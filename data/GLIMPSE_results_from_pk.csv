index,id,consensuality_scores
0,https://openreview.net/forum?id=B1-Hhnslg,"{'The paper is an extension of the matching networks by Vinyals et al. in NIPS2016.': 1.0986114740371704, 'Instead of using all the examples in the support set during test, the method represents each class by the mean of its learned embeddings.': 1.0986123085021973, 'The training procedure and experimental setting are very similar to the original matching networks.': 1.0986123085021973, 'I am not completely sure about its advantages over the original matching networks.': 1.0986123085021973, 'It seems to me when dealing with 1-shot case, these two methods are identical since there is only one example seen in this class, so the mean of the embedding is the embedding itself.': 1.0970332622528076, 'When dealing with 5-shot case, original matching networks compute the weighted average of all examples, but it is at most 5x cost.': 1.0908472537994385, 'The experimental results reported for prototypical nets are only slightly better than matching networks.': 1.0986123085021973, 'I  think it is a simple, straightforward,  novel extension, but I am not fully convinced its advantages.': 1.0986014604568481, 'This paper proposes an improved version of matching networks, with better scalability properties with respect to the support set of a few-shot classifier.': 1.0986123085021973, 'Instead of considering each support point individually, they learn an embedding function that aggregates over items of each class within the support set (eq. 1).': 1.0986123085021973, 'This is combined with episodic few-shot training with randomly-sampled partitions of the training set classes, so that the training and testing scenarios match closely.': 1.0986123085021973, 'Although the idea is quite straightforward, and there are a great many prior works on zero-shot and few-shot learning, the proposed technique is novel to my knowledge, and achieves state-of-the-art results on several  benchmark datasets.': 1.0986123085021973, 'One addition that I think would improve the paper is a clearer description of the training algorithm (perhaps pseudocode).': 1.0986123085021973, 'In its current form the paper a bit vague about this.': 1.0986123085021973, '*** Paper Summary ***': 1.0986123085021973, 'This paper simplify matching network by considering only a single prototype per class which is obtained as the average of the embedding of the training class samples.': 1.0270631313323975, 'Empirical comparisons with matching networks are reported.': 1.0986123085021973, '*** Review ***': 1.0986123085021973, 'The paper reads well and clearly motivate the work.': 1.0986123085021973, 'This work of learning metric learning propose to simplify an earlier work (matching network) which is a great objective.': 1.0986123085021973, 'However, I am not sure it achieve better results than matching networks.': 1.0986123085021973, 'The space of learning embeddings to optimize nearest neighbor classification has been explored before, but the idea of averaging the propotypes is interesting (as a non-linear extension of Mensink et al 2013).': 1.033754587173462, 'I would suggest to improve the discussion of related work and to consolidate the results section to help distinguish between the methods you outperform and the one you do not.': 0.9951473474502563, 'The related work section can be extended to include work on learning distance metric to optimize a nearest neighbor classification, see Weinberger et al, 2005 and subsequent work.': 0.6355160474777222, 'Extensions to perform the same task with neural networks can be found in Min et al, 09 that purse a goal very close to yours.': 1.0984922647476196, 'Regarding approaches pursuing similar goals with a different learning objective, you cite siamese network with pairwise supervision.': 1.0986123085021973, 'The learning to rank (for websearch) litterature with triplet supervision or global ranking losses is also highly relevant, ie. one example ""the query"" defines the class and the embedding space need to be such that positive/relevant document are closer to the query than the others.': 1.0986123085021973, 'I would suggest to start with Chris Burges 2010 tutorial.': 1.0986123085021973, 'One learning class': 1.0986114740371704, 'I am not sure the reported results correctly reflect the state of the art for all tasks.': 1.0844303369522095, 'The results are positive on Omniglot': 1.0978642702102661, 'but I feel that you should also report the better results of matching networks on miniImageNet with fine tuning and full contextual embeddings.': 1.0934499502182007, 'It can be considered misleading not to report it.': 1.0986123085021973, 'On Cub 200, I thought that the state-of-the-art was 50.1%, when using features from GoogLeNet (Akata et al 2015), could you comment on this?': 1.098557949066162, 'Overall, paper could greatly be improved, both in the discussion of related work and with a less partial reporting of prior empirical results.': 1.0986123085021973, '*** References ***': 1.0986123085021973, 'Large Margin Nearest Neighbors.': 1.0986123085021973, 'Weinberger et al, 2005': 1.0986123085021973, 'From RankNet to LambdaRank to LambdaMART: An Overview, Chris J.C. Burges, June 23, 2010': 1.0986123085021973, 'A Deep Non-linear Feature Mapping for Large-Margin kNN Classification, Min et al, 09': 1.0986123085021973}"
1,https://openreview.net/forum?id=B1-q5Pqxl,"{'The paper looks at the problem of locating the answer to a question in a text (For this task the answer is always part of the input text).': 1.0986123085021973, 'For this the paper proposes to combine two existing works: Match-LSTM to relate question and text representations and Pointer Net to predict the location of the answer in the text.': 1.0986123085021973, 'Strength:': 1.0986123085021973, 'The suggested approach makes sense for the task and achieves good performance, (although as the authors mention, recent concurrent works achieve better results)': 1.0986123085021973, 'The paper is evaluated on the SQuAD dataset and achieves significant improvements over prior work.': 1.0986123085021973, 'Weaknesses:': 1.0986123085021973, '1. It is unclear from the paper how well it is applicable to other problem scenarios where the answer is not a subset of the input text.': 1.0986123085021973, '2. Experimental evaluation': 1.0986123085021973, '2.1. It is not clear why the Bi-Ans-Ptr in Table 2 is not used for the ensemble although it achieves the best performance.': 1.0986123085021973, '2.2. It would be interested if this approach generalizes to other datasets.': 1.0986123085021973, 'Other (minor/discussion points)': 1.0986123085021973, 'The task and approach seem to have some similarity of locating queries in images and visual question answering.': 1.0986123085021973, 'The authors might want to consider pointing to related works in this direction.': 1.0986119508743286, 'I am wondering how much this task can be seen as a “guided extractive summarization”, i.e. where the question guides the summarization process.': 1.0986123085021973, 'Page 6, last paragraph: missing “.”: “… searching This…”': 1.0986123085021973, 'Summary:': 1.0986123085021973, 'While the paper presents an interesting combination of two approaches for the task of answer extraction, the novelty is moderate.': 1.0986108779907227, 'While the experimental results are encouraging, it remains unclear how well this approach generalizes to other scenarios as it seems a rather artificial task.': 1.0986123085021973, 'The paper presents a deep neural network for the task of machine comprehension on the SQuAD dataset.': 1.0889698266983032, 'The proposed model is based on two previous works': 1.0986123085021973, 'match-LSTM and Pointer Net.': 1.0966147184371948, 'Match-LSTM produces attention over each word in the given question for each word in the given passage, and sequentially aggregates this matching of each word in the passage with the words in the question.': 1.0982847213745117, 'The pointer net is used to generate the answer by either generating each word in the answer or by predicting the starting and ending tokens in the answer from the provided passage.': 1.0756617784500122, 'The experimental results show that both the variants of the proposed model outperform the baseline presented in the SQuAD paper.': 0.4133390784263611, 'The paper also shows some analysis of the results obtained such as variation of performance across answer lengths and question types.': 1.0942999124526978, 'Strengths:': 1.0986123085021973, '1. A novel end-to-end model for the task of machine comprehension rather than using hand-crafted features.': 1.0986087322235107, '2. Significant performance boost over the baseline presented in the SQuAD paper.': 1.0985991954803467, '3. Some insightful analyses of the results such as performance is better when answers are short, ""why"" questions are difficult to answer.': 1.070894718170166, 'Weaknesses/Questions/Suggestions:': 1.0986121892929077, '1. The paper does not show quantitatively how much modelling attention in match-LSTM and answer pointer layer helps. So, it would be insightful if authors could compare the model performance with and without attention in match-LSTM, and with and without attention in answer pointer layer.': 1.0777888298034668, '2. It would be good if the paper could provide some insights into why there is a huge performance gap between boundary model and sequence model in the answer pointer layer.': 1.0823595523834229, '3. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required.': 0.7177720069885254, '4. Could authors please explain why the activations resulting from {h^p}_i and {h^r}_{i-1} in G_i in equation 2 are being repeated across dimension of Q. Why not learn different activations for each dimension?': 0.41325169801712036, '5. I wonder why Bi-Ans-Ptr is not used in the ensemble model (last row in table 2) when it is shown that Bi-Ans-Ptr improves performance by 1.2% in F1.': 0.49613824486732483, '6. Could authors please discuss and compare the DCR model (in table 2) in the paper in more detail?': 0.4504033923149109, 'Review Summary: The paper presents a reasonable end-to-end model for the task of machine comprehension on the SQuAD dataset, which outperforms the baseline model significantly.': 1.0986120700836182, 'However, it would be good if more analyses / ablation studies / insights are included regarding': 1.0947548151016235, 'how much attention helps, why is boundary model better than sequence model, how does the performance change when the reasoning required becomes difficult.': 1.0986123085021973, 'SUMMARY.': 1.0986123085021973, 'This paper proposes a new neural network architectures for solving the task of reading comprehension question answering where the goal is answering a questions regarding a given text passage.': 1.0986123085021973, 'The proposed model combines two well-know neural network architectures match-lstm and pointer nets.': 1.0986123085021973, 'First the passage and the questions are encoded with a unidirectional LSTM.': 1.0986123085021973, 'Then the encoded words in the passage and the encoded words in the questions are combined with an attention mechanism so that each word of the passage has a certain degree of compatibility with the question.': 1.0986123085021973, 'For each word in the passage the word representation and the weighted representation of the query is concatenated and passed to an forward lstm.': 1.0986123085021973, 'The same process is done in the opposite direction with a backward lstm.': 1.0986123085021973, 'The final representation is a concatenation of the two lstms.': 1.0986123085021973, 'As a decoded a pointer network is used.': 1.0986123085021973, 'The authors tried with two approaches: generating the answer word by word, and generating the first index and the last index of the answer.': 1.0986123085021973, 'The proposed model is tested on the Stanford Question Answering Dataset.': 1.0986123085021973, 'An ensemble of the proposed model achieves performance close to state-of-the-art models.': 1.0986123085021973, 'OVERALL JUDGMENT': 1.0986123085021973, 'I think the model is interesting mainly because of the use of pointer networks as a decoder.': 1.0986123085021973, 'One thing that the authors could have tried is a multi-hop approach.': 1.0986123085021973, 'It has been shown in many works to be extremely beneficial in the joint encoding of passage and query.': 1.0986123085021973, 'The authors can think of it as a deep match-lstm.': 1.0986123085021973, 'The analysis of the model is interesting and insightful.': 1.0986123085021973, 'The sharing of the code is good.': 1.0986123085021973}"
2,https://openreview.net/forum?id=B16Jem9xe,"{'I just noticed I submitted my review as a pre-review question - sorry about this.': 1.0986123085021973, 'Here it is again, with a few more thoughts added...': 0.4838534891605377, 'The authors present a great and - as far as I can tell - accurate and honest overview of the emerging theory about GANs from a likelihood ratio estimation/divergence minimisation perspective.': 1.0986123085021973, 'It is well written and a good read, and one I would recommend to people who would like to get involved in GANs.': 1.0901001691818237, 'My main problem with this submission is that it is hard as a reviewer to pin down what precisely the novelty is - beyond perhaps articulating these views better than other papers have done in the past.': 1.0986123085021973, 'A sentence from the paper ""But it has left us unsatisfied since we have not gained the insight needed to choose between them.”': 1.0976207256317139, ""summarises my feeling about this paper: this is a nice 'unifying review’ type paper that - for me - lacks a novel insight."": 0.33615732192993164, 'In summary, my assessment is mixed: I think this is a great paper, I enjoyed reading it.': 1.087641716003418, 'I was left a bit disappointed by the lack of novel insight, or a singular key new idea which you often expect in conference presentations, and this is why I’m not highly confident about this as a conference submission (and hence my low score)': 1.0986123085021973, 'I am open to be convinced either way.': 1.0985652208328247, 'Detailed comments:': 1.0986123085021973, 'I think the authors should probably discuss the connection of Eq.': 1.0985835790634155, '(13) to KLIEP: Kullback-Leibler Importance Estimation by Shugiyama and colleagues.': 1.0981858968734741, 'I don’t quite see how the part with equation (13) and (14) fit into the flow of the paper.': 1.0389271974563599, 'By this point the authors have established the view that GANs are about estimating likelihood ratios - and then using these likelihood ratios to improve the generator.': 1.0986114740371704, 'These paragraphs read like: we also tried to derive another particular formulation for doing this but we failed to do it in a practical way.': 0.74677574634552, 'There is a typo in spelling Csiszar divergence': 0.9817811846733093, 'Equation (15) is known (to me) as Least Squares Importance Estimation by Kanamori et al (2009).': 1.0983412265777588, 'A variant of least-squares likelihood estimation uses the kernel trick, and finds a function from an RKHS that best represents the likelihood ratio between the two distributions in a least squares sense.': 1.0985914468765259, 'I think it would be interesting to think about how this function is related to the witness function commonly used in MMD and what the properties of this function are compared to the witness function - perhaps showing the two things for simple distributions.': 1.0957485437393188, 'I have stumbled upon the work of Sugiyama and collaborators on direct density ratio estimation before, and I found that work very insightful.': 1.0986123085021973, 'Generally, while some of this work is cited in this paper, I felt that the authors could do more to highlight the great work of this group, who have made highly significant contributions to density ratio estimation, albeit with a different goal in mind.': 0.4727920889854431, 'On likelihood ratio estimation: some methods approximate the likelihood ratio directly (such as least-squares importance estimation), some can be thought of more as approximating the log of this quantity (logistic regression, denoising autoencoders).': 1.0974133014678955, 'An unbiased estimate of the ratio will provide a biased estimate of the logarithm and vice versa.': 0.5227369666099548, 'To me it feels like estimating the log of the ratio directly is more useful, and in more generality estimating the convex function of the ratio which is used to define the f-divergence seems like a good approach.': 1.0486692190170288, 'Could the authors comment on this?': 0.4075838327407837, 'I think the hypothesis testing angle is oversold in the paper.': 1.0649693012237549, 'I’m not sure what additional insight is gained by mixing in some hypothesis testing terminology.': 1.0054913759231567, 'Other than using quantities that appear in hypothesis testing as tests statistics, his work does not really talk about hypothesis testing, nor does it use any tools from the hypothesis testing literature.': 0.96033775806427, 'In this sense, this paper is in contrast with Sutherland et al (in review for ICLR) who do borrow concepts from two-sample testing to optimise hyperparameters of the divergence used.': 1.0985791683197021, 'The paper provides an exposition of multiple ways of learning in implicit generative models, of which generative adversarial networks are an example.': 1.0986123085021973, 'The paper is very clear, the exposition is insightful, and the presented material is clearly important.': 1.0986123085021973, 'It is hard to assess ""novelty"" of this work, as the individual pieces are not novel, and yet the exposition of all of them in the same space with clear outline of the connections between them is novel.': 1.0986123085021973, 'I believe this work is significant - it provides a bridge for language and methods used in multiple parts of statistics and machine learning.': 1.0986123085021973, 'This has the potential to accelerate progress.': 1.0986123085021973, 'I recommend publishing this paper at ICLR, even though it is not the ""typical"" paper that get published at this conference (in that it doesn\'t offer empirical validation, nor makes a particular claim about relative merits of different methods).': 1.0986123085021973, 'Thank you for an interesting read.': 1.0869210958480835, ""Given the huge interest in generative modelling nowadays, this paper is very timely and does provide very clear connections between methods that don't use maximum likelihood for training."": 1.0986121892929077, 'It made a very useful observation that the generative and the discriminative loss do **not** need to be coupled with each other.': 1.0986123085021973, 'I think this paper in summary provides some very useful insights to the practitioners on how to select the objective function to train the implicit generative model.': 1.0986123085021973, ""The only reason that I decided to hold back my strong acceptance recommendation is that I don't understand the acceptance criteria of ICLR."": 1.0986123085021973, 'First this paper has the style very similar to the Sugiyama': 1.0986123085021973, 'et al.': 1.0986123085021973, 'papers that are cited (e.g. presenting in different perspectives that were all covered in those papers but in a different context), making me unsure about how to evaluate the novelty.': 1.0986123085021973, ""Second this paper has no experiment nor mathematical theorem, and I'm not exactly sure what kinds of contributions the ICLR committee is looking for."": 1.0986123085021973}"
3,https://openreview.net/forum?id=B16dGcqlx,"{'This paper proposed a novel adversarial framework to train a model from demonstrations in a third-person perspective, to perform the task in the first-person view.': 1.0986123085021973, 'Here the adversarial training is used to extract a novice-expert (or third-person/first-person) independent feature so that the agent can use to perform the same policy in a different view point.': 1.0986123085021973, 'While the idea is quite elegant and novel (I enjoy reading it), more experiments are needed to justify the approach.': 1.0986123085021973, 'Probably the most important issue is that there is no baseline, e.g., what if we train the model with the image from the same viewpoint?': 1.0986123085021973, 'It should be better than the proposed approach but how close are they?': 1.0986123085021973, 'How the performance changes when we gradually change the viewpoint from third-person to first-person?': 1.0986123085021973, 'Another important question is that maybe the network just blindly remembers the policy, in this case, the extracted feature could be artifacts of the input image that implicitly counts the time tick in some way (and thus domain-agonistic), but can still perform reasonable policy.': 1.0986123085021973, 'Since the experiments are conduct in a synthetic environment, this might happen.': 1.0986121892929077, 'An easy check is to run the algorithm on multiple viewpoint and/or with blurred/differently rendered images, and/or with random initial conditions.': 1.0986123085021973, 'Other ablation analysis is also needed.': 1.0986123085021973, 'For example, I am not fully convinced by the gradient flipping trick used in Eqn. 5, and in the experiments there is no ablation analysis for that (GAN/EM style training versus gradient flipping trick).': 1.0986123085021973, 'For the experiments, Fig.': 1.098274827003479, '4,5,6 does not have error bars and is not very convincing.': 0.6548051834106445, 'The paper presents an interesting new problem setup for imitation learning: an agent tries to imitate a trajectory demonstrated by an expert but said trajectory is demonstrated in a different state or observation space than the one accessible by the agent (although the dynamics of the underlying MDP are shared).': 1.0986123085021973, 'The paper proposes a solution strategy that combines recent work on domain confusion losses with a recent IRL method based on generative adversarial networks.': 1.0986123085021973, 'I believe the general problem to be relevant and agree with the authors that it results in a more natural formulation for imitation learning that might be more widely applicable.': 1.0986123085021973, 'There are however a few issues with the paper in its current state that make the paper fall short of being a great exploration of a novel idea.': 1.0986120700836182, 'I will list these concerns in the following (in arbitrary order)': 1.0986108779907227, 'The paper feels at times to be a bit hurriedly written (this also mainly manifests itself in the experiments, see comment below) and makes a few fairly strong claims in the introduction that in my opinion are not backed up by their approach.': 1.0986084938049316, 'For example: ""Advancements in this class of algorithms would significantly improve the state of robotics, because it will enable anyone to easily teach robots new skills""; given that the current method to my understanding has the same issues that come with standard GAN training (e.g. instability etc.) and requires a very accurate simulator to work well (since TRPO will require a large number of simulated trajectories in each step) this seems like an overstatement.': 1.098594307899475, 'There are some sentences that are ungrammatical or switch tense in the middle of the sentence making the paper harder to read than necessary, e.g. Page 2: ""we find that this simple approach has been able to solve the problems""': 1.0985859632492065, 'The general idea of third person imitation learning is nice, clear and (at least to my understanding) also novel.': 1.0761027336120605, 'However, instead of exploring how to generally adapt current IRL algorithms to this setting the authors pick a specific approach that they find promising (using GANs for IRL) and extend it.': 1.0985891819000244, 'A significant amount of time is then spent on explaining why current IRL algorithms will fail in the third-person setting.': 1.0985865592956543, 'I fail to see why the situation for the GAN based approach is any different than that of any other existing IRL algorithm.': 1.0956984758377075, 'To be more clear: I see no reason why e.g. behavioral cloning could not be extended with a domain confusion loss in exactly the same way as the approach presented.': 1.0986108779907227, 'To this end it would have been nice to rather discuss which algorithms can be adapted in the same way (and also test them) and which ones cannot.': 0.9027387499809265, 'One straightforward approach to apply any IRL algorithm would for example be to train two autoencoders for both domains that share higher layers + a domain confusion loss on the highest layer, should that not result in features that are directly usable?': 1.0985051393508911, 'If not, why?': 1.0986123085021973, 'While the general argument that existing IRL algorithms will fail in the proposed setting seems reasonable it is still unfortunate that no attempts have been made to validate this empirically.': 1.0966212749481201, 'No comparison is made regarding what happens when one e.g. performs supervised learning (behavioral cloning) using the expert observations and then transfers to the changed domain.': 1.0545645952224731, 'How well would this work in practice ?': 1.0362358093261719, 'Also, how fast can different IRL algorithms solve the target task in general (assuming a first person perspective) ?': 1.0986123085021973, 'Although I like the idea of presenting the experiments as being directed towards answering a specific set of questions I feel like the posed questions somewhat distract from the main theme of the paper.': 1.0986123085021973, 'Question 2 suddenly makes the use of additional velocity information to be a main point of importance and the experiments regarding Question 3 in the end only contain evaluations regarding two hyperparameters (ignoring all other parameters such as the parameters for TRPO, the number of rollouts per iteration, the number of presented expert episodes and  the design choices for the GAN).': 0.9694563746452332, 'I understand that not all of these can be evaluated thoroughly in a conference paper but I feel like some more experiments or at least some discussion would have helped here.': 1.0986123085021973, 'The presented experimental evaluation somewhat hides the cost of TRPO training with the obtained reward function.': 1.0986123085021973, 'How many roll-outs are necessary in each step?': 1.0986123085021973, 'The experiments lack some details: How are the expert trajectories obtained?': 1.0783390998840332, 'The domains for the pendulum experiment seem identical except for coloring of the pole, is that correct (I am surprised this small change seems to have such a detrimental effect)?': 1.0816563367843628, 'Figure 3 shows average performance over 5 trials, what about Figure 5 (if this is also average performance, what is the variance here)?': 1.0963815450668335, 'Given that GANs are not easy to train, how often does the training fail/were you able to re-use the hyperparameters across all experiments?': 1.0986084938049316, 'UPDATE:': 1.0986123085021973, 'I updated the score.': 1.0983529090881348, 'Please see my response to the rebuttal below.': 1.0986123085021973, 'The paper extends the imitation learning paradigm to the case where the demonstrator and learner have different points of view.': 1.0986123085021973, 'This is an important contribution, with several good applications.': 1.0986123085021973, 'The main insight is to use adversarial training to learn a policy that is robust to this difference in perspective.': 1.0986123085021973, 'This problem formulation is quite novel compared to the standard imitation learning literature (usually first-order perspective), though has close links to the literature on transfer learning (as explained in Sec.2).': 1.0986123085021973, 'The basic approach is clearly explained, and follows quite readily from recent literature on imitation learning and adversarial training.': 1.0986121892929077, 'I would have expected to see comparison to the following methods added to Figure 3:': 0.4457644820213318, '1)': 1.0986123085021973, 'Standard 1st person imitation learning using agent A data, and apply the policy on agent A.': 1.0986095666885376, 'This is an upper-bound on how well you can expect to do, since you have the correct perspective.': 1.0986123085021973, '2)  Standard 1st person imitation learning using agent A data, then apply the policy on agent B.  Here, I expect it might do less well than 3rd person learning, but worth checking to be sure, and showing what is the gap in performance.': 0.41148829460144043, '3)': 1.0986123085021973, 'Reinforcement learning using agent A data, and apply the policy on agent A.': 0.952936589717865, 'I expect this might do better than 3rd person imitation learning': 1.0986121892929077, 'but it might depend on the scenario (e.g. difficulty of imitation vs exploration; how different are the points of view between the agents).': 0.9941859841346741, 'I understand this is how the expert data is collected for the demonstrator, but I don’t see the performance results from just using this procedure on the learner (to compare to Fig.3 results).': 1.098129153251648, 'Including these results would in my view significantly enhance the impact of the paper.': 0.903808057308197}"
4,https://openreview.net/forum?id=B184E5qee,"{'The authors present a simple method to affix a cache to neural language models, which provides in effect a copying mechanism from recently used words.': 1.0986123085021973, 'Unlike much related work in neural networks with copying mechanisms, this mechanism need not be trained with long-term backpropagation, which makes it efficient and scalable to much larger cache sizes.': 1.0986123085021973, 'They demonstrate good improvements on language modeling by adding this cache to RNN baselines.': 1.0986123085021973, 'The main contribution of this paper is the observation that simply using the hidden states h_i': 1.0908904075622559, 'as keys for words x_i, and h_t as the query vector, naturally gives a lookup mechanism that works fine without tuning by backprop.': 1.09689462184906, 'This is a simple observation and might already exist as folk knowledge among some people, but it has nice implications for scalability and the experiments are convincing.': 1.0986123085021973, 'The basic idea of repurposing locally-learned representations for large-scale attention where backprop would normally be prohibitively expensive is an interesting one, and could probably be used to improve other types of memory networks.': 1.0986123085021973, 'My main criticism of this work is its simplicity and incrementality when compared to previously existing literature.': 1.09860098361969, 'As a simple modification of existing NLP models, but with good empirical success, simplicity and practicality, it is probably more suitable for an NLP-specific conference.': 1.098610758781433, 'However, I think that approaches that distill recent work into a simple, efficient, applicable form should be rewarded and that this tool will be useful to a large enough portion of the ICLR community to recommend its publication.': 1.0985887050628662, 'This paper proposes a simple extension to a neural network language model by adding a cache component.': 0.6879522204399109, 'The model stores <previous hidden state, word> pairs in memory cells and uses the current hidden state to control the lookup.': 1.0986120700836182, 'The final probability of a word is a linear interpolation between a standard language model and the cache language model.': 1.0974252223968506, 'Additionally, an alternative that uses global normalization instead of linear interpolation is also presented.': 1.0845681428909302, 'Experiments on PTB, Wikitext, and LAMBADA datasets show that the cache model improves over standard LSTM language model.': 0.37150251865386963, 'There is a lot of similar work on memory-augmented/pointer neural language models, and the main difference is that the proposed method is simple and scales to a large cache size.': 1.0986120700836182, 'However, since the technical contribution is rather limited, the experiments need to be more thorough and conclusive.': 1.0980582237243652, 'While it is obvious from the results that adding a cache component improves over language models without memory, it is still unclear that this is the best way to do it (instead of, e.g., using pointer networks).': 1.0986121892929077, 'A side-by-side comparison of models with pointer networks vs. models with cache with roughly the same number of parameters is needed to convincingly argue that the proposed method is a better alternative (either because it achieves lower perplexity, faster to train but similar test perplexity, faster at test time, etc.)': 1.0986123085021973, 'Some questions:': 1.0986123085021973, 'In the experiment results, for your neural cache model, are those results with linear interpolation or global normalization, or the best model?': 1.098611831665039, 'Can you show results for both?': 1.0986108779907227, 'Why is the neural cache model worse than LSTM on Ctrl (Lambada dataset)?': 1.0909302234649658, 'Please also show accuracy on this dataset.': 0.5785760283470154, 'It is also interesting that the authors mentioned that training the cache component instead of only using it at test time gives little improvements.': 1.0986114740371704, 'Are the results about the same or worse?': 1.0925625562667847, 'This paper not only shows that a cache model on top of a pre-trained RNN can improve language modeling, but also illustrates a shortcoming of standard RNN models in that they are unable to capture this information themselves.': 1.0986123085021973, 'Regardless of whether this is due to the small BPTT window (35 is standard) or an issue with the capability of the RNN itself, this is a useful insight.': 1.0969350337982178, 'This technique is an interesting variation of memory augmented neural networks with a number of advantages to many of the standard memory augmented architectures.': 1.0986123085021973, 'They illustrate the neural cache model on not just the Penn Treebank but also WikiText-2 and WikiText-103, two datasets specifically tailored to illustrating long term dependencies with a more realistic vocabulary size.': 1.0986123085021973, 'I have not seen the ability to refer up to 2000 words back previously.': 1.0986123085021973, 'I recommend this paper be accepted.': 1.0986123085021973, 'There is additionally extensive analysis of the hyperparameters on these datasets, providing further insight.': 1.0986123085021973, 'I recommend this interesting and well analyzed paper be accepted.': 1.0986123085021973}"
5,https://openreview.net/forum?id=B186cP9gx,"{'This paper investigates the hessian of small deep networks near the end of training.': 1.3812731504440308, 'The main result is that many eigenvalues are approximately zero, such that the Hessian is highly singular, which means that a wide amount of theory does not apply.': 1.2905149459838867, 'The overall point that deep learning algorithms are singular, and that this undercuts many theoretical results, is important but it has already been made: Watanabe.': 1.3862943649291992, '“Almost All Learning Machines are Singular”, FOCI 2007.': 0.1674322783946991, 'This is one paper in a growing body of work investigating this phenomenon.': 1.3182801008224487, 'In general, the references for this paper could be fleshed out much further—a variety of prior work has examined the Hessian in deep learning, e.g., Dauphin et al.': 1.3859333992004395, '“Identifying and attacking the saddle point problem in high dimensional non-convex optimization” NIPS 2014 or the work of Amari and others.': 1.3859144449234009, 'Experimentally, it is hard to tell how results from the small sized networks considered here might translate to much larger networks.': 1.3862943649291992, 'It seems likely that the behavior for much larger networks would be different.': 1.234648585319519, 'A reason for optimism, though, is the fact that a clear bulk/outlier behavior emerges even in these networks.': 1.3862942457199097, 'Characterizing this behavior for simple systems is valuable.': 1.3862943649291992, 'Overall, the results feel preliminary but likely to be of interest when further fleshed out.': 1.0632011890411377, 'This paper is attacking an important problem, but should do a better job situating itself in the related literature and undertaking experiments of sufficient size to reveal large-scale behavior relevant to practice.': 1.3862943649291992, 'The paper analyzes the properties of the Hessian of the training objective for various neural networks and data distributions.': 1.3862943649291992, 'The authors study in particular, the eigenspectrum of the Hessian, which relates to the difficulty and the local convexity of the optimization problem.': 1.3862943649291992, 'While there are several interesting insights discussed in this paper such as the local flatness of the objective function, as well as the study of the relation between data distribution and Hessian, a somewhat lacking aspect of the paper is that most described effects are presented as general, while tested only in a specific setting, without control experiments, or mathematical analysis.': 1.3862943649291992, 'For example, regarding the concentration of eigenvalues to zero in Figure 6, it is unclear whether the concentration effect is really caused by training (e.g. increasing insensitivity to local perturbations), or the consequence of a specific choice of scale for the initial parameters.': 1.3862943649291992, 'In Figure 8, the complexity of the data is not defined.': 0.8362882137298584, 'It is not clear whether two fully overlapping distributions (the Hessian would then become zero?) is considered as complex or simple data.': 1.3862943649291992, 'Some of the plots legends (Fig. 1 and 2) and labels are unreadable in printed format.': 1.3862943649291992, ""Plots of Figure 3 don't have the same range for the x-axis."": 0.44500043988227844, 'The image of Hessian matrix of Figure 1 does not render properly in printed format.': 1.3862943649291992, 'The work presents some empirical observations to support the statement that “the Hessian of the loss functions in deep learning is degenerate”.': 1.3862943649291992, 'But what does this statement refer to?': 1.3862557411193848, 'To my understanding, there are at least three interpretations:': 0.9405614137649536, '(i)': 1.0282704830169678, 'The Hessian of the loss functions in deep learning is degenerate at any point in the parameter space, i.e., any network weight matrices.': 1.3862943649291992, '(ii) The Hessian of the loss functions in deep learning is degenerate at any critical point.': 1.3305596113204956, '(iii) The Hessian of the loss functions in deep learning is degenerate at any local minimum, or any global minimum.': 0.6871575117111206, 'None of these interpretations is solidly supported by the observations provided in the paper.': 1.3619396686553955, 'More comments are as follows:': 1.386283040046692, '1) The authors state that “we don’t have much information on what the actual Hessian looks like.”': 1.02235746383667, 'Then I just wonder what Hessian is investigated.': 1.3862943649291992, 'Is it the actual one or approximate one?': 1.0730775594711304, 'Please clarify and provide the references for computing the actual Hessian.': 1.3862866163253784, '2) It is not clear whether the optimization was done by a batch gradient descent algorithm, i.e., batch back propagation (BP) algorithm, or a stochastic BP algorithm.': 0.8452231287956238, 'If the training was done via a stochastic BP algorithm, it is hard to conclude that the the Neural Network has been trained to its local minimum.': 1.3755522966384888, 'When it was done by a full-batch BP algorithm, what was the accumulating point?': 0.4053419828414917, 'Was it local minimum or global minimum?': 1.3786442279815674, '3) Since the negative log likelihood function was used as at the end of training, it is essentially a joint learning approach in both the Newton weight matrices and the negative log likelihood vector.': 1.027113914489746, 'Certainly, the whole loss function is not convex in these two parameters.': 1.3862937688827515, 'But if least squares error function is used at the end, would it make any difference in claiming the degeneracy of the Hessian?': 1.386290192604065, '4) Finally, the statement “There are still negative eigenvalues even when they are small in magnitude” is very puzzling.': 1.192006230354309, 'Potential reasons are:': 1.3859912157058716, '(a)': 1.3685455322265625, 'If the training algorithm did converge, the accumulating points were not local minima, i.e., they were saddle points.': 1.3854310512542725, '(b) Training algorithms did not converge, or have not converged yet.': 0.7548527717590332, '(c) The calculation of the actual Hessian might be inaccurate.': 0.9024868607521057, 'Studying the Hessian in deep learning, the experiments in this paper suggest that the eigenvalue distribution is concentrated around zero and the non zero eigenvalues are related to the complexity of the input data.': 0.5231049060821533, 'I find most of the discussions and experiments to be interesting and insightful.': 1.3862920999526978, 'However, the current paper could be significantly improved.': 1.3862943649291992, 'Quality:': 1.3862943649291992, 'It seems that the arguments in the paper could be enhanced by more effort and more comprehensive experiments.': 1.3862943649291992, 'Performing some of the experiments discussed in the conclusion could certainly help a lot.': 1.3862943649291992, 'Some other suggestions:': 1.3079217672348022, '1- It would be very helpful to add other plots showing the distribution of eigenvalues for some other machine learning method for the purpose of comparison to deep learning.': 0.5620876550674438, '2- There are some issues about the scaling of the weights and it make sense to normalize the weights each time before calculating the Hessian otherwise the result might be misleading.': 1.2898274660110474, '3- It might worth trying to find a quantity that measures the singularity of Hessian because it is difficult to visually conclude something from the plots.': 1.0304245948791504, '4- Adding some plots for the Hessian during the optimization is definitely needed because we mostly care about the Hessian during the optimization not after the convergence.': 0.27525845170021057, 'Clarity:': 1.2882672548294067, '1- There is no reference to figures in the main text which makes it confusing for the reading to know the context for each figure.': 0.622964084148407, 'For example, when looking at Figure 1, it is not clear that the Hessian is calculated at the beginning of optimization or after convergence.': 1.3840882778167725, '2- The texts in the figures are very small and hard to read.': 0.5086957812309265}"
6,https://openreview.net/forum?id=B1E7Pwqgl,"{'The authors proposes an interesting idea of connecting the energy-based model (descriptor) and': 1.0986123085021973, 'the generator network to help each other.': 1.0986123085021973, 'The samples from the generator are used as the initialization': 1.0986123085021973, 'of the descriptor inference.': 1.0986123085021973, 'And the revised samples from the descriptor is in turn used to update': 1.0986123085021973, 'the generator as the target image.': 1.0986123085021973, 'The proposed idea is interesting.': 1.0986123085021973, 'However, I think the main flaw is that the advantages of having that': 1.0986123085021973, 'architecture are not convincingly demonstrated in the experiments.': 1.0851948261260986, 'For example, readers will expect': 1.023107647895813, 'quantative analysis on how initializing with the samples from the generator helps?': 1.01492440700531, 'Also, the only': 1.0983604192733765, 'quantative experiment on the reconstruction is also compared to quite old models.': 1.0979961156845093, 'Considering that': 1.0986123085021973, 'the model is quite close to the model of Kim & Bengio 2016, readers would also expect a comparison': 1.0985206365585327, 'to that model.': 1.0986123085021973, '** Minor': 1.0986123085021973, ""I'm wondering if the analysis on the convergence is sound when considering the fact that samples"": 1.0986123085021973, 'from SGLD are biased samples (with fixed step size).': 1.0788977146148682, 'Can you explain a bit more on how you get Eqn 8?': 1.0986123085021973, 'when p(x|y) is also dependent on W_G?': 1.0986011028289795, 'This paper introduces CoopNets, an algorithm which trains a Deep-Energy Model (DEM, the “descriptor”) with the help of an auxiliary directed bayes net, e.g. “the generator”.': 1.0915534496307373, 'The descriptor is trained via standard maximum likelihood, with Langevin MCMC for sampling.': 1.0984816551208496, 'The generator is trained to generate likely samples under the DEM in a single, feed-forward ancestral sampling step.': 1.09861159324646, 'It can thus be used to shortcut expensive MCMC sampling, hence the reference to “cooperative training”.': 1.074706792831421, 'The above idea is interesting and novel, but unfortunately is not sufficiently validated by the experimental results.': 1.098180890083313, 'First and foremost, two out of the three experiments do not feature a train /test split, and ignore standard training and evaluation protocols for texture generation (see [R1]).': 1.0986123085021973, 'Datasets are also much too small.': 1.0934282541275024, 'As such these experiments only seem to confirm the ability of the model to overfit.': 1.097615361213684, 'On the third in-painting tasks, baselines are almost non-existent: no VAEs, RBMs, DEM, etc which makes it difficult to evaluate the benefits of the proposed approach.': 1.09857177734375, 'In a future revision, I would also encourage the authors to answer the following questions experimentally.': 1.0986076593399048, 'What is the impact of the missing rejection step in Langevin MCMC (train with, without ?).': 1.098608374595642, 'What is the impact of the generator on the burn-in process of the Markov chain (show sample auto-correlation).': 1.0986123085021973, 'How bad is approximation of training the generator from ({\\tilde{Y}, \\hat{X}) instead of ({\\tilde{Y}, \\tilde{X}) ?': 1.0983316898345947, 'Run comparative experiments.': 1.0986123085021973, 'The paper would also greatly benefit from a rewrite focusing on clarity, instead of hyperbole (“pioneering work” in reference to closely related, but non-peer reviewed work) and prose (“tale of two nets”).': 1.0746625661849976, 'For example, the authors fail to specify the exact form of the energy function: this seems like a glaring omission.': 1.0874720811843872, 'PROS:': 1.0986123085021973, '+ Interesting and novel idea': 1.0986123085021973, 'CONS:': 1.0986123085021973, 'Improper experimental protocols': 1.0986123085021973, 'Missing baselines': 1.0986123085021973, 'Missing diagnostic experiments': 1.0981727838516235, '[R1] Heess, N., Williams, C. K. I., and Hinton, G. E. (2009).': 1.0664434432983398, 'Learning generative texture models with extended fields of-experts.': 1.0846887826919556, 'This paper proposed a new joint training scheme for two probabilistic models of signals (e.g. images) which are both deep neural network based and are termed generator and descriptor networks.': 1.0986123085021973, 'In the new scheme, termed cooperative training, the two networks train together and assist each other: the generator network provides samples that work as initial samples for the descriptor network, and the descriptor network updates those samples to help guide training of the generator network.': 1.0986123085021973, 'This is an interesting approach for coupling the training of these two models.': 1.0986120700836182, 'The paper however is quite weak on the empirical studies.': 1.0986123085021973, 'In particular:': 1.0986123085021973, 'The training datasets are tiny, from sets of 1 image to 5-6.': 1.098551869392395, 'What is the reason for not using larger sets?': 0.7818742394447327, 'I think the small datasets are leading to over training and are really masking the true value of the proposed cooperative training approach.': 1.0986123085021973, 'For most of the experiments presented in the paper it is hard to assess the specific value brought by the proposed cooperative training approach because baseline results are missing.': 1.0986123085021973, 'There are comparisons provided for face completion experiments - but even there comparisons with descriptor or generator network trained separately or with other deep auto-encoders are missing.': 1.0986123085021973, 'Thus it is hard to conclude if and how much gain is obtained by cooperative training over say individually training the descriptor and generator networks.': 1.0986123085021973, 'Another comment is that in the “related work” section, I think relation with variational auto encoders (Kingma and Welling 2013) should be included.': 1.098603367805481, 'Despite limitations mentioned above, I think the ideas presented in the paper are intuitively appealing and worth discussing at ICLR.': 0.7868688702583313, 'Paper would be considerably strengthened by adding more relevant baselines and addressing the training data size issues.': 1.0965052843093872}"
7,https://openreview.net/forum?id=B1ElR4cgg,"{'This is a parallel work with BiGAN.': 1.0986123085021973, 'The idea is using auto encoder to provide extra information for discriminator.': 1.0986123085021973, 'This approach seems is promising from reported result.': 1.0986123085021973, 'After reading the rebuttal, I decided to increase my score.': 1.0986123085021973, 'I think ALI somehow stabilizes the GAN training as demonstrated in Fig.': 1.0986123085021973, '8': 1.0986123085021973, 'and learns a reasonable inference network.': 1.0986123085021973, 'Initial Review:': 1.0986123085021973, 'This paper proposes a new method for learning an inference network in the GAN framework.': 1.0986123085021973, ""ALI's objective is to match the joint distribution of hidden and visible units imposed by an encoder and decoder network."": 1.0986123085021973, 'ALI is trained on multiple datasets, and it seems to have a good reconstruction even though it does not have an explicit reconstruction term in the cost function.': 1.0986120700836182, 'This shows it is learning a decent inference network for GAN.': 1.0986123085021973, 'There are currently many ways to learn an inference network for GANs: One can learn an inference network after training the GAN by sampling from the GAN and learning a separate network to map X to Z.': 1.0986123085021973, 'There is also the infoGAN approach (not cited) which trains the inference network at the same time with the generative path.': 1.0986123085021973, ""I think this paper should have an extensive comparison with these other methods and have a discussion for why ALI's inference network is superior to previous works."": 1.0986123085021973, ""Since ALI's inference network is stochastic, it would be great if different reconstructions of a same image is included."": 1.0986123085021973, 'I believe the inference network of the BiGAN paper is deterministic which is the main difference with this work.': 1.0986123085021973, 'So maybe it is worth highlighting this difference.': 1.0986123085021973, ""The quality of samples is very good, but there is no quantitative experiment to compare ALI's samples with other GAN variants."": 1.0986123085021973, 'So I am not sure if learning an inference network has contributed to better generative samples.': 1.0986123085021973, 'Maybe including an inception score for comparison can help.': 1.0986123085021973, 'There are two sets of semi-supervised results:': 1.0986123085021973, 'The first one concatenate the hidden layers of the inference network and uses an L2-SVM afterwards.': 1.0986123085021973, 'Ideally, concatenating feature maps is not the best way for semi-supervised learning and one would want to train the semi-supervised path at the same time with the generative path.': 0.572421133518219, 'It would have been much more interesting if part of the hidden code was a categorical distribution and another part of it was a continuous distribution like Gaussian, and the inference network on the categorical latent variable was used directly for classification (like semi-supervised VAE).': 1.0067651271820068, 'In this case, the inference network would be trained at the same time with the generative path.': 0.9492177963256836, 'Also if the authors can show that ALI can disentangle factors of variations with a discrete latent variable like infoGAN, it will significantly improve the quality of the paper.': 0.9932981729507446, 'The second semi-supervised learning results show that ALI can match the state-of-the-art.': 0.9972081780433655, 'But my impression is that the significant gain is mainly coming from the adaptation of Salimans et al. (2016) in which the discriminator is used for classification.': 1.094831943511963, 'It is unclear to me why learning an inference network help the discriminator do a better job in classification.': 0.6043635606765747, 'How do we know the proposed method is improving the stability of the GAN?': 1.0976706743240356, 'My understanding is that one of the main points of learning an inference network is to learn a mapping from the image to the high-level features such as class labels.': 1.0603594779968262, 'So it would have been more interesting if the inference path was directly used for semi-supervised learning as I explained above.': 1.0986123085021973, 'This paper extends the GAN framework to allow for latent variables.': 1.0986123085021973, 'The observed data set is expanded by drawing latent variables z from a conditional distribution q(z|x).': 1.0983942747116089, 'The joint distribution on x,z is then modeled using a joint generator model p(x,z)=p(z)p(x|z).': 0.553029477596283, 'Both q and p are then trained by trying to fool a discriminator.': 1.0986123085021973, 'This constitutes a worthwhile extension of GANs: giving GANs the ability to do inference opens up many applications that could previously only be addressed by e.g. VAEs.': 1.0661206245422363, 'The results are very promising.': 1.0986123085021973, ""The CIFAR-10 samples are the best I've seen so far (not counting methods that use class labels)."": 1.034500002861023, 'Matching the semi-supervised results from Salimans et al. without feature matching also indicates the proposed method may improve the stability of training GANs.': 1.098010540008545}"
8,https://openreview.net/forum?id=B1G9tvcgx,"{'This paper proposes a multimodal neural machine translation that is based upon previous work using variational methods but attempts to ground semantics with images.': 1.0986123085021973, 'Considering way to improve translation with visual information seems like a sensible thing to do when such data is available.': 1.0986120700836182, 'As pointed out by a previous reviewer, it is not actually correct to do model selection in the way it was done in the paper.': 1.0985960960388184, 'This makes the gains reported by the authors very marginal.': 1.0941487550735474, ""In addition, as the author's also said in their question response, it is not clear if the model is really learning to capture useful image semantics."": 1.0986123085021973, 'As such, it is unfortunately hard to conclude that this paper contributes to the direction that originally motivated it.': 1.0986123085021973, 'The paper proposes an approach to the task of multimodal machine translation, namely to the case when an image is available that corresponds to both source and target sentences.': 1.0986123085021973, 'The idea seems to be to use a latent variable model and condition it on the image.': 1.0985870361328125, 'In practice from Equation 3 and Figure 3 one can see that the image is only used during training to do inference.': 1.0984920263290405, 'That said, the approach appears flawed, because the image is not really used for translation.': 1.0986123085021973, 'Experimental results are weak.': 1.072783350944519, 'If the model selection was done properly, that is using the validation set, the considered model would only bring 0.6 METEOR and 0.2 BLEU advantage over the baseline.': 1.0980350971221924, 'In the view of the overall variance of the results, these improvements can not be considered significant.': 1.098610281944275, 'The qualitative analysis in Subsection 4.4 appears inconclusive and unconvincing.': 1.0986120700836182, 'Overall, there are major issues with both the approach and the execution of the paper.': 1.0986078977584839, 'I have problems understanding the motivation of this paper.': 0.9418176412582397, ""The authors claimed to have captured a latent representation of text and image during training and can translate better without images at test time, but didn't demonstrate convincingly that images help (not to mention the setup is a bit strange when there are no images at test time)."": 1.0986121892929077, 'What I see are only speculative comments: ""we observed some gains, so these should come from our image models"".': 1.0986123085021973, ""The qualitative analysis doesn't convince me that the models have learned latent representations; I am guessing the gains are due to less overfitting because of the participation of images during training."": 1.0986123085021973, 'The dataset is too small to experiment with NMT.': 0.9638610482215881, 'I\'m not sure if it\'s fair to compare their models with NMT and VNMT given the following description in Section 4.1 ""VNMT is fine-tuned by NMT and our models are fine-tuned with VNMT"".': 0.6209138631820679, 'There should be more explanation on this.': 1.051066517829895, 'Besides, I have problems with the presentation of this paper.': 0.7505499124526978, '(a) There are many symbols being used unnecessary.': 1.0830785036087036, 'For example: f & g are used for x (source) and y (target) in Section 3.1.': 1.0951162576675415, ""(b) The ' symbol is not being used in a consistent manner, making it sometimes hard to follow the paper."": 0.3715382218360901, ""For example, in section 3.1.2, there are references about h'_\\pi obtained from Eq."": 1.0986123085021973, '(3) which is about h_\\pi (yes, I understand what the authors mean, but there can be better ways to present that).': 1.0986123085021973, ""(c) I'm not sure if it's correct in Section 3.2.2 h'_z is computed from \\mu and \\sigma."": 1.0986123085021973, ""So how \\mu' and \\sigma' are being used ?"": 1.0986123085021973, '(d) G+O-AVG should be something like G+O_{AVG}.': 1.0986123085021973, ""The minus sign makes it looks like there's an ablation test there."": 1.0986123085021973, 'Similarly for other symbols.': 1.0986123085021973, 'Other things: no explanations for Figure 2 & 3.': 1.0986123085021973, ""There's a missing \\pi symbol in Appendix A before the KL derivation."": 1.0986123085021973}"
9,https://openreview.net/forum?id=B1GOWV5eg,"{'This paper shows that extending deep RL algorithms to decide which action to take as well as how many times to repeat it leads to improved performance on a number of domains.': 1.0986123085021973, 'The evaluation is very thorough and shows that this simple idea works well in both discrete and continuous actions spaces.': 1.0986123085021973, 'A few comments/questions:': 1.0986123085021973, 'Table 1 could be easier to interpret as a figure of histograms.': 1.0986123085021973, 'Figure 3 could be easier to interpret as a table.': 1.0986123085021973, 'How was the subset of Atari games selected?': 1.0986123085021973, 'The Atari evaluation does show convincing improvements over A3C on games requiring extended exploration (e.g. Freeway and Seaquest), but it would be nice to see a full evaluation on 57 games.': 1.0986120700836182, 'This has become quite standard and would make it possible to compare overall performance using mean and median scores.': 1.0986123085021973, 'It would also be nice to see a more direct comparison to the STRAW model of Vezhnevets et al., which aims to solve some of the same problems as FiGAR.': 1.0986123085021973, 'FiGAR currently discards frames between action decisions.': 1.0986123085021973, 'There might be a tradeoff between repeating an action more times and throwing away more information.': 1.0986123085021973, 'Have you thought about separating these effects?': 1.0986123085021973, 'You could train a model that does process intermediate frames.': 1.0986123085021973, 'Just a thought.': 1.0986123085021973, 'Overall, this is a nice simple addition to deep RL algorithms that many people will probably start using.': 1.0986123085021973, ""I'm increasing my score to 8 based on the rebuttal and the revised paper."": 1.0986123085021973, 'This paper provides a simple method to handle action repetitions.': 1.0986123085021973, 'They make the action a tuple (a,x), where a is the action chosen, and x the number of repetitions.': 1.0986123085021973, 'Overall they report some improvements over A3C/DDPG, dramatic in some games, moderate in other.': 1.0986123085021973, 'The idea seems natural and there is a wealth of experiment to support it.': 1.0986123085021973, 'Comments:': 1.0986123085021973, 'The scores reported on A3C in this paper and in the Mnih et al. publication (table S3) differ significantly.': 1.0986123085021973, 'Where does this discrepancy come from?': 1.0986123085021973, ""If it's from a different training regime (fewer iterations, for instance), did the authors confirm that running  their replication to the same settings as Mnih et al provide similar results?"": 1.0986123085021973, 'It is intriguing that the best results of FiGAR are reported on games where few actions repeat dominate.': 1.0981298685073853, 'This seems to imply that for those, the performance overhead of FiGAR over A3C is high since A3C uses an action repeat of 4 (and therefore has 4 times fewer gradient updates).': 1.098610758781433, 'A3C could be run for a comparable computation cost with a lower action repeat, which would probably result in increased performance of A3C.  Nevertheless,  the automatic determination of the appropriate action repeat is interesting, even if the overall message seems to be to not repeat actions too often.': 1.0986123085021973, 'Slightly problematic notation, where r sometimes denotes rewards, sometimes denotes elements of the repetition set R (top of page 5)': 0.6540219783782959, ""In the equation at the bottom of page 5 - since the sum is not indexed over decision steps, not time steps, shouldn't the rewards r_k be modified to be the sum of rewards (appropriately discounted) between those time steps?"": 1.0986123085021973, 'The section on DDPG is confusingly written.': 1.0986123085021973, '""Concatenating"" loss is a strange operation; doesn\'t FiGAR correspond to a loss to roughly looks like Q(x,mu(x)) +': 1.0986123085021973, 'R log p(x) (with separate loss for learning the critic)?': 1.0986123085021973, 'It feels that REINFORCE should be applied for the repetition variable x (second term of the sum) and reparametrization for the action a (first term)?': 0.5459320545196533, ""Is the 'name_this_game' name in the tables  intentional?"": 1.0986123085021973, 'A potential weakness of the method is that the agent must decide to commit to an action for a fixed number of steps, independently of what happens next.': 0.8176798224449158, 'Have the authors considered a scheme in which, at each time step, the agent decides to stick with the current decision or not?': 1.039368748664856, '(It feels like it might be a relatively simple modification of FiGAR).': 1.0986123085021973, 'This paper proposes a simple but effective extension to reinforcement learning algorithms, by adding a temporal repetition component as part of the action space, enabling the policy to select how long to repeat the chosen action for.': 1.0863384008407593, 'The extension applies to all reinforcement learning algorithms, including both discrete and continuous domains, as it is primarily changing the action parametrization.': 1.0984797477722168, 'The paper is well-written, and the experiments extensively evaluate the approach with 3 different RL algorithms in 3 different domains (Atari, MuJoCo, and TORCS).': 1.097623348236084, 'Here are some comments and questions, for improving the paper:': 1.0986123085021973, 'The introduction states that ""all DRL algorithms repeatedly execute a chosen action for a fixed number of time steps k"".': 1.0984901189804077, 'This statement is too strong, and is actually disproved in the experiments — repeating an action is helpful in many tasks, but not in all tasks.': 1.0980769395828247, 'The sentence should be rephrased to be more precise.': 1.0986123085021973, 'In the related work, a discussion of the relation to semi-MDPs would be useful to help the reader better understand the approach and how it compares and differs (e.g. the response from the pre-review questions)': 0.35022681951522827, 'Experiments:': 1.0986123085021973, 'Can you provide error bars on the experimental results?': 1.0986123085021973, '(from running multiple random seeds)': 1.0986123085021973, 'It would be useful to see experiments with parameter sharing in the TRPO experiments, to be more consistent with the other domains, especially since it seems that the improvement in the TRPO experiments is smaller than that of the other two domains.': 1.0916718244552612, 'Right now, it is hard to tell if the smaller improvement is because of the nature of the task, because of the lack of parameter sharing, or something else.': 1.0191829204559326, 'The TRPO evaluation is different from the results reported in Duan et al.': 1.0986106395721436, 'ICML ’': 1.0986123085021973, '16.': 1.0986123085021973, 'Why not use the same benchmark?': 1.0986123085021973, 'Videos only show the policies learned with FiGAR, which are uninformative without also seeing the policies learned without FiGAR.': 1.095338225364685, 'Can you also include videos of the policies learned without FiGAR, as a comparison point?': 1.0986120700836182, 'How many laps does DDPG complete without FiGAR?': 1.0986123085021973, 'The difference in reward achieved seems quite substantial (557K vs. 59K).': 1.0986123085021973, 'Can the tables be visualized as histograms?': 1.0986123085021973, 'This seems like it would more effectively and efficiently communicate the results.': 1.0986123085021973, 'Minor comments:': 1.0986123085021973, 'On the plot in Figure 2, the label for the first bar should be changed from 1000 to 3500.': 1.0986123085021973, '“idea of deciding when necessary” - seems like it would be better to say “idea of only deciding when necessary""': 1.0986123085021973, '""spaces.': 1.0986123085021973, 'Durugkar et al.” — missing a space.': 1.0986123085021973, '“R={4}” — why 4?': 1.0986123085021973, 'Could you use a letter to indicate a constant instead?': 1.0986123085021973, '(or a different notation)': 1.0986123085021973}"
10,https://openreview.net/forum?id=B1Igu2ogg,"{'This paper discusses a method for computing vector representations for documents by using a skip-gram style learning mechanism with an added regularizer in the form of a global context vector with various bits of drop out.': 1.0986123085021973, 'While none of the individual components proposed in this paper are new, I believe that the combination in this fashion is.': 1.0986053943634033, 'Further, I appreciated the detailed analysis of model behaviour in section 3.': 1.0986123085021973, 'The main downside to this submission is in its relative weakness on the empirical front.': 1.0986123085021973, 'Arguably there are more interesting tasks than sentiment analysis and k-way classification!': 0.38956549763679504, 'Likewise, why waste 2/3 of a page on t-sne projections rather than use that space for further analysis?': 1.0986123085021973, ""While I am a bit disappointed by this reduced evaluation and agree with the other reviewers concerning soft baselines, I think this paper should be accepted: it's an interesting algorithm, nicely composed and very efficient, so it's reasonable to assume that other readers might have use for some of the ideas presented here."": 1.0986123085021973, 'This paper presents a framework for creating document representations.': 1.09861159324646, 'The main idea is to represent a document as an average of its word embeddings with a data-dependent regularization that favors informative or rare words while forcing common words to be close to 0.': 1.0986119508743286, 'Experiments on sentiment analysis and document classification show that the proposed method has the lowest error rates compared to baseline document embedding methods.': 1.0604604482650757, 'While I like the motivation of finding the best way to encode a document into a vector, the paper does not offer significant technical contributions.': 1.098092794418335, 'Most of the techniques are not new, and the main selling point is the simplicity and speed of the proposed method.': 0.9965472221374512, 'For this reason, I would like to see good results for more than two tasks to be convinced that this is the best way to learn document representations.': 1.0986065864562988, 'For RNN-LM, is the LM trained to minimize classification error, or is it trained  as a language model?': 1.0986052751541138, 'Did you use the final hidden state as the representation, or the average of all hidden states?': 1.0986123085021973, 'One of the most widely used method to represent documents now is to have a bidirectional LSTM and concatenate the final hidden states as the document representation.': 1.0986123085021973, 'I think it would be useful to know how the proposed method compares to this approach for tasks such as document classification or sentiment analysis.': 1.0986123085021973, ""This paper proposes learning document embeddings as a sum of the constituent word embeddings, which are jointly learned and randomly dropped out ('corrupted') during training."": 1.0986123085021973, 'While none of the pieces of this model are particularly novel, the result is an efficient learning algorithm for document representation with good empirical performance.': 1.046754002571106, ""Joint training of word and document embeddings is not a new idea, nor is the idea of enforcing the document to be represented by the sum of its word embeddings (see, e.g. '“The Sum of Its Parts”: Joint Learning of Word and Phrase Representations with Autoencoders' by Lebret and Collobert)."": 1.0011194944381714, 'Furthermore, the corruption mechanism is nothing other than traditional dropout on the input layer.': 1.0986123085021973, 'Coupled with the word2vec-style loss and training methods, this paper offers little on the novelty front.': 1.098600149154663, 'On the other hand, it is very efficient at generation time, requiring only an average of the word embeddings rather than a complicated inference step as in Doc2Vec.': 1.0986123085021973, 'Moreover, by construction, the embedding captures salient global information about the document': 1.0985993146896362, 'it captures specifically that information that aids in local-context prediction.': 0.49946287274360657, 'For such a simple model, the performance on sentiment analysis and document classification is quite encouraging.': 1.0986123085021973, 'Overall, despite the lack of novelty, the simplicity, efficiency, and performance of this model make it worthy of wider readership and study, and I recommend acceptance.': 1.0986121892929077}"
11,https://openreview.net/forum?id=B1IzH7cxl,"{'The authors propose a recurrent variational neural network approach to modelling volatility in financial time series.': 1.0986123085021973, 'This model consists of an application of Chung et al.’s': 1.0986123085021973, '(2015) VRNN model to volatility forecasting, wherein a Variational Autoencoder (VAE) structure is repeated at each time step of the series.': 1.0986123085021973, 'The paper is well written and easy to follow (although this reviewer suggests applying a spelling checking, since the paper contains a number of harmless typos).': 1.0986123085021973, 'The paper’s main technical contribution is to stack two levels of recurrence, one for the latent process and one for the observables.': 1.0986123085021973, 'This appears to be a novel, if minor contribution.': 1.0986123085021973, 'The larger contribution is methodological, in areas of time series modelling that are both of great practical importance and have hitherto been dominated by rigid functional forms.': 1.0986123085021973, 'The demonstration of the applicability and usefulness of general-purpose non-linear models for volatility forecasting would be extremely impactful.': 1.0986123085021973, 'I have a few comments and reservations with the paper:': 1.0986123085021973, '1) Although not  mentioned explicitly, the authors’ framework are couched in terms of carrying out one-timestep-ahead forecasts of volatility.': 1.0986123085021973, 'However, many applications of volatility models, for instance for derivative pricing, require longer-horizon forecasts.': 1.0986123085021973, 'It would be interesting to discuss how this model could be extended to forecast at longer horizons.': 1.0986123085021973, '2) In Section 4.4, there’s a mention that a GARCH(1,1) is conditionally deterministic.': 1.0986123085021973, 'This is true only when forecasting 1 time-step in the future.': 1.0986123085021973, 'At longer horizons, the GARCH(1,1) volatility forecast is not deterministic.': 1.0986123085021973, '3) I was initially unhappy with the limitations of the experimental validation, limited to comparison with a baseline GARCH model.': 1.0986123085021973, 'However, the authors provided more comparisons in the revision, which adds to the quality of the results, although the models compared against cannot be considered state of the art.': 1.098476767539978, 'It would be well advised to look into R packages such as `stochvol’ and ‘fGarch’ to get implementations of a variety of models that can serve as useful baselines, and provide convincing evidence that the modelled volatility is indeed substantially better than approaches currently entertained by the finance literature.': 0.6121336221694946, '4)': 1.0986123085021973, 'In Section 5.2, more details should be given on the network, e.g. number of hidden units, as well as the embedding dimension D_E (section 5.4)': 1.098609209060669, '5)': 0.5170272588729858, 'In Section 5.3, more details should be given on the data generating process for the synthetic data experiments.': 1.0852670669555664, '6)': 1.0986117124557495, 'Some results in the appendix are very puzzling: around jumps in the price series, which are places where the volatility should spike, the model reacts instead by huge drops in the volatility (Figure 4(b) and (c), respectively around time steps 1300 and 1600).': 1.0985691547393799, 'This should be explained and discussed.': 1.0985653400421143, 'All in all, I think that the paper provides a nice contribution to the art of volatility modelling.': 1.081169843673706, 'In spite of some flaws, it provides a starting point for the broader impact of neural time series processing in the financial community.': 1.0955018997192383, 'The authors propose a recurrent neural network approach for constructing a': 1.0986123085021973, 'stochastic volatility model for financial time series.': 1.0938490629196167, 'They introduce an': 1.0976184606552124, 'inference network based on a recurrent neural network that computes the': 0.8416264653205872, 'approximation to the posterior distribution for the latent variables given the': 0.6623166799545288, 'past data.': 1.0986123085021973, 'This variational approximation is used to maximize the marginal': 1.0974040031433105, 'likelihood in order to learn the parameters of the model.': 1.067734718322754, 'The proposed method': 1.0969963073730469, 'is validated in experiments with synthetic and real-world time series, showing': 1.0712568759918213, 'to outperform parametric GARCH models and a Gaussian process volatility model.': 1.0695390701293945, 'Quality:': 1.0986123085021973, 'The method proposed seems technically correct, with the exception that in': 0.41021960973739624, 'equation (19) the inference model is doing filtering and not smoothing, in the': 1.0985976457595825, ""sense that the posterior for z_t' only depends on those other z_t and x_t"": 0.974738597869873, ""values with t<t', but in the true posterior p(Z|X) each z_t depends on all the"": 0.46854066848754883, 'X.': 1.0986123085021973, 'This means the proposed learning method is inefficient.': 0.4079698920249939, 'The reference below': 1.0383775234222412, 'shows how to perform smoothing too and the results in that paper show indeed that': 0.8088740110397339, 'smoothing produces better results for learning the model.': 0.46940597891807556, 'Sequential Neural Models with Stochastic Layers Fraccaro, Marco and S\\o nderby,': 0.725266695022583, 'S\\o ren Kaae and Paquet, Ulrich and Winther, Ole In NIPS 2016.': 1.0729546546936035, 'It is not clear if the method proposed in the above reference would perform better': 0.7105955481529236, 'just because of using smoothing when learning the model parameters.': 1.0554766654968262, 'Clarity:': 0.6864228844642639, 'The paper is clearly written and easy to read.': 1.0969816446304321, 'For the results on real-world data, in Table 1, how is the NLL computed?': 1.080999732017517, 'Is the': 1.0954046249389648, 'average NLL across the 162 time series?': 1.0986071825027466, 'Originality:': 0.530305027961731, 'The method proposed is not very original.': 1.0939078330993652, 'Previous work has already used the': 0.737506091594696, 'variational approach with the reparametrization trick to learn recurrent neural': 0.5144148468971252, 'networks with stochastic units (see the reference above).': 1.096764326095581, 'It seems that the main': 1.0986074209213257, 'contribution of the authors is to apply this type of techniques to the problem': 1.0182406902313232, 'of modeling financial time series.': 1.0986086130142212, 'Significance:': 1.0985866785049438, 'The results shown are significant, the method proposed by the authors': 1.0984610319137573, 'outperforms previous approaches.': 1.0733519792556763, 'However, there is a huge amount of techniques': 1.010188102722168, 'available for modeling financial time-series.': 0.4969526529312134, 'The number of GARCH variants is': 1.09861159324646, 'probably close to hundreds, each one claiming to be better than the others.': 0.42516931891441345, 'This makes difficult to quantify how important the results are.': 0.1059216856956482, 'Thank you for an interesting read.': 1.0985833406448364, 'I found the application of VRNN type generative model to financial data very promising.': 1.064550757408142, ""But since I don't have enough background knowledge to judge whether the performance gap is significant or not, I wouldn't recommend acceptance at this stage."": 1.0980720520019531, ""To me, the biggest issue for this paper is that I'm not sure if the paper contains significant novelty."": 1.097965121269226, 'The RNN-VAE combination has been around for more than a year and this paper does not propose significant changes to it.': 0.9027766585350037, 'Maybe this paper fits better to an application targeting conference, rather than ICLR.': 1.0679928064346313, ""But I'm not exactly sure about ICLR's acceptance criteria, and maybe the committee actually prefer great performances and interesting applications?"": 1.0986114740371704}"
12,https://openreview.net/forum?id=B1KBHtcel,"{'This paper addresses automated argumentation mining using pointer network.': 1.0986117124557495, 'Although the task and the discussion is interesting, the contribution and the novelty is marginal because this is a single-task application of PN among many potential tasks.': 1.09861159324646, 'This paper addresses the problem of argument mining, which consists of finding argument types and predicting the relationships between the arguments.': 1.0985194444656372, 'The authors proposed a pointer network structure to recover the argument relations.': 1.098360538482666, 'They also propose modifications on pointer network to perform joint training on both type and link prediction tasks.': 1.098589539527893, 'Overall the model is reasonable, but I am not sure if ICLR is the best venue for this work.': 1.0970933437347412, 'My first concern of the paper is on the novelty of the model.': 1.093475341796875, 'Pointer network has been proposed before.': 1.0979310274124146, 'The proposed multi-task learning method is interesting, but the authors only verified it on one task.': 1.0984221696853638, 'This makes me feel that maybe the submission is more for a NLP conference rather than ICLR.': 1.0985462665557861, 'The authors stated that the pointer network is less restrictive compared to some of the existing tree predicting method.': 1.0956330299377441, 'However, the datasets seem to only contain single trees or forests, and the stack-based method can be used for forest prediction by adding a virtual root node to each example (as done in the dependency parsing tasks).': 1.0986123085021973, 'Therefore, I think the experiments right now cannot reflect the advantages of pointer network models unfortunately.': 1.0817680358886719, 'My second concern of the paper is on the target task.': 1.098599910736084, 'Given that the authors want to analyze the structures between sentences, is the argumentation mining the best dataset?': 1.0986123085021973, 'For example, authors could verify their model by applying it to the other tasks that require tree structures such as dependency parsing.': 1.0986026525497437, 'As for NLP applications, I found that the assumption that the boundaries of AC are given is a very strong constraint, and could potentially limit the usefulness of the proposed model.': 1.0569621324539185, 'Overall, in terms of ML, I also feel that baseline methods the authors compared to are probably strong for the argument mining task, but not necessary strong enough for the general tree/forest prediction tasks (as there are other tree/forest prediction methods).': 1.0950900316238403, 'In terms of NLP applications, I think the assumption of having AC boundaries is too restrictive, and maybe ICLR is not the best venture for this submission.': 1.0981268882751465, 'This paper proposes a model for the task of argumentation mining (labeling the set of relationships between statements expressed as sentence-sized spans in a short text).': 1.0986123085021973, 'The model combines a pointer network component that identifies links between statements and a classifier that predicts the roles of these statements.': 1.0986123085021973, 'The resulting model works well: It outperforms strong baselines, even on datasets with fewer than 100 training examples.': 1.0986123085021973, ""I don't see any major technical issues with this paper, and the results are strong."": 1.0986123085021973, ""I am concerned, though, that the paper doesn't make a substantial novel contribution to representation learning."": 0.44883766770362854, 'It focuses on ways to adapt reasonably mature techniques to a novel NLP problem.': 1.0986123085021973, 'I think that one of the ACL conferences would be a better fit for this work.': 0.28349727392196655, 'The choice of a pointer network for this problem seems reasonable, though (as noted by other commenters)': 1.0986123085021973, 'the paper does not make any substantial comparison with other possible ways of producing trees.': 0.7698712348937988, 'The paper does a solid job at breaking down the results quantitatively, but I would appreciate some examples of model output and some qualitative error analysis.': 1.0986109972000122, 'Detail notes:': 1.0985788106918335, 'Figure 2 appears to have an error.': 1.0986123085021973, 'You report that the decoder produces a distribution over input indices only, but you show an example of the network pointing to an output index in one case.': 1.0986123085021973, 'I don\'t think ""Wei12"" is a name.': 1.0986123085021973}"
13,https://openreview.net/forum?id=B1M8JF9xx,"{'# Review': 1.0986119508743286, 'This paper proposes a quantitative evaluation for decoder-based generative models that use Annealed Importance Sampling (AIS) to estimate log-likelihoods.': 1.0986069440841675, 'Quantitative evaluations are indeed much needed since for some models, like Generative Adversarial Networks (GANs) and Generative Moment Matching Networks (GMMNs), qualitative evaluation of samples is still frequently used to assess their generative capability.': 1.0968782901763916, 'Even though, there exist quantitative evaluations like Kernel Density Estimation (KDE), the authors show how AIS is more accurate than KDE and how it can be used to perform fine-grained comparison between generative models (GAN, GMMs and Variational Autoencoders (VAE)).': 1.0348840951919556, 'The authors report empirical results comparing two different decoder architectures that were both trained, on the continuous MNIST dataset, using the VAE, GAN and GMMN objectives.': 1.098611831665039, 'They also trained an Importance Weighted Autoencoder (IWAE) on binarized MNIST and show that, in this case, the IWAE bound underestimates the true log-likelihoods by at least 1 nat (which is significant for this dataset) according to the AIS evaluation of the same model.': 1.0982775688171387, '# Pros': 1.0986087322235107, 'Their evaluation framework is public and is definitely a nice contribution to the community.': 1.0906741619110107, 'This paper gives some insights about how GAN behaves from log-likelihood perspective.': 1.098611831665039, 'The authors disconfirm the commonly proposed hypothesis that GAN are memorizing training data.': 1.0985828638076782, 'The authors also observed that GANs miss important modes of the data distribution.': 1.0986123085021973, '# Cons/Questions': 1.0984501838684082, 'It is not clear for me why sometimes the experiments were done using different number of examples (100, 1000, 10000) coming from different sources (trainset, validset, testset or simulation/generated by the model).': 1.0981241464614868, 'For instance, in Table 2 why results were not reported using all 10,000 examples of the testing set?': 1.09846830368042, 'It is not clear why in Figure 2c, AIS is slower than AIS+encoder?': 1.0985678434371948, 'Is the number of intermediate distributions the same in both?': 0.9179630279541016, '16 independent chains for AIS seems a bit low from what I saw in the literature (e.g. in [Salakhutdinov & Murray, 2008] or [Desjardins etal., 2011], they used 100 chains).': 0.47722023725509644, 'Could it be that increasing the number of chains helps tighten the confidence interval reported in Table 2?': 1.098092794418335, 'I would have like the authors to give their intuitions as to why GAN50 has a BDMC gap of 10 nats, i.e. 1 order of magnitude compared to the others?': 1.0929263830184937, '# Minor comments': 0.11407194286584854, 'Table 1 is not referenced in the text and lacks description of what the different columns represent.': 1.0753151178359985, 'Figure 2(a), are the reported values represents the average log-likelihood of 100 (each or total?)': 1.082862377166748, 'training and validation examples of MNIST (as described in Section 5.3.2).': 1.0986123085021973, ""Figure 2(c), I'm guessing it is on binarized MNIST?"": 1.0986123085021973, 'Also, why are there fewer points for AIS compared to IWAE and AIS+encoder?': 1.0986123085021973, 'Are the BDMC gaps mentioned in Section 5.3.1 the same as the ones reported in Table2 ?': 1.0986123085021973, 'Typo in caption of Figure 3: ""(c) GMMN-10"" but actually showing GMMN-50 according to the graph title and subcaption.': 1.0986123085021973, 'Summary:': 1.0986123085021973, 'This paper describes how to estimate log-likelihoods of currently popular decoder-based generative models using annealed importance sampling (AIS) and HMC.': 1.0986114740371704, 'It validates the method using bidirectional Monte Carlo on the example of MNIST, and compares the performance of GANs and VAEs.': 1.0775548219680786, 'Review:': 1.0986123085021973, 'Although this seems like a fairly straight-forward application of AIS to me (correct me if I missed an important trick to make this work), I very much appreciate the educational value and empirical contributions of this paper.': 1.0965763330459595, 'It should lead to clarity in debates around the density estimation performance of GANs, and should enable more people to use AIS.': 1.0986119508743286, 'Space permitting, it might be a good idea to try to expand the description of AIS.': 1.0986028909683228, 'All the components of AIS are mentioned and a basic description of the algorithm is given, but the paper doesn’t explain well “why” the algorithm does what it does/why it works.': 1.0774163007736206, 'I was initially confused by the widely different numbers in Figure 2.': 1.0986123085021973, 'On first glance my expectation was that this Figure is comparing GAN, GMMN and IWAE (because of the labeling at the bottom and because of the leading words in the caption’s descriptions).': 1.0983785390853882, 'Perhaps mention in the caption that (a) and (b) use continuous MNIST and (c) uses discrete MNIST.': 1.0979336500167847, '“GMMN-50” should probably be “GMMN-10”.': 0.9275951981544495, 'Using reconstructions for evaluation of models may be a necessary but is not sufficient condition for a good model.': 0.7564848065376282, 'Depending on the likelihood, a posterior sample might have very low density under the prior, for example.': 0.8038269281387329, 'It would be great if the authors could point out and discuss the limitations of this test a bit more.': 1.0014081001281738, 'Minor:': 1.0986123085021973, 'Perhaps add a reference to MacKay’s density networks (MacKay, 1995) for decoder-based generative models.': 1.0182435512542725, 'In Section 2.2, the authors write “the prior over z can be drastically different than the true posterior p(z|x), especially in high dimension”.': 1.0897010564804077, 'I think the flow of the paper could be improved here, especially for people less familiar with importance sampling/AIS.': 1.0981518030166626, 'I don’t think the relevance of the posterior for importance sampling is clear at this point in the paper.': 0.7013970613479614, 'In Section 2.3 the authors claim that is often more “meaningful” to estimate p(x) in log-space because of underflow problems.': 0.4252952039241791, '“Meaningful” seems like the wrong word here.': 0.45441651344299316, 'Perhaps revise to say that it’s more practical to estimate log p(x) because of underflow problems, or to say that it’s more meaningful to estimate log p(x) because of its connection to compression/surprise/entropy.': 1.050494909286499, 'The paper describes a method to evaluate generative models such as VAE, GAN and GMMN.': 1.0962074995040894, 'This is very much needed in our community where we still eyeball generated images to judge the quality of a model.': 1.0986123085021973, 'However, the technical increment over the NIPS 16 paper: “Measuring the reliability of MCMC inference with bidirectional Monte Carlo” is very small, or nonexistent (but please correct me if I am wrong!).': 1.0981365442276, '(Grosse et al).': 0.40547195076942444, 'The relative contribution of this paper is the application of this method to generative models.': 1.0986123085021973, 'In section 2.3 the authors seem to make a mistake.': 1.0986123085021973, 'They write E[p’(x)] <= p(x) but I think they mean: E[log p’(x)]': 1.0986123085021973, '<= log E[p’(x)]': 1.0986123085021973, '= log p(x).': 1.0986123085021973, 'Also,  for what value of x?': 1.0986123085021973, 'If p(x) is normalized it can’t be true for all values of x. Anyways, I think there are typos here and': 1.0986123085021973, 'there and the equations could be more precise.': 1.0986123085021973, 'On page 5 top of the page it is said that the AIS procedure can be initialized with q(z|x) instead of p(z).': 1.0986123085021973, 'However, it is unclear what value of x is then picked?': 1.0986123085021973, 'Is it perhaps Ep(x)[q(z|x)] ?': 1.0986123085021973, 'I am confused with the use of the term overfitting (p8 bottom).': 1.0986123085021973, 'Does a model A overfit relative to a another model B if the test accuracy of A is higher than that of B even though the gap between train and test accuracy is also higher for B than for A. I think not.': 1.0986123085021973, 'Perhaps the last sentence on page 8 should say that VAE-50 underfits less than GMMN-50?': 1.0986123085021973, 'The experimental results are interesting in that it exposes the fact that GANs and GMMNs seem to have much lover test accuracy than VAE despite the fact that their samples look great.': 1.0986123085021973}"
14,https://openreview.net/forum?id=B1MRcPclx,"{'The paper proposed a simple and effective model for QA.': 1.097163200378418, 'The paper is easy to read and result is impressive on the synthetic and real dataset.': 1.0986123085021973, 'The one question is the paper is called query-reduction, but there is no place to show this reduction explicitly.': 1.0986123085021973, 'The paper proposes a simplistic recurrent architecture that has a very small temporal dependence in its definition (which, in turn, allows it to be parallelized across the temporal axis).': 1.0986123085021973, 'Paper is clear to follow, the architecture has appropriate motivation and the experiments seem to be very thorough and over an appropriate range of synthetic and real datasets.': 1.0986087322235107, 'Results are very impressive on its own especially considering the simplicity of the model.': 0.8347699046134949, 'Activation and gate visualizations are informative.': 0.873883843421936, 'Overall, the work is sufficient as a conference contribution.': 0.4993240535259247, 'I appreciate the effort by authors in responding to questions and incorporating revisions.': 0.6884557008743286, 'This work introduces a new RNN model and shows how it can be used for QA in bAbI-like settings.': 1.0986121892929077, 'I think the model is simple and interesting, and the results are strong.': 1.0985971689224243, 'I would like the authors to also show results training on all tasks simultaneously, but other than that, I think it is an interesting model and it is nicely described and tested.': 1.0982903242111206}"
15,https://openreview.net/forum?id=B1PA8fqeg,"{'The multiagent system is proposed as a generalization of neural network.': 1.0976089239120483, 'The proposed system can be used with less restrictive network structures more efficiently by computing only those necessary computations in the graph.': 1.0986123085021973, ""Unfortunately, I don't find the proposed system different from the framework of artificial neural network."": 1.0986121892929077, ""Although for today's neural network structures are designed to have a lot of matrix-matrix multiplications, but it is not limited to have such architecture."": 1.0986123085021973, 'In other words, the proposed multiagent system can be framed in the artificial neural network with more complicated layer/connectivity structures while considering each neuron as layer.': 1.0986123085021973, 'The computation efficiency is argued among different sparsely connected denoising autoencoder in multiagent system framework only but the baseline comparison should be against the fully-connected neural network that employs matrix-matrix multiplication.': 1.0986123085021973, 'The paper reframes feed forward neural networks as a multi-agent system.': 1.098524570465088, 'It seems to start from the wrong premise that multi-layer neural networks were created expressed as full matrix multiplications.': 1.0986123085021973, 'This ignores the decades-long history of development of artificial neural networks, inspired by biological neurons, which thus started from units with arbitrarily sparse connectivity envisioned as computing in parallel.': 1.0986123085021973, 'The matrix formulation is primarily a notational convenience; note also that when working with sparse matrix operations (or convolutions) zeros are neither stored not multiplied by.': 1.0986123085021973, 'Besides the change in terminology, essentially renaming neurons agents, I find the paper brings nothing new and interesting to the table.': 1.0986028909683228, 'Pulling in useful insights from a different communitiy such as multi-agent systems would be most welcome.': 1.0986123085021973, 'But for this to be compelling, it would have to be largely unheard-of elements in neural net research, with clear supporting empirical evidence that they significantly improve accuracy or efficiency.': 1.0986123085021973, 'This is not achieved in the present paper.': 1.0986117124557495, 'Unfortunately, the paper is not clear enough for me to understand what is being proposed.': 1.098379373550415, 'At a high-level the authors seem to propose a generalization of the standard layered neural architecture (of which MLPs are a special case), based on arbitrary nodes which communicate via messages.': 1.0986123085021973, 'The paper then goes on to show that their layer-free architecture can perform the same computation as a standard MLP.': 1.0986123085021973, 'This logic appears circular.': 1.0985966920852661, 'The low level details of the method are also confusing: while the authors seem to be wanting to move away from layers based on matrix-vector products, Algorithm 4 nevertheless resorts to matrix-vector products for the forward and backwards pass.': 1.0986123085021973, 'Although the implementation relies on asynchronously communicating nodes, the “locking” nature of the computation makes the two entirely equivalent.': 1.0986123085021973}"
16,https://openreview.net/forum?id=B1TTpYKgx,"{'This paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks.': 1.0985857248306274, 'Random networks (deep networks with random Gaussian weights, hard tanh or ReLU activation) are studied according to several criterions: number of neutron transitions, activation patterns, dichotomies and trajectory length.': 1.0698118209838867, ""There doesn't seem to be a solid justification for why the newly introduced measures of expressivity really measure expressivity."": 1.0692133903503418, 'For instance the trajectory length seems a very discutable measure of expressivity.': 1.0709811449050903, 'The only justification given for why it should be a good measure of expressivity is proportionality with other measures of expressivity in the specific case of random networks.': 1.0959588289260864, 'The paper is too obscure and too long.': 1.0672438144683838, 'The work may have some interesting ideas but it does not seem to be properly replaced in context.': 1.0692318677902222, 'Some findings seem trivial.': 1.098307490348816, 'detailed comments': 1.0986123085021973, 'p2': 1.0986123085021973, '""Much of the work examining achievable functions relies on unrealistic architectural assumptions such as layers being exponentially wide""': 1.098611831665039, 'I don’t think so.': 1.0986084938049316, 'In ""Deep Belief Networks are Compact Universal Approximators"" by Leroux et al., proof is given that deep but narrow feed-forward neural networks with sigmoidal units can represent any Boolean expression i.e. A neural network with 2n−1 + 1 layers of n units (with n the number of input neutron).': 1.0646940469741821, '“Comparing architectures in such a fashion limits the generality of the conclusions”': 1.0821359157562256, 'To my knowledge much of the previous work has focused on mathematical proof, and has led to very general conclusions on the representative power of deep networks (one example being Leroux et al again).': 1.0973999500274658, 'It is much harder to generalise the approach you propose, based on random networks which are not used in practice.': 1.0986123085021973, '“[we study] a family of networks arising in practice: the behaviour of networks after random initialisation”': 1.0986123085021973, 'These networks arise in practice as an intermediate step that is not used to perform computations; this means that the representative power of such intermediate networks is a priori irrelevant.': 1.0986123085021973, 'You would need to justify why it is not.': 1.0986123085021973, '“results on random networks provide natural baselines to compare trained networks with”': 1.0986123085021973, 'random networks are not “natural” for the study of expressivity of deep networks.': 1.0986123085021973, 'It is not clear how the representative power of random networks (what kind of random networks seems an important question here) is linked to the representative power of (i) of the whole class of networks or (ii) the class of networks after training.': 1.0986123085021973, 'Those two classes of networks are the ones we would a priori care about and you would need to justify why the study of random networks helps in understanding either (i) or (ii).': 1.0986123085021973, 'p5': 1.0986123085021973, '“As FW is a random neural network […] it would suggest that points far enough away from each other would have independent signs, i.e. a direct proportionality between the length of z(n)(t) and the number of times it crosses the decision boundary.”': 1.0986123085021973, 'As you say, it seems that proportionality of the two measures depends on the network being random.': 1.0986123085021973, 'This seems to invalidate generalisation to other networks, i.e. if the networks are not random, one would assume that path lengths are not proportional.': 1.0986123085021973, 'p6': 1.0986123085021973, 'the expressivity w.r.t.': 1.0986123085021973, 'remaining depth seems a trivial concerns, completely equivalent to the expressivity w.r.t. depth.': 1.0986123085021973, 'This makes the remark in figure 5 that the number of achievable dichotomies only depends *only* on the number of layers above the layer swept seem trivial': 1.098427414894104, 'p7': 1.0986123085021973, 'in figure 6 a network width of 100 for MNIST seems much too small.': 0.9633457660675049, 'Accordingly performance is very poor and it is difficult to generalise the results to relevant situations.': 1.0986056327819824, 'SUMMARY': 1.0985817909240723, 'This paper studies the expressive power of deep neural networks under various related measures of expressivity.': 1.0983539819717407, ""It discusses how these measures relate to the `trajectory length', which is shown to depend exponentially on the depth of the network, in expectation (at least experimentally, at an intuitive level, or theoretically under certain assumptions)."": 0.4361492991447449, 'The paper also emphasises the importance of the weights in the earlier layers of the network, as these have a larger influence on the represented classes of functions, and demonstrates this in an experimental setting.': 0.6523076295852661, 'PROS': 1.0986121892929077, 'The paper further advances on topics related to the expressive power of feedforward neural networks with piecewise linear activation functions, in particular elaborating on the relations between various points of view.': 0.3857845067977905, 'CONS': 1.0985257625579834, 'The paper further advances and elaborates on interesting topics, but to my appraisal it does not contribute significantly new aspects to the discussion.': 0.8336090445518494, 'COMMENTS': 1.098610520362854, 'The paper is a bit long (especially the appendix) and seems to have been written a bit in a rush.': 1.0914133787155151, 'Overall the main points are presented clearly, but the results and conclusions could be clearer about the assumptions / experimental vs theoretical nature.': 0.8237890601158142, 'The connection to previous works could also be clearer.': 0.9590481519699097, 'On page 2 one finds the statement ``Furthermore, architectures are often compared via ‘hardcoded’ weight values': 0.9829860329627991, ""a specific function that can be represented efficiently by one architecture is shown to only be inefficiently approximated by another.''"": 1.051884412765503, 'This is partially true, but it neglects important parts of the discussion conducted in the cited papers.': 0.9674679636955261, 'In particular, the paper [Montufar, Pascanu, Cho, Bengio 2014] discusses not one hard coded function, but classes of functions with a given number of linear regions.': 1.0327969789505005, 'That paper shows that deep networks generically* produce functions with at least a given number of linear regions, while shallow networks never do.': 1.098507046699524, '* Generically meaning that, after fixing the number of parameters, any function represented by the network, for parameter values form an open, positive -measure, neighbourhood, belongs to the class of functions which have at least a certain number of linear regions.': 0.5033544301986694, 'In particular, such statements can be directly interpreted in terms of networks with random weights.': 0.9972701072692871, 'One of the measures for expressivity discussed in the present paper is the number of Dichotomies.': 0.43258368968963623, 'In statistical learning theory, this notion is used to define the VC-dimension.': 0.8716329336166382, 'In that context, a high value is associated with a high statistical complexity, meaning that picking a good hypothesis requires more data.': 0.8625394701957703, ""On page 2 one finds the statement ``We discover and prove the underlying reason for this – all three measures are directly proportional to a fourth quantity, trajectory length.''"": 0.7389050722122192, 'The expected trajectory length increasing exponentially with depth can be interpreted as the increase (or decrease) in the scale by a composition of the form a*...*a': 0.5055641531944275, 'x, which scales the inputs by a^d.': 1.097744107246399, 'Such a scaling by itself certainly is not an underlying cause for an increase in the number of dichotomies or activation patterns or transitions.': 1.0866551399230957, 'Here it seems that at least the assumptions on the considered types of trajectories also play an important role.': 0.6205115914344788, 'This is probably related to another observation from page 4: ``if the variance of the bias is comparatively too large...': 1.0488938093185425, ""then we no longer see exponential growth.''"": 1.0986123085021973, 'OTHER SPECIFIC COMMENTS': 1.0986123085021973, 'In Theorem 1': 1.0911237001419067, ""Here it would be good to be more specific about ``random neural network'', i.e., fixed connectivity structure with random weights, and also about the kind of one-dimensional trajectory, i.e., finite in length, closed, differentiable almost everywhere, etc."": 0.8958418369293213, ""The notation ``g \\geq O(f)'' used in the theorem reads literally as |g| \\geq \\leq k |f| for some k>0, for large enough arguments."": 1.0582762956619263, 'It could also be read as g being not smaller than some function that is bounded above by f, which holds for instance whenever g\\geq 0.': 1.091313362121582, 'For expressing asymptotic lower bounds one can use the notation \\Omega (see https://en.wikipedia.org/wiki/Big_O_notation).': 0.5173371434211731, 'It would be helpful to mention that the expectation is being taken with respect to the network weights and that these are normally distributed with variance \\sigma.': 0.6128794550895691, 'Theorem 2.': 1.0986123085021973, 'Here it would be good to be more specific about the kind of sign transitions.': 0.8229036331176758, 'Is this about transitions at any units of the network, or about sign transitions at the scalar output of the entire network.': 0.5480023622512817, 'Theorem 3 is quite trivial.': 0.41272127628326416, 'The bijection between transitions and activation patterns is not clear.': 0.7466967105865479, 'Take a regular n-gon in the plane and a circle that crosses each edge twice.': 1.0717746019363403, 'This makes 2n transitions but only n+1 activation patterns.': 1.0832545757293701, 'Theorem 4.': 1.0986123085021973, 'Where is the proof of this statement?': 0.6286286115646362, ""How does this relate to the simple fact that each activation pattern corresponds to the vector indicating the units that are `active'?"": 1.0986123085021973, 'MINOR COMMENTS': 1.0986123085021973, ""The names of the theorems (e.g. ``Bound on ...'' in Theorem 1) could be separated more clearly from the statements, for instance using bold font, a dot, or parentheses."": 1.0986123085021973, ""On page 4, in Latex one can use \\gg for the `much larger' symbol."": 1.0986123085021973, 'On page 4, explain the notation \\delta z_\\orth.': 1.0986123085021973, ""On page 4, explain that ``latent image'' refers to the image in the last layer."": 1.0986123085021973, 'Why are there no error bars in Figure 2?': 1.0986123085021973, 'On page 5 explain that the hyperplane is in the last hidden layer.': 1.0986123085021973, ""On page 5, ``is transitioning for any input''."": 1.0986123085021973, 'This is not clearly stated, since a transition takes place at a point in a trajectory of inputs, not for a single input.': 1.0986123085021973, 'The y-axis labels in Figure 1 (c) and (d) are too small.': 1.0986123085021973, 'Why are there no error bars in Figure 1 (a) and (b)?': 1.0986123085021973, 'The caption could at least mention that shown are the averages over experiments.': 1.0986123085021973, 'In Figure 4 (b) the curves are occluded by the labels.': 1.0986123085021973, 'The numbering of results is confusing.': 1.0986123085021973, 'In the Appendix some numbers are repeated with the main part and some are missing.': 1.0986123085021973, 'On page 19.': 1.0986123085021973, 'Theorem 6.': 1.0986123085021973, 'As far as I remember Stanley also provides an elementary proof of case with hyperplanes in general position.': 1.0986123085021973, 'Many other works also provide elementary proofs using the same induction arguments in what is known as the sweep hyperplane method.': 1.0986123085021973, 'Summary of the paper:': 1.0986123085021973, 'Authors study in this paper quantities related to the expressivity of neural networks.': 1.0986123085021973, 'The analysis is done for a random network.': 1.0986123085021973, 'authors define the ‘trajectory length’ of a one dimensional trajectory as the length of the trajectory as the points (in a m- dimensional space) are embedded by layers of the network.': 1.0986123085021973, 'They provide growth factors as function of hidden units k, and number of layers d.  the growth factor is exponential in the number of layers.': 1.0986123085021973, 'Authors relates this trajectory length to authors quantities : ‘transitions’,’activation patterns ’ and ‘Dichotomies’.': 1.0986123085021973, 'As a consequence of this study authors suggest that training only  earlier layers in the network  leads higher accuracy then just training later layers.': 1.0986123085021973, 'Experiments are presented on MNIST and CIFAR10.': 1.0986123085021973, 'Clarity:': 1.0986123085021973, 'The  paper is a little hard to follow, since  the motivations are not clear in the introduction and the definitions across the paper are not clear.': 1.0986123085021973, 'Novelty:': 1.0986123085021973, 'Studying the trajectory length as function of transforming the data by a multilayer network is   new and interesting idea.': 1.0986123085021973, 'The relation to transition numbers is in term of the growth factor, and not as a quantity to quantity relationship.': 1.0847556591033936, 'Hence it is hard to understand what are the implications.': 1.0982409715652466, 'Significance:': 0.5141568183898926, 'The geometry of the input set (of dimension m)  shows up only weakly in the activation patterns analysis.': 1.0986120700836182, 'The trajectory study should tell us how the network organizes the input set.': 1.098611831665039, 'As observed in the experiments the network becomes contractive/selective as we train the network.': 1.0983552932739258, 'It would be interesting to study those phenomenas using this trajectory length , as a measure for disentangling nuisance factors ( such as invariances etc.).': 1.0986051559448242, 'In the supervised setting the network need not to be contractive every where , so it needs to be selective to the class label, a  theoretical study of the selectivity and contraction using the trajectory length would be more appealing.': 1.0975918769836426, 'Detailed comments:': 1.0985171794891357, 'Theorem 1:': 1.098610758781433, 'As raised by reviewer one the definition of a one dimensional input trajectory is missing.': 1.0730632543563843, 'What does theorem 1 tells us about the design and the architecture to use in neural networks as promised in the introduction is not clear.': 1.0873234272003174, 'The connection to transitions in Theorem 2 is rather weak.': 1.0670915842056274, 'Theorem 2:': 1.098561406135559, 'in the proof of theorem 2 it not clear what is meant by T and t. Notations are confusing, the expectation is taken with respect to which weight: is it W_{d+1} or (W_{d+1} and W_{d})?': 0.5705779194831848, ""I understand you don't want to overload notation but maybe E_{d+1} can help keeping track."": 1.0986123085021973, ""I don't see how the recursion is applied if T and t in it, have different definitions."": 1.0986123085021973, 'seems T_{d+1} for you is a random variable and t_{d} is fixed.': 1.0986123085021973, 'Are you fixing W_d': 1.0986123085021973, 'and then looking at W_{d+1} as  random?': 1.0986123085021973, 'In the same proof:  the recursion  is for d>1  ?': 1.0986123085021973, ""your analysis is for W \\in R^{k\\times k}, you don't not study the W \\in \\mathbb{R}^{k\\times m}."": 1.0986123085021973, 'In this case you can not assume assume that |z^(0)|=1.': 1.0986123085021973, 'should d=1, be analyzed alone to know how it scales with m?': 1.0986123085021973, 'Theorem 4 in main text:': 1.0986123085021973, 'Is the proof missing?': 1.0986123085021973, 'or Theorem 4 in the main text is Theorem 6 in the appendix?': 1.0986123085021973, 'Figures 8 and 9:': 1.0986123085021973, ""the trajectory length reduction in the training isn't that just the network becoming contractive to enable mapping the training points to the labels?"": 1.0986123085021973, 'See for instance  on contraction in deep networks https://arxiv.org/pdf/1601.04920.pdf': 1.0986123085021973, 'How much the plot depends on the shape of the trajectory?': 1.0986123085021973, 'have you tried other then circular trajectory?': 1.0986123085021973, 'In these plots the 2 mnist points had same label ? or different label?': 1.0986123085021973, 'both cases should be studied, to see the tradeoff between contraction and selectivity to the class label.': 1.0986123085021973}"
17,https://openreview.net/forum?id=B1YfAfcgl,"{'Overview:': 1.3862943649291992, 'This paper introduces a biasing term for SGD that, in theoretical results and a toy example, yields solutions with an approximately equal or lower generalization error.': 1.3862943649291992, 'This comes at a computational cost of estimating the gradient of the biasing term for each iteration through stochastic gradient Langevin dynamics, approximating an MCMC sample of the log partition function of a modified Gibbs distribution.': 1.3862943649291992, 'The cost is equivalent to adding an inner for-loop to the standard SGD algorithm for each minibatch.': 1.3862943649291992, 'Pros:': 1.3862943649291992, 'Reviews and distills many results and theorems from past 2 decades that suggest a promising way forward for increasing the generalizability of deep neural networks': 1.3862943649291992, 'Generally very well written and well presented results, with interesting discussion of eigenvalues of Hessian as a way to characterize “flat” minima': 1.3862943649291992, 'Promising mathematical arguments suggest that E-SGD has generalization error bounded below by SGD, motivating further research in the area': 1.3862943649291992, 'Cons / points suggested for a rebuttal:': 1.3862943649291992, '(1) One claim of the paper given in the abstract is ”experiments on competitive baselines demonstrate that Entropy-SGD leads to improved generalization and has the potential to accelerate training.“': 1.3862943649291992, 'This does not appear to be supported by the current set of experiments.': 1.3862943649291992, 'As the authors comment in the discussion section, “In our experiments, Entropy-SGD results in a comparable generalization error as SGD, but always has a lower cross-entropy loss.”': 1.3862943649291992, ""It's not clear to me how to reconcile those two claims."": 1.3862943649291992, '(2) Similarly, the claim of accelerated training is not convincingly supported in the present version of the paper.': 1.3862943649291992, 'Vanilla SGD requires a single forward pass through all M minibatches during one epoch for a parameter update, but the new method, E-SGD requires, L*M forward passes during one epoch where L is the number of Langevin updates, which require a minibatch sample each.': 1.3862943649291992, 'This could in fact mean that E-SGD has worse computational complexity to reach the same point.': 1.3862943649291992, 'In a remark on p.9, the authors note that a single epoch is defined to be “the number of parameter updates required to run through the dataset once.”': 1.3862943649291992, 'It’s not clear to me how this answers the objection to a factor of L additional computations required for the inner-loop SGLD iterations.': 1.3862943649291992, 'SGLD appears to introduces a potentially costly tradeoff that must be carefully managed by a user of E-SGD.': 1.3862055540084839, '(3) As the previous two points suggest, the paper could use some attention to the magnitude of the claims.': 1.3862943649291992, 'For example, the introduction reads “Actively biasing towards wide valleys aids generalization, in fact, we can optimize solely the free energy term to obtain similar generalization error as SGD on the original loss function.“': 1.3862943649291992, 'According the the values reported on pp.9-10, only on MNIST is the generalization error, using only the free energy term (the log partition function of the modified Gibbs distribution), equivalent to using only the SGD loss function.': 1.3862943649291992, 'This corresponds to setting rho to 0 in equation (6).': 1.3862943649291992, 'On CIFAR-10, rho = 0.01 is used.': 1.3862943649291992, '(4) Another contribution of this paper, the characterization of the optimization landscape in terms of the eigenvalues of the Hessian and low generalization error being associated with flat local extrema, is helpful and interesting.': 1.3862943649291992, 'I found the plots clear and useful.': 1.3862943649291992, 'As another reviewer has already pointed out, there are high-level similarities to “Flat Minima” by Hochreiter and Schmidhuber (1997).': 1.3862943649291992, 'The authors have responded already by adding a paragraph that helpfully explores some differences with H&S 1997.': 1.3862943649291992, 'However, the similarities should also be carefully identified and mentioned.': 1.3862943649291992, 'H&S 1997 includes detailed theoretical analysis that could be helpful for future work in this area, and has independently discovered a similar approach to training generalizable networks.': 1.3862943649291992, ""(5) It's not clear how the assumption about the eigenvalues that were made in section 4.4 / Appendix B affect the application of this result to real-world problems."": 1.3862943649291992, 'What magnitude of c>0 needs to be chosen?': 1.3862943649291992, 'Does this correspond to a measurable characteristic of the dataset?': 1.370397686958313, ""It's a little mysterious in the current version of the paper."": 1.2907742261886597, '__': 1.3862943649291992, 'Note__:': 1.3861415386199951, 'An earlier version of the review (almost identical to the present one) for an earlier version of the paper (available on arXiV) can be found here: http://www.shortscience.org/paper?bibtexKey=journals/corr/1611.01838#csaba': 1.3862906694412231, 'The only change concerns relation to previous work.': 1.3813358545303345, 'Problem__:': 1.3862943649291992, 'The problem considered is to derive an improved version of SGD for training neural networks (or minimize empirical loss) by modifying the loss optimized to that the solution found is more likely to end up in the vicinity of a minimum where the loss changes slowly (""flat minima"" as in the paper of Hochreiter and Schmidhuber from 1997).': 1.386231780052185, 'Motivation__:': 1.3862943649291992, 'It is hypothetised that flat minima ""generalize"" better.': 0.9116505980491638, 'Algorithmic approach__: Let  be the (empirical) loss to be minimized.': 1.3862943649291992, 'Modify this to': 1.3862943649291992, 'with some  tunable parameters.': 1.3862943649291992, 'For': 1.3862943649291992, ', the term': 1.345752477645874, 'becomes very small, so effectively the second term is close to a constant times the integral of  over a ball centered at  and having a radius of .': 1.3687013387680054, 'This is a smoothened version of , hence one expects that by making this term more important then the first term, a procedure minimizing': 1.3862884044647217, 'will be more likely to end up at a flat minima of .': 1.3862941265106201, 'Since the gradient is somewhat complicated, an MCMC algorithm is proposed (""stochastic gradient Langevin dynamics"" from Welling and Teh, 2011).': 1.382089614868164, '__Results__:': 1.3862943649291992, 'There is a theoretical result that quantifies the increased smoothness of': 1.2699856758117676, ', which is connected to stability and ultimately to generalization through citing a result of Hardt et al. (2015).': 1.1267753839492798, 'Empirical results show better validation error on two datasets: MNIST and CIFAR-10 (the respective networks are LeNet and All-CNN-C).': 1.3812072277069092, 'The improvement is in terms of reaching the same validation error as with an ""original SGD"" but with fewer ""epochs"".': 1.0260392427444458, 'Soundness, significance__: The proof of the __theoretical result__ relies on an arbitrary assumption that there exists some  such that no eigenvalue of the hessian of  lies in the set  (the reason for the assumption is because otherwise a uniform improvement cannot be shown).': 0.898235023021698, 'For  the improvement of the smoothness (first and second order) is a factor of .': 1.1349215507507324, ""The proof uses Laplace's method and is more a sketch than a rigorous proof (error terms are dropped; it would be good to make this clear in the statement of the result)."": 1.3536741733551025, 'In the experiments the modified procedure did not consistently reach a smaller validation error.': 1.1290391683578491, ""The authors did not present running times, hence it is unclear whether the procedure's increased computation cost is offset by the faster convergence."": 1.3862943649291992, '__Evaluation__:': 1.3862943649291992, 'It is puzzling why a simpler smoothing, e.g.,': 1.3862943649291992, '(with  being the density of a centered probability distribution) is not considered.': 1.3862943649291992, 'The authors note that something ""like this"" may be infeasible in ""deep neural networks"" (the note is somewhat vague).': 1.370884895324707, 'However, under mild conditions,': 1.3862943649291992, ', hence, for ,': 0.9596541523933411, 'is an unbiased estimate of': 1.3862943649291992, ', whose calculation is as cheap as that of vanilla SGD.': 1.3861382007598877, 'Also, how much the smoothness of  changes when using this approach is quite well understood.': 1.3303353786468506, '__Related work__:': 1.3862937688827515, 'It is also strange that the specific modification that appears in this paper was proposed by others (Baldassi et al.), whom this paper also cites, but without giving these authors credit for introducing local entropy as a smoothing technique.': 1.3581808805465698, 'The paper introduces a new regularization term which encourages the optimizer': 1.3862882852554321, 'to search for a flat local minimum of reasonably low loss instead of seeking a': 1.3862942457199097, 'sharp region of a low loss.': 1.3862943649291992, 'This is motivated by some empirical observations that': 1.3862943649291992, 'local minima of good generalization performance tend to have flat shape.': 1.3862943649291992, 'To achieve this, a regularization term based on the free local energy is proposed': 1.3862943649291992, 'and the gradient of this term, which do not have tractable closed-form solution,': 0.7357219457626343, 'is obtained by performing Monte Carlo estimation using SGLD sampler.': 1.3747929334640503, 'In the': 1.352723479270935, 'experiments, the authors show some evidence of the flatness of good local': 0.8492650985717773, 'minima, and also the performance of the proposed method in comparison to the': 1.2431858777999878, 'Adam optimizer.': 1.2235033512115479, 'The paper is well and clearly written.': 1.3843473196029663, 'I enjoyed reading the paper.': 1.3862943649291992, 'The connection': 1.3470858335494995, 'to the concept of free energy in optimization framework seems interesting.': 1.3323047161102295, 'The': 1.3862943649291992, 'motivation of pursuing flatness is also well analyzed with a few experiments.': 1.3862924575805664, ""I'm"": 1.3854328393936157, 'wondering if the first term in eqn.': 1.3862941265106201, '(8) is correct.': 1.386264681816101, ""I guess it should be f(x') not f(x)?"": 1.177791714668274, ""Also, I'm wondering why the authors did not add the experiment results on RNN in"": 1.370054006576538, 'the evaluation of the performance because char-lstm for text generation was': 1.3862355947494507, 'already used for the flatness experiments.': 0.744472861289978, 'I think adding more experiments on': 1.3679120540618896, 'various models and applications of deep architectures (e.g., RNN, seq2seq, etc.)': 1.3827133178710938, ""will make the author's claim more persuasive."": 1.3862364292144775, 'I also found the mixed usage of the': 1.2827438116073608, 'terminology, e.g., free energy and free entropy, a bit confusing.': 0.9967072606086731, 'This paper presents a principled approach to finding flat minima.': 1.3862940073013306, 'The motivation to seek such minima is due to their better generalization ability.': 1.3862943649291992, 'The idea is to add to the original loss function a new term that exploits both width and depth of the objective function.': 1.3862301111221313, 'In fact, the regularization term can be interpreted as Gaussian convolution of the exponentiated loss.': 1.3862941265106201, 'Therefore, the introduced regularization term is essentially Gaussian smoothed version of the exponentiated loss.': 1.3862943649291992, 'The smoothing obviously tends to suppress sharp minima.': 1.3862943649291992, 'Overall, developing such regularization term based on thermodynamics concepts is very interesting.': 1.3862943649291992, 'I have a couple of concerns that the authors may want to clarify in the rebuttal.': 1.3862943649291992, '1. When reporting the generalization performance, the experiments report the number of epochs; showing the proposed algorithm reaches better generalization in fewer epochs than plain SGD. Is this the number of epochs it takes by line 7 of your algorithm, or it is the total number of epochs (line 3 and 7 all combined)? If the former, it is not a fair comparison. If you multiply the number of epochs of SGD (line 7) by the number iterations it takes to approximate Langevin dynamics, it seems you obtain little gain against plain SGD.': 1.3862943649291992, '2. The proposed algorithm approximates the smoothed ""exponentiated"" loss (by smoothing I refer to convolution with the Gaussian). I am wondering how it compares against simpler idea of smoothing the original loss (dropping exponentiation)? Is the difference only in the motivation (e.g. thermodynamics interpretation) or it is deeper, e.g. the proposed scheme lends itself to more accurate approximation and/or achieves better generalization bound (in terms of the attained smoothness)? Smoothing the cost function without exponentiation allows simpler approximation (Monte Carlo integration instead of MCMC), e.g. see section 5.3 of https://arxiv.org/pdf/1601.04114': 1.2834986448287964, '3. Section 4.4. Thank you for revising the statements related to the eigenvalues of the Hessian. However, even in the revised version, there seems to be some discrepancy. You ""assume no eigenvalue of the Hessian lies in the set [−2γ −c, c] for some small c > 0"". This essentially says the eigenvalues are far from zero. Such assumption seems to be in the opposite direction of the reality: the plots of eigenvalues (Figure 1) show most eigenvalues are indeed close to zero.': 1.2557698488235474, ""4. Theorem 3 from Hardt 2015: The way you quote it differs from the original paper. Are you referring to the Theorem 3.12 of Hardt's paper? If so, why the difference, including elimination of dependency on constant c in the exponent of T?"": 1.0840880870819092}"
18,https://openreview.net/forum?id=B1ZXuTolx,"{'The paper proposes a modified DAE objective where it is the mapped representation of the corrupted input that is pushed closer to the representation of the uncorrupted input.': 1.0985313653945923, 'This thus borrows from both denoising (DAE) for the stochasticity and from the contractive (CAE) auto-encoders objectives (which the paper doesn’t compare to) for the representational closeness, and as such appears rather incremental.': 1.0986099243164062, 'In common with the CAE, a collapse of the representation can only be avoided by additional external constraints, such as tied weights, batch normalization or other normalization heuristics.': 1.0986064672470093, 'While I appreciates that the authors added a paragraph discussing this point and the usual remediations after I had raised it in an earlier question, I think it would deserve a proper formal treatment.': 1.0986121892929077, 'Note that such external constraints do not seem to arise from the information-theoretic formalism as articulated by the authors.': 1.0904067754745483, 'This casts doubt regarding the validity or completeness of the proposed formal motivation as currently exposed.': 0.45335879921913147, 'What the extra regularization does from an information-theoretic perspective remains unclearly articulated (e.g. interpretation of lambda strength?).': 1.0986123085021973, 'On the experimental front, empirical support for the approach is very weak: few experiments on synthetic and small scale data.': 1.0986047983169556, ""The modified DAE's test errors on MNIST are larger than those of Original DAE all the time expect for one precise setting of lambda, and then the original DAE performance is still within the displayed error-bar of the modified DAE."": 1.0986063480377197, 'So, it is unclear whether the improvement is actually statistically significant.': 1.0960731506347656, 'The work introduced a new form of regularization for denoising autoencoders, which explicitly enforces robustness in the encoding phrase w.r.t.': 1.0986123085021973, 'input perturbation.': 1.0611772537231445, 'The author motivates the regularization term as minimizing the conditional entropy of the encoding given the input.': 0.42274659872055054, 'The modifier denoising autoencoders is evaluated on some synthetic datasets as well as MNIST, along with regular auto-encoders and denoising autoencoders.': 1.098609447479248, 'The work is fairly similar to several existing extensions to auto-encoders, e.g., contractive auto encoders, which the author did not include in the comparison.': 1.0986123085021973, 'The experiment section needs more polishing.': 1.0986123085021973, 'More details should be provided to help understand the figures in the section.': 0.8849135637283325, 'The paper proposes to add an additional term to the denoising-autoencoder objective.': 1.0986123085021973, 'The new term is well motivated, it introduces an asymmetry between the encoder and decoder, forcing the encoder to represent a compressed, denoised version of the input.': 1.0986123085021973, 'The authors propose to avoid the trivial solution introduced by the new term by using tied weights or normalized Euclidean distance error (the trivial solution occurs by scaling the magnitude of the code down in the encoder, and back up in the decoder).': 1.0986123085021973, 'The proposed auto-encoder scheme is very similar to a host of other auto-encoders that have been out in the literature for some time.': 1.0986123085021973, 'The authors evaluate the proposed scheme on toy-data distributions in 2D as well as MNIST.': 1.0986123085021973, 'Although the work is well motivated, it certainly seems like an empirically unproven and incremental improvement to an old idea.': 1.0986123085021973}"
19,https://openreview.net/forum?id=B1akgy9xx,"{'Strengths': 0.6863767504692078, 'interesting to explore the connection between ReLU DNN and simplified SFNN': 0.6208838224411011, 'small task (MNIST)  is used to demonstrate the usefulness of the proposed training methods experimentally': 0.693144679069519, 'the proposed, multi-stage training methods are simple to implement (despite lacking theoretical rigor)': 0.6927328705787659, 'Weaknesses': 0.6931383013725281, 'no results are reported on real tasks with large training set': 0.691399872303009, 'not clear exploration on the scalability of the learning methods when training data becomes larger': 0.6930798888206482, 'when the hidden layers become stochastic, the model shares uncertainty representation with deep Bayes networks or deep generative models (Deep Discriminative and Generative Models for Pattern Recognition , book chapter in “Pattern Recognition and Computer Vision”, November 2015, Download PDF).': 0.5827518701553345, 'Such connections should be discussed, especially wrt the use of uncertainty representation to benefit pattern recognition (i.e. supervised learning via Bayes rule) and to benefit the use of domain knowledge such as “explaining away”.': 0.41654321551322937, 'would like to see connections with variational autoencoder models and training, which is also stochastic with hidden layers': 0.685650110244751, 'This paper builds connections between DNN, simplified stochastic neural network (SFNN) and SFNN and proposes to use DNN as the initialization model for simplified SFNN.': 0.6918127536773682, 'The authors evaluated their model on several small tasks with positive results.': 0.6930369734764099, 'The connection between different models is interesting.': 0.6929503083229065, 'I think the connection between sigmoid DNN and Simplified SFNN is the same as mean-field approximation that has been known for decades.': 0.6556357145309448, 'However, the connection between ReLU DNN and simplified SFNN is novel.': 0.6917812824249268, 'My main concern is whether the proposed approach is useful when attacking real tasks with large training set.': 0.6931471824645996, 'For tasks with small training set I can see that stochastic units would help generalize well.': 0.6931471824645996}"
20,https://openreview.net/forum?id=B1ckMDqlg,"{'This paper proposes a method for significantly increasing the number of parameters in a single layer while keeping computation in par with (or even less than) current SOTA models.': 1.091195821762085, 'The idea is based on using a large mixture of experts (MoE) (i.e. small networks), where only a few of them are adaptively activated via a gating network.': 1.0932655334472656, 'While the idea seems intuitive, the main novelty in the paper is in designing the gating network which is encouraged to achieve two objectives: utilizing all available experts (aka importance), and distributing computation fairly across them (aka load).': 1.0986119508743286, 'Additionally, the paper introduces two techniques for increasing the batch-size passed to each expert, and hence maximizing parallelization in GPUs.': 0.42033660411834717, 'Experiments applying the proposed approach on RNNs in language modelling task show that it can beat SOTA results with significantly less computation, which is a result of selectively using much more parameters.': 1.0818238258361816, 'Results on machine translation show that a model with more than 30x number of parameters can beat SOTA while incurring half of the effective computation.': 0.7514470815658569, 'I have the several comments on the paper:': 1.0986123085021973, 'I believe that the authors can do a better job in their presentation.': 1.0986123085021973, 'The paper currently is at 11 pages (which is too long in my opinion), but I find that Section 3.2 (the crux of the paper) needs better motivation and intuitive explanation.': 1.068328619003296, 'For example, equation 8 deserves more description than currently devoted to it.': 1.0986123085021973, 'Additional space can be easily regained by moving details in the experiments section (e.g. architecture and training details) to the appendix for the curious readers.': 1.0984071493148804, 'Experiment section can be better organized by finishing on experiment completely before moving to the other one.': 1.098611831665039, 'There are also some glitches in the writing, e.g. the end of Section 3.1.': 1.0986120700836182, 'The paper is missing some important references in conditional computation (e.g. https://arxiv.org/pdf/1308.3432.pdf) which deal with very similar issues in deep learning.': 0.4068734347820282, 'One very important lesson from the conditional computation literature is that while we can in theory incur much less computation, in practice (especially with the current GPU architectures)': 1.093589186668396, 'the actual time does not match the theory.': 1.0986123085021973, 'This can be due to inefficient branching in GPUs.': 1.0986123085021973, 'It would be nice if the paper includes a discussion of how their model (and perhaps implementation) deal with this problem, and why it scales well in practice.': 1.0935214757919312, 'Table 1 and Table 3 contain repetitive information, and I think they should be combined in one (maybe moving Table 3 to appendix).': 1.0985511541366577, 'One thing I do not understand is how does the number of ops/timestep relate to the training time.': 1.0986123085021973, 'This also related to the pervious comment.': 1.0986123085021973, 'Paper Strengths:': 1.0986123085021973, 'Elegant use of MoE for expanding model capacity and enabling training large models necessary for exploiting  very large datasets in a computationally feasible manner': 1.0986123085021973, 'The effective batch size for training the MoE drastically increased also': 1.0986123085021973, 'Interesting experimental results on the effects of increasing the number of MoEs, which is expected.': 1.0986123085021973, 'Paper Weaknesses:': 1.0858943462371826, 'there are many different ways of increasing model capacity to enable the exploitation of very large datasets; it would be very nice to discuss  the use of MoE and other alternatives in terms of computational efficiency and other factors.': 0.7849066257476807, 'This paper describes a method for greatly expanding network model size (in terms of number of stored parameters) in the context of a recurrent net, by applying a Mixture of Experts between recurrent net layers that is shared between all time steps.': 1.0986123085021973, 'By process features from all timesteps at the same time, the effective batch size to the MoE is increased by a factor of the number of steps in the model; thus even for sparsely assigned experts, each expert can be used on a large enough sub-batch of inputs to remain computationally efficient.': 1.0986123085021973, 'Another second technique that redistributes elements within a distributed model is also described, further increasing per-expert batch sizes.': 1.0986123085021973, 'Experiments are performed on language modeling and machine translation tasks, showing significant gains by increasing the number of experts, compared to both SoA as well as explicitly computationally-matched baseline systems.': 1.0986123085021973, 'An area that falls a bit short is in presenting plots or statistics on the real computational load and system behavior.': 1.0986123085021973, 'While two loss terms were employed to balance the use of experts, these are not explored in the experiments section.': 1.0799052715301514, 'It would have been nice to see the effects of these more, along with the effects of increasing effective batch sizes, e.g. measurements of the losses over the course of training, compared to the counts/histogram distributions of per-expert batch sizes.': 0.5650136470794678, 'Overall I think this is a well-described system that achieves good results, using a nifty placement for the MoE that can overcome what otherwise might be a disadvantage for sparse computation.': 1.096574306488037, 'Small comment:': 1.0986123085021973, ""I like Fig 3, but it's not entirely clear whether datapoints coincide between left and right plots."": 1.0986123085021973, 'The H-H line has 3 points on left but 5 on the right?': 1.0986123085021973, 'Also would be nice if the colors matched between corresponding lines.': 1.0986123085021973}"
21,https://openreview.net/forum?id=B1ewdt9xe,"{'An interesting architecture that accumulates and continuously corrects mistakes as you see more and more of a video sequence.': 0.9244377017021179, 'Clarity: The video you generated seems very helpful towards understanding the information flow in your network, it would be nice to link to it from the paper.': 1.0986123085021973, '""Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).""': 1.0861928462982178, 'It is not clear how different are the train and test sequences at the moment, since standard benchmarks do not really exist for video prediction and each author picks his/her favorite.': 1.0933293104171753, 'Underperforming Finn et al 206 at the H3.6m Walking videos is a bit disappointing.': 1.098602294921875, 'Learning about the physical structure and semantics of the world from video (without supervision) is a very hot area in computer vision and machine learning.': 1.0986123085021973, ""In this paper, the authors investigate how the prediction of future image frames (inherently unsupervised) can help to deduce object/s structure and it's properties (in this case single object pose, category, and steering angle, (after a supervised linear readout step))"": 1.0986123085021973, 'I enjoyed reading this paper, it is clear, interesting and proposes an original network architecture (PredNet) for video frame prediction that has produced promising results on both synthetic and natural images.': 1.0986123085021973, 'Moreover, the extensive experimental evaluation and analysis the authors provide puts it on solid ground to which others can compare.': 1.0986123085021973, 'The weaknesses:': 1.0861780643463135, 'the link to predictive coding should be better explained in the paper if it is to be used as a motivation for the prednet model.': 0.967987060546875, ""any idea that the proposed method is learning an implicit `model' of the `objects' that make up the `scene' is vague and far fetched, but it sounds great."": 0.9499620199203491, 'Minor comment:': 1.0986123085021973, 'Next to the number of labeled training examples (Fig.5), it would be interesting to see how much unsupervised training data was used to train your representations.': 1.0986123085021973, 'Paper Summary': 0.9080967307090759, 'This paper proposes an unsupervised learning model in which the network': 0.5217305421829224, 'predicts what its state would look like at the next time step (at input layer': 0.9406294822692871, 'and potentially other layers).': 1.0971978902816772, 'When these states are observed, an error signal': 0.4457511901855469, 'is computed by comparing the predictions and the observations.': 1.0985654592514038, 'This error': 1.0975335836410522, 'signal is fed back into the model.': 1.0986123085021973, 'The authors show that this model is able to': 1.0973554849624634, 'make good predictions on a toy dataset of rotating 3D faces as well as on': 1.0985841751098633, 'natural videos.': 1.0986123085021973, 'They also show that these features help perform supervised': 1.0986123085021973, 'tasks.': 1.0986123085021973, 'Strengths': 1.0986123085021973, 'The model is an interesting embodiment of the idea of predictive coding': 1.0538204908370972, 'implemented using a end-to-end backpropable recurrent neural network architecture.': 1.0615031719207764, 'The idea of feeding forward an error signal is perhaps not used as widely as it could': 1.0986123085021973, 'be, and this work shows a compelling example of using it.': 1.0422853231430054, 'Strong empirical results and relevant comparisons show that the model works well.': 1.0835241079330444, 'The authors present a detailed ablative analysis of the proposed model.': 1.0986087322235107, 'Weaknesses': 1.0986123085021973, 'The model (esp.': 1.0444865226745605, 'in Fig 1) is presented as a generalized predictive model': 1.0429000854492188, 'where next step predictions are made at each layer.': 0.6327322721481323, 'However, as discovered by': 1.0316038131713867, 'running the experiments, only the predictions at the input layer are the ones': 1.096429705619812, 'that actually matter and the optimal choice seems to be to turn off the error': 1.0971496105194092, 'signal from the higher layers.': 0.41677844524383545, 'While the authors intend to address this in future': 0.002577841281890869, 'work, I think this point merits some more discussion in the current work, given': 1.0960462093353271, 'the way this model is presented.': 0.4678308367729187, 'The network currently lacks stochasticity and does not model the future as a': 0.9295412302017212, 'multimodal distribution (However, this is mentioned as potential future work).': 1.0977652072906494, 'Quality': 1.0986123085021973, 'The experiments are well-designed and a detailed analysis is provided': 1.0986123085021973, 'in the appendix.': 1.0952316522598267, 'Clarity': 0.581002414226532, 'The paper is well-written and easy to follow.': 0.9102579355239868, 'Originality': 0.40544527769088745, 'Some deep models have previously been proposed that use predictive coding.': 0.864212155342102, 'However, the proposed model is most probably novel in the way it feds back the': 0.013651702553033829, 'error signal and implements the entire model as a single differentiable': 0.27329951524734497, 'network.': 1.097993016242981, 'Significance': 1.0985548496246338, 'This paper will be of wide interest to the growing set of researchers working': 1.0986120700836182, 'in unsupervised learning of time series.': 1.098260760307312, 'This helps draw attention to': 1.0924447774887085, 'predictive coding as an important learning paradigm.': 1.03248131275177, 'Overall': 1.0986123085021973, 'Good paper with detailed and well-designed experiments.': 1.0985313653945923, 'The idea of feeding': 1.0986123085021973, 'forward the error signal is not being used as much as it could be in our': 1.0986123085021973, 'community.': 1.0986123085021973, ""This work helps to draw the community's attention to this idea."": 1.0986123085021973}"
22,https://openreview.net/forum?id=B1gtu5ilg,"{'This paper proposes a model to learn across different views of objects.': 0.8145821690559387, 'The key insight is to use a triplet loss that encourages two different views of the same object to be closer than an image of a different object.': 0.4886227250099182, 'The approach is evaluated on object instance and category retrieval and compared against baseline CNNs (untrained AlexNet and AlexNet fine-tuned for category classification) using fc7 features with cosine distance.': 0.4388653337955475, 'Furthermore, a comparison against human perception on the ""Tenenbaum objects” is shown.': 1.098599910736084, 'Positives: Leveraging a triplet loss for this problem may have some novelty (although it may be somewhat limited given some concurrent work; see below).': 1.0927215814590454, 'The paper is reasonably written.': 1.0986123085021973, 'Negatives: The paper is missing relevant references of related work in this space and should compare against an existing approach.': 1.0986123085021973, 'More details:': 1.0986123085021973, 'The “image purification” paper is very related to this work:': 1.0557377338409424, '[A] Joint Embeddings of Shapes and Images via CNN Image Purification.': 1.0970327854156494, 'Hao Su*, Yangyan Li*, Charles Qi, Noa Fish, Daniel Cohen-Or, Leonidas Guibas.': 0.4627723693847656, 'SIGGRAPH Asia 2015.': 1.0986123085021973, 'There they learn to map CNN features to (hand-designed) light field descriptors of 3D shapes for view-invariant object retrieval.': 1.0986119508743286, 'If possible, it would be good to compare directly against this approach (e.g., the cross-view retrieval experiment in Table 1 of [A]).': 1.0986123085021973, 'It appears that code and data is available online (http://shapenet.github.io/JointEmbedding/).': 1.098258137702942, 'Somewhat related to the proposed method is recent work on multi-view 3D object retrieval:': 1.0982263088226318, '[B] Multi-View 3D Object Retrieval With Deep Embedding Network.': 1.0402511358261108, 'Haiyun Guo, Jinqiao Wang, Yue Gao, Jianqiang Li, and Hanqing Lu.': 0.6872169375419617, 'IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 25, NO. 12, DECEMBER 2016.': 0.8841641545295715, 'There they developed a triplet loss as well, but for multi-view retrieval (given multiple images of the same object).': 1.087062120437622, 'Given the similarity of the developed approach, it somewhat limits the novelty of the proposed approach in my view.': 0.727075457572937, 'Also related are approaches that predict a volumetric representation of an input 2D image (going from image to canonical orientation of 3D shape):': 1.0213615894317627, '[C] R. Girdhar, D. Fouhey, M. Rodriguez, A. Gupta.': 0.3556963801383972, 'Learning a Predictable and Generative Vector Representation for Objects.': 0.6869246363639832, 'ECCV 2016.': 1.0986123085021973, '[D] Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling.': 0.9632534384727478, 'Jiajun Wu*, Chengkai Zhang*, Tianfan Xue, William T. Freeman, and Joshua B. Tenenbaum.': 0.36023423075675964, 'NIPS 2016.': 1.098611831665039, 'For the experiments, I would like to see a comparison using different feature layers (e.g., conv4, conv5, pool4, pool5) and feature comparison (dot product, Eucllidean).': 1.0967941284179688, 'It has been shown that different layers and feature comparisons perform differently for a given task, e.g.,': 1.0359272956848145, '[E] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views.': 0.8732802271842957, 'Francisco Massa, Bryan C. Russell, Mathieu Aubry.': 0.7246912121772766, 'Conference on Computer Vision and Pattern Recognition (CVPR), 2016.': 1.0986123085021973, '[F] Understanding Deep Features with Computer-Generated Imagery.': 1.0986123085021973, 'Mathieu Aubry and Bryan C. Russell.': 1.0986123085021973, 'IEEE International Conference on Computer Vision (ICCV), 2015.': 1.038370132446289, 'I think learning a deep feature representation that is supervised to group dissimilar views of the same object is interesting.': 1.0986123085021973, ""The paper isn't technically especially novel but that doesn't bother me at all."": 1.0985009670257568, 'It does a good job exploring a new form of supervision with a new dataset.': 1.0986123085021973, ""I'm also not bothered that the dataset is synthetic, but it would be good to have more experiments with real data, as well."": 1.0985496044158936, 'I think the paper goes too far in linking itself to human vision.': 1.0986123085021973, 'I would prefer the intro not have as much cognitive science or neuroscience.': 1.0986073017120361, 'The second to fourth paragraphs of the intro in particular feels like it over-stating the contribution of this paper as somehow revealing some truth about human vision.': 1.0986003875732422, 'Really, the narrative is much simpler': 0.9670522212982178, '""we often want deep feature representations that are viewpoint invariant.': 1.0974384546279907, 'We supervise a deep network accordingly.': 1.0985523462295532, 'Humans also have some capability to be viewpoint invariant which has been widely studied [citations]"".': 1.076430320739746, 'I am skeptical of any claimed connections bigger than that.': 0.4230756163597107, 'I think 3.1 should not be based on tree-to-tree distance comparisons but instead based on the entire matrix of instance-to-instance similarity assessments.': 1.0986123085021973, 'Why do the lossy conversion to trees first?': 1.0986123085021973, 'I don\'t think ""Remarkably"" is justified in the statement ""Remarkably, we found that OPnets similarity judgement matches a set of data on human similarity judgement, significantly better than AlexNet""': 1.098610520362854, 'I\'m not an expert on human vision, but from browsing online and from what I\'ve learned before it seems that ""object persistence"" frequently relates to the concept of occlusion.': 1.0986119508743286, 'Occlusion is never mentioned in this paper.': 1.0986088514328003, 'I feel like the use of human vision terms might be misleading or overclaiming.': 1.0986123085021973, 'On one hand this paper is fairly standard in that it uses deep metric learning with a Siamese architecture.': 1.0986123085021973, 'On the other, the connections to human perception involving persistence is quite interesting.': 1.0986123085021973, ""I'm not an expert in human vision, but the comparison in general and the induced hierarchical groupings in particular seem like something that should interest people in this community."": 1.0986123085021973, 'The experimental suite is ok': 1.0986123085021973, 'but I was disappointed that it is 100% synthetic.': 1.0968914031982422, 'The authors could have used a minimally viable real dataset such as ALOI http://aloi.science.uva.nl .': 1.0985602140426636, 'In summary, the mechanics of the proposed approach are not new, but the findings about the transfer of similarity judgement to novel object classes are interesting.': 1.0986121892929077}"
23,https://openreview.net/forum?id=B1hdzd5lg,"{'This paper proposes a new gating mechanism to combine word and character representations.': 1.0984611511230469, 'The proposed model sets a new state-of-the-art on the CBT dataset; the new gating mechanism also improves over scalar gates without linguistic features on SQuAD and a twitter classification task.': 1.0986119508743286, 'Intuitively, the vector-based gate working better than the scalar gate is unsurprising, as it is more similar to LSTM and GRU gates.': 1.0986121892929077, 'The real contribution of the paper for me is that using features such as POS tags and NER help learn better gates.': 1.0986123085021973, 'The visualization in Figure 3 and examples in Table 4 effectively confirm the utility of these features, very nice!': 1.0986123085021973, 'In sum, while the proposed gate is nothing technically groundbreaking, the paper presents a very focused contribution that I think will be useful to the NLP community.': 1.0986123085021973, 'Thus, I hope it is accepted.': 1.0969080924987793, 'I think the problem here is well motivated, the approach is insightful and intuitive, and the results are convincing of the approach (although lacking in variety of applications).': 1.0986121892929077, 'I like the fact that the authors use POS and NER in terms of an intermediate signal for the decision.': 1.0451209545135498, 'Also they compare against a sufficient range of baselines to show the effectiveness of the proposed model.': 1.0986123085021973, ""I am also convinced by the authors' answers to my question, I think there is sufficient evidence provided in the results to show the effectiveness of the inductive bias introduced by the fine-grained gating model."": 1.0986123085021973, 'SUMMARY.': 1.0986123085021973, 'The paper proposes a gating mechanism to combine word embeddings with character-level word representations.': 1.0985053777694702, 'The gating mechanism uses features associated to a word to decided which word representation is the most useful.': 0.4180063009262085, 'The fine-grain gating is applied as part of systems which seek to solve the task of cloze-style reading comprehension question answering, and Twitter hashtag prediction.': 1.0986123085021973, 'For the question answering task, a fine-grained reformulation of gated attention for combining document words and questions is proposed.': 1.0986123085021973, 'In both tasks the fine-grain gating helps to get better accuracy, outperforming state-of-the-art methods on the CBT dataset and performing on-par with state-of-the-art approach on the SQuAD dataset.': 1.0986123085021973, 'OVERALL JUDGMENT': 1.077897548675537, 'This paper proposes a clever fine-grained extension of a scalar gate for combining word representation.': 0.45623257756233215, 'It is clear and well written.': 0.7191102504730225, 'It covers all the necessary prior work and compares the proposed method with previous similar models.': 1.0986121892929077, 'I liked the ablation study that shows quite clearly the impact of individual contributions.': 1.0986123085021973, 'And I also liked the fact that some (shallow) linguistic prior knowledge e.g., pos tags ner tags, frequency etc. has been used in a clever way.': 1.0986123085021973, 'It would be interesting to see if syntactic features can be helpful.': 1.0870598554611206}"
24,https://openreview.net/forum?id=B1jnyXXJx,"{'Summary:': 1.0986123085021973, 'This paper proposes a regularizer that is claimed to help escaping from the saddle points.': 1.0986123085021973, 'The method is inspired from physics, such that thinking of the optimization process is moving a positively charged particle would over the error surface which would be pushed away from saddle points due to the saddle point being positively changed as well.': 1.0986123085021973, 'Authors of the paper show results over several different datasets.': 0.4157671630382538, 'Overview of the Review:': 1.0929651260375977, 'Pros:': 1.0686538219451904, '- The idea is very interesting.': 1.0986123085021973, '- The diverse set of results on different datasets.': 1.0986123085021973, 'Cons:': 1.0986123085021973, '- The justification is not strong enough.': 1.0986123085021973, '- The paper is not well-written.': 1.0986123085021973, '- Experiments are not convincing enough.': 1.09861159324646, 'Criticisms:': 1.0986123085021973, 'I liked the idea and the intuitions coming from the paper.': 0.4377926290035248, 'However, I think this paper is not written well.': 0.4374025762081146, 'There are some variables introduced in the paper and not explained good-enough, for example in 2.3, the authors start to talk about p without introducing and defining it properly.': 1.0986120700836182, 'The only other place it appears before is Equation 6.': 1.0986119508743286, 'The Equations need some work as well, some work is needed in terms of improving the flow of the paper, e.g., introducing all the variables properly before using them.': 1.0986089706420898, 'Equation 6 appears without a proper explanation and justification.': 0.8715915679931641, 'It is necessary to explain it what it means properly since I think this is one of the most important equation in this paper.': 1.0986123085021973, 'More analysis on what it means in terms of optimization point of view would also be appreciated.': 0.582371711730957, 'is not a parameter, it is a function which has its own hyper-parameter .': 1.0986121892929077, 'It would be interesting to report validation or test results on a few tasks as well.': 1.0986123085021973, 'Since this method introduced as an additional cost function, its effect on the validation/test results would be interesting as well.': 1.0986108779907227, 'The authors should discuss more on how they choose the hyper-parameters of their models.': 1.0986123085021973, 'The Figure 2 and 3 does not add too much to the paper and they are very difficult to understand or draw any conclusions from.': 1.0869500637054443, 'There are lots of Figures under 3.4.2 without any labels of captions.': 1.09859037399292, 'Some of them are really small and difficult to understand since the labels on the figures appear very small and somewhat unreadable.': 1.0983939170837402, 'A small question:': 1.0986123085021973, '* Do you also backpropagate through': 1.0986123085021973, '?': 1.0986123085021973, 'This paper proposes a novel method for accelerating optimization near saddle points.': 1.0986123085021973, 'The basic idea is to repel the current parameter vector from a running average of recent parameter values.': 1.0986123085021973, 'This method is shown to optimize faster than a variety of other methods in a variety of datasets and architectures.': 1.0986123085021973, 'On the surface, the proposed method seems extremely close to momentum.': 1.0985335111618042, 'It would be very valuable to think of a clear diagram illustrating how it differs from momentum and why it might be better near a saddle point.': 1.0986123085021973, 'The illustration of better convergence on the toy saddle example is not what I mean here—optimization speed comparisons are always difficult due to the many details and hyper parameters involved, so seeing it work faster in one specific application is not as useful as a conceptual diagram which shows a critical case where CPN will behave differently from—and clearly qualitatively better than—momentum.': 1.0986123085021973, 'Another way of getting at the relationship to momentum would be to try to find a form for R_t(f) that yields the exact momentum update.': 1.0986123085021973, 'You could then compare this with the R_t(f) used in CPN.': 0.4380604028701782, 'The overly general notation  etc should be dropped—Eqn 8 is enough.': 1.0986106395721436, 'The theoretical results (Eqn 1 and Thm 1) should be removed, they are irrelevant until the joint density can be specified.': 1.0986123085021973, 'Experimentally, it would be valuable to standardize the results to allow comparison to other methods.': 1.0986024141311646, 'For instance, recreating Figure 4 of Dauphin et al, but engaging the CPN method rather than SFN, would clearly demonstrate that CPN can escape something that momentum cannot.': 1.0801602602005005, 'I think the idea here is potentially very valuable, but needs more rigorous comparison and a clear relation to momentum and other work.': 1.0986123085021973, 'The research direction taken by this paper is of great interest.': 1.098578929901123, 'However, the empirical results are not great enough to pay for the weaknesses of the proposed approach (see Section 6).': 1.0986123085021973, '""Throughout this paper the selection of hyper-parameters was kept rather simple.""': 1.0986123085021973, 'but the momentum term of CPN is set to 0.95': 1.0986123085021973, 'and not 0.9 as in all/most optimizers CPN is compared to.': 1.0986123085021973, 'I suppose that the positive effect of CPN (if any) is mostly due to its momentum term.': 1.0986123085021973}"
25,https://openreview.net/forum?id=B1kJ6H9ex,"{'This is a very nicely written paper which unifies some value-based and policy-based (regularized policy gradient) methods, by pointing out connections between the value function and policy which have not been established before.': 0.7517151832580566, 'The theoretical results are new and insightful, and will likely be useful in the RL field much beyond the specific algorithm being proposed in the paper.': 0.4277191460132599, 'This being said, the paper does exploit the theory to produce a unified version of Q-learning and policy gradient, which proves to work on par or better than the state-of-art algorithms on the Atari suite.': 1.0846943855285645, 'The empirical section is very well explained in terms of what optimization were done.': 1.0975027084350586, 'One minor comment I had was related to the stationary distribution used for a policy - there are subtleties here between using a discounted vs non-discounted distribution which are not crucial in the tabular case, but will need to be addressed in the long run in the function approximation case.': 0.7410842180252075, 'This being said, there is no major problem for the current version of the paper.': 1.0981858968734741, 'Overall, the paper is definitely worthy of acceptance, and will likely influence a broad swath of RL, as it opens the door to further theoretical results as well as algorithm development.': 0.41804587841033936, 'Nice paper, exploring the connection between value-based methods and policy gradients, formalizing the relation between the softmax-like policy induced by the Q-values and a regularized form of PG.': 1.0985898971557617, 'Presentation:': 1.0986123085021973, 'Although that seems to be the flow in the first part of the paper, I think it could be cast as a extension/ generalization of the dueling Q-network – for me that would be a more intuitive exposition of the new algorithm and findings.': 1.09860360622406, 'Small concern in general case derivation:': 1.0985949039459229, 'Section 3.2: Eq. (7) the expectation (s,a) is wrt to \\pi, which is a function of \\theta': 0.5502018332481384, 'that dependency seems to be ignored, although it is key to the PG update derivation.': 1.075981616973877, ""If these policies(the sampling policy for the expectation and \\pi) are close enough it's usually okay"": 1.0986123085021973, ""but except for particular cases (trust-region methods & co), that's generally not true."": 1.0986123085021973, 'Thus, you might end up solving a very different problem than the one you actually care solving.': 1.0986123085021973, 'Results:': 1.0986123085021973, 'A comparison with the dueling architecture could be added as that would be the closest method (it would be nice to see if and in which game you get an improvement)': 1.0986123085021973, 'Overall: strong paper, good theoretical insights.': 1.0986123085021973, 'This paper shows how policy gradient and Q-Learning may be combined together, improving learning as demonstrated in particular in the Atari Learning Environment.': 1.0986123085021973, 'The core idea is to note that entropy-regularized policy gradient leads to a Boltzmann policy based on Q values, thus linking pi & Q together and allowing both policy gradient and Q-Learning updates to be applied.': 1.0986123085021973, 'I think this is a very interesting paper, not just for its results and the proposed algorithm (dubbed PGQ), but mostly because of the links it draws between several techniques, which I found quite insightful.': 1.0986123085021973, 'That being said, I also believe it could have done a better job at clearly exposing these links: I found it somewhat difficult to follow, and it took me a while to wrap my head around it, even though the underlying concepts are not that complex.': 1.0986123085021973, 'In particular:': 1.0986123085021973, 'The notation \\tilde{Q}^pi is introduced in a way that is not very clear, as ""an estimate of the Q-values"" while eq. 5 is an exact equality (no estimation)': 1.0986123085021973, 'It is not clear to me what section 3.2 is bringing exactly, I wonder if it could just be removed to expand some other sections with more explanations.': 1.0986123085021973, 'The links to dueling networks (Wang et al, 2016) are in my opinion not explicit and detailed enough (in 3.3 & 4.1): as far as I can tell the proposed architecture ends up being very similar to such networks and thus it would be worth telling more about it (also in experiments my understanding is that the ""variant of asynchronous deep Q-learning"" being used is essentially such a dueling network, but it is not clearly stated).': 1.0941720008850098, 'I also believe it should be mentioned that PGQ can also be seen as combining Q-Learning with n-step expected Sarsa using a dueling network: this kind of example helps better understand the links between methods': 1.0877206325531006, 'Overall I wish section 3.3 was clearer, as it draws some very interesting links, but it is hard to see where this is all going when reading the paper for the first time.': 1.0985983610153198, 'One confusing point is w.r.t.': 1.0980513095855713, 'to the relationship with section 3.2, that assumes a critic outputting Q values while in 3.3 the critic outputs V.': 1.0986123085021973, 'The ""mu"" distribution also comes somewhat out of nowhere.': 1.098419427871704, 'I hope the authors can try and improve the readability of the paper in a final version, as well as clarify the points raised in pre-review questions (in particular related to experimental details, the derivation of eq. 4, and the issue of the discounted distribution of states).': 1.0986123085021973, 'Minor remarks:': 1.0986123085021973, 'The function r(s, a) used in the Bellman equation in section 2 is not formally defined.': 1.0986123085021973, ""It looks a bit weird because the expectation is on s' and b' but r(s, a) does not depend on them (so either it should be moved out of the expectation, or the expectation should also be over the reward, depending on how r is defined)"": 1.0986123085021973, 'The definition of the Boltzmann policy at end of 2.1 is a bit confusing since there is a sum over ""a"" of a quantity that does not depend (clearly) on ""a""': 1.0986123085021973, 'I believe 4.3 is for the tabular case but this is not clearly stated': 1.0986123085021973, 'Any idea why in Fig.': 1.0986123085021973, '1 the 3 algorithms do not all converge to the same policy?': 1.0986123085021973, 'In such a simple toy setting I would expect it to be the case.': 1.0986123085021973, 'Typos:': 1.0986123085021973, '""we refer to the classic text Sutton & Barto (1998)"" => missing ""by""?': 1.0986123085021973, '""Online policy gradient typically require an estimate of the action-values function"" => requires & value': 1.0986123085021973, '""the agent generates experience from interacting the environment"" => with the environment': 1.0986123085021973, 'in eq. 12 (first line) there is a comma to remove near the end, just before the dlog pi': 1.0985982418060303, '""allowing us the spread the influence of a reward"" => to spread': 1.0986121892929077, '""in the off-policy case tabular case"" => remove the first case': 1.0985816717147827, '""The green line is Q-learning where at the step an update is performed"" => at each step': 1.0986123085021973, 'In Fig. 2 it says A2C instead of A3C': 1.0986123085021973, 'NB: I did not have time to carefully read Appendix A': 1.0986123085021973}"
26,https://openreview.net/forum?id=B1mAJI9gl,"{'The paper proposes to provide a theoretical explanation for why deep convolutional neural networks are invertible (at-least, when going back from certain intermediate layers to the image itself).': 1.0986123085021973, 'It does so by considering the invertibility of a single layer, assuming the convolutional filters essentially correspond to incoherent measurements satisfying RIP.': 1.0986123085021973, 'In my opinion, while this is an interesting direction of research, the paper is not ready for publication.': 1.0986123085021973, 'I feel the treatment does not go sufficiently towards explaining the phenomenon in deep neural networks.': 1.0986123085021973, 'Even after reading the response from the authors, I feel the results are only a minor variation of the standard results from compressive sensing for sparse reconstruction with incoherent measurements.': 1.0986080169677734, 'A deep neural network is fundamentally different from a single layer': 1.0986123085021973, 'it is the ""deep"" part that makes the forward task work.': 1.0986123085021973, 'As the authors note, there is significant deterioration when IHT is applied recursively': 1.0986123085021973, 'therefore, at best the theory explains the partial invertibility of a single layer.': 1.0986123085021973, ""That a single layer is approximately invertible isn't surprising, that a cascade of layers *is*."": 1.0986123085021973, 'For any theoretical analysis of this phenomenon to be useful, I believe it must go beyond analyzing a single compressive measurement-type layer, and try to explain how much of the same theory holds for a cascade.': 1.0986123085021973, ""I say this because it's entirely possible that the sparse recovery theory breaks down beyond a single layer, and invertibility ends up being a property caused by correlations between the weights of different layers."": 1.0986123085021973, 'In other words, there is no way to tell from the current results for individual layers whether they are in fact a step towards explaining the invertibility of whole networks.': 1.0986123085021973, 'The authors propose a theoretical framework to analyze the recoverability of sparse activations in intermediate layers of deep networks, using theoretical tools from compressed sensing.': 1.0986123085021973, 'They relate the computations that are performed by a CNN and a particular recovery algorithm (Iterative Hard Thresholding, IHT).': 1.0986119508743286, 'They present proofs of necessary conditions for recoverability to hold, and also show detailed empirical evidence of how they hold in practice.': 1.0986123085021973, 'This is a well-written paper that presents a new angle on why the current CNN architectures work so well.': 1.0986121892929077, 'The authors give a brief but sufficient review of the fundamentals of compressed sensing, present their main result relating feed-forward networks and IHT (a surprising result), and progress naturally to a detailed experimental section.': 1.0985159873962402, 'The introductory analysis at the beginning of Section 3, in particular, delivers the gist of why the method should work with very approachable and simple math, which is not common in theoretical papers.': 1.0985534191131592, 'The increasing complexity of the experiments, done in small steps, shows a nice progression from artificial distributions to a realistic experiment.': 1.0986123085021973, 'A few aspects should be improved.': 1.0986123085021973, ""First of all, although the treatment of ReLU non-linearities is sufficient, it is assumed with little discussion that max-pooling non-linearities shouldn't present a problem as well."": 1.0948574542999268, 'A discussion of how this is inverted (e.g., with pooling switches) is needed.': 1.0986123085021973, 'The relationship between feed-forward nets and Algorithm 1 assumes tied weights.': 1.0986123085021973, 'It might be worthwhile to mention that the result is stronger for the case of RNNs, where this is the case by design.': 1.0986123085021973, 'Although it might be obvious, it might help some readers to briefly note that the reconstruction algorithm is meant to be applied to each layer sequentially, basing the activations of each layer on the one above it (in back-propagation order).': 1.0986123085021973, 'Finally, the filter coherence measure must be defined either mathematically or with a proper reference.': 1.0986123085021973, 'Summary of the paper': 1.0986123085021973, 'The paper studies the invertiblity of convolutional neural network in the random model.': 1.0986123085021973, 'A reconstruction algorithm similar to IHT is proposed for layer-wise inversion of the network.': 1.0986123085021973, 'Clarity:': 1.0986123085021973, 'The paper is confusing wrt to standard notations in deep learning.': 1.0986123085021973, 'Comments:': 0.8985233306884766, 'The paper makes two simplifications in the analysis of a CNN, that makes it map to a model based compressive sensing framework:': 1.0985490083694458, '1-  The non linearity (RELU) is dropped.': 1.0087090730667114, 'This is a big simplification, for random gaussian weights for instance we know by JL that we can preserve L_2 distance, when RELU is applied the metric changes (see for instance the kernel for n=1 in  http://cseweb.ucsd.edu/~saul/papers/nips09_kernel.pdf).': 1.0986123085021973, '2- The pooling operation is modified to be shrinkage operator, that keeps the maximum value in a block  and sets to the zero other values, hence the dimensionality of the pooled representation is the same of the un-pooled representations.': 0.49877896904945374, 'Note that in that cases the locations of where the maximum happens is known.': 1.0982400178909302, 'This is not the case in the ’standard’ max pooling definition.': 1.0645015239715576, 'IHT does not map to a forward of a CNN as described in the paper.': 1.0986123085021973, '(see next point)': 0.4164893627166748, '3- It maybe that the notations  used in the paper are implying some confusions wrt to the standard notations in deep learning.': 0.5262780785560608, '- Let z be the standard  pooled representation of dimension ‘k’.': 0.8484994173049927, '- Define U as the unpooling  operation U(z, locations of maximum) :=  M(Wx,k).': 1.0986123085021973, 'Hence your model of inversion is assuming the knowledge of the ’standard’ pooling representation and the switches (max locations).': 1.0986123085021973, 'Referring to M as a pooling operation is misleading and confusing, it is the ‘standard’ unpooling operation.': 1.0986123085021973, '- Under this notations W^{\\top} U(z, locations of maximum) is a sensible reconstruction algorithm , with the simplification of ignoring the RELU.': 1.0986123085021973, 'Under these notations,  IHT is similar to a backward of the encoding neural network not  a forward.': 1.0986123085021973, 'It is known if you take the derivative with respect to the input in a neural network, you get the transpose convolution also known as ‘deconvolution’ (which is not a correct naming) , hence this IHT iteration is nothing else then the well known transposed convolution.': 1.098609209060669, '4- I suggest the authors to rewrite the paper taking into account standard definitions and notation in deep learning as discussed in point 3, and to clearly state that the recovery is done under the knowledge of the switches.': 1.0986123085021973, '=====': 1.0986123085021973, 'After reading the authors rebuttal and the revisions , the reviewer maintains his main concerns with the paper.': 1.0986123085021973, 'Many improvements are still needed for the paper to be ready to be published in ICLR, at the notation , presentation and the theory levels.': 1.0986123085021973}"
27,https://openreview.net/forum?id=B1oK8aoxe,"{'Interesting work on hierarchical control, similar to the work of Heess et al.': 1.0986123085021973, 'Experiments are strong and manage to complete benchmarks that previous work could not.': 1.0986123085021973, 'Analysis of the experiments is a bit on the weaker side.': 1.0986123085021973, '(1) Like other reviewers, I find the use of the term ‘intrinsic’ motivation somewhat inappropriate (mostly because of its current meaning in RL).': 1.0986123085021973, 'Pre-training robots with locomotion by rewarding speed (or rewarding grasping for a manipulating arm) is very geared towards the tasks they will later accomplish.': 1.0986123085021973, 'The pre-training tasks from Heess et al., while not identical, are similar.': 1.0986123085021973, '(2) The Mutual Information regularization is elegant and works generally well, but does not seem to help in the more complex mazes 1,2 and 3.': 1.0986123085021973, 'The authors note this - is there any interpretation or analysis for this result?': 1.0986123085021973, '(3) The factorization between S_agent and S_rest should be clearly detailed in the paper.': 1.0986123085021973, 'Duan et al specify S_agent, but for replicability, S_rest should be clearly specified as well - did I miss it?': 1.0986123085021973, '(4) It would be interesting to provide some analysis of the switching behavior of the agent.': 1.0986123085021973, 'More generally, some further analysis of the policies (failure modes, effects of switching time on performance) would have been welcome.': 1.0986123085021973, '*Edited the score 6->7.': 1.0986123085021973, 'The paper presents a method for hierarchical RL using stochastic neural networks.': 1.0986123085021973, 'The paper has introduced using information-theoretic measure of option identifiability as an additional reward for learning a diverse mixture of sub-policies.': 1.0986123085021973, 'One nice result in the paper is the comparison with strong baseline which directly combines the intrinsic rewards with sparse rewards and shows that this supposedly smooth reward can’t solve tasks.': 1.0986123085021973, 'Besides the argument made from the authors on difficulty on long-term credit assignment/benefits from hierarchical abstraction, one possible explanation for this might be the diversity requirement imposed in sub-policy training, which is assumed to be off in the baseline case.': 1.0986123085021973, 'Wonder if this can shed insights into improving the baseline and proposing new end-to-end hierarchical policy learning as hierarchical REPS/option-critic etc.': 1.0986123085021973, 'papers do.': 1.0986123085021973, 'Nice visualizations.': 1.0986123085021973, 'The paper presents a promising direction, and it may be strengthened further by possibly addressing some of the following points.': 1.098334789276123, '1) Limited diversification of sub-policies: Both concatenation and bilinear integration allow only minimal differentiations in sub-policies through first hidden weight, which is not a problem in the tested tasks because they essentially require same locomotion policies with minimal diversification, but such limitation can be more obvious in other tasks where ideal sub-policies are more diverse.': 0.7267513275146484, 'Thus it is interesting to see it apply on harder, non-locomotion domains, where ideal sub-policies are not that similar, e.g. for manipulation, solving some task from one state can be very different from solving it from another state.': 1.0986123085021973, '2) Limitation on hierarchical policies: Manager network is trained while the sub-policies are fixed.': 0.9736956357955933, 'Furthermore, the time steps for sub-policies are fixed.': 0.7573322653770447, 'This requires “intrinsic” rewards and their learned sub-policies to be very good for solving down-stream tasks.': 1.0807524919509888, 'It would be nice to see some more discussions/results on handling such cases, ideally connecting to end-to-end hierarchical policy learning.': 0.945968747138977, '3) Intrinsic/unsupervised rewards seem domain-specific/supervised rewards: Because of (2), this seems unavoidable.': 0.5595321655273438, 'I like the setting presented in this paper but I have several criticism/questions:': 1.0986123085021973, '(1) What are the failure model of this work?': 1.0984523296356201, 'As richness of behaviors get complex, I expected this approach to have issues with the diversity of skills that could be discovered.': 1.0986123085021973, '(2) Looking at Sec 5.3': 1.0979632139205933, '"" let X be a random variable denoting the grid in which the agent is currently situated""': 1.0986123085021973, 'is the space discretized?': 1.0986123085021973, ""And if so why and what happens if it isn't."": 1.0986123085021973, '(3) Expanding on the first point, does the approach work with more complicated embodiment?': 1.0986123085021973, 'Say a 5-link swimmer instead of 2?': 1.0986123085021973, 'I think this is important to assess the generality of this approach': 1.0986123085021973, '(4) Authors claim that ""Recently, Heess et al.': 1.0986123085021973, '(2016) have independently proposed to learn a range of skills in a pre-training environment that will be useful for the downstream tasks, which is similar to our framework.': 1.0986123085021973, 'However, their pre-training setup requires a set of goals to be specified.': 1.0986123085021973, 'In comparison, we use intrinsic rewards as the only signal to the agent during the pre-training phase, the construction of which only requires very minimal domain knowledge.""': 1.0986123085021973, ""I don't entirely agree with this."": 1.0986123085021973, 'The rewards that this paper proposes are also quite hand-crafted and specific to a seemingly limited set of control tasks.': 1.0986123085021973}"
28,https://openreview.net/forum?id=B1s6xvqlx,"{'[UPDATE]': 1.0986123085021973, 'After going through the response from the author and the revision, I increased my review score for two reasons.': 1.0986123085021973, '1. I thank the reviewers for further investigating the difference between yours and the other work (Scheduled sampling, Unsupervised learning using LSTM) and providing some insights about it.': 1.0128240585327148, 'This paper at least shows empirically that 100%-Pred scheme is better for high-dimensional video and for long-term predictions.': 1.0986123085021973, 'It would be good if the authors briefly discuss this in the final revision (either in the appendix or in the main text).': 1.0986123085021973, '2. The revised paper contains more comprehensive results than before.': 1.0986123085021973, 'The presented result and discussion in this paper will be quite useful to the research community as high-dimensional video prediction involves large-scale experiments that are computationally expensive.': 1.0986109972000122, 'Summary': 1.0986123085021973, 'This paper presents a new RNN architecture for action-conditional future prediction.': 1.0922837257385254, 'The proposed architecture combines actions into the recurrent connection of the LSTM core, which performs better than the previous state-of-the-art architecture': 1.090888500213623, '[Oh et al.].': 1.0986123085021973, 'The paper also explores and compares different architectures such as frame-dependent/independent mode and observation/prediction-dependent architectures.': 0.7907238602638245, 'The experimental result shows that the proposed architecture with fully prediction-dependent training scheme achieves the state-of-the-art performance on several complex visual domains.': 1.0985807180404663, 'It is also shown that the proposed prediction architecture can be used to improve exploration in a 3D environment.': 1.0944737195968628, 'Novelty': 1.0986123085021973, 'The novelty of the proposed architecture is not strong.': 1.0899744033813477, 'The difference between [Oh et al.]': 1.09635591506958, 'and this work is that actions are combined into the LSTM in this paper, while actions are combined after LSTM in [Oh et al.].': 0.67681884765625, 'The jumpy prediction was already introduced by [Srivastava et al.] in the deep learning area.': 0.42907100915908813, 'Experiment': 1.09861159324646, 'The experiments are well-designed and thorough.': 0.7208865880966187, 'Specifically, the paper evaluates different training schemes and compares different architectures using several rich domains (Atari, 3D worlds).': 0.1886960119009018, 'Besides, the proposed method achieves the state-of-the-art results on many domains and presents an application for model-based exploration.': 0.991734504699707, 'Clarity': 1.0986123085021973, 'The paper is well-written and easy to follow.': 0.7315770387649536, 'Overall': 1.0986123085021973, 'Although the proposed architecture is not much novel, it achieves promising results on Atari games and 3D environments.': 0.9059717655181885, 'In addition, the systematic evaluation of different architectures presented in the paper would be useful to the community.': 0.8277433514595032, '[Reference]': 1.0986123085021973, 'Nitish Srivastava, Elman Mansimov, Ruslan Salakhutdinov.': 0.84679114818573, 'Unsupervised Learning with LSTMs.': 0.6740429401397705, 'ICML 2016.': 1.0986123085021973, 'The paper presents an action-conditional recurrent network that can predict frames in video games hundreds of steps in the future.': 1.0986123085021973, 'The paper claims three main contributions:': 1.0986123085021973, '1. modification to model architecture (used in Oh et al.) by using action at time t-1 to directly predict hidden state at t': 0.7734952569007874, '2. exploring the idea of jumpy predictions (predictions multiple frames in future without using intermediate frames)': 0.7283740043640137, '3. exploring different training schemes (trade-off between observation and prediction frames for training LSTM)': 0.7899573445320129, '1. modification to model architecture': 1.0353318452835083, '+': 1.0986123085021973, 'The motivation seems good that in past work (Oh et al.)': 1.0986123085021973, 'the action at t-1 influences x_t, but not the state h_t of the LSTM.': 1.0986123085021973, 'This could be fixed by making the LSTM state h_t dependent on a_{t-1}': 1.0986123085021973, 'However, this is of minor technical novelty.': 1.0986123085021973, 'Also, as pointed in reviewer questions, a similar effect could be achieved by adding a_t-1 as an input to the LSTM at time t.': 1.0986123085021973, 'This could be done without modifying the LSTM architecture as stated in the paper.': 1.0721206665039062, 'While the authors claim that combining a_t-1 with h_t-1 and s_t-1 performs worse than the current method which combines a_t-1 only with h_t-1, I would have liked to see the empirical difference in combining a_t-1 only with s_t-1 or only with h_t-1.': 1.0983916521072388, 'Also, a stronger motivation is required to support the current formulation.': 1.0979392528533936, 'Further, the benefits of this change in architecture is not well analyzed in experiments.': 1.0807387828826904, 'Fig.': 1.0986123085021973, '5(a) provides the difference between Oh et al. (with traditional LSTM) and current method.': 0.08330130577087402, 'However, the performance difference is composed of 2 components (difference in training scheme and architecture).': 1.0951200723648071, 'This contribution of the architecture to the performance is not clear from this experiment.': 1.0986120700836182, 'The authors did claim in the pre-review phase that Fig.': 1.0704501867294312, '12 (a) shows the difference in performance only due to architecture for ""Seaquest"".': 0.6680322885513306, 'However, from this plot it appears that the gain at 100-steps (~15)  is only a small fraction of the overall gain in Fig.': 1.088689923286438, '5 (a) (~90).': 0.5844021439552307, 'It is difficult to judge the significance of the architecture modification from this result for one game.': 1.0979008674621582, '2. Exploring the idea of jumpy predictions:': 1.0297324657440186, '+ As stated by the authors, omitting the intermediate frames while predicting future frames could significantly sppedup simulations.': 0.40260136127471924, 'The results in Fig.': 1.0986123085021973, '5(b) present some interesting observations that omitting intermediate frames does not lead to significant error-increase for at least a few games.': 0.11729966104030609, 'However, it is again not clear whether the modification in the current model leads to this effect or it could be achieved by previous models like Oh et al.': 1.0296745300292969, 'While, the observations themselves are interesting, it would have been better to provide a more detailed analysis for more games.': 0.9379670023918152, 'Also, the novelty in dropping intermediate frames for speedup is marginal.': 1.0976642370224, '3. Exploring different training schemes': 1.0986123085021973, '+ This is perhaps the most interesting observation presented in the paper.': 1.0986123085021973, 'The authors present the difference in performance for different training schemes in Fig. 2(a).': 1.0986123085021973, 'The training schemes are varied based on the fraction of training phase which only uses observation frames and the fraction that uses only prediction frames.': 1.0986123085021973, 'The results show that this change in training can significantly affect prediction results and is the biggest contributor to performance improvement compared to Oh et al.': 1.0986123085021973, 'While this observation is interesting, this effect has been previously explored in detail in other works like schedule sampling (Bengio et al.) and to some extent in Oh et al.': 1.0986123085021973, 'Clarity of presentation:': 1.007831335067749, 'The exact experimental setup is not clearly stated for some of the results.': 1.0986002683639526, 'For instance, the paper does not say that Fig.': 1.0501329898834229, '2(a) uses the same architecture as Oh et al.': 0.4259644150733948, 'However, this is stated in the response to reviewer questions.': 1.0976331233978271, '4 is difficult to interpret.': 0.9320960640907288, 'The qualitative difference between Oh et al. and current method could be highlighted explicitly.': 1.0892456769943237, 'Minor: The qualitative analysis section requires the reader to navigate to various video-links in order to understand the section. This leads to a discontinuity in reading and is particularly difficult while reading a printed-copy.': 1.0637469291687012, 'Overall, the paper presents some interesting experimental observations.': 1.0986120700836182, 'However, the technical novelty and contribution of the proposed architecture and training scheme is not clear.': 1.091661810874939, 'The authors propose a recurrent neural network architecture that is able to output more accurate long-term predictions of several game environments than the current state-of-the-art.': 1.0986087322235107, 'The original network architecture was inspired by inability of previous methods to accurately predict many time-steps into the future,': 1.0986119508743286, 'and their inability to jump directly to a future prediction without iterating through all intermediate states.': 0.7823573350906372, 'The authors have provided an extensive experimental evaluation on several benchmarks with promising results.': 0.43944981694221497, 'In general the paper is well written and quite clear in its explanations.': 0.5640316009521484, 'Demonstrating that this kind of future state prediction is useful for 3D maze exploration is a plus.': 0.5197527408599854, '# Minor comments:': 1.0929670333862305, ""`jumpy predictions have been developed in low-dimensional observation spaces' - cite relevant work in the paper."": 0.6657729744911194, '# Typos': 1.0986123085021973, ""Section 3.1 - `this configuration is all experiments'"": 1.09861159324646}"
29,https://openreview.net/forum?id=B1vRTeqxg,"{'The authors propose a new model to learn symbolic expression representations.': 1.0929254293441772, 'They do a reasonably extensive evaluation with similar approaches and motivate their approach well.': 1.0986123085021973, 'As expressed in the preliminary questions, I think the authors could improve the motivation for their subexpforce loss.': 1.0986123085021973, 'At the top of page 6 the authors mention that they compare to two-layer MLP w/o residual connections.': 1.0986123085021973, 'I think having a direct comparison between a model with and w/o the subexpforce loss would be helpful too and should be included (i.e. keep the residual connections and normalization).': 1.0986123085021973, 'My main concern is the evaluation ""score"".': 1.0985608100891113, 'It appears to be precision on a per query basis.': 1.0986123085021973, 'I believe a more standard metric, precision-recall or roc would be more informative.': 1.0986123085021973, ""In particular the chosen metric is expected to perform better when the equivalence classes are larger, since this isn't taken into account in the denominator, but the likelihood of a random expression matching the query increases."": 1.0986123085021973, 'The goal of this paper is to learn vector representation of boolean and polynomial expressions, such that equivalent expressions have similar representations.': 1.049795389175415, 'The model proposed in the paper is based on recursive neural network, as introduced by Socher et al. (2012).': 1.0848406553268433, 'Given the syntactic parse tree of a formula (either boolean or polynomial), the representation for a node is obtained by applying a MLP on the representation of the children.': 1.0831700563430786, 'This process is applied recursively to obtain the representation of the full expression.': 1.0898839235305786, 'Contrary to Socher et al. (2012), this paper proposes to use more than one layer (this is especially important to capture XOR operation, which is not surprising at all).': 1.0986000299453735, 'The paper also introduces a reconstruction error (called SubexpForce), so that the expression of children can be recovered from the expression of the parent (if I understood correctly).': 0.8617374300956726, 'The model is trained using a classification loss, where the label of a given expression corresponds to its equivalence class.': 0.823214054107666, 'The method is then evaluated on randomly generated data, and compared to baselines such as tf-idf, GRU RNN or standard recursive neural network.': 1.0980855226516724, 'While I do agree with the authors that learning good representation for symbolic expressions (and to capture compositionality) is an important task, I am not entirely convinced by the experimental setting proposed in this paper.': 1.0967124700546265, 'Indeed, as stated in the paper, the task of deciding if two boolean expressions are equivalent is NP-hard, and I do not understand if the model can do better than implicitly computing the truth table of expressions.': 1.0847703218460083, 'While sometimes a bit hard to follow, the paper is technically sound.': 0.44503408670425415, 'In particular, the proposed model is well adapted to the problem and outperforms the baselines.': 1.0753189325332642, 'pros:': 1.0365302562713623, '- the model is relatively simple and sound.': 1.0986123085021973, '- using a classification loss over equivalence classes (should be compared with using similarity).': 1.0986123085021973, 'cons:': 1.0986123085021973, '- not convinced by the setting: I do not believe that you can really do better than the truth table for boolean expr (or computing the value of the polynomial expression for randomly chosen points in [0, 1]^n).': 1.045149803161621, '- some part of the paper are a bit hard to follow (e.g. justification of the SubexpForce, discussion of why softmax does not work, etc...).': 0.4063541889190674, '- comparison between classification loss and similarity loss is missing.': 1.0986123085021973, 'This work proposes to compute embeddings of symbolic expressions (e.g., boolean expressions, or polynomials) such that semantically equivalent expressions are near each other in the embedded space.': 0.9941920638084412, 'The proposed model uses recursive neural networks where the architecture is made to match that of the parse tree of a given symbolic expression.': 1.0986123085021973, 'To train the model parameters, the authors create a dataset of expressions where semantic equivalence relationships are known and minimize a loss function so that equivalent expressions are closer to each other than non-equivalent expressions via a max-margin loss function.': 1.005500316619873, 'The authors also use a “subexpression forcing” mechanism which, if I understand it correctly, encourages the embeddings to respect some kind of compositionality.': 1.0983563661575317, 'Results are shown on a few symbolic expression datasets created by the authors and the proposed method is demonstrated to outperform baselines pretty convincingly.': 1.098608136177063, 'I especially like the PCA visualization where the action of negating an expression is shown to correspond roughly to negating the embedding in its vector space — it is a lot like the man - woman + queen = king type embeddings that we see in the word2vec and glove style papers.': 1.0986123085021973, 'The weakest part of the paper is probably that the setting seems somewhat contrived — I can’t really think of a real setting where it is easy to have a training set of known semantic equivalences, but still more worth it to use a neural network to do predictions.': 1.0985980033874512, 'The authors have also punted on dealing with variable names, assuming that distinct variables refer to different entities in the domain.': 1.0985878705978394, 'This is understandable, as variable names add a whole new layer of complexity on an already difficult problem, but also seems high limiting.': 1.0985552072525024, 'For example, the proposed methods would not be useable in an “equation search engine” unless we were able to accurately canonicalize variable names in some way.': 1.0986123085021973, 'Other miscellaneous points:': 0.9721637964248657, '* Regarding problem hardness, I believe that the problem of determining if two expressions are equivalent is actually undecidable — see the “word problem for Thue systems”.': 1.0635769367218018, 'Related to this, I was not able to figure out how the authors determine ground truth equivalence in their training sets.': 1.0965375900268555, 'They say that expressions are simplified into a canonical form and grouped, but this seems to not be possible in general, so one question is — is it possible that equivalent expressions in the training data would have been mapped to different canonical forms?': 1.0772664546966553, 'Would it have been easier/possible to construct and compare truth tables?': 0.9309483170509338, '* The “COMBINE” operation uses what the authors describe as a residual-like connection.': 1.0470550060272217, 'Looking at the equations, the reason why this is not actually a residual connection is because of the weight matrix that is multiplied by the lower level l_0 features.': 1.0063620805740356, 'A true residual connection would have passed the features through unchanged (identity connection) and would have also been better at fighting gradient explosion….': 1.098610758781433, 'so is there a reason why this was used rather than an identity connection?': 1.0986123085021973, '* In table 3, the first tf-idf entry: a + (c+a) *': 1.0986123085021973, 'c seems equivalent to a + (c * (a+c))': 1.0986123085021973, '* Vertical spacing between Figure 4 caption and body of text is very small and looks like the caption continues into the body of the text.': 1.0986123085021973}"
30,https://openreview.net/forum?id=BJ--gPcxl,"{'This paper presents a semi-supervised algorithm for regularizing deep convolutional neural networks.': 0.6397894620895386, 'They propose an adversarial approach for image inpainting where the discriminator learns to identify whether an inpainted image comes from the data distribution or the generator, while at the same time it learns to recognize objects in an image from the data distribution.': 1.0985080003738403, 'In experiments, they show the usefulness of their algorithm in which the features learned by the discriminator result in comparable or better object recognition performance to the reported state-of-the-art in two datasets.': 1.0986123085021973, 'Overall, the proposed idea seems a simple yet an effective way for regularize CNNs to improve the classification performance.': 1.035882592201233, 'This paper proposes a method to incorporate super-resolution and inpainting in the GAN framework for semi-supervised learning using the GAN discriminative features on larger images.': 1.0986123085021973, 'The core idea of the paper is not very novel.': 1.0986123085021973, 'The usefulness of the GAN discriminative features for semi-supervised learning is already established in previous works such as CatGAN, DCGAN and Salimans et al.': 1.0986123085021973, 'However this paper does a good job in actually getting the semi-supervised GAN framework working on larger images such as STL-10 and Pascal datasets using the proposed context conditioning approach, and achieves the state-of-the-art on these datasets.': 1.0986123085021973, 'I think that the authors should provide the SSL-GAN baseline for the PASCAL dataset as it is very important to compare the contribution of the context conditioning idea with the standard way of using GAN for semi-supervised learning, i.e., SSL-GAN.': 1.0986120700836182, ""I can't see why the SSL-GAN can not be applied to the 64*64 and 96*96 version of the Pascal dataset (Table 3)."": 1.0986121892929077, 'If they have trouble training the vanilla GAN on Pascal even on the 64*64 image size, this should be mentioned in the paper and be explained.': 1.0986123085021973, 'I am concerned about this specially because CC-GAN almost matches the SSL-GAN baseline on STL-10, and CC-GAN2, to me, seems like a hacky way to improve upon the core CC-GAN idea.': 1.0986121892929077, 'So it would be great to compare CC-GAN and SSL-GAN on some other dataset, even if it is a downsampled PASCAL dataset.': 1.0985993146896362, 'After rebuttal:': 1.0986123085021973, 'Thanks for reporting the AlexNet results.': 1.0986123085021973, 'The fact that they are not great is not so bad by itself, and as the authors mention, it would be interesting to understand why this happens.': 1.007562279701233, 'But the fact that these results  were not in the paper (and in fact still are not there) is disturbing.': 0.6235253810882568, 'Moreover, some claims in the paper look wrong in the light of these results, for example:': 0.7492669224739075, '""This suggests that our gains stem from the CC-GAN method rather than the use of a better architecture.""': 0.5193705558776855, '""Since discrimination of real/fake in-paintings is more closely related to the target task of object classification than extracting a feature representation suitable for in-filling, it is not surprising that we are able to exceed the performance of Pathak et al. (2016) on PASCAL classification.""': 0.7795326709747314, 'These statements, and possibly other parts of the paper, have to be updated.': 1.085923671722412, 'I think the paper cannot be published in its current form.': 0.4050028324127197, 'Perhaps after a revision.': 1.0986123085021973, 'Initial review:': 1.0986123085021973, 'The paper demonstrates an application of generative adversarial networks (GAN) to unsupervised feature learning.': 1.0031496286392212, 'The authors show that the representation learned by the discriminator of a conditional GAN trained for image inpainting performs well on image classification.': 1.0985041856765747, 'As a side-effect, fairly convincing inpaintings are produced.': 1.0700414180755615, 'The proposed method combines two existing ideas: using the discriminator of a GAN as a feature learner [Radford et al. 2015] and performing unsupervised feature learning with image inpainting': 1.0985852479934692, '[Pathak et al. 2016].': 1.0986123085021973, 'Therefore conceptual novelty of the paper is limited.': 1.098608136177063, 'On the plus side, the authors implement their idea well and demonstrate state-of-the-art results on STL-10 and good results on Pascal VOC (although Pascal experiments are incomplete, see below).': 1.0978704690933228, 'Overall, I am in the borderline mode, and I will gladly raise the score if the authors address my concerns regarding the experiments.': 1.0848314762115479, '1) Experimental evaluation on Pascal VOC is not quite satisfactory.': 1.0986123085021973, 'Comparison with prior work is unfair because the network architecture used by the authors (VGG) is different from the architecture used by all existing methods (AlexNet).': 1.0985169410705566, 'It is great that the authors do not try to hide this fact in the paper, but I do not understand why the authors are not willing to simply run their method with AlexNet architecture, although two commenters asked them to do so.': 0.7686099410057068, 'Such an experiment would strongly support authors’ claims.': 1.0986064672470093, 'Current reasoning that “we thought it reasonable to use more current models while making the difference clear” is not convincing.': 1.0986123085021973, 'It is great that better architectures lead to better results, but it is also very important to properly compare to prior work.': 1.0986123085021973, 'On a related topic, Doersch et al. also tried using VGG architecture, would it be possible to compare to that?': 1.0986123085021973, 'Yet another question: why are you not comparing to [Noroozi&Favaro, ECCV 2016] ?': 1.0986123085021973, 'I would also like the authors to address the comment by Richard Zhang.': 1.0986123085021973, '2)': 1.0986123085021973, 'Qualitative inpainting results are incomplete: comparison with previous methods (for instance, [Pathak et al 2016]) is missing, and it is impossible to compare different versions of the proposed method because different images are used for different variants.': 1.0986123085021973, 'I realize there may be too little space in the main paper to show all the results, but many more results should be shown in the supplementary material.': 1.0986123085021973, 'Quantitative results are missing.': 1.0986123085021973, 'Currently the inpainting results are just interesting pictures to look at, but they do not add as much to the paper as they could.': 1.0986123085021973}"
31,https://openreview.net/forum?id=BJ0Ee8cxx,"{'1. The hierarchical memory is fixed, not learned, and there is no hierarchical in the experimental section, only one layer for softmax layer.': 1.0981807708740234, '2. It shows the 10-mips > 100-mips > 1000-mips, does it mean 1-mips is the best one we should adopt?': 1.0772068500518799, '3. Approximated k-mips is worse than even original method. Why does it need exact k-mips? It seems the proposed method is not robust.': 1.0801023244857788, 'The paper proposes an algorithm for training memory networks which have very large memories.': 1.0985093116760254, 'Training such models in traditional ways, by using soft-attention mechanism over all the memory slots is not only slow, it is also harder to train due to dispersion of gradients.': 1.0986123085021973, 'The paper proposes to use the k-mips algorithm over the memories to choose a subset of the memory slots over which the attention is applied.': 1.0423288345336914, 'Since the cost of exact k-mips is the same as doing full attention, the authors propose to use approximate k-mips, which while faster to compute, results in inferior performance.': 1.065686583518982, 'An artifact of using k-mips is that one cannot learn the memory slots.': 1.09853196144104, 'Hence they are pre-trained and kept fixed during entire training.': 1.0986121892929077, 'The experimental section shows the efficacy of using k-mips using the SimpleQuestions dataset.': 1.0986121892929077, 'The exact k-mips results in the same performance as the full attention.': 1.0883208513259888, 'The approximate k-mips results in deterioration in performance.': 0.8736640214920044, 'The paper is quite clearly written and easy to understand.': 1.0985194444656372, 'I think the ideas proposed in the paper are not super convincing.': 1.0986123085021973, 'I have a number of issues with this paper.': 1.0967376232147217, ""1. The k-mips algorithm forces the memories to be fixed. This to me is a rather limiting constraint, especially on problems/dataset which will require multiple hops of training to do compounded reasoning. As a results I'm not entirely sure about the usefulness of this technique."": 1.0704632997512817, '2. Furthermore, the exact k-mips is the sample complexity as the full attention. The only way to achieve speedup is to use approx k-mips. That, as expected, results in a significant drop in performance.': 1.082673192024231, '3. The paper motivates the ideas by proposing solutions to eliminate heuristics used to prune the memories. However in Section 3.1 the authors themselves end up using multiple heuristics to make the training work. Agreed, that the used heuristics are not data dependent, but still, it feels like they are kicking the can down the road as far as heuristics are concerned.': 1.062493920326233, '4. The experimental results are not very convincing. First there is no speed comparison. Second, the authors do not compare with methods other than k-mips which do fast nearest neighbor search, such as, FLANN.': 1.0842387676239014, 'I find this paper not very compelling.': 1.0986086130142212, 'The basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable.': 1.0986123085021973, 'However, this was precisely the point of Rae et al.': 1.0986123085021973, ""There are a  number of standardized neighbor searchers; I don't understand why the authors choose to use their own (which they do not benchmark against the standards)."": 1.0986123085021973, 'Moreover, they test on a problem where there is no clear need for (vector based) fast-nn, because one can use hashing on the text.': 1.0986123085021973, 'I also find the repeated distinction between ""mips"" and ""nns"" distracting; most libraries that can do one can do the other, or inputs can be modified  to switch between the problems; indeed the authors do this when they convert to the  ""mcss"" problem.': 1.0986123085021973}"
32,https://openreview.net/forum?id=BJ3filKll,"{'The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds.': 1.0986123085021973, 'Specifically, the paper focuses on what the authors call ""monotonic chains of linear segments"", which are essentially sets of intersecting tangent planes.': 1.0986123085021973, 'The paper presents a construction that efficiently models such manifolds in a deep net, and presents a basic error analysis of the resulting construction.': 1.0986123085021973, 'While the presented results are novel to the best of my knowledge, they are hardly surprising (1) given what we already know about the representational power of deep networks and (2) given that the study selects a deep network architecture and a data structure that are very ""compatible"".': 1.0985877513885498, 'In particular, I have three main concerns with respect to the results presented in this paper:': 1.0986123085021973, '(1) In the last decade, there has been quite a bit of work on learning data representations from sets of local tangent planes.': 1.0986123085021973, 'Examples that spring to mind are local tangent space analysis of Zhang & Zha (2002), manifold charting by Brand (2002) and alignment of local models by Verbeek, Roweis, and Vlassis (2003).': 1.0986123085021973, 'None of this work is referred to in related work, even though it seems highly relevant to the analysis presented here.': 1.0986123085021973, 'For instance, it would be interesting to see how these old techniques compare to the deep network trained to produce the embedding of Figure 6.': 1.0986123085021973, 'This may provide some insight into the inductive biases the deep net introduces: does it learn better representations that non-parametric techniques because it has better inductive biases, or does it learn worse representations because the loss being optimized is non-convex?': 1.0986114740371704, '(2) It is difficult to see how the analysis generalizes to more complex data in which local linearity assumptions on the data manifold are vacuous given the sparsity of data in high-dimensional space, or how it generalizes to deep network architectures that are not pure ReLU networks.': 1.0986120700836182, 'For instance, most modern networks use a variant of batch normalization; this already appears to break the presented analyses.': 1.0986123085021973, '(3) The error bound presented in Section 4 appears vacuous for any practical setting, as the upper bound on the error is exponential in the total curvature (a quantity that will be quite large in most practical settings).': 1.0986123085021973, 'This is underlined by the analysis of the Swiss roll dataset, of which the authors state that the ""bound for this case is very loose"".': 1.0986123085021973, 'The fact that the bound is already so loose for this arguably very simple manifold makes that the error analysis may tell us very little about the representational power of deep nets.': 1.0985418558120728, 'I would encourage the authors to address issue (1) in the revision of the paper.': 1.0985203981399536, 'Issue (2) and (3) may be harder to address, but is essential that they are addressed for the line of work pioneered by this paper to have an impact on our understanding of deep learning.': 1.0578060150146484, 'Minor comments:': 1.0986123085021973, 'In prior work, the authors only refer to fully supervised siamese network approaches.': 0.5105661153793335, 'These approaches differ from that taken by the authors, as their approach is unsupervised.': 1.0923807621002197, ""It should be noted that the authors are not the first to study unsupervised representation learners parametrized by deep networks: other important examples are deep autoencoders (Hinton & Salakhutdinov, 2006 and work on denoising autoencoders from Bengio's group) and parametric t-SNE (van der Maaten, 2009)."": 1.0984797477722168, 'What loss do the authors use in their experiments?': 1.0756490230560303, 'Using ""the difference between the ground truth distance ... and the distance computed by the network"" seems odd, because it encourages the network to produce infinitely large distances (to get a loss of minus infinity).': 1.056641936302185, 'Is the difference squared?': 1.092264175415039, 'Summary:': 1.0986123085021973, 'In this paper, the authors look at the ability of neural networks to represent low dimensional manifolds efficiently e.g. embed them into a lower dimensional Euclidian space.': 1.0986121892929077, 'They define a class of manifolds, monotonic chains (affine spaces that intersect, with hyperplanes separating monotonic intervals of spaces) and give a construction to embed such a chain with a neural network with one hidden layer.': 1.0986123085021973, 'They also give a bound on the number of parameters required to do so, and examine what happens when the manifold is noisy.': 1.0986123085021973, 'Experiments involve looking at embedding synthetic data from a monotonic chain using a distance preservation loss.': 1.0986098051071167, 'This experiment supports the theoretical bound on number of parameters needed to embed the monotonic chain.': 1.0986123085021973, 'Another experiment varies the elevation and azimuth of of faces, which are known to lie on a monotonic chain, on a regression loss.': 1.0986123085021973, 'Comments:': 1.0986123085021973, 'The direction of investigation in the paper (looking at what happens to manifolds in a neural network), is very compelling, and I strongly encourage the authors to continue exploring this direction.': 1.0986123085021973, 'However, the current version of the paper could use some more work:': 1.0986123085021973, 'The experiments are all with a regression loss and a shallow network, and as part of the reason for interest in this question is the very large, high dimensional datasets we use now, which require a deeper network, it seems important to address this case.': 1.0986123085021973, 'It also seems important to confirm that embedding works well when *classification* loss is used, instead of regression': 1.0986123085021973, 'The theory sections could do with being more clearly written': 1.0986123085021973, 'I’m not as familiar with the literature in this area, and while the proof method used is relatively elementary, it was difficult to understand what exactly was being proved': 1.0986123085021973, 'e.g. formally stating what could be expected of an embedding that “accurately and efficiently” preserves a monotonic chain, etc.': 1.0986123085021973, 'SUMMARY': 1.0986123085021973, 'This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers.': 1.0979336500167847, 'PROS': 1.0986123085021973, 'Interesting, easy to follow view on some of the capabilities of neural networks, highlighting the dimensionality reduction aspect, and pointing at possible directions for further investigation.': 1.0986096858978271, 'CONS': 1.0986123085021973, 'The paper presents a construction illustrating certain structures that can be captured by a network, but it does not address the learning problem (although it presents experiments where such structures do emerge, more or less).': 1.0986114740371704, 'COMMENTS': 1.0986123085021973, 'It would be interesting to study the ramifications of the presented observations for the case of deep(er) networks.': 1.0986123085021973, 'Also, to study to what extent the proposed picture describes the totality of functions that are representable by the networks.': 1.0986123085021973, 'MINOR COMMENTS': 1.0986123085021973, 'Figure 1 could be referenced first in the text.': 1.0986123085021973, ""``Color coded'' where the color codes what?"": 1.0986123085021973, 'Thank you for thinking about revising the points from my first questions.': 1.0986123085021973, 'Note: Isometry on the manifold.': 1.0986123085021973, 'On page 5, mention how the orthogonal projection on S_k is realized in the network.': 1.0986123085021973, ""On page 6 ``divided into segments'' here `segments' is maybe not the best word."": 1.0986123085021973, ""On page 6 ``The mean relative error is 0.98''"": 1.0986123085021973, 'what is the baseline here, or what does this number mean?': 1.0986123085021973}"
33,https://openreview.net/forum?id=BJ46w6Ule,"{'The paper addresses the problem of learning compact binary data representations.': 1.0986123085021973, 'I have a hard time understanding the setting and the writing of the paper is not making it any easier.': 1.0986123085021973, ""For example I can't find a simple explanation of the problem and I am not familiar with these line of research."": 1.097658395767212, ""I read all the responses provided by authors to reviewer's questions and re-read the paper again and I still do not fully understand the setting and thus can't really evaluate the contributions of these work."": 1.0986123085021973, 'The related work section does not exist and instead the analysis of the literature is somehow scattered across the paper.': 1.0986123085021973, 'There are no derivations provided.': 1.0911189317703247, 'Statements often miss references, e.g. the ones in the fourth paragraph of Section 3.': 1.0984313488006592, 'This makes me conclude that the paper still requires significant work before it can be published.': 1.0986123085021973, 'The goal of this paper is to learn “ a collection of experts that are individually': 0.9198915958404541, 'meaningful and that have disjoint responsibilities.”': 1.09816575050354, 'Unlike a standard mixture model, they “use a different mixture for each dimension d.”': 1.0986121892929077, 'While the results seem promising, the paper exposition needs significant improvement.': 1.0765717029571533, 'Comments:': 1.0986123085021973, 'The paper jumps in with no motivation at all.': 0.42791974544525146, 'What is the application, or even the algorithm, or architecture that this is used for?': 1.0954499244689941, 'This should be addressed at the beginning.': 0.5289623737335205, 'The subsequent exposition is not very clear.': 1.011907696723938, 'There are assertions made with no justification, e.g. “the experts only have a small variance for some subset of the variables while the variance of the other variables is large.”': 1.098610520362854, 'Since you’re learning both the experts and the weights, can this be rephrased in terms of dictionary learning?': 1.098494529724121, 'Please discuss the relevant related literature.': 1.0986121892929077, 'The horse data set is quite small with respect to the feature dimension, and so the conclusions may not necessarily generalize.': 1.0985151529312134, 'This paper proposes a new kind of expert model where a sparse subset of most reliable experts is chosen instead of the usual logarithmic opinion pool of a PoE.': 1.0986123085021973, 'I find the paper very unclear.': 1.0986123085021973, 'I tried to find a proper definition of the joint model p(x,z) but could not extract this from the text.': 1.0986123085021973, 'The proposed “EM-like” algorithm should then also follow directly from this definition.': 1.0977829694747925, 'At this point I do not see if such as definition even exists.': 1.0984346866607666, 'In other words, is there is an objective function on which the iterates of the proposed algorithm are guaranteed to improve on the train data?': 1.0986123085021973, 'We also note that the “product of unifac models” from Hinton tries to do something very similar where only a subset of the experts will get activated to generate the input: http://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf': 1.0986123085021973, 'I tried to derive the update rule on top of page 4 from the “conditional objective for p(x|h)” in sec. 3.2': 1.050973892211914, 'But I am getting something different (apart form the extra smoothing factors eps and mu_o).': 1.0985969305038452, 'Does this follow?': 1.0986123085021973, '(If we define R=R_nk, mu-mu_k and X=X_n, I get mu = (XR)*inv(R^TR) as the optimal solution, which then needs to be projected back onto the probability simplex).': 0.9974645972251892, 'The experiments are only illustrative.': 1.0986123085021973, 'They don’t compare with other methods (such as an RBM or VAE) nor do they give any quantitative results.': 1.0976380109786987, 'We are left with eyeballing some images.': 1.0838291645050049, 'I have no idea whether what we see is impressive or not.': 1.097407579421997}"
34,https://openreview.net/forum?id=BJ5UeU9xx,"{'The paper presents a theoretically well motivated for visualizing what parts of the input feature map are responsible for the output decision.': 1.0612883567810059, 'The key insight is that features that maximally change the output and are simultaneously more unpredictable from other features are the most important ones.': 1.0986123085021973, 'Most previous work has focused on finding features that maximally change the output without accounting for their predictability from other features.': 1.0986106395721436, 'Authors build upon ideas presented in the work of Robnik-Šikonja & Kononenko (2008).': 1.0986123085021973, 'The results indicate that the proposed visualization mechanism based on modeling conditional distribution identifies more salient regions as compared to a mechanism based on modeling marginal distribution.': 1.035443663597107, 'I like that authors have presented visualization results for a single image across multiple networks and multiple classes.': 1.097085952758789, 'There results show that the proposed method indeed picks up on class-discriminative features.': 1.097634196281433, 'Authors have provided a link to visualizations for a random sample of images in a comment – I encourage the authors to include this in the appendix of the paper.': 1.0986123085021973, 'My one concern with the paper is – Zeiler et al., proposed a visualization method by greying small square regions in the image.': 1.0977191925048828, 'This is similar to computing the visualization using the marginal distribution.': 1.0985034704208374, 'Authors compute the marginal visualization using 10 samples, however in the limit of infinite samples the image region would be gray.': 1.0986121892929077, 'The conditional distribution is computed using a normal distribution that provides some regularization and therefore estimating the conditional and marginal distributions using 10 samples each is not justified.': 1.0986123085021973, 'I would like to see the comparison when grey image patches (akin to Zeiler et al.) are used for visualization against the approach based on the conditional distribution.': 0.9335917234420776, 'The authors propose a way to visualize which areas of an image provide mostly influence a certain DNN response mostly.': 1.0985904932022095, ""They apply some very elegant and convincing improvements to the basic method by Robnik-Sikonja and Konononko from 2008 to DNNs, thus improving it's analysis and making it usable for images and DNNs."": 1.0936279296875, 'The authors provide a very thorough analysis of their methods and show very convincing examples (which they however handpicked.': 1.098608374595642, 'It would be very nice to have maybe at least one figure showing the analysis on e.g. 24 random picks from ImageNet).': 0.4586198329925537, 'One thing I would like to see is how their method compares to some other methods they mention in the introduction (like gradient-based ones or deconvolution based ones).': 1.0985831022262573, 'They paper is very clearly written, all necessary details are given and the paper is very nice to read.': 1.0986117124557495, 'Alltogether: The problem of understanding how DNNs function and how they draw their conclusions is discussed a lot.': 1.0985887050628662, ""The author's method provides a clear contribution that can lead to further progress in this field (E.g. I like figure 8 showing how AlexNet, GoogLeNet and VGG differ in where they collect evidence from)."": 1.0986123085021973, 'I can think of several potential applications of the method and therefore consider it of high significance.': 1.0954995155334473, 'Update: The authors did a great job of adopting all of my suggestions.': 0.6116580367088318, 'Therefore I improve the rating from 8 to 9.': 1.0986123085021973, 'The authors of this work propose an interesting approach to visualizing the predictions made by a deep neural network.': 1.09860098361969, 'The manuscript is well written is provides good insight into the problem.': 1.0986123085021973, ""I also appreciate the application to medical images, as simply illustrating the point on ImageNet isn't interesting enough."": 1.0986028909683228, 'I do have some questions and comments.': 1.0986114740371704, '1.  As the authors correctly point out in 3.1, approximating the conditional probability of a feature x_i by the marginal distribution p(x_i) is not realistic. They advocate for translation invariance, i.e. the position of the pixel in the image shouldn\'t affect the probability, and suggest that the pixels appearance depends on the small neighborhood around it. However, it is well known that global context makes an big impact on the semantics of pixels. In ""Objects in Contexts"", authors show that a given neighborhood of pixels can take different semantic meanings based on the global context in the image. In the context of deep neural networks, works such as ""ParseNet"" also illustrate the importance of global context on the spatial label distribution. This does not necessarily invalidate this approach, but is a significant limitation. It would be great if the authors provided a modification to (4) and empirically verified the change.': 0.40913596749305725, '2. Figure 7 shows the distribution over top 3 predictions before and after softmax. It is expected that even fairly uniform distributions will transform toward delta functions after softmax normalization. Is there an additional insight here?': 0.9524886608123779, '4. Finally, in 4.1, the authors state that it takes 30 minutes to analyze a single image with GooLeNet on a GPU? Why is this so computationally expensive? Such complexity seems to make the algorithm impractical and analyzing datasets of statistical relevance seems prohibitive.': 1.0985623598098755}"
35,https://openreview.net/forum?id=BJ6oOfqge,"{'This paper presents a model for semi-supervised learning by encouraging feature invariance to stochastic perturbations of the network and/or inputs.': 1.0986123085021973, 'Two models are described:  One where an invariance term is applied between different instantiations of the model/input a single training step, and a second where invariance is applied to features for the same input point across training steps via a cumulative exponential averaging of the features.': 1.0986123085021973, 'These models evaluated using CIFAR-10 and SVHN, finding decent gains of similar amounts in each case.': 1.098608136177063, 'An additional application is also explored at the end, showing some tolerance to corrupted labels as well.': 1.0986123085021973, 'The authors also discuss recent work by Sajjadi &al that is very similar in spirit, which I think helps corroborate the findings here.': 1.0986123085021973, 'My largest critique is it would have been nice to see applications on larger datasets as well.': 1.0985913276672363, 'CIFAR and SVHN are fairly small test cases, though adequate for demonstration of the idea.': 1.0954492092132568, 'For cases of unlabelled data especially, it would be good to see tests with on the order of 1M+ data samples, with 1K-10K labeled, as this is a common case when labels are missing.': 1.0986119508743286, 'On a similar note, data augmentations are restricted to only translations and (for CIFAR) horizontal flips.': 1.0986121892929077, 'While ""standard,"" as the paper notes, more augmentations would have been interesting to see': 0.45839372277259827, 'particularly since the model is designed explicitly to take advantage of random sampling.': 0.45673635601997375, 'Some more details might also pop up, such as the one the paper mentions about handling horizontal flips in different ways between the two model variants.': 1.0986123085021973, 'Rather than restrict the system to a particular set of augmentations, I think it would be interesting to push it further, and see how its performance behaves over a larger array of augmentations and (even fewer) numbers of labels.': 1.0978577136993408, 'Overall, this seems like a simple approach that is getting decent results, though I would have liked to see more and larger experiments to get a better sense for its performance characteristics.': 1.098517656326294, 'Smaller comment: the paper mentions ""dark knowledge"" a couple times in explaining results, e.g. bottom of p.6.': 1.0985994338989258, 'This is OK for a motivation, but in analyzing the results I think it may be possible to have something more concrete.': 0.9943203330039978, 'For instance, the consistency term encourages feature invariance to the stochastic sampling more strongly than would a classification loss alone.': 1.0986119508743286, 'This work explores taking advantage of the stochasticity of neural network outputs under randomized augmentation and regularization techniques to provide targets for unlabeled data in a semi-supervised setting.': 0.6847738027572632, 'This is accomplished by either applying stochastic augmentation and regularization on a single image multiple times per epoch and encouraging the outputs to be similar (Π-model) or by keeping a weighted average of past epoch outputs and penalizing deviations of current network outputs from this running mean (temporal ensembling).': 1.0986117124557495, 'The core argument is that these approaches produce ensemble predictions which are likely more accurate than the current network and are thus good targets for unlabeled data.': 1.0984879732131958, 'Both approaches seem to work quite well on semi-supervised tasks and some results show that they are almost unbelievably robust to label noise.': 0.6385223865509033, 'The paper is clearly written and provides sufficient details to reproduce these results in addition to providing a public code base.': 0.024010729044675827, 'The core idea of the paper is quite interesting and seems to result in higher semi-supervised accuracy than prior work.': 0.4126065671443939, 'I also found the attention to and discussion of the effect of different choices of data augmentation to be useful.': 1.0961556434631348, 'I am a little surprised that a standard supervised network can achieve 30% accuracy on SVHN given 90% random training labels.': 1.0088099241256714, 'This would only give 19% correctly labeled data (9% by chance + 10% unaltered).': 1.0916787385940552, 'I suppose the other 81% would not provide a consistent training signal such that it is possible, but it does seem quite unintuitive.': 1.090579867362976, 'I tried to look through the github for this experiment but it does not seem to be included.': 1.0986123085021973, 'As for the resistance of Π-model and temporal ensembling to this label noise, I find that somewhat more believable given the large weights placed on the consistency constraint for this task.': 1.096952199935913, 'The authors should really include discussion of w(t) in the main paper.': 1.0986121892929077, 'Especially because the tremendous difference in w_max in the incorrect label tolerance experiment (10x for Π-model and 100x for temporal ensembling from the standard setting).': 1.0986123085021973, 'Could the authors comment towards the scalability for larger problems?': 1.0986123085021973, 'For ImageNet, you would need to store around 4.8 gigs for the temporal ensembling method or spend 2x as long training with Π-model.': 1.0985218286514282, 'Can the authors discuss sensitivity of this approach to the amount and location of dropout layers in the architecture?': 0.17046982049942017, 'Preliminary rating:': 1.0986123085021973, 'I think this is a very interesting paper with quality results and clear presentation.': 0.6569927930831909, 'Minor note:': 1.0852998495101929, ""2nd paragraph of page one 'without neither' -> 'without either'"": 1.030521273612976, 'This paper presents a semi-supervised technique for “self-ensembling” where the model uses a consensus prediction (computed from previous epochs) as a target to regress to, in addition to the usual supervised learning loss.': 1.0986123085021973, 'This has connections to the “dark knowledge” idea, ladder networks work is shown in this paper to be a promising technique for scenarios with few labeled examples (but not only).': 1.0986123085021973, 'The paper presents two versions of the idea: one which is computationally expensive (and high variance) in that it needs two passes through the same example at a given step, and a temporal ensembling method that is stabler, cheaper computationally but more memory hungry and requires an extra hyper-parameter.': 1.0986123085021973, 'My thoughts on this work are mostly positive.': 1.0948625802993774, 'The drawbacks that I see are that the temporal ensembling work requires potentially a lot of memory, and non-trivial infrastructure / book-keeping for imagenet-sized experiments.': 1.0986123085021973, 'I am quite confused by the Figure 2 / Section 3.4 experiments about tolerance to noisy labels: it’s *very* incredible to me that by making 90% of the labels random one can still train a classifier that is either 30% accurate or ~78% accurate (depending on whether or not temporal ensembling was used).': 1.0986123085021973, 'I don’t see how that can happen, basically.': 1.097914457321167, 'Minor stuff:': 1.0986123085021973, 'Please bold the best-in-category results in your tables.': 1.0986123085021973, 'I think it would be nice to talk about the ramp-up of w(t) in the main paper.': 1.0189868211746216, 'The authors should consider putting the state of the art results for the fully-supervised case in their tables, instead of just their own.': 0.5210593342781067, 'I am confused as to why the authors chose not to use more SVHN examples.': 1.0985116958618164, 'The stated reason that it’d be “too easy” seems a bit contrived: if they used all examples it would also make it easy to compare to previous work.': 0.9283080101013184}"
36,https://openreview.net/forum?id=BJ8fyHceg,"{'This paper suggests combining LSTMs, trained on a large midi corpus, with a handcrafted reward function that helps to fine-tune the model in a musically meaningful way.': 1.0986063480377197, 'The idea to use hand-crafted rewards in such a way is great and seems promising for practical scenarios, where a musician would like to design a set of rules, rather than a set of melodies.': 1.0986123085021973, ""Even though some choices made along the way seem rather ad-hoc and simplistic from a music theoretical perspective, the results sound like an improvement upon the note RNN baseline, but we also don't know how cherry picked these results are."": 1.0986123085021973, 'I am not convinced that this approach will scale to much more complicated reward functions necessary to compose real music.': 1.0986121892929077, 'Maybe LSTMs are the wrong approach altogether if they have so much trouble learning to produce pleasant melodies from such a relatively big corpus of data.': 1.0986123085021973, ""Aren't there any alternative differentiable models that are more suitable?"": 1.0986123085021973, 'What about dilated convolution based approaches?': 1.0986123085021973, ""What I don't like about the paper is that the short melodies are referenced as compositions while being very far from meaningful music, they are not even polyphonic after all."": 1.0986123085021973, 'I think it would be great if such papers would be written with the help or feedback of people that have real musical training and are more critical towards these details.': 1.0986123085021973, 'What I like about the paper is that the authors make an effort to understand what is going on, table 1 is interesting for instance.': 1.0986117124557495, 'However, Figure 3 should have included real melody excerpts with the same sound synthesis/sample setup.': 1.098610281944275, 'Besides that, more discussion on the shortcomings of the presented method should be added.': 0.9321343302726746, 'In summary, I do like the paper and idea': 1.088212490081787, 'and I can imagine that such RL based fine-tuning approaches will indeed be useful for musicians.': 0.15809795260429382, 'Even though the novelty might be limited, the paper serves as a documentation on how to achieve solid results in practice.': 1.0986123085021973, 'The authors propose a solution for the task of synthesizing melodies.': 0.9947078824043274, 'The authors claim that the ""language-model""-type approaches with LSTMs generate melodies with certain shortcomings.': 1.0986123085021973, 'They tend to lack long-range structure, to repeat notes etc.': 1.0985147953033447, 'To solve this problem the authors suggest that the model could be first trained as a pure LM-style LSTM and then trained with reinforcement learning to optimize an objective which includes some non-differentiable music-theory related constraints.': 1.0986123085021973, 'The reinforcement learning methodology is appropriate but straightforward and closely resembles previous work for text modeling and dialogue generation.': 1.0986123085021973, ""By itself the methodology doesn't offer a new technique."": 0.3988944888114929, ""To me, the paper's contribution then comes down to the novelty / utility / impact of the application."": 1.0986123085021973, 'The authors clearly put substantial of effort into crafting the rules and user study and that is commendable.': 1.0986123085021973, 'On the other hand, music itself is dealt with somewhat naively.': 1.0986123085021973, 'While the user study reflects hard work, it seems premature.': 1.098605751991272, 'The semi-plausible piano melodies here are only music in the way that LSTM Shakespeare passes as poetry.': 1.0986123085021973, ""So it's analogous to conducting a user study comparing LSTM Shakespeare to n-gram Shakespeare."": 1.098599910736084, ""I'd caution the author's against the uncritical motivation that a problem has previously been studied."": 1.0986123085021973, ""Research contains abundant dead ends (not to say this is necessarily one) and the burden to motivate research shouldn't be forgotten."": 1.0985480546951294, 'This is especially true when the application is the primary thrust of a paper.': 0.46932709217071533, 'Generally the authors should be careful about describing this model as ""composing"".': 1.0986123085021973, 'By analogy to a Shakespeare-LSTM, the language model is not really composing English prose.': 1.0986123085021973, 'The relationship between constructing a statistical sequence model and creating art - an activity that involves communication grounded in real-world semantics should not be overstated.': 0.3483925759792328, ""I appreciate the authors' efforts to respond to some criticisms of the problem setup and encourage them to anticipate these arguments in the paper and to better motivate the work in the future."": 1.0858372449874878, 'If the main contribution is the application (the methods have been used elsewhere), then the motivation is of central importance.': 1.0985801219940186, 'I also appreciate their contention that the field benefits from multiple datasets and not simply relying on language modeling.': 0.9018783569335938, 'Further, they are correct in asserting that MIDI can capture all the information in a score (not merely ""Gameboy music"", and that for some musics (e.g. European classical)': 1.098423957824707, 'the score is of central importance.': 1.0986123085021973, 'However, the authors may overstate the role of a score in jazz music.': 0.41406744718551636, ""Overall, for me, the application, while fun, doesn't add enough to the impact of the paper."": 0.9460074305534363, ""And the methodology, while appropriate, doesn't stand on its own."": 0.5807123184204102, 'Update': 1.0986123085021973, 'Thanks for your modifications and arguments.': 0.8525189161300659, ""I've revised my scores to add a point."": 0.6314904093742371, 'This paper uses a combination of likelihood and reward based learning to learn sequence models for music.': 1.0986123085021973, 'The ability to combine likelihood and reward based learning has been long known, as a result of the unification of inference and learning first appearing in the ML literature with the EM formalism of Attias (2003) for fixed horizons, extended by Toussaint and Storkey (2006), to general horizon settings, Toussaint et al. (2011) to POMDPs and generalised further by Kappen et Al.': 1.0986123085021973, '(2012) and Rawlik': 1.0986123085021973, 'et Al. (2012).': 1.0986123085021973, 'These papers introduced the basic unification, and so any additional probabilistic or data driven objective can be combined with the reinforcement learning signal: it is all part of a unified reward/likelihood.': 1.0986123085021973, 'Hence the optimal control target under unification is p(b=1|\\tau)E_p(A,S) \\prod_t \\pi(a_t|s_t): i.e. the probability of getting reward, and probability of the policy actions under the known data-derived distribution, thereby introducing the log p(a_t|s_t) into (9) too.': 1.0985984802246094, 'The interpretation of the secondary objective as the prior is an alternative approach under a stochastic optimal control setting, but not the most natural one given the whole principle of SOC of matching control objectives to inference objectives.': 1.0986123085021973, 'The SOC off policy objective still does still contain the KL term so the approach would still differ from the approach of this paper.': 1.0986123085021973, 'Though the discussion of optimal control is good, I think some further elaboration of the history and how reward augmentation can work in SOC would be valuable.': 1.0986123085021973, 'This would allow SOC off-policy methods to be compared with the DQN directly, like for like.': 1.0985500812530518, 'The motivation of the objective (3) is sensible but could be made clearer via the unification argument above.': 1.0986121892929077, 'Then the paper uses DCN to take a different approach from the variational SOC for achieving that objective.': 1.0986120700836182, 'Another interesting point of discussion is the choice of E_pi \\log p(a_t|s_t) – this means the policy must “cover” the model.': 1.0986108779907227, 'But one problem in generation is that a well-trained model is often underfit, resulting in actions that, over the course of a number of iterations, move the state into data-unsupported parts of the space.': 1.097072720527649, 'As a result the model is no longer confident and quickly tends to be fairly random.': 0.5688994526863098, 'This approach (as opposed to a KL(p||pi) – which is not obvious how to implement) cannot mitigate against that, without a very strong signal (to overcome the tails of a distribution).': 1.0625587701797485, 'In music, with a smaller discrete alphabet, this is likely to be less of a problem than for real valued policy densities, with exponentially decaying tails.': 1.0986123085021973, 'Some further discussion of what you see in light of this issue would be valuable: the use of c to balance things seems critical, and it seems clear from Figure 2 that the reward signal needed to be very high to push the log p signal into the right range.': 1.0986123085021973, 'Altogether, in the music setting this paper provides a reasonable demonstration that augmentation of a sequence model with an additional reward constraint is valuable.': 1.09861159324646, 'It demonstrates that DQN is one way of learning that signal, but AFAICS it does not compare learning the same signal via other techniques.': 0.6929682493209839, 'Instead for the comparator techniques it reverts to treating the p(a|s) as a “prior” term rather than a reward term, leaving a bit of a question as to whether DQN is particularly appropriate.': 0.38720351457595825, 'Another interesting question for the discussion is whether the music theory reward could be approximated by a differentiable model, mitigating the need for an RL approach at all.': 0.8223986625671387}"
37,https://openreview.net/forum?id=BJ9fZNqle,"{'The authors introduce some new prior and approximate posterior families for variational autoencoders, which are compatible with the reparameterization trick, as well as being capable of expressing multiple modes.': 1.0986123085021973, 'They also introduce a gating mechanism between prior and posterior.': 1.0986123085021973, 'They show improvements on bag of words document modeling, and dialogue response generation.': 1.0986123085021973, 'The original abstract is overly strong in its assertion that a unimodal latent prior p(z) cannot fit a multimodal marginal int_z p(x|z)p(x)dz with a DNN response model p(x|z) (""it cannot possibly capture more complex aspects of the data distribution"", ""critical restriction"", etc).': 1.0986123085021973, 'While the assertion that a unimodal latent prior is necessary to model multimodal observations is false, there are sensible motivations for the piecewise constant prior and posterior.': 1.0986123085021973, 'For example, if we think of a VAE as a sort of regularized autoencoder where codes are constrained to ""fill up"" parts of the prior latent space, then there is a sphere-packing argument to be made that filling a Gaussian prior with Gaussian posteriors is a bad use of code space.': 1.0986123085021973, ""Although the authors don't explore this much, a hypercube-based tiling of latent code space is a sensible idea."": 1.0984584093093872, 'As stated, I found the message of the paper to be quite sloppy with respect to the concept of ""multi-modality.""': 1.0801756381988525, 'There are 3 types of multimodality at play here: multimodality in the observed marginal distribution p(x), which can be captured by any deep latent Gaussian model, multimodality in the prior p(z), which makes sense in some situations (e.g. a model of MNIST digits could have 10 prior modes corresponding to latent codes for each digit class), and multimodality in the posterior z for a given observation x_i, q(z_i|x_i).': 0.388887494802475, 'The final type of multimodality is harder to argue for, except in so far as it allows the expression of flexibly shaped distributions without highly separated modes.': 0.5591890811920166, ""I believe flexible posterior approximations are important to enable fine-grained and efficient tiling of latent space, but I don't think these need to have multiple strong modes."": 0.42356958985328674, 'I would be interested to see experiments demonstrating otherwise for real world data.': 0.9688301086425781, 'I think this paper should be more clear about the different types of multi-modality and which parts of their analysis demonstrate which ones.': 0.9020649194717407, 'I also found it unsatisfactory that the piecewise variable analysis did not show different components of the multi-modal prior corresponding to different words, but rather just a separation between the Gaussian and the piecewise variables.': 0.6295607686042786, 'As I mention in my earlier questions, I found it surprising that the learned variance and mean for the Gaussian prior helps so dramatically with G-NVDM likelihood when the powerful networks transforming to and from latent space should make it scale-invariant.': 1.014871597290039, 'Explicitly separating out the contributions of a reimplemented base model, prior-posterior interpolation and the learned prior parameters would strengthen these experiments.': 1.098586916923523, 'Overall, the very strong improvements on the text modeling task over NVDM seem hard to understand, and I would like to see an ablation analysis of all the differences between that model and the proposed one.': 0.529350757598877, 'The fact that adding more constant components helps for document modeling is interesting, and it would be nice to see more qualitative analysis of what the prior modes represent.': 0.8721320629119873, 'I also would be surprised if posterior modes were highly separated, and if they were it would be interesting to explore if they corresponded to e.g. ambiguous word-senses.': 0.806659460067749, 'The experiments on dialog modeling are mostly negative results, quantitatively.': 1.0678188800811768, 'The observation that the the piecewise constant variables encode time-related words and the Gaussian variables encode sentiment is interesting, especially since it occurs in both sets of experiments.': 0.4976518750190735, 'This is actually quite interesting, and I would be interested in seeing analysis of why this is the case.': 1.0913310050964355, 'As above, I would like to see an analysis of the sorts of words that are encoded in the different prior modes and whether they correspond to e.g. groups of similar holidays or days.': 1.097144603729248, 'In conclusion, I think the piecewise constant variational family is a good idea, although it is not well-motivated by the paper.': 0.9021121263504028, 'The experimental results are very good for document modeling, but without ablation analysis against the baseline it is hard to see why they should be with such a small modification in G-NVDM.': 1.0679073333740234, 'The fact that H-NVDM performs better is interesting, though.': 0.977712869644165, 'This paper should better motivate the need for different types of multi-modality, and demonstrate that those sorts of things are actually being captured by the model.': 0.7391063570976257, 'As it is, the paper introduces an interesting variational family and shows that it performs better for some tasks, but the motivation and analysis is not clearly focused.': 0.4203222393989563, 'To demonstrate that this is a broadly applicable family, it would also be good to do experiments on a more standard datasets like MNIST.': 0.9387284517288208, 'Even without an absolute log-likelihood improvement, if the method yielded interpretable multiple modes this would be a valuable contribution.': 1.0986120700836182, 'This paper proposes a piecewise constant parameterisation for neural variational models so that it could explore the multi-modality of the latent variables and develop more powerful neural models.': 1.0986123085021973, 'The experiments of neural variational document models and variational hierarchical recurrent encoder-decoder models show that the introduction of the piecewise constant distribution helps achieve better perplexity on modelling documents and seemly better performance on modelling dialogues.': 1.0986123085021973, 'The idea of having a piecewise constant prior for latent variables is interesting, but the paper is not well-written (even 14 pages long) and the design of the experiments fails to demonstrate the most of the claims.': 1.0986123085021973, 'The detailed comments are as follows:': 1.0986123085021973, 'The author explains the limitations of the VAEs with standard Gaussian prior in the last paragraph of 3.1 and the last paragraph of 5.1.': 1.0986123085021973, 'Hence, a multimodal prior would help the VAEs overcome the issues of optimisation.': 1.0983837842941284, 'However, there is a lack of evidence showing the multimodality of the prior helps break the bottleneck.': 1.0986123085021973, 'In the last paragraph of 6.1, the author claimed the decoder parameter matrix is directly affected by the latent variables.': 1.0986123085021973, 'But what the connects the decoder is a combination of a piecewise constant and Gaussian latent variables.': 1.0986016988754272, 'No matter what is discovered in the experiments, it only shows z=<z_gaussian, z_piecewise> is multimodal.': 1.0986123085021973, 'However, z=<z_gaussian1, z_gaussian2> can be multimodal as well.': 1.0986123085021973, 'None of the claims in this paragraph stands.': 1.0583438873291016, 'In the quantitative evaluation of NVDM, there is an incremental model from z=z_gaussian to z=<z_gaussian, z_piecewise>.': 1.09861159324646, 'As the prior is learned together with the variational posterior, a more flexible prior would alleviate the regularisation imposed by the KL term.': 1.0986121892929077, 'Certainly, more parameters are applied as well, so a fair comparison would at least be z=<z_gaussian, z_piecewise> and z=<z_gaussian1, z_gaussian2> which equals to a double sized z_gaussian.': 1.098610520362854, 'The results shown in Table 3 are implausible.': 0.6138535737991333, 'I cannot believe the author used gradients to evaluate the model.': 1.0986121892929077, 'Eq. 5 is confusing, adding a multiplication sign might help.': 0.40315574407577515, '3.1 can be deleted because people attending ICLR are familiar with VAEs.': 1.0975525379180908, 'Typos:': 1.0985841751098633, 'as well as the well as the generated prior->  as well as the generated prior': 0.4533144235610962, ""UPDATE: I have read the authors' rebuttal and also the other comments in this paper's thread."": 1.0620348453521729, 'My thoughts have not changed.': 1.0986123085021973, 'The authors propose using a mixture prior rather than a uni-modal': 1.098569631576538, 'prior for variational auto-encoders.': 0.4073342978954315, 'They argue that the simple': 1.0966402292251587, 'uni-modal prior ""hinders the overall expressivity of the learned model': 0.41864097118377686, 'as it cannot possibly capture more complex aspects of the data': 1.0986123085021973, 'distribution.""': 1.0338200330734253, 'I find the motivation of the paper suspicious because while the prior': 1.0984214544296265, 'may be uni-modal, the posterior distribution is certainly not.': 0.19922061264514923, 'Furthermore, a uni-modal distribution on the latent variable space can': 0.7195479869842529, 'certainly still lead to the capturing of complex, multi-modal data': 0.6994403600692749, 'distributions.': 1.0981190204620361, '(As the most trivial case, take the latent variable': 1.0986123085021973, 'space to be a uniform distribution; take the likelihood to be a': 1.0986123085021973, ""point mass given by applying the true data distribution's inverse CDF"": 1.0986123085021973, 'to the uniform.': 1.0986123085021973, 'Such a model can capture any distribution.)': 1.0986123085021973, 'In addition, multi-modality is arguably an overfocused concept in the': 1.0986123085021973, 'literature, where the (latent variable) space is hardly anymore worth': 1.0986123085021973, 'capturing from a mixture of simple distributions when it is often a': 1.0986123085021973, 'complex nonlinear space.': 1.0986123085021973, 'It is unclear from the experiments how much': 1.0986123085021973, ""the influence of the prior's multimodality influences the posterior to"": 1.0986123085021973, 'capture more complex phenomena, and whether this is any better than': 1.0986123085021973, 'considering a more complex (but still reparameterizable) distribution': 1.0986123085021973, 'on the latent space.': 1.0986123085021973, 'I recommend that this paper be rejected, and encourage the authors to': 1.0986123085021973, 'more extensively study the effect of different priors.': 1.0986123085021973, ""I'd also like to make two additional comments:"": 1.0986123085021973, 'While there is no length restriction at ICLR, the 14 page document can': 1.0986123085021973, 'be significantly condensed without loss of describing their innovation': 1.0986123085021973, 'or clarity.': 1.0986123085021973, 'I recommend the authors do so.': 1.0986123085021973, ""Finally, I think it's important to note the controversy in this paper."": 1.0986123085021973, 'It was submitted with many significant incomplete details (e.g., no experiments,': 1.0986123085021973, 'many missing citations, a figure placed inside that was pencilled in': 1.0986123085021973, 'by hand, and several missing paragraphs).': 1.0986123085021973, 'These details were not': 1.0986123085021973, 'completed until roughly a week(?) later.': 1.0986123085021973, 'I recommend the chairs discuss': 1.0986123085021973, 'this in light of what should be allowed next year.': 1.0986123085021973}"
38,https://openreview.net/forum?id=BJAA4wKxg,"{""The system described works comparably to bi-directional LSTM baseline for NMT, and CNN's are naturally parallelizable."": 1.0986123085021973, ""Key ideas include the use of two stacked CNN's (one for each of encoding and decoding) for translation, with res connections and position embeddings."": 1.0986123085021973, ""The use of CNN's for translation has been attempted previously (as described by the authors), but presumably it is the authors' combination of various architectural choices (attention, position embeddings, etc) that make the present system competitive with RNN's, whereas earlier attempts were not."": 1.0986123085021973, ""They describe system's sensitivity to some of these choices (e.g. experiments to choose appropriate number of layers in each of the CNN's)."": 1.0985292196273804, 'The experimental results are well reported in detail.': 1.0986123085021973, 'One or two figures would definitely be required to help clarify the architecture.': 0.4572686553001404, 'This paper is less about new ways of learning representations than about the combination of choices made (over the set of existing techniques) in order to get the good results that they do on the reported NMT tasks.': 1.0986121892929077, 'In this respect, while I am fairly confident that the paper represents good work in machine learning, I am not quite as confident about its fit for this particular conference.': 1.0986123085021973, 'This paper is the first (I believe) to establish a simple yet important result that Convnets for NMT encoders can be competitive to RNNs.': 1.0986123085021973, 'The authors present a convincing set of results over many translation tasks and compare with very competitive baselines.': 1.0986123085021973, 'I also appreciate the detailed report on training and generation speed.': 1.0986123085021973, ""I find it's very interesting when position embeddings turn out to be hugely important (beside residual connections); unfortunately, there is little analysis to shed more lights on this aspect and perhaps compare other ways of capturing positions (a wild guess might be to use embeddings that represent some form of relative positions)."": 1.0986123085021973, 'The only concern I have (similar to the other reviewer) is that this paper perhaps fits better in an NLP conference.': 1.0985846519470215, ""One minor comment: it's slight strange that this well-executed paper doesn't have a single figure on the proposed architecture :)"": 1.0986123085021973, 'It will also be even better to draw a figure for the biLSTM architecture as well (it does take some effort to understand the last paragraph in Section 2, especially the part on having a linear layer to compute z).': 1.0985190868377686, 'The paper reports a very clear and easy to understand result that a convolutional network can be used instead of the recurrent encoder for neural machine translation.': 1.0986123085021973, 'Apart from the known architectural elements, such as convolution, pooling, residual connections, position embeddings, the paper features one unexpected architectural twist: two stacks of convolutions, one for computing alignment and another for computing the representations.': 1.0986121892929077, 'The empirical evidence that this was necessary is provided, however the question of *why* it is necessary remains open.': 1.098516821861267, 'The experimental evaluation is very extensive and leaves no doubt that the proposed approach works well.': 1.0986123085021973, 'The convnet-based model was faster at evaluation, but it is not very clear what is the main speed-up factor.': 1.0986123085021973, 'It’s however hard to argue against the fact that the speed advantage of convnets is likely to increase if a more parallel implementation is considered.': 1.0986123085021973, 'My main concern is whether or not the paper is appropriate for ICLR, because the contribution is quite incremental and rather application-specific.': 1.0986123085021973, 'ACL, EMNLP and other NLP conferences would be a better venue, I think.': 1.0986123085021973}"
39,https://openreview.net/forum?id=BJAFbaolg,"{'This paper trains a generative model which transforms noise into model samples by a gradual denoising process.': 1.0986121892929077, 'It is similar to a generative model based on diffusion.': 1.0959972143173218, 'Unlike the diffusion approach:': 1.0677077770233154, 'It uses only a small number of denoising steps, and is thus far more computationally efficient.': 1.0838451385498047, 'Rather than consisting of a reverse trajectory, the conditional chain for the approximate posterior jumps to q(z(0) | x), and then runs in the same direction as the generative model.': 1.0986121892929077, 'This allows the inference chain to behave like a perturbation around the generative model, that pulls it towards the data.': 1.0986123085021973, '(This also seems somewhat related to ladder networks.)': 1.029958963394165, 'There is no tractable variational bound on the log likelihood.': 1.0986123085021973, 'I liked the idea, and found the visual sample quality given a short chain impressive.': 1.0986123085021973, 'The inpainting results were particularly nice, since one shot inpainting is not possible under most generative modeling frameworks.': 1.0986120700836182, ""It would be much more convincing to have a log likelihood comparison that doesn't depend on Parzen likelihoods."": 1.0984501838684082, 'Detailed comments follow:': 1.088789701461792, 'Sec. 2:': 1.0986123085021973, '""theta(0) the"" -> ""theta(0) be the""': 1.0986111164093018, '""theta(t) the"" -> ""theta(t) be the""': 1.0985565185546875, '""what we will be using"" -> ""which we will be doing""': 1.098568081855774, 'I like that you infer q(z^0|x), and then run inference in the same order as the generative chain.': 1.0986117124557495, 'This reminds me slightly of ladder networks.': 1.0711555480957031, '""q*.': 1.0986123085021973, 'Having learned"" -> ""q*.': 1.098609209060669, '[paragraph break] Having learned""': 0.30569595098495483, 'Sec 3.3:': 1.093576192855835, '""learn to inverse"" -> ""learn to reverse""': 1.098007082939148, 'Sec. 4:': 1.0957034826278687, '""For each experiments"" -> ""For each experiment""': 1.0986121892929077, 'How sensitive are your results to infusion rate?': 1.0986056327819824, 'Sec. 5: ""appears to provide more accurate models"" I don\'t think you showed this': 1.0980809926986694, ""there's no direct comparison to the Sohl-Dickstein paper."": 0.6991754770278931, 'Fig 4.': 1.0986123085021973, 'neat!': 1.0986123085021973, 'The paper presents a method for training a generative model via an iterative denoising procedure.': 1.0986123085021973, 'The denoising process is initialized with a random sample from a crude approximation to the data distribution and produces a high quality sample via multiple denoising steps.': 1.0986123085021973, 'Training is performed by setting-up a Markov chain that slowly blends propositions from the current denoising model with a real example from the data distribution; using this chain the current denoising model is updated towards reproducing the changed, ""better"", samples from the blending process.': 1.0986123085021973, 'This is a clearly written paper that considers an interesting approach for training generative models.': 1.0986123085021973, 'I was intrigued by the simplicity of the presented approach and really enjoyed reading the paper.': 1.0846329927444458, 'The proposed method is novel although it has clear ties to other recent work aiming to use denoising models for sampling from distributions such as the work by Sohl-Dickstein and the recent work on using DAEs as generative models.': 1.0986123085021973, 'I think this general direction of research is important.': 1.0986121892929077, 'The proposed procedure takes inspiration from the perspective of generating samples by minimizing an energy function via transitions along a Markov chain and, if successful, it can potentially sidestep many problems of current procedures for training directed generative models such as:': 1.0986123085021973, 'convergence and mode coverage problems as in generative adversarial networks': 0.6043327450752258, 'problems with modeling multi-modal distributions which can arise when a too restrictive approximate inference model is paired with a powerful generative model': 1.0809766054153442, 'That being said, another method that seems promising for addressing these issues that also has some superficially similarity to the presented work is the idea of combining Hamiltonian Monte Carlo inference with variational inference as in [1].': 1.0685374736785889, 'As such I am not entirely convinced that the method presented here will be able to perform better than the mentioned paper; although it might be simpler to train.': 1.0961339473724365, ""Similarly, although I agree that using a MCMC chain to generate samples via a MC-EM like procedure is likely very costly I am not convinced such a procedure won't at least also work reasonably well for the simple MNIST example."": 1.0985931158065796, 'In general a more direct comparison between different inference methods using an MCMC chain like procedure would be nice to have but I understand that this is perhaps out of the scope of this paper.': 1.098503589630127, 'One thing that I would have expected, however, is a direct comparison to the procedure from Sohl-Dickstein in terms of sampling steps and generation quality as it is so directly related.': 1.097374439239502, 'Other major points (good and bad):': 0.4104147255420685, 'Although in general the method is explained well some training details are missing.': 1.0981100797653198, 'Most importantly it is never mentioned how alpha or omega are set (I am assuming omega is 0.01 as that is the increase mentioned in the experimental setup).': 0.48926278948783875, 'It is also unclear how alpha affects the capabilities of the generator.': 1.0628671646118164, ""While it intuitively seems reasonable to use a small alpha over many steps to ensure slow blending of the two distributions it is not clear how necessary this is or at what point the procedure would break (I assume alpha = 1 won't work as the generator then would have to magically denoise a sample from the relatively uninformative draw from p0 ?)."": 1.0811240673065186, 'The authors do mention in one of the figure captions that the denoising model does not produce good samples in only 1-2 steps but that might also be an artifact of training the model with small alpha (at least I see no a priori reason for this).': 1.0964503288269043, 'More experiments should be carried out here.': 1.094597578048706, 'No infusion chains or generating chains are shown for any of the more complicated data distributions, this is unfortunate as I feel these would be interesting to look at.': 1.095887541770935, 'The paper does a good job at evaluating the model with respect to several different metrics.': 1.0938243865966797, 'The bound on the log-likelihood is nice to have as well!': 0.8155838251113892, 'Unfortunately the current approach does not come with any theoretical guarantees.': 0.3397732079029083, 'It is unclear for what choices of alpha the procedure will work and whether there is some deeper connection to MCMC sampling or energy based models.': 1.0986123085021973, 'In my eyes this does not subtract from the value of the paper but would perhaps be worth a short sentence in the conclusion.': 1.0986123085021973, 'Minor points:': 1.0986123085021973, 'The second reference seems broken': 1.0986123085021973, 'Figure 3 starts at 100 epochs and, as a result, contains little information.': 1.0632163286209106, 'Perhaps it would be more useful to show the complete training procedure and put the x-axis on a log-scale ?': 1.0961346626281738, 'The explanation regarding the convolutional networks you use makes no sense to me.': 0.7695587873458862, 'You write that you use the same structure as in the ""Improved GANs"" paper which, unlike your model, generates samples from a fixed length random input.': 1.0984545946121216, ""I thus suppose you don't really use a generator with 1 fully connected network followed by up-convolutions but rather have several stages of convolutions followed by a fully connected layer and then up-convolutions ?"": 1.084083080291748, 'The choice of parametrizing the variance via a sigmoid output unit is somewhat unusual, was there a specific reason for this choice ?': 0.6645562648773193, 'footnote 1 contains errors: ""This allow to"" -> ""allows to"",  ""few informations"" -> ""little information"".': 0.9225908517837524, '""This force the network"" -> ""forces""': 1.0986123085021973, 'Page 1 error: etc...': 1.0967530012130737, 'Page 4 error: ""operator should to learn""': 1.0157734155654907, '[1] Markov Chain Monte Carlo and Variational Inference: Bridging the Gap, Tim Salimans and Diedrik P. Kingma and Max Welling, ICML 2015': 0.4183270335197449, '>>> Update <<<<': 1.0986123085021973, 'Copied here from my response below:': 1.0984455347061157, 'I believe the response of the authors clarifies all open issues.': 0.9222850799560547, 'I strongly believe the paper should be accepted to the conference.': 0.7529380321502686, 'The only remaining issue I have with the paper is that, as the authors acknowledge the architecture of the generator is likely highly sub-optimal and might hamper the performance of the method in the evaluation.': 1.0060468912124634, 'This however does not at all subtract from any of the main points of the paper.': 0.5551607608795166, 'I am thus keeping my score as a clear accept.': 1.0652693510055542, 'I want to emphasize that I believe the paper should be published (just in case the review process results in some form of cut-off threshold that is high due to overall ""inflated"" review scores).': 0.7401416301727295, 'Summary:': 1.0982849597930908, 'This paper introduces a heuristic approach for training a deep directed generative model, where similar to the transition operator of a Markov chain each layer samples from the same conditional distribution.': 1.0986121892929077, 'Similar to optimizing a variational lower bound, the approach is to approximate the gradient by replacing the posterior over latents with an alternative distribution.': 0.3960002064704895, 'However, the approximating distribution is not updated to improve the lower bound but heuristically constructed in each step.': 1.0985921621322632, 'A further difference to variational optimization is that the conditional distributions are optimized greedily rather than following the gradient of the joint log-likelihood.': 1.0986123085021973, 'Review:': 1.0986100435256958, 'The proposed approach is interesting and to me seems worth exploring more.': 0.6371416449546814, 'Given that there are approaches for training the same class of models which are 1) theoretically more sound, 2) of similar computational complexity, and 3) work well in practice (e.g. Rezende & Mohamed, 2015), I am nevertheless not sure of its potential to generate impact.': 1.0986104011535645, 'My bigger concern, however, is that the empirical evaluation is still quite limited.': 1.0986088514328003, 'I appreciate the authors included proper estimates of the log-likelihood.': 1.0986123085021973, 'This will enable and encourage future comparisons with this method on continuous MNIST.': 1.0986123085021973, 'However, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE.': 1.0986123085021973, '(From the paper: “Therefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.”': 1.0986123085021973, '“Such observation models can easily achieve much higher log-likelihood scores, […].”)': 1.0986123085021973, 'Comparisons with inpainting results using other methods would have been nice.': 1.0986123085021973, 'How practical is the proposed approach compared to other approaches?': 1.0986123085021973, 'Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed approach seems to be both efficient and effective for inpainting.': 1.0986120700836182, 'Not making this a bigger point and performing the proper evaluations seems like a missed opportunity.': 1.0986123085021973, 'Minor:': 1.0986123085021973, '– I am missing citations for “ordered visible dimension sampling”': 1.0986123085021973, '– Typos and frequent incorrect use of \\citet and \\citep': 1.0986123085021973}"
40,https://openreview.net/forum?id=BJC8LF9ex,"{'This paper presents a modified gated RNN caled GRU-D that deals with time series which display a lot of missing values in their input.': 1.0986123085021973, 'They work on two fronts.': 1.0986123085021973, 'The first deals with the missing inputs directly by using a learned convex combination of the previous available value (forward imputation) and the mean value (mean imputation).': 1.0986123085021973, 'The second includes dampening the recurrent layer not unlike a second reset gate, but parametrized according to the time elapsed since the last available value of each attributes.': 1.0986123085021973, 'Positives': 1.0986123085021973, 'Clear definition of the task (handling missing values for classification of time series)': 1.0986123085021973, 'Many interesting baselines to test the new model against.': 1.0986123085021973, 'The model presented deals with the missing values in a novel, ML-type way (learn new dampening parameters).': 1.0986123085021973, 'The extensive tests done on the datasets is probably the greatest asset of this paper.': 1.0986123085021973, 'Negatives': 1.0986123085021973, 'The paper could use some double checking for typos.': 1.0986123085021973, 'The Section A.2.3 really belongs in the main article as it deals with important related works.': 1.0986123085021973, 'Swap it with the imprecise diagrams of the model if you need space.': 1.0041414499282837, 'No mention of any methods from the statistics litterature.': 1.0974023342132568, 'Here are the two main points of this review that informs my decision:': 1.0977327823638916, '1. The results, while promising, are below expectations. The paper hasn’t been able to convince me that GRU-simple (without intervals) isn’t just as well-suited for the task of handling missing inputs as GRU-D. In the main paper, GRU-simple is presented as the main baseline. Yet, it includes a lot of extraneous parameters (the intervals) that, according to Table 5, probably hurts the model more than it helps it. Having a third of it’s parameters being of dubious value, it brings the question of the fairness of the comparison done in the main paper, especially since in the one table where GRU-simple (without intervals) is present, GRU-D doesn’t significantly outperforms it.': 1.0639463663101196, '2. My second concern, and biggest, is with some claims that are peppered through the paper. The first is about the relationship with the presence rate of data in the dataset and the diagnostics. I might be wrong, but that only indicates that the doctor in charge of that patient requested the relevant analyses be done according to the patient’s condition. That would mean that an expert system based on this data would always seem to be one step behind.': 1.0969218015670776, 'The second claim is the last sentence of the introduction, which sets huge expectations that were not met by the paper.': 1.0847759246826172, 'Another is that “simply concatenating masking and time interval vectors fails to exploit the temporal structure of missing values” is unsubstantiated and actually disproven later in the paper.': 1.0858365297317505, 'Yet another is the conclusion that since GRU models displayed the best improvement between a subsample of the dataset and the whole of it means that the improvement is going to continue to grow as more data is added.': 0.8380861878395081, 'This fails to consider that non-GRU models actually started with much better results than most GRU ones.': 1.0986123085021973, 'Lastly is their claim to capture informative missingness by incorporating masking and time intervals directly inside the GRU architecture.': 1.0986123085021973, 'While the authors did make these changes, the fact that they also concatenate the mask to the input, just like GRU-simple (without intervals), leads me to question the actual improvement made by GRU-D.': 1.0986123085021973, 'Given that, while I find that the work that has been put into the paper is above average, I wouldn’t accept that paper without a reframing of the findings and a better focus on the real contribution of this paper, which I believe is the novel way to parametrize the choice of imputation method.': 1.0986123085021973, 'This paper proposed a way to deal with supervised multivariate time series tasks involving missing values.': 1.0986123085021973, 'The high level idea is still using the recurrent neural network (specifically, GRU in this paper) to do sequence supervised learning, e.g., classification, but modifications have been made to the input and hidden layers of RNNs to tackle the missing value problem.': 1.0986123085021973, 'pros:': 1.0986123085021973, '1) the insight of utilizing missing value is critical.': 1.0986123085021973, 'the observation of decaying effect in the healthcare application is also interesting;': 1.0986123085021973, '2) the experiment seems to be solid; the baseline algorithms and analysis of results are also done properly.': 1.0986123085021973, 'cons:': 1.0986123085021973, '1) the novelty of this work is not enough.': 1.0986123085021973, 'Adding a decaying smooth factor to input and hidden layers seems to be the main modification of the architecture.': 1.0986123085021973, '2) the datasets used in this paper are small.': 1.0986123085021973, '3) the decaying effect might not be able to generalize to other domains.': 1.0986123085021973, 'The authors propose a RNN-method for time-series classification with missing values, that can make use of potential information in missing values.': 1.0986123085021973, 'It is based on a simple linear imputation of missing values with learnable parameters.': 1.0986123085021973, 'Furthermore, time-intervals between missing values are computed and used to scale the RNN computation downstream.': 1.0986123085021973, 'The authors demonstrate that their method outperforms reasonable baselines on (small to mid-sized) real world datasets.': 1.0986123085021973, 'The paper is clearly written.': 1.0986123085021973, 'IMO the authors propose a reasonable approach for dealing with missing values for their intended application domain, where data is not abundant and requires smallish models.': 1.0986123085021973, 'I’m somewhat sceptical if the benefits would carry over to big datasets, where more general, less handcrafted multi-layer RNNs are an option.': 1.0986123085021973}"
41,https://openreview.net/forum?id=BJC_jUqxe,"{'I like the idea in this paper that use not just one but multiple attentional vectors to extract multiple representations for a sentence.': 1.0978190898895264, 'The authors have demonstrated consistent gains across three different tasks Age, Yelp, & SNLI.': 1.097352147102356, ""However, I'd like to see more analysis on the 2D representations (as concerned by another reviewer) to be convinced."": 1.094017744064331, 'Specifically, r=30 seems to be a pretty large value when applying to short sentences like tweets or those in the SNLI dataset.': 1.0731620788574219, ""I'd like to see the effect of varying r from small to large value."": 1.0870007276535034, 'With large r value, I suspect your models might have an advantage in having a much larger number of parameters (specifically in the supervised components) compare to other models.': 1.0986003875732422, 'To make it transparent, the model sizes should be reported.': 0.9005547165870667, ""I'd also like to see performances on the dev sets or learning curves."": 1.07896089553833, 'In the conclusion, the authors remark that ""attention mechanism reliefs the burden of LSTM"".': 1.0970906019210815, ""If the 2D representations are effective in that aspect, I'd expect that the authors might be able to train with a smaller LSTM."": 0.9023306965827942, 'Testing the effect of LSTM dimension vs  will be helpful.': 0.9942364692687988, 'Lastly, there is a problem in the presentation of the paper in which there is no training objective defined.': 0.9598991274833679, 'Readers have to read until the experimental sections to guess that the authors perform supervised learning and back-prop through the self-attention mechanism as well as the LSTM.': 1.0986088514328003, '* Minor comments:': 1.0986123085021973, 'Typos: netowkrs, toghter, performd': 1.0986121892929077, 'Missing year for the citation of (Margarit & Subramaniam)': 1.0976207256317139, 'In figure 3, attention plotswith and without penalization look similar.': 1.0985993146896362, 'This paper proposes a method for representing sentences as a 2d matrix by utilizing a self-attentive mechanism on the hidden states of a bi-directional LSTM encoder.': 1.0986123085021973, 'This work differs from prior work mainly in the 2d structure of embedding, which the authors use to produce heat-map visualizations of input sentences and to generate good performance on several downstream tasks.': 1.0986113548278809, 'There is a substantial amount of prior work which the authors do not appropriately address, some of which is listed in previous comments.': 1.0986086130142212, 'The main novelty of this work is in the 2d structure of embeddings, and as such, I would have liked to see this structure investigated in much more depth.': 1.0986039638519287, 'Specifically, a couple important relevant experiments would have been:': 1.0986123085021973, '* How do the performance and visualizations change as the number of attention vectors (r) varies?': 0.6479918956756592, '* For a fixed parameter budget, how important is using multiple attention vectors versus, say, using a larger hidden state or embedding size?': 0.8950921893119812, 'I would recommend changing some of the presentation in the penalization term section.': 1.0836966037750244, 'Specifically, the statement that ""the best way to evaluate the diversity is definitely the Kullback Leibler divergence between any 2 of the summation weight vectors"" runs somewhat counter to the authors\' comments about this topic below.': 1.0986119508743286, 'In Fig.': 0.6671864986419678, '(2), I did not find the visualizations to provide particularly compelling evidence that the multiple attention vectors were doing much of interest beyond a single attention vector, even with penalization.': 1.0982656478881836, 'To me this seems like a necessary component to support the main claims of this paper.': 1.0983905792236328, ""Overall, while I found the architecture interesting, I am not convinced that the model's main innovation"": 0.9203959703445435, 'the 2d structure of the embedding matrix': 1.0986123085021973, 'is actually doing anything important or meaningful beyond what is being accomplished by similar attentive embedding models already present in the literature.': 1.0926384925842285, 'Further experiments demonstrating this effect would be necessary for me to give this paper my full endorsement.': 1.093414545059204, 'This paper introduces a sentence encoding model (for use within larger text understanding models) that can extract a matrix-valued sentence representation by way of within-sentence attention.': 1.0986123085021973, 'The new model lends itself to (slightly) more informative visualizations than could be gotten otherwise, and beats reasonable baselines on three datasets.': 1.0986120700836182, 'The paper is reasonably clear, I see no major technical issues, and the proposed model is novel and effective.': 1.09856379032135, 'It could plausibly be relevant to sequence modeling tasks beyond NLP.': 1.0986123085021973, 'I recommend acceptance.': 1.0986120700836182, ""There is one fairly serious writing issue that I'd like to see fixed, though: The abstract, introduction, and related work sections are all heavily skewed towards unsupervised learning."": 1.0986037254333496, ""The paper doesn't appear to be doing unsupervised learning, and the ideas are no more nor less suited to unsupervised learning than any other mainstream ideas in the sentence encoding literature."": 1.09859299659729, 'Details:': 1.0986123085021973, 'You should be clearer about how you expect these embeddings to be used, since that will be of certain interest to anyone attempting to use the results of this work.': 1.0986071825027466, 'In particular, how you should convert the matrix representation into a vector for downstream tasks that require one.': 1.0985963344573975, 'Some of the content of your reply to my comment could be reasonably added to the paper.': 1.0982434749603271, 'A graphical representation of the structure of the model would be helpful.': 1.0824482440948486, ""The LSTMN (Cheng et al., EMNLP '16) is similar enough to this work that an explicit comparison would be helpful."": 0.9172890782356262, 'Again, incorporating your reply to my comment into the paper would be more than adequate.': 1.0985370874404907, ""Jiwei Li et al. (Visualizing and Understanding Neural Models in NLP, NAACL '15) present an alternative way of visualizing the influence of words on sentence encodings without using cross-sentence attention."": 1.0986121892929077, 'A brief explicit comparison would be nice here.': 1.0985645055770874}"
42,https://openreview.net/forum?id=BJFG8Yqxl,"{'The paper proposes the group sparse autoencoder that enforces sparsity of the hidden representation group-wise, where the group is formed based on labels (i.e., supervision).': 1.0986121892929077, 'The p-th group hidden representation is used for reconstruction with group sparsity penalty, allowing learning more discriminative, class-specific patterns in the dataset.': 1.0986007452011108, 'The paper also propose to combine both group-level and individual level sparsity as in Equation (9).': 1.0986123085021973, 'Clarity of the paper is a bit low.': 1.0986123085021973, ""Do you use only p-th group's activation for reconstruction?"": 1.0986123085021973, 'If it is true, then for Equation (9) do you use all individual hidden representation for reconstruction or still using the subset of representation corresponding to that class only?': 1.0986123085021973, 'In Equation (7), RHS misses the summation over p, and wondering it is a simple typo.': 1.0986123085021973, 'Is the algorithm end-to-end trainable?': 1.0986123085021973, 'It seems to me that the group sparse CNN is no more than the GSA whose input data is the feature extracted from sequential CNNs (or any other pretrained CNNs).': 1.0986123085021973, 'Other comments are as follows:': 1.0986123085021973, 'Furthermore the group sparse autoencoder is (semi-) supervised method since it uses label information to form a group, whereas the standard sparse autoencoder is fully unsupervised.': 1.0986119508743286, ""That being said, it is not surprising that group sparse autoencoder learns more class-specific pattern whereas sparse autoencoder doesn't."": 1.0986123085021973, 'I think the fair comparison should be to autoencoders that combines classification for their objective function.': 1.0986123085021973, 'Although authors claim that GSA learns more group-relevant features, Figure 3 (b) is not convincing enough to support this claim.': 1.0986123085021973, ""For example, the first row contains many filters that doesn't look like 1 (e.g., very last column looks like 3)."": 1.0986123085021973, 'Other than visual inspection, do you observe improvement in classification using proposed algorithm on MNIST experiments?': 1.0986123085021973, 'The comparison to the baseline model is missing.': 1.0908440351486206, ""I believe the baseline model shouldn't be the sequential CNN, but the sequential CNN + sparse autoencoder."": 1.0986123085021973, 'In addition, more control experiment is required that compares between the Equation (7)-(9), with different values of \\alpha and \\beta.': 1.098611831665039, 'Missing reference:': 1.0986123085021973, 'Shang et al., Discriminative Training of Structured Dictionaries via Block Orthogonal Matching Pursuit, SDM 2016 - they consider block orthgonal matching pursuit for dictionary learning whose blocks (i.e., projection matrices) are constructed based on the class labels for discirminative training.': 0.7078145146369934, 'This paper proposed the group sparse auto-encoder for feature extraction.': 1.0781042575836182, 'The author then stack the group sparse auto-encoders on top of CNNs to extract better question sentence representation for QA tasks.': 1.0981154441833496, 'Pros:': 1.0986123085021973, 'group-sparse auto-encoder seems new to me.': 1.0986123085021973, 'extensive experiments on QA tasks.': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'The idea is somewhat incremental.': 1.0986123085021973, 'Writing need to be improved.': 1.0986123085021973, 'Lack of ablation studies to show the effectiveness of the proposed approach.': 1.0986123085021973, ""Moreover, I am not convinced by the author's answer regarding the baseline."": 1.0986123085021973, 'A separate training stages of CNN+SGL for comparison is fine.': 1.0986123085021973, 'The purpose is to validate and analyze why the proposed SGA is preferred rather than group lasso, e.g. joint training could improve, or the proposed group-sparse regularization outperforms l_21 norm, etc.': 1.0986123085021973, ""However, we can't see it from the current experiments."": 1.0986123085021973, 'This paper propose to classify questions by leveraging corresponding answers.': 1.0986123085021973, 'The proposed method uses group sparse autoencoders to model question groups.': 1.0986123085021973, 'The proposed method offers improved accuracy over baselines.': 1.0986123085021973, 'But the baseline used is a little stale.': 1.0986123085021973, 'Would be interesting to see how it compares to more recent CNN and RNN based methods.': 1.0986123085021973, 'It would also be interesting to see the contribution of each components.': 1.0986123085021973, 'For example, how much GSA contributed to the improvement.': 1.0986123085021973}"
43,https://openreview.net/forum?id=BJK3Xasel,"{'This paper proposes a nonparametric neural network model, which automatically learns the size of the model during the training process.': 1.0986123085021973, 'The key idea is to randomly add zero units and use sparse regularizer to automatically null out the weights that are irrelevant.': 1.0986123085021973, 'The idea sounds to be a random search approach over discrete space with the help of sparse regularization to eliminate useless units.': 1.0986123085021973, 'This is an important problem and the paper gives interesting results.': 1.0982235670089722, 'My main comments are listed below:': 0.43424028158187866, 'What is the additional computation complexity of the algorithm?': 0.4768694341182709, 'The decomposition of each fan-in weights into a parallel component and an orthogonal component and the transformation into radial-angular coordinates may require a lot of extra computation time.': 1.0986123085021973, 'The authors may need to discuss the extra amount of operations relative to the parametric neural network.': 1.0986123085021973, 'Furthermore, it would be useful to show some running time experiments.': 1.0986123085021973, 'It is observed that nonparametric networks return small networks on the convex dataset so that it is inferior to parametric networks.': 1.0986123085021973, 'Any insight on this?': 1.097921371459961, 'I agree with reviewer 2 on the interesting part of the paper.': 0.45696309208869934, 'The idea of removing or adding units is definitely an interesting direction, that will make a model grow or shrink along the lines required by the problem and the data, not the user prior knowledge.': 1.0986119508743286, 'The authors offer an interesting theoretical result that proves that under fan out or fan in regularization the optimum of the error function is achieved for finite number of parameters - so the net does not grow indefinitely, until it over-fits perfectly the data.': 1.0986123085021973, 'That reminds me of more traditional approaches such as Lasso or Elastic Net, in which the regularization produces sparse weights.': 1.0985491275787354, 'I would  have like more intuition to be given for this theorem.': 0.6277260184288025, 'It is a nice result, somewhat expected (at last for me it is intuitive) and I would have liked such intuition to be given some space in the paper.': 1.0986121892929077, 'For example, less discussion of prior work (that is nice too, but not as important as discussing and studying the main result of the paper) could make more room for addressing the theoretical results.': 1.0960341691970825, 'Please also see below (point 2) for some suggestions.': 0.4054648280143738, 'I have a few other comments to make:': 1.0985904932022095, '1. An interesting experiment would be to show that a model such as yours, where the nodes (neurons) are added or removed automatically can outperform a net with the same number of nodes (at the end, after complete learning), in which the size and number of nodes per layer are fixed from the start. This would prove the efficiency of the idea. This is where your method is interesting: do you save nodes that are not needed and replace them with nodes that are needed? Do you optimize performance vs. memory?': 0.10977015644311905, 'I understand that experiments along this line are given in Figure 2, with mixed results.': 1.0986123085021973, 'The Figure i must say, is not very clear, but it is possible to interpret under careful inspection.': 0.9158363938331604, 'In some the non-parametric nets are doing better and others are doing worse than the parametric ones.': 1.0925471782684326, 'Even in such case i could see the usefulness of the method as it helps discovering the structure.': 1.0986123085021973, ""What i don't fully understand is why they can do better sometimes than the end net which could be trained from scratch: why is the nonparametric version of learning better than the parametric version, when the final net is known in advance?"": 1.0746474266052246, 'Could you give more insight?': 1.0986123085021973, '2. Can you better discuss the meaning and implications of Theorem 1. I feel this theorem is just put there with no proper discussion. Beyond the proofs, from the Appendix, what is the key insight of the Theorem? What does it say, in plain English? To me, the conclusion seems almost natural and obvious. Is there some powerful insight?': 1.011038064956665, 'As i have mentioned previously, i feel this theoretical result deserves more space, with even more experiments to back it up.': 1.0986123085021973, 'For example, can regularizer parameter lambda  be predicted given the data - is there a property in the data that can help guessing the right lambda?': 1.0986123085021973, 'My feeling is that lambda is the key factor for determining the final net structure.': 1.0986108779907227, 'Is this true?': 1.096818208694458, 'How much does the structure of the final net depend on the initialization?': 1.091590166091919, 'Do you get different nets if you start from different random weights?': 1.0986052751541138, 'How different are they?': 1.0986121892929077, 'What happens when fan in and fan out regularizers are combined?': 1.0985604524612427, 'Do you still have the same theoretical result?': 1.09861159324646, 'I have a few additional questions:': 1.0948526859283447, '1. Why do you say that adding zero units changes the regularizer value? For example, does L2 norm change if you add zero values?': 0.8052349090576172, '2. Zero units are defined as having either the fan in or the fan out weights being zero. I think that what you meant is that both fan in and fan out weights are zero, otherwise you cannot remove the unit and keep the same output f. This should be clarified better I think.': 0.5579646229743958, 'I changed my rating to 7, while hoping that the authors will address my comments above.': 1.0986123085021973, 'This paper addresses the problem of allowing networks to change the number of units that are used during training.': 1.0908124446868896, 'This is done in a simple but elegant and well-motivated way.': 0.31215161085128784, 'Units with zero input or output weights are added or removed during training, while a group sparsity norm for regularization is used to encourage unit weights to go to zero.': 1.0986114740371704, 'The main theoretical contribution is to show that with proper regularization, the loss is minimized by a network with a finite number of units.': 1.0938416719436646, 'In practice, this result does not guarantee that the resulting network will not over- or under-fit the training data, but some initial experiments show that this does not seem to be the case.': 1.0986121892929077, 'One potential advantage of approaches that learn the number of units to use in a network is to ease the burden of tuning hyperparameters.': 1.0986114740371704, 'One disadvantage of this approach (and maybe any such approach) is that it does not really solve this problem.': 1.0986108779907227, 'The network still has several hyperparameters that implicitly control the number of units that will emerge, including parameters that control how often new units are added and how rapidly weights may decay to zero.': 1.0986123085021973, 'It is not clear whether these hyperparameters will be easier or harder to tune than the ones in standard approaches.': 1.0986087322235107, 'In fairness, the authors do not claim that they have made training easier, but it is a little disappointing that this does not seem to be the case.': 0.7509890198707581, 'The authors do emphasize that they are able to train networks that use fewer units to achieve comparable performance to networks trained parametrically.': 1.0986108779907227, 'This is potentially important, because smaller networks can reduce run-time at testing, and power consumption and memory footprint, which is important on mobile devices in particular.': 1.0986121892929077, 'However, the authors do not compare experimentally to existing approaches that attempt to reduce the size of parametrically trained networks (eg., by pruning trained networks)': 1.098559856414795, 'so it is not clear whether this approach is really competitive with the best current approaches to reducing the size of trained networks.': 0.4535716474056244, 'Another potential disadvantage of the proposed approach is that the same hyperparameters control both the number of units that will appear in the network and the training time.': 0.3329317569732666, 'Therefore, training might potentially be much slower for this approach than for a parametric approach with fixed hyperparameters.': 1.0985883474349976, 'In practice, many parametric approaches require methods like grid search to choose hyperparameters, which can be very slow, but in many other cases experience with similar problems can make the choice of hyperparameters relatively easy.': 1.0947425365447998, 'This means that the cost of grid search is not always paid, but the slowness of the authors’ approach may be endemic.': 0.7198985815048218, 'The authors do not discuss how this issue will scale as much larger networks are trained.': 1.0564849376678467, 'It is a concern that this approach may not be practical for large-scale networks, because training will be very slow.': 1.0986123085021973, 'In general, the experiments are helpful and encouraging, but not comprehensive or totally convincing.': 1.0986123085021973, 'I would want to see experiments on much larger problems before I was convinced that this approach can really be practical or widely useful.': 1.0986123085021973, 'Overall, I found this to be an interesting and clearly written paper that makes a potentially useful point.': 1.0986123085021973, 'The overall vision of building networks that can grow and adapt through life-long learning is inspiring, and this type of work might be needed to realize such a vision.': 1.0986123085021973, 'But the current results remain pretty speculative.': 1.0986123085021973}"
44,https://openreview.net/forum?id=BJKYvt5lg,"{'UPDATE: The authors addressed all my concerns in the new version of the paper, so I raised my score and now recommend acceptance.': 1.0985931158065796, 'This paper combines the recent progress in variational autoencoder and autoregressive density modeling in the proposed PixelVAE model.': 1.0985713005065918, 'The paper shows that it can match the NLL performance of a PixelCNN with a PixelVAE that has a much shallower PixelCNN decoder.': 1.0968087911605835, 'I think the idea of capturing the global structure with a VAE and modeling the local structure with a PixelCNN decoder makes a lot of sense and can prevent the blurry reconstruction/samples of VAE.': 1.0986037254333496, 'I specially like the hierarchical image generation experiments.': 1.0986123085021973, 'I have the following suggestions/concerns about the paper:': 1.0985692739486694, '1) Is there any experiment showing that using the PixelCNN as the decoder of VAE will result in better disentangling of high-level factors of variations in the hidden code?': 0.3934342861175537, 'For example, the authors can train a PixelVAE and VAE on MNIST with 2D hidden code and visualize the 2D hidden code for test images and color code each hidden code based on the digit and show that the digits have a better separation in the PixelVAE representation.': 1.0981101989746094, 'A semi-supervised classification comparison between VAE and the PixelVAE will also significantly improve the quality of the paper.': 1.0955244302749634, '2) A similar idea is also presented in a concurrent ICLR submission ""Variational Lossy Autoencoder"".': 0.6944485902786255, 'It would be interesting to have a discussion included in the paper and compare these works.': 1.0580466985702515, '3)': 1.0985593795776367, 'The answer to the pre-review questions made the architecture details of the paper much more clear, but I still ask the authors to include the exact architecture details of all the experiments in the paper and/or open source the code.': 1.098606824874878, 'The clarity of the presentation is not satisfying and the experiments are difficult to reproduce.': 0.7886448502540588, '4) As pointed out in my pre-review question, it would be great to include two sets of MNIST samples maybe in an appendix section.': 0.26095181703567505, 'One with PixelCNN and the other with PixelVAE with the same pixelcnn depth to illustrate the hidden code in PixelVAE actually captures the global structure.': 1.0961112976074219, 'I will gladly raise the score if the authors address my concerns.': 1.097446322441101, 'The paper combines a hierarchical Variational Autoencoder with PixelCNNs to model the distribution of natural images.': 1.098609209060669, 'They report good (although not state of the art) likelihoods on natural images and briefly start to explore what information is encoded by the latent representations in the hierarchical VAE.': 1.0986123085021973, 'I believe that combining the PixelCNN with a VAE, as was already suggested in the PixelCNN paper, is an important and interesting contribution.': 1.0832090377807617, 'The encoding of high-, mid- and low-level variations at the different latent stages is interesting but seems not terribly surprising, since the size of the image regions the latent variables model is also at the corresponding scale.': 1.0986123085021973, 'Showing that the PixelCNN improves the latent representation of the VAE with regard to some interesting task would be a much stronger result.': 1.0953482389450073, 'Also, while the paper claims, that combining the PixelCNN with the VAE reduces the number of computationally expensive autoregressive layers, it remains unclear how much more efficient their whole model is than an PixelCNN with comparable likelihood.': 1.0985872745513916, 'In general, I find the clarity of the presentation wanting.': 0.5215781331062317, 'For example, I agree with reviewer1 that the exact structure of their model remains unclear from the paper and would be difficult to reproduce.': 1.0985716581344604, 'All in all this is a nice paper.': 0.3509819507598877, 'I think the model is quite clever, attempting to get the best of latent variable models and auto-regressive models.': 1.0986123085021973, 'The implementation and specific architecture choices (as discussed in the pre-review) also seem reasonable.': 1.0986123085021973, 'On the experimental side, I would have liked to see something more than NLL measurements and samples - maybe show this is useful for other tasks such as classification?': 1.098608374595642, ""Though I don't think this is a huge leap forward this is certainly a nice paper and I recoemmend acceptance."": 1.0986120700836182}"
45,https://openreview.net/forum?id=BJO-BuT1g,"{'This paper addresses the problem of efficient neural stylization.': 1.0814584493637085, 'Instead of training a separate network for N different styles (as is done, e.g., in Johnson et al.), this paper extends the instance normalization work of Ulyanov et al. to train a single network and learn a smaller set “conditional instance normalization” parameters dependent on the desired output style.': 1.0983233451843262, 'The conditional instance normalization applies a learnt affine transformation on normalized feature maps at each layer in the network.': 0.5854634046554565, 'Qualitative results are shown.': 1.0986123085021973, 'I have not worked in this area, but I’m generally aware of the main issues in transferring artistic style.': 1.0986120700836182, 'The paper addresses a known challenge of incorporating different styles into the same net, which have a number of practical benefits.': 0.4297402501106262, 'As far as I can tell the results look compelling.': 1.0986002683639526, 'As I’m less confident in my expertise in this area, I’m happy to support another reviewer who is willing to champion this paper.': 1.0986119508743286, 'My main comments are on the paper writing.': 1.0986123085021973, 'As far as I understand, the main novelty of the approach starts in Section 2.2, and before that is review of prior art.': 1.008862018585205, 'If this is indeed the case, one suggestion is to remove the subsection heading for 2.1 so it’s grouped with the first part of Section 2, and to cite related work for the feedforward network (e.g., Johnson et al.) in the text and in Fig 2': 1.0975456237792969, 'so it’s clear.': 1.097619891166687, 'In fact, I’m wondering if Figs 2 and 3 can be combined somehow so that the contribution is clearer in the figures.': 0.4307161271572113, 'I was at first confused by Eq (5) as x and z are not defined anywhere.': 0.9916722774505615, 'Also, it may be helpful to write out everything explicitly as is done in the instance normalization paper.': 0.42782720923423767, 'In Eq (4), perhaps you could write T_s to emphasize that there are separate networks for different styles.': 0.8542011976242065, 'Fig 5 left: I’m assuming the different colors correspond to the different styles.': 1.0985684394836426, 'If so, perhaps mention this in the caption.': 1.0834332704544067, 'Also, this figure is hard to read.': 0.1446515917778015, 'Maybe instead show single curves with error bars that are averages over the loss curves for N-styles and individual styles.': 1.0345803499221802, 'Typos:': 1.0986123085021973, 'Page 1: Shouldn’t it be “VGG-16” network (not ""VGG-19”)?': 1.0986123085021973, 'Page 2: “newtork” => “network”.': 1.0986123085021973, 'Paragraph after Eq. (5): “much less” => “fewer”.': 1.0986123085021973, 'The paper introduces an elegant method to train a single feed-forward style transfer network with a large number of styles.': 1.0986123085021973, 'This is achieved by a global, style-dependent scale and shift parameter for each feature in the network.': 1.0986123085021973, 'Thus image style is encoded in a very condensed subset of the network parameters, with only two parameters per feature map.': 1.0986030101776123, 'This enables to easily incorporate new styles into an existing network by fine-tuning.': 1.0986123085021973, 'At the same time, the quality of the generated stylisations is comparable to existing feed-forward single-style transfer networks.': 0.8597164154052734, 'While this also means that the stylisation results in the paper are limited by the quality of current feed-forward methods, the proposed method seems general enough to be combined with future improvements in feed-forward style transfer.': 1.0986098051071167, 'Finally, the paper shows that having multiple styles encoded in one feature space allows to gradually interpolate between different styles to generate new mixtures of styles.': 1.066611886024475, 'This is comparable to interpolating between the Gram Matrices of different style images in the iterative style transfer algorithm by Gatys et al.': 0.48058122396469116, 'and comes with similar limitations: Right now the parameters of the style feature space are hard to interpret and therefore there is little control over the stylisation outcome when moving in that feature space.': 1.0985207557678223, 'Here I see the most potential for improvement of the paper: The parameterisation of style in terms of scale and shift parameters of individual features seems like a promising basis to achieve interpretable style features.': 1.0986114740371704, 'It would be a great addition to explore to what extend statements such as “The parameters of neuron N in layer L encodes e.g. the colour or brush-strokes of the styles” can be made.': 1.0138434171676636, 'I agree that this is a potentially laborious endeavour, but even just qualitative statements of this kind that are demonstrated with the respective manipulations in the stylisation would be very interesting.': 1.0478962659835815, 'In conclusion, this is a good paper presenting an elegant and valuable contribution that will have considerable impact on the design of feed-forward stylisation networks.': 1.0980380773544312, 'CONTRIBUTIONS': 1.0986123085021973, 'The authors propose a simple architectural modification (conditional instance normalization) for the task of feedforward neural style transfer that allows a single network to apply many different styles to input images.': 1.0986123085021973, 'Experiments show that the proposed multi-style networks produce qualitatively similar images as single-style networks, train as fast as single-style networks, and achieve comparable losses as single-style networks.': 1.0986123085021973, 'In addition, the authors shows that new styles can be incrementally added to multi-style networks with minimal finetuning, and that convex combinations of per-style parameters can be used for feedforward style blending.': 1.0986123085021973, 'The authors have released open-source code and pretrained models allowing others to replicate the experimental results.': 1.0986123085021973, 'NOVELTY': 1.0986123085021973, 'The problem setup is very similar to prior work on feedforward neural style transfer, but the paper is the first to my knowledge that uses a single network to apply different styles to input images; the proposed conditional instance normalization layer is also novel.': 1.0986123085021973, 'This paper is also the first that demonstrates feedforward neural style blending; though not described in published literature, optimization-based neural style blending had previously been demonstrated in https://github.com/jcjohnson/neural-style.': 1.0986123085021973, 'MISSING CITATION': 1.0986123085021973, 'The following paper was concurrent with Ulyanov et al (2016a) and Johnson et al in demonstrating feedforward neural style transfer, though it did not use the Gram-based formulation of Gatys et al:': 1.0986123085021973, 'Li and Wand, ""Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks"", ECCV 2016': 1.0986123085021973, 'CLARITY': 1.0986123085021973, 'The paper is very well written and easy to follow.': 1.0986123085021973, 'SIGNIFICANCE': 1.0986123085021973, 'Though simple, the proposed method is a significant addition to the growing field of neural style transfer.': 1.0986123085021973, 'Its benefits are especially clear for mobile applications, which are often constrained in both disk space and bandwidth.': 1.0984147787094116, 'Using the proposed method, only a single trained network needs to be transmitted and stored on the mobile device; in addition the ability of the proposed method to incrementally learn new styles means that new styles can be added by transmitting only a small number of new style-specific parameters to a mobile device.': 1.089161992073059, 'EVALUATION': 1.0986123085021973, 'Like many other papers on neural style transfer, the results are mostly qualitative.': 0.4753047227859497, 'Following existing literature, the authors use style and content loss as a quantitative measure of quality, but these metrics are unfortunately not always well-correlated with the perceptual quality of the results.': 0.8738810420036316, 'I find the results of this paper convincing, but I wish that this and other papers on this topic could find a way to evaluate their results more quantitatively.': 0.48713141679763794, 'SUMMARY': 1.0986121892929077, 'The problem and method are slightly incremental, but the several improvements over prior work make this paper a significant addition to the growing literature on neural style transfer.': 1.0114206075668335, 'The paper is well-written and its experiments convincingly validate the benefits of the method.': 1.0476157665252686, 'Overall I believe the paper would be a valuable addition to the conference.': 1.0986123085021973, 'Pros': 1.0986123085021973, 'Simple modification to feedforward neural style transfer with several improvements over prior work': 0.5841407775878906, 'Strong qualitative results': 1.0986123085021973, 'Well-written': 1.0986123085021973, 'Open-source code has already been released': 1.0986123085021973, 'Cons': 1.0986123085021973, 'Slightly incremental': 1.0986123085021973, 'Somewhat lacking in quantitative evaluation, but not any more so than prior work on this topic': 1.0986090898513794}"
46,https://openreview.net/forum?id=BJRIA3Fgg,"{'This paper studies knowledge transfer problem from small capacity network to bigger one.': 1.3862943649291992, 'This is a follow-up work of Net2Net (ICLR 2015) and NetMorph(ICML 2016).': 1.3862943649291992, 'Comments': 1.3862943649291992, '1) This paper studies macroscopic problem, with the morphing process composed by multiple atomic operations.': 1.3862943649291992, 'While the atomic operations are proposed in Net2Net and NetMorph, there has not been study of the general modularized process principally.': 1.386293888092041, 'Thus this paper asks a novel question.': 1.3862943649291992, '2) The solution by composing multiple atomic transformations seems to be quite reasonable.': 1.2322219610214233, '3) In the “related work” section, it is better to change “network morphism” to “knowledge transfer” or in the subsection title, most of these works are known as knowledge transfer and it helps to connect to the existing works.': 1.0180212259292603, '4)': 1.3862943649291992, 'The author shows experiments on variants of ResNet.': 1.3862943649291992, 'While the experiment shows that initializing from ResNet gives better error rate than the ones trained from scratch, it is unclear what the source This paper studies knowledge transfer problem from small capacity network to bigger one.': 1.3862943649291992, 'While the experiment shows that initializing from ResNet gives better error rate than the ones trained from scratch, it is unclear what the source is.': 1.3862943649291992, '5) One major advantage of this type of knowledge transfer (Net2Net, NetMorph) is to speedup training and model exploration.': 0.4385641813278198, 'There seems to be no experiments demonstrate such advantage (possibly due to the lose initialization of BatchNorm).': 1.3853020668029785, 'This is the major drawback of this paper.': 0.7929013967514038, '6)': 0.7217339277267456, 'The method proposed by the author can in principle do quite complicated transformation, e.g. transform  an entire resnet from a single conv layer, the experiment only consists of simple module transformations, which in some way can be covered by atomic operations.': 1.3862677812576294, 'It would be more interesting to see what the results of more complicated transformations are (even if they are not as effective).': 0.638335108757019, 'In summary, this paper studies a novel problem of knowledge transfer in a macroscopic level.': 1.3862943649291992, 'The method could be of interest to the ICLR community.': 1.3862943649291992, 'The experiments should be improved (comment 5) to make the results more convincing and practically useful and I strongly encourage the authors to do so.': 1.3862943649291992, 'This paper presents works on neural network / CNN architecture morphing.': 1.3862943649291992, 'Results are not reported on ImageNet larger ResNet and new network architecture such as Xception and DenseNet - which are maybe too new!': 1.3862942457199097, 'Also most results are reported in small datasets and network, which do not offer confidence in the usability in production systems.': 1.3862943649291992, 'My biggest issue is that computational time and effort for these techniques is not mentioned in detail.': 1.3862943649291992, 'We always want to be able to quantify the extra effort of understanding and using a new technique, especially if the results are minor.': 1.3862943649291992, 'The paper proposes a methodology for morphing a trained network to different architecture without having to retrain from scratch.': 1.0711007118225098, 'The manuscript reads well and the description is easy to follow.': 1.031009554862976, 'However, the results are not very convincing as the selected baselines are considerably far from the state of the art.': 1.1055206060409546, 'The paper should include comparisons with state of the art, for example wide residual networks.': 1.0989652872085571, 'Tables should also report number of parameters for each architecture, this would help fair comparison.': 1.2512218952178955, 'The paper presents an interesting incremental approach for exploring new convolutional network hierarchies in an incremental manner after a baseline network has reached a good recognition performance.': 1.3862943649291992, 'The experiments are presented for the CIFAR-100 and ImageNet benchmarks by morphing various ResNet models into better performing models with somewhat more computation.': 1.3862903118133545, 'Although the baselines are less strong than those presented in the literature, the paper claims significant error reduction for both ImageNet and CIFAR-100.': 1.383551001548767, 'The main idea of the paper is to rewrite convolutions into multiple convolutions while expanding the number of filters.': 1.3862940073013306, 'It is quite unexpected that this approach yields any improvements over the baseline model at all.': 1.3862943649291992, 'However, for some of the basic tenets of network morphing, experimental evidence is not given in the paper.': 1.3862942457199097, 'Here are some fundamental questions raised by the paper:': 1.3862926959991455, 'How does the quality of morphed networks compares to those with the same topology trained from scratch?': 1.3862943649291992, 'How does the incremental training time after morphing relate to that of the network trained from scratch?': 1.3862943649291992, 'Where is the extra computational cost of the morphed networks come from?': 1.3859366178512573, 'Why is the quality of the baseline ResNet models lag behind those that are reported in the literature and github?': 1.3862015008926392, '(E.g. the github ResNet-101 model is supposed to have 6.1% top-5 recall vs 6.6 reported in the paper)': 0.8736051917076111, 'More evidence for the first three points would be necessary to evaluate the validity of the claims of the paper.': 1.386292576789856, 'The paper is written reasonably well and can be understood quite well, but the missing evidence and weaker baselines make it looks somewhat less convincing.': 1.3835498094558716, 'I would be inclined to revise up the score if a more experimental evidence were given for the main message of the paper (see the points above).': 1.3862943649291992}"
47,https://openreview.net/forum?id=BJVEEF9lx,"{'A method for training neural networks to mimic abstract data structures is presented.': 1.0986123085021973, 'The idea of training a network to satisfy an abstract interface is very interesting and promising, but empirical support is currently too weak.': 1.0986123085021973, 'The paper would be significantly strengthened if the method could be shown to be useful in a realistic application, or be shown to work better than standard RNN approaches on algorithmic learning tasks.': 0.9221477508544922, 'The claims about mental representations are not well supported.': 1.0986123085021973, 'I would remove the references to mind and brain, as well as the more philosophical points, or write a paper that really emphasizes one of these aspects and supports the claims.': 1.0975619554519653, 'The paper presents a framework to formulate data-structures in a learnable way.': 1.0986123085021973, 'It is an interesting and novel approach that could generalize well to interesting datastructures and algorithms.': 1.0986123085021973, 'In its current state (Revision of Dec. 9th), there are two strong weaknesses remaining: analysis of related work, and experimental evidence.': 1.0986123085021973, 'Reviewer 2 detailed some of the related work already, and especially DeepMind (which I am not affiliated with) presented some interesting and highly related results with its neural touring machine and following work.': 1.0986123085021973, 'While it may be of course very hard to make direct comparisons in the experimental section due to complexity of the re-implementation, it would at least be very important to mention and compare to these works conceptually.': 1.0986123085021973, 'The experimental section shows mostly qualitative results, that do not (fully) conclusively treat the topic.': 1.0986123085021973, 'Some suggestions for improvements:': 1.0986123085021973, '* It would be highly interesting to learn about the accuracy of the stack and queue structures, for increasing numbers of elements to store.': 1.0985145568847656, '* Can a queue / stack be used in arbitrary situations of push-pop operations occuring, even though it was only trained solely with consecutive pushes / consecutive pops?': 1.0819205045700073, ""Does it in this enhanced setting `diverge' at some point?"": 1.0986123085021973, '*': 1.0986123085021973, ""The encoded elements from MNIST, even though in a 28x28 (binary?) space, are elements of a ten-element set, and can hence be encoded a lot more efficiently just by `parsing' them, which CNNs can do quite well."": 1.0986119508743286, ""Is the NN `just' learning to do that?"": 1.0986123085021973, 'If so, its performance can be expected to strongly degrade when having to learn to stack more than 28*28/4=196 numbers (in case of an optimal parser and loss-less encoding).': 1.0986123085021973, 'To argue more in this direction, experiments would be needed with an increasing number of stack / queue elements.': 1.0986123085021973, 'Experimenting with an MNIST parsing NN in front of the actual stack/queue network could help strengthening or falsifying the claim.': 1.0986123085021973, ""The claims about `mental representations' have very little support throughout the paper."": 1.0986123085021973, 'If indication for correspondence to mental models, etc., could be found, it would allow to hold the claim.': 1.0986123085021973, 'Otherwise, I would remove it from the paper and focus on the NN aspects and maybe mention mental models as motivation.': 1.0986123085021973, 'The paper presents a way to ""learn"" approximate data structures.': 1.0986123085021973, 'They train neural networks (ConvNets here) to perform as an approximate abstract data structure by having an L2 loss (for the unrolled NN) on respecting the axioms of the data structure they want the NN to learn.': 1.0782697200775146, 'E.g. you NN.push(8), NN.push(6), NN.push(4), the loss is proportional to the distance with what is NN.pop()ed three times and 4, 6, 8 (this example is the one of Figure 1).': 1.0895650386810303, 'There are several flaws:': 1.0986123085021973, '- In the case of the stack: I do not see a difference between this and a seq-to-seq RNN trained with e.g. 8, 6, 4 as input sequence, to predict 4, 6, 8.': 0.14155124127864838, ""- While some of the previous work is adequately cited, there is an important body of previous work (some from the 90s) on learning Peano's axioms, stacks, queues, etc. that is not cited nor compared to. For instance [Das et al. 1992], [Wiles & Elman 1995], and more recently [Graves et al. 2014], [Joulin & Mikolov 2015], [Kaiser & Sutskever 2016]..."": 1.0519546270370483, '- Using MNIST digits, and not e.g. a categorical distribution on numbers, is adding complexity for no reason.': 1.0986123085021973, '- (Probably the biggest flaw) The experimental section is too weak to support the claims. The figures are adequate, but there is no comparison to anything. There is also no description nor attempt to quantify a form of ""success rate"" of learning such data structures, for instance w.r.t the number of examples, or w.r.t to the size of the input sequences. The current version of the paper (December 9th 2016) provides, at best, anecdotal experimental evidence to support the claims of the rest of the paper.': 1.0933456420898438, 'While an interesting direction of research, I think that this paper is not experimentally sound enough for ICLR.': 1.0986123085021973}"
48,https://openreview.net/forum?id=BJYwwY9ll,"{'This work develops a method to quickly produce an ensemble of deep networks that outperform a single network trained for an equivalent amount of time.': 1.0986114740371704, ""The basis of this approach is to use a cyclic learning rate to quickly settle the model into a local minima and saving a model snapshot at this time before quickly raising the learning rate to escape towards a different minima's well of attraction."": 1.0986123085021973, 'The resulting snapshots can be collected throughout a single training run and achieve reasonable performance compared to baselines and have some of the gains of traditional ensembles (at a much lower cost).': 1.0986123085021973, 'This paper is well written, has clear and informative figures/tables, and provides convincing results across a broad range of models and datasets.': 1.0986120700836182, 'I especially liked the analysis in Section 4.4.': 1.0986123085021973, 'The publicly available code to ensure reproducibility is also greatly appreciated.': 1.0986123085021973, 'I would like to see more discussion of the accuracy and variability of each snapshot and further comparison with true ensembles.': 1.0986113548278809, 'Preliminary rating:': 1.0986123085021973, 'This is an interesting work with convincing experiments and clear writing.': 0.6830196976661682, 'Minor note:': 1.0959054231643677, 'Why is the axis for lambda from -1 to 2 in Figure 5 where lambda is naturally between 0 and 1.': 1.0984653234481812, ""I don't have much to add to my pre-review questions."": 1.0986123085021973, ""The main thing I'd like to see that would strengthen my review further is a larger scale evaluation, more discussion of the hyperparameters, etc."": 1.0986119508743286, ""Where test error are reported for snapshot ensembles it would be useful to report statistics about the performance of individual ensemble members for comparison (mean and standard deviation, maybe best single member's error rate)."": 1.0986123085021973, 'The work presented in this paper proposes a method to get an ensemble of neural networks at no extra training cost (i.e., at the cost of training a single network), by saving snapshots of the network during training.': 1.098608136177063, 'Network is trained using a cyclic (cosine) learning rate schedule; the snapshots are obtained when the learning rate is at the lowest points of the cycles.': 1.0985983610153198, 'Using these snapshot ensembles, they show gains in performance over a single network on the image classification task on a variety of datasets.': 1.0985959768295288, 'Positives:': 0.4931587278842926, '1. The work should be easy to adopt and re-produce, given the simple techinque and the experimental details in the paper.': 1.0880215167999268, '2. Well written paper, with clear description of the method and thorough experiments.': 0.5340880751609802, 'Suggestions for improvement / other comments:': 1.0983716249465942, '1. While it is fair to compare against other techniques assuming a fixed computational budget, for a clear perspective, thorough comaprisons with ""true ensembles"" (i.e., ensembles of networks trained independently) should be provided.': 0.6769128441810608, 'Specificially, Table 4 should be augmented with results from ""true ensembles"".': 1.0970406532287598, '2. Comparison with true ensembles is only provided for DenseNet-40 on CIFAR100 in Figure 4. The proposed snapshot ensemble achieves approximately 66% of the improvement of ""true ensemble"" over the single baseline model. This is not reflected accurately in the authors\' claim in the abstract: ""[snapshot ensembles] **almost match[es]** the results of far more expensive independently trained [true ensembles].""': 0.6748945713043213, '3. As mentioned before: to understand the diversity of snapshot ensembles, it would help to the diversity against different ensembling technique, e.g. (1) ""true ensembles"", (2) ensembles from dropout as described by Gal et. al, 2016 (Dropout as a Bayesian Approximation).': 0.8327531814575195}"
49,https://openreview.net/forum?id=BJ_MGwqlg,"{'The paper studies the impact of using customized number representations on accuracy, speed, and energy consumption of neural network inference.': 1.0986123085021973, 'Several standard computer vision architectures including VGG and GoogleNet are considered for the experiments, and it is concluded that floating point representations are preferred over fixed point representations, and floating point numbers with about 14 bits are sufficient for the considered architectures resulting in a small loss in accuracy.': 1.0986123085021973, 'The paper provides a nice overview of floating and fixed point representations and focuses on an important aspect of deep learning that is not well studied.': 1.0986123085021973, 'There are several aspects of the paper that could be improved, but overall, I am leaned toward weak accept assuming that the authors address the issues below.': 1.0986123085021973, '1-': 1.0986123085021973, 'The paper is not clear that it is only focusing on neural network inference.': 1.0986123085021973, 'Please include the word ""inference"" in the title / abstract to clarify this point and mention that the findings of the paper do not necessarily apply to neural network training as training dynamics could be different.': 1.0986123085021973, '2- The paper does not discuss the possibility of adopting quantization tricks during training, which may result in the use of fewer bits at inference.': 1.0986123085021973, '3-': 1.0986123085021973, 'The paper is not clear whether in computing the running time and power consumption, it includes all of the modules or only multiply-accumulate units?': 1.0986123085021973, 'Also, how accurate are these numbers given different possible designs and the potential difference between simulation and production?': 1.0986123085021973, 'Please elaborate on the details of simulation in the paper.': 1.0986123085021973, '4-': 1.0986123085021973, 'The whole discussion about ""efficient customized precision search"" seem unimportant to me.': 1.0986123085021973, 'When such important hardware considerations are concerned, even spending 20x simulation time is not that important.': 1.0986123085021973, 'The exhaustive search process could be easily parallelized and one may rather spend more time at simulation at the cost of finding the exact best configuration rather than an approximation.': 1.0986123085021973, 'That said, weak configurations could be easily filtered after evaluating just a few examples.': 1.0986123085021973, ""5- Nvidia's Pascal GP100 GPU supports FP16."": 1.0986123085021973, 'This should be discussed in the paper and relevant Nvidia papers / documents should be cited.': 1.0986123085021973, 'More comments:': 1.0983575582504272, 'Parts of the paper discussing ""efficient customized precision search"" are not clear to me.': 1.06722891330719, 'As future work, the impact of number representations on batch normalization and recurrent neural networks could be studied.': 1.0985839366912842, 'This paper explores the performance-area-energy-model accuracy tradeoff encountered in designing custom number representations for deep learning inference.': 1.0807888507843018, 'Common image-based benchmarks: VGG, Googlenet etc are used to demonstrate that fewer than1 6 bits in a custom floating point representation can lead to improvement in runtime performance and energy efficiency with only a small loss in model accuracy.': 1.0986123085021973, 'Questions:': 1.0986123085021973, '1. Does the custom floating point number representation take into account support for de-normal numbers?': 0.7891184687614441, '2. Is the custom floating point unit clocked at the same frequency as the baseline 32-bit floating point unit? If not, what are the different frequencies used and how would this impact the overall system design in terms of feeding the data to the floating point units from the memory': 0.8653582334518433, 'Comments:': 1.0986123085021973, '1. I would recommend using the IEEE half-precision floating point (1bit sign, 5bit exponent, and 10bit mantissa) as a baseline for comparison. At this point, it is well known in both the ML and the HW communities that 32-bit floats are an overkill for DNN inference and major HW vendors already include support for IEEE half-precision floats.': 0.40548375248908997, '2. In my opinion, the claim that switching to custom floating point  lead to a YY.ZZ x savings in energy is misleading. It might be true that the floating-point unit itself might consume less energy due to smaller bit-width of the operands, however a large fraction of the total energy is spent in data movement to/from the memories. As a result, reducing the floating point unit’s energy consumption by a certain factor will not translate to the same reduction in the total energy. A reader not familiar with such nuances (for example a typical member of the ML community), may be mislead by such claims.': 0.6696386337280273, '3. On a similar note as comment 2, the authors should explicitly mention that the claimed speedup is that of the floating point unit only, and it will not translate to the overall workload speedup. Although the speedup of the compute unit is roughly quadratic in the bit-width, the bandwidth requirements scale linearly with bit-width. As a result, it is possible that these custom floating point units may be starved on memory bandwidth, in which case the claims of speedup and energy savings need to be revisited.': 0.6222070455551147, '4. The authors should also comment on the complexities and overheads introduced in data accesses, designing the various system buses/ data paths when the number representation is not byte-aligned. Moving to a custom 14-bit number representation (for example) can improve the performance and energy-efficiency of the floating point unit, but these gains can be partially eroded due to the additional overhead in supporting non-byte aligned memory accesses.': 0.41396021842956543, 'The paper provides a first study of customized precision hardware for large convolutional networks, namely alexnet, vgg and googlenet.': 1.0986123085021973, 'It shows that it is possible to achieve larger speed-ups using floating-point precision (up to 7x) when using fewer bits, and better than using fixed-point representations.': 1.0986123085021973, 'The paper also explores predicting custom floating-point precision parameters directly from the neural network activations, avoiding exhaustive search, but i could not follow this part.': 1.0986123085021973, 'Only the activations of the last layer are evaluated, but on what data ?': 1.0986123085021973, 'On all the validation set ?': 1.0986123085021973, 'Why would this be faster than computing the classification accuracy ?': 1.0986123085021973, 'The results should be useful for hardware manufacturers, but with a catch.': 1.0986123085021973, 'All popular convolutional networks now use batch normalization, while none of the evaluated ones do.': 1.0986123085021973, 'It may well be that the conclusions of this study will be completely different on batch normalization networks, and fixed-point representations are best there, but that remains to be seen.': 1.0986123085021973, 'It seems like something worth exploring.': 1.0986123085021973, 'Overall there is not a great deal of novelty other than being a useful study on numerical precision trade-offs at neural network test time.': 1.0986123085021973, 'Training time is also something of interest.': 1.0986123085021973, 'There are a lot more researchers trying to train new networks fast than trying to evaluate old ones fast.': 1.0986123085021973, 'I am also no expert in digital logic design, but my educated guess is that this paper is marginally below the acceptance threshold.': 1.0986123085021973}"
50,https://openreview.net/forum?id=BJa0ECFxe,"{'Paper summary': 1.0986123085021973, 'This paper develops a generalization of dropout using information theoretic': 1.0986123085021973, 'principles.': 1.0986123085021973, 'The basic idea is that when learning a representation z of input x': 1.0986123085021973, 'with the aim of predicting y, we must choose a z such that it carries the least': 1.0986123085021973, 'amount of information about x, as long as it can predict y.': 1.0986123085021973, 'This idea can be': 1.0986123085021973, 'formalized using the Information Bottleneck Lagrangian.': 1.0986123085021973, 'This leads to an': 1.0986123085021973, 'optimization problem which is similar to the one derived for variational': 1.0986123085021973, 'dropout, the difference being that Information dropout allows for a scaling': 1.0193613767623901, 'factor associated with the KL divergence term that encourages noise.': 1.0928001403808594, 'The amount': 1.0981886386871338, 'of noise being added is made a parameterized function of the data and this': 1.088150143623352, 'function is optimized along with the rest of the model.': 0.679226815700531, 'Experimental results on': 0.4160580635070801, 'CIFAR-10 and MNIST show (small) improvements over binary dropout.': 1.0986123085021973, 'Strengths': 1.0986121892929077, 'The paper highlights an important conceptual link between probabilistic': 1.0986123085021973, 'variational methods and information theoretic methods, showing that dropout': 1.029125690460205, 'can be generalized using both formalisms to arrive at very similar models.': 1.0878146886825562, 'The presentation of the model is excellent.': 1.0950461626052856, 'The experimental results on cluttered MNIST are impressive.': 1.0851383209228516, 'Weaknesses': 1.0986123085021973, 'The results on CIFAR-10 in Figure 3(b) seem to be on a validation set (unless': 0.9984756708145142, 'the axis label is a typo).': 1.0967674255371094, 'It is not clear why the test set was not used.': 1.0986002683639526, 'This': 1.0986123085021973, 'makes it hard to compare to results reported in Springenberg et al, as well as': 0.5819351673126221, 'other results in literature.': 0.45751309394836426, 'Quality': 1.0986123085021973, 'The theoretical exposition is high quality.': 1.0985968112945557, 'Figure 2 gives a nice qualitative': 1.0976595878601074, 'assessment of what the model is doing.': 1.0986121892929077, 'However, the experimental results': 0.6398403644561768, 'section can be made better, for example, by matching the results on CIFAR-10 as': 0.5630264282226562, 'reported in Springenberg et al. and trying to improve on those using information': 0.7569327354431152, 'dropout.': 1.0875835418701172, 'Clarity': 1.093942403793335, 'The paper is well written and easy to follow.': 0.9689117670059204, 'Originality': 0.9260690212249756, 'The derivation of the information dropout optimization problem using IB': 1.0985842943191528, 'Lagrangian is novel.': 0.4536130130290985, 'However, the final model is quite close to variational': 0.5498164892196655, 'Significance': 1.0986123085021973, 'This paper will be of general interest to researchers in representation learning': 1.0833312273025513, 'because it highlights an alternative way to think about latent variables (as': 0.9828780889511108, 'information bottlenecks).': 1.0985944271087646, 'However, unless the model can be shown to achieve': 1.0976204872131348, 'significant improvements over simple dropout, its wider impact is likely to be': 1.0986119508743286, 'limited.': 1.0986123085021973, 'Overall': 1.0986123085021973, 'The paper presents an insightful theoretical derivation and good preliminary': 1.0986123085021973, 'results.': 1.0986123085021973, 'The experimental section can be improved.': 1.0986123085021973, 'Minor comments and suggestions': 1.0986123085021973, 'expecially -> especially': 1.0986123085021973, 'trough -> through': 1.0986123085021973, 'There is probably a minus sign missing in the expression for H(y|z) above Eq (2).': 1.0986123085021973, ""Figure 3(a) has error bars, but 3(b) doesn't."": 1.0986123085021973, 'It might be a good idea to have those': 1.0986123085021973, 'for Figure 3(b) as well.': 1.0986123085021973, 'Please consider comparing Figure 2 with the activity map of a standard CNN': 1.0986123085021973, 'trained with binary dropout, so we can see if similar filtering out is': 1.0986123085021973, 'happening there already.': 1.0985780954360962, 'An interesting connection is made between dropout, Tishby et al\'s ""information bottleneck"" and VAEs.': 1.0986120700836182, ""Specifically, classification of 'y' from 'x' is split in two faces: an inference model z ~ q(z|x), a prior p(z), and a classifier y ~ p(y|z)."": 1.0927481651306152, ""By optimizing the objective E_{(x,y)~data} [ E_{z~q(z|x)}[log p(x|y)] + lambda * KL(q(z|x)||p(z))], with lambda <= 1, an information bottleneck 'z' is formed, where lambda controls an upper bound on the number of bits traveling through 'z'."": 1.0983244180679321, ""The objective is equivalent to a VAE objective with downweighted KL(posterior|prior), an encoder that takes as input 'x', and a decoder that only predicts 'x'."": 1.0967859029769897, 'Related work (section 2) is discussed sufficiently.': 0.40780168771743774, 'In section 3, would be better to remind us the definition of mutual information.': 1.0128673315048218, 'Connection to VAEs in section 5 is interesting.': 1.0985480546951294, 'Unfortunately, the MNIST/CIFAR-10 results are not great.': 1.0934239625930786, 'Since the method is potentially more flexible than other forms of dropout, this is slightly disappointing.': 0.693301796913147, ""It's unclear why the CIFAR-10 results seem to be substantially worse than the results originally reported for that architecture."": 1.0986123085021973, ""It's unclear which version of 'beta' was used in figure 3a."": 1.0986123085021973, 'Overall I think the theory presented in the paper is promising.': 1.0986123085021973, 'However, the paper lacks sufficiently convincing experimental results, and I encourage the authors to do further experiments that prove significant improvements, at least on CIFAR-10, perhaps on larger problems.': 1.0986123085021973, 'The authors propose ""information dropout"", a variation of dropout with an information theoretic interpretation.': 1.0986123085021973, 'A dropout layer limits the amount of information that can be passed through it, and the authors quantify this using a variational bound.': 1.0986123085021973, 'It remains unclear why such an information bottleneck is a good idea from a theoretical standpoint.': 1.0986123085021973, 'Bayesian interpretations lend a theoretical basis to parameter noise, but activation noise has no such motivation.': 1.0986123085021973, 'The information bottleneck indeed limits the information that can be passed through, but there is no rigorous argument for why this should improve generalization.': 1.0986123085021973, 'The experiments are not convincing.': 1.0986123085021973, 'The CIFAR-10 results are worse than those in the paper that originally proposed the network architecture they use (Springenberg et al).': 1.0986123085021973, 'The VAE results on MNIST are also horrible.': 1.0986123085021973}"
51,https://openreview.net/forum?id=BJbD_Pqlg,"{'The author works to compare DNNs to human visual perception, both quantitatively and qualitatively.': 1.0983939170837402, 'Their first result involves performing a psychophysical experiment both on humans and on a model and then comparing the results (actually I think the psychophysical data was collected in a different work, and is just used here).': 1.098488450050354, 'The specific psychophysical experiment determined, separately for each of a set of approx.': 1.0986120700836182, '1110 images, what the noise level of additive noise would have to be to make a just-noticeable-difference for humans in discriminating the noiseless image from the noisy one.': 1.0986120700836182, 'The authors then define a metric on neural networks that allows them to measure what they posit might be a similar property for the networks.': 1.0986088514328003, 'They then correlate the pattern of noise levels between neural networks that the humans.': 1.0986123085021973, 'Deep neural networks end up being much better predictors of the human pattern of noise levels than simpler measure of image perturbation (e.g. RMS contrast).': 1.0975114107131958, 'A second result involves comparing DNNs to humans in terms of their pattern errors in a series of highly controlled experiments using stimuli that illustrate classic properties of human visual processing': 1.0984504222869873, 'including segmentation, crowding and shape understanding.': 1.0986123085021973, 'They then used an information-theoretic single-neuron metric of discriminability to assess similar patterns of errors for the DNNs.': 1.098610758781433, 'Again, top layers of DNNs were able to reproduce the human patterns of difficulty across stimuli, at least to some extent.': 1.0986104011535645, 'A third result involves comparing DNNs to humans in terms of their pattern of contrast sensitivity across a series of sine-grating images at different frequencies.': 1.0970948934555054, '(There is a classic result from vision research as to what this pattern should be, so it makes a natural target for comparison to models.)': 0.7479640245437622, 'The authors define a DNN correlate for the propertie in terms of the cross-neuron average of the L1-distance between responses to a blank image and responses to a sinuisoid of each contrast and frequency.': 0.6022493839263916, 'They then qualitatively compare the results of this metric for DNNs models to known results from the literature on humans, finding that, like humans, there is an apparent bandpass response for low-contrast gratings and a mostly constant response at high contrast.': 0.5918312668800354, 'Pros:': 1.0986123085021973, '*': 1.0986123085021973, 'The general concept of comparing deep nets to psychophysical results in a detailed, quantitative way, is really nice.': 1.0247045755386353, '* They nicely defined a set of ""linking functions"", e.g. metrics that express how a specific behavioral result is to be generated from the neural network.': 1.0986049175262451, '(Ie. the L1 metrics in results 1 and 3 and the information-theoretic measure in result 2.)': 0.5895712971687317, 'The framework for setting up such linking functions seems like a great direction to me.': 0.8974135518074036, 'The actual psychophysical data seems to have been handled in a very careful and thoughtful way.': 1.0457192659378052, ""These folks clearly know what they're doing on the psychophysical end."": 1.0929044485092163, 'Cons:': 1.0986123085021973, ""* To my mind, the biggest problem wit this paper is that that it doesn't say something that we didn't really know already."": 1.098594069480896, 'Existing results have shown that DNNs are pretty good models of the human visual system in a whole bunch of ways, and this paper adds some more ways.': 0.4061902165412903, 'What would have been great would be:': 1.0986123085021973, '(a) showing that they metric of comparison to humans that was sufficiently sensitive that it could pull apart various DNN models, making one clearly better than the others.': 1.0986123085021973, '(b) identifying a wide gap between the DNNs and the humans that is still unfilled.': 1.0986123085021973, 'They sort of do this, since while the DNNs are good at reproducing the human judgements in Result 1, they are not perfect': 1.0986123085021973, 'gap is between 60% explained variance and 84% inter-human consistency.': 1.0986123085021973, ""This 24% gap is potentially important, so I'd really like to see them have explored that gap more"": 1.0986123085021973, 'e.g. (i) widening the gap by identifying which images caused the gap most and focusing a test on those, or (ii) closing the gap by training a neural network to get the pattern 100% correct and seeing if that made better CNNs as measured on other metrics/tasks.': 0.8741933703422546, 'In other words, I would definitely have traded off not having results 2 and 3 for a deeper exploration of result 1.': 1.0952316522598267, ""I think their overall approach could be very fruitful, but it hasn't really been carried far enough here."": 0.9050034880638123, '* I found a few things confusing about the layout of the paper.': 0.6240964531898499, 'I especially found that the quantitative results for results 2 and 3 were not clearly displayed.': 0.568187415599823, 'Why was figure 8 relegated to the appendix?': 1.097833275794983, 'Where are the quantifications of model-human similarities for the data shown in Figure 8?': 0.7226894497871399, ""Isn't this the whole meat of their second result?"": 1.0951550006866455, 'This should really be presented in a more clear way.': 1.0985618829727173, '* Where is the quantification of model-human similarity for the data show in Figure 3?': 1.0915719270706177, ""Isn't there a way to get the human contrast-sensitivity curve and then compare it to that of models in a more quantitively precise way, rather than just note a qualitative agreement?"": 0.537500262260437, ""It seems odd to me that this wasn't done."": 0.48157328367233276, 'This paper compares the performance, in terms of sensitivity to perturbations, of multilayer neural networks to human vision.': 1.0986123085021973, 'In many of the tasks tested, multilayer neural networks exhibit similar sensitivities as human vision.': 1.0986123085021973, 'From the tasks used in this paper one may conclude that multilayer neural networks capture many properties of the human visual system.': 1.0975284576416016, 'But of course there are well known adversarial examples in which small, perceptually invisible perturbations cause catastrophic errors in categorization, so against that backdrop it is difficult to know what to make of these results.': 1.0986086130142212, 'That the two systems exhibit similar phenomenologies in some cases could mean any number of things, and so it would have been nice to see a more in depth analysis of why this is happening in some cases and not others.': 1.0986123085021973, 'For example, for the noise perturbations described in the the first section, one sees already that conv2 is correlated with human sensitivity.': 1.0986121892929077, 'So why not examine how the first layer filters are being combined to produce this contextual effect?': 1.09861159324646, 'From that we might actually learn something about neural mechanisms.': 0.8032848834991455, 'Although I like and am sympathetic to the direction the author is taking here, I feel it just scratches the surface in terms of analyzing perceptual correlates in multilayer neural nets.': 1.0985409021377563, 'The paper reports several connections between the image representations in state-of-the are object recognition networks and findings from human visual psychophysics:': 1.0986123085021973, '1) It shows that the mean L1 distance in the feature space of certain CNN layers is predictive of human noise-detection thresholds in natural images.': 1.0953797101974487, '2) It reports that for 3 different 2-AFC tasks for which there exists a condition that is hard and one that is easy for humans, the mutual information between decision label and quantised CNN activations is usually higher in the condition that is easier for humans.': 0.7002984285354614, '3) It reproduces the general bandpass nature of contrast/frequency detection sensitivity in humans.': 1.0986049175262451, 'While these findings appear interesting, they are also rather anecdotal and some of them seem to be rather trivial (e.g. findings in 2).': 1.0986123085021973, 'To make a convincing statement it would be important to explore what aspects of the CNN lead to the reported findings.': 1.0986123085021973, 'One possible way of doing that could be to include good baseline models to compare against.': 1.0985887050628662, 'As I mentioned before, one such baseline should be reasonable low-level vision model.': 1.0986123085021973, 'Another interesting direction would be to compare the results for the same network at different training stages.': 1.0986123085021973, 'In that way one might be able to find out which parts of the reported results can be reproduced by simple low-level image processing systems,  which parts are due to the general deep network’s architecture and which parts arise from the powerful computational properties (object recognition performance) of the CNNs.': 1.0986123085021973, 'In conclusion, I believe that establishing correspondences between state-of-the art CNNs and human vision is a potentially fruitful approach.': 1.0986123085021973, 'However to make a convincing point that found correspondences are non-trivial, it is crucial to show that non-trivial aspects of the CNN lead to the reported findings, which was not done.': 1.0904306173324585, 'Therefore, the contribution of the paper is limited since I cannot judge whether the findings really tell me something about a unique relation between high-performing CNNs and the human visual system.': 0.968402624130249, 'UPDATE:': 1.0986123085021973, 'Thank you very much for your extensive revision and inclusion of several of the suggested baselines.': 1.0986123085021973, 'The results of the baseline models often raise more questions and make the interpretation of the results more complex, but I feel that this reflects the complexity of the topic and makes the work rather more worthwhile.': 1.0147615671157837, 'One further suggestion: As the experiments with the snapshots of the CaffeNet shows, the direct relationship between CNN performance and prediction accuracy of biological vision known from Yamins et al. 2014 and Cadieu et al. 2014 does not necessarily hold in your experiments.': 0.9173855781555176, 'I think this should be discussed somewhere in the paper.': 1.0986123085021973, 'All in all, I think that the paper now constitutes a decent contribution relating state-of-the art CNNs to human psychophysics and I would be happy for this work to be accepted.': 1.098530888557434, 'I raise the my rating for this paper to 7.': 1.0986123085021973}"
52,https://openreview.net/forum?id=BJh6Ztuxl,"{'The authors present a methodology for analyzing sentence embedding techniques by checking how much the embeddings preserve information about sentence length, word content, and word order.': 1.0963774919509888, 'They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors.': 1.0491136312484741, 'The experiments are thorough and provide interesting insights into the representational power of common sentence embedding strategies, such as the fact that word ordering is surprisingly low-entropy conditioned on word content.': 1.0986090898513794, 'Exploring what sort of information is encoded in representation learning methods for NLP is an important and under-researched area.': 1.0986013412475586, 'For example, the tide of word-embeddings research was mostly stemmed after a thread of careful experimental results showing most embeddings to be essentially equivalent, culminating in ""Improving Distributional Similarity with Lessons Learned from Word Embeddings"" by Levy, Goldberg, and Dagan.': 1.0964381694793701, 'As representation learning becomes even more important in NLP this sort of research will be even more important.': 0.614489734172821, 'While this paper makes a valuable contribution in setting out and exploring a methodology for evaluating sentence embeddings, the evaluations themselves are quite simple and do not necessarily correlate with real-world desiderata for sentence embeddings (as the authors note in other comments, performance on these tasks is not a normative measure of embedding quality).': 1.0986123085021973, ""For example, as the authors note, the ability of the averaged vector to encode sentence length is trivially to be expected given the central limit theorem (or more accurately, concentration inequalities like Hoeffding's inequality)."": 1.0982928276062012, 'The word-order experiments were interesting.': 1.0985653400421143, 'A relevant citation for this sort of conditional ordering procedure is ""Generating Text with Recurrent Neural Networks"" by Sutskever, Martens, and Hinton, who refer to the conversion of a bag of words into a sentence as ""debagging.""': 1.0985376834869385, 'Although this is just a first step in better understanding of sentence embeddings, it is an important one': 1.0986123085021973, 'and I recommend this paper for publication.': 1.0986123085021973, 'This paper analyzes various unsupervised sentence embedding approaches by means of a set of auxiliary prediction tasks.': 1.0986123085021973, 'By examining how well classifiers can predict word order, word content, and sentence length, the authors aim to assess how much and what type of information is captured by the different embedding models.': 1.0986123085021973, 'The main focus is on a comparison between and encoder-decoder model (ED) and a permutation-invariant model, CBOW.': 1.0986123085021973, '(There is also an analysis of skip-thought vectors, but since it was trained on a different corpus it is hard to compare).': 1.0986123085021973, 'There are several interesting and perhaps counter-intuitive results that emerge from this analysis and the authors do a nice job of examining those results and, for the most part, explaining them.': 1.0986123085021973, 'However, I found the discussion of the word-order experiment rather unsatisfying.': 1.0986123085021973, ""It seems to me that the appropriate question should have been something like, 'How well does model X do compared to the theoretical upper bound which can be deduced from natural language statistics?'"": 1.0986123085021973, ""This is investigated from one angle in Section 7, but I would have preferred to the effect of natural language statistics discussed up front rather than presented as the explanation to a 'surprising' observation."": 1.0986095666885376, 'I had a similar reaction to the word-order experiments.': 1.0986123085021973, 'Most of the interesting results, in my opinion, are about the ED model.': 1.0986123085021973, 'It is fascinating that the LSTM encoder does not seem to rely on natural-language ordering statistics': 1.0986123085021973, 'it seems like doing so should be a big win in terms of per-parameter expressivity.': 1.0986123085021973, ""I also think that it's strange that word content accuracy begins to drop for high-dimensional embeddings."": 1.0986123085021973, 'I suppose this could be investigated by handicapping the decoder.': 1.0986123085021973, 'Overall, this is a very nice paper investigating some aspects of the information content stored in various types of sentence embeddings.': 1.0986123085021973, 'I recommend acceptance.': 1.0986123085021973, 'This paper presents a set of experiments investigating what kinds of information are captured in common unsupervised approaches to sentence representation learning.': 1.0986123085021973, 'The results are non-trivial and somewhat surprising.': 1.0986123085021973, 'For example, they show that it is possible to reconstruct word order from bag of words representations, and they show that LSTM sentence autoencoders encode interpretable features even for randomly permuted nonsense sentences.': 1.0986123085021973, 'Effective unsupervised sentence representation learning is an important and largely unsolved problem in NLP, and this kind of work seems like it should be straightforwardly helpful towards that end.': 1.0986123085021973, 'In addition, the experimental paradigm presented here is likely more broadly applicable to a range of representation learning systems.': 1.0986123085021973, 'Some of the results seem somewhat strange, but I see no major technical concerns, and think that that they are informative.': 1.0986123085021973, 'One minor red flag:': 1.0986123085021973, 'The massive drop in CBOW performance in Figures 1b and 4b are not explained, and seem implausible enough to warrant serious further investigation.': 1.0986123085021973, 'Can you be absolutely certain that those results would appear with a different codebase and different random seed implementing the same model?': 1.0986123085021973, 'Fortunately, this point is largely orthogonal to the major results of the paper.': 1.0986123085021973, 'Two writing comments:': 1.0986123085021973, ""I agree that the results with word order and CBOW are surprising, but I think it's slightly misleading to say that CBOW is predictive of word order."": 1.0986123085021973, ""It doesn't represent word order at all, but it's possible to probabilistically reconstruct word order from the information that it does encode."": 1.0986123085021973, 'Saying that ""LSTM auto-encoders are more effective at encoding word order than word content"" doesn\'t really make sense.': 1.0986123085021973, ""These two quantities aren't comparable."": 1.0986123085021973}"
53,https://openreview.net/forum?id=BJhZeLsxx,"{'This work proposed a simple but strong baseline for parametric texture synthesis.': 1.0986123085021973, 'In empirical experiments, samples generated by the baseline composed by multi-scale and random filters sometime rival the VGG-based model which has multi-layer and pre-trained filters.': 1.0986121892929077, 'The authors concluded that texture synthesis does not necessarily depend on deep hierarchical representations or the learned feature maps.': 1.0986123085021973, 'This work is indeed interesting and insightful.': 1.0981804132461548, 'However, the conclusions are needed to be further testified (especially for deep hierarchical representations).': 1.0985991954803467, 'Firstly, all of generated samples by both VGG and single layer model are not perfect and much worse than the results from non-parametric methods.': 1.0972799062728882, 'Besides VGG-based model seems to do better in inpainting task in Figure 7.': 1.0986123085021973, 'Last but not least, would a hierarchical model (instead of lots of filters with different size) handle multi-scale more efficiently?': 1.0986123085021973, 'This paper provides an interesting analysis of the conditions which enable generation of natural looking textures.': 1.0986123085021973, 'The results is quite surprising, and analysis is quite thorough.': 1.0986071825027466, 'I do think the evaluation methods require more work, but as other reviewers mentioned this could be an interesting line of work moving forwards and does not take too much from this current paper which, I think, should be accepted.': 1.0986123085021973, 'The framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures.': 1.0986123085021973, 'This paper investigates in detail which kind of deep or shallow networks may work well in this framework.': 1.0476677417755127, 'One of the main findings is that that very shallow nets, consisting of a single filter bank with random weights, work surprisingly well, and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19.': 1.0986123085021973, 'More broadly, the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis.': 1.0922709703445435, 'Figure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly.': 1.0986123085021973, 'This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function, which is a reasonable explanation.': 1.0986123085021973, 'In the new supplementary material, the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results, which is a useful complement to the original experiments.': 1.0985780954360962, 'The main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective.': 1.0969862937927246, 'It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture, hence with very good visual quality.': 1.0986123085021973, 'Such trivial statistics would also be very shallow.': 1.0986123085021973, 'The aim is instead to capture a texture distribution, and measuring how well a method meets this challenge remains an open problem.': 1.0810084342956543, 'Hence, while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics), it is difficult to conclusively confirm that this is the case.': 1.0985996723175049, 'The authors indicate that diversity could be measured in terms of entropy.': 1.09813392162323, 'This is reasonable, but, as they acknowledge in their answers to questions, difficult to do in practice.': 1.0836752653121948, 'Furthermore, this would still not account for the other aspect of the problem, namely visual quality.': 1.0976520776748657, 'They also suggest to perform a psychophysical assessment, which may be the only practical way of addressing this problem, but deem that to be material for future work.': 1.0970360040664673, 'Overall, since evaluation of image generation is such an hard problem, I think the paper still has sufficient strengths to warrant publication in ICLR.': 1.0986123085021973, 'Still, some form of psychophysical assessment would be useful to confirm the intuitions that, at present, can only be obtained by inspecting the figure sin the paper and in the supplementary material.': 1.0985337495803833}"
54,https://openreview.net/forum?id=BJjn-Yixl,"{'This paper introduces an attention-based recurrent network that learns to compare images by attending iteratively back and forth between a pair of images.': 1.0986123085021973, 'Experiments show state-of-the-art results on Omniglot, though a large part of the performance gain comes from when extracted convolutional features are used as input.': 1.0986123085021973, 'The paper is significantly improved from the original submission and reflects changes based on pre-review questions.': 1.0986123085021973, 'However, while there was an attempt made to include more qualitative results e.g. Fig. 2, it is still relatively weak and could benefit from more examples and analysis.': 1.0986119508743286, 'Also, why is the attention in Fig.': 1.0986123085021973, '2 always attending over the full character?': 1.091578483581543, 'Although it is zooming in, shouldn’t it attend to relevant parts of the character?': 1.0986123085021973, 'Attending to the full character on a solid background seems a trivial solution where it is then unclear where the large performance gains are coming from.': 0.042204514145851135, 'While the paper is much more polished now, it is still lacking in details in some respects, e.g. details of the convolutional feature extractor used that gives large performance gain.': 1.0985662937164307, 'This paper presents an attention based recurrent approach to one-shot learning.': 1.09861159324646, 'It reports quite strong experimental results (surpassing human performance/HBPL) on the Omniglot dataset, which is somewhat surprising because it seems to make use of very standard neural network machinery.': 1.0986123085021973, 'The authors also note that other have helped verify the results (did Soumith Chintala reproduce the results?) and do provide source code.': 1.0986123085021973, ""After reading this paper, I'm left a little perplexed as to where the big performance improvements are coming from as it seems to share a lot of the same components of previous work."": 1.0986106395721436, ""If the author's could report result from a broader suite of experiments like in previous work (e.g matching networks), it would much more convincing."": 1.0986123085021973, 'An ablation study would also help with understanding why this model does so well.': 1.0986123085021973, 'This paper describes a method that estimates the similarity between a set of images by alternatively attend each image with a recurrent manner.': 1.0986123085021973, ""The idea of the paper is interesting, which mimic the human's behavior."": 1.0986121892929077, 'However, there are several cons of the paper:': 1.0986027717590332, ""1. The paper is now well written. There are too many 'TODO', 'CITE' in the final version of the paper, which indicates that the paper is submitted in a rush or the authors did not take much care about the paper. I think the paper is not suitable to be published with the current version."": 0.9469350576400757, '2. The missing of the experimental results. The paper mentioned the LFW dataset. However, the paper did not provide the results on LFW dataset. (At least I did not find it in the version of Dec. 13th)': 0.9124727249145508, '3. The experiments of Omniglot dataset are not sufficient. I suggest that the paper provides some illustrations about how the model the attend two images (e.g. the trajectory of attend).': 0.7246900796890259}"
55,https://openreview.net/forum?id=BJluGHcee,"{'This paper proposes a generative model for mixtures of basic local structures where the dependency between local structures is a tensor.': 1.0986123085021973, 'They use tensor decomposition and the result of their earlier paper on expressive power of CNNs along with hierarchical Tucker to provide an inference mechanism.': 1.0986123085021973, 'However, this is conditioned on the existence of decomposition.': 1.0986123085021973, 'The authors do not discuss how applicable their method is for a general case, what is the subspace where this decomposition exists/is efficient/has low approximation error.': 1.0986123085021973, 'Their answer to this question is that in deep learning era these theoretical analysis is not needed.': 1.0986123085021973, 'While this claim is subjective, I need to emphasize that the paper does not clarify this claim and does not mention the restrictions.': 1.0986123085021973, 'Hence, from theoretical perspective, the paper has flaws and the claims are not justified completely.': 1.0986123085021973, 'Some claims cannot be justified with the  current results in tensor literature as the authors also mentioned in the discussions.': 1.0986123085021973, 'Therefore, they should have corrected their claims in the paper and made the clarifications that this approach is restricted to a clear subclass of tensors.': 1.0986100435256958, 'If we ignore the theoretical aspect and only consider the paper from empirical perspective, the experiments the appear in the paper are not enough to accept the paper.': 1.076832890510559, 'MNIST and CIFAR-10 are very simple baselines and more extensive experiments are required.': 1.087623953819275, 'Also, the experiments for missing data are not covering real cases and are too synthetic.': 1.0984127521514893, 'Also, the paper lacks the extension beyond images.': 1.098591685295105, 'Since the authors repeatedly mention that their approach goes beyond images, and since the theory part is not complete, those experiments are essential for acceptance of this paper.': 1.0983772277832031, 'This paper uses Tensors to build generative models.': 1.0986123085021973, 'The main idea is to divide the input into regions represented with mixture models, and represent the joint distribution of the mixture components with a tensor.': 1.0977215766906738, 'Then, by restricting themselves to tensors that have an efficient decomposition, they train convolutional arithmetic circuits to generate the probability of the input and class label, providing a generative model of the input and labels.': 1.0084351301193237, 'This approach seems quite elegant.': 0.40984252095222473, 'It is not completely clear to me how the authors choose the specific architecture for their model, and how these choices relate to the class of joint distributions that they can represent, but even if these choices are somewhat heuristic, the overall framework provides a nice way of controlling the generality of the distributions that are represented.': 1.0979382991790771, 'The experiments are on simple, synthetic examples of missing data.': 0.9120625853538513, 'This is somewhat of a limitation, and the paper would be more convincing if it could include experiments on a real-world problem that contained missing data.': 1.0986123085021973, 'One issue here is that it must be known which elements of the input are missing, which somewhat limits applicability.': 1.0986123085021973, 'Could experiments be run on problems relating to the Netflix challenge, which is the classic example of a prediction problem with missing data?': 1.0986123085021973, 'In spite of these limitations, the experiments provide appropriate comparisons to prior work, and form a reasonable initial evaluation.': 1.0986123085021973, 'I was a little confused about how the input of missing data is handled experimentally.': 1.0986123085021973, 'From the introductory discussion my impression was that the generative model was built over region patches in the image.': 1.0986123085021973, 'This led me to believe that they would marginalize over missing regions.': 1.0986123085021973, 'However, when the missing data consists of IID randomly missing pixels, it seems that every region will be missing some information.': 1.0986123085021973, 'Why is it appropriate to marginalize over missing pixels?': 1.0986123085021973, 'Specifically,  in Equation 6 represents a local region, and the ensuing discussion shows how to marginalize over missing regions.': 1.0986123085021973, 'How is this done when only a subset of a region is missing?': 1.0986123085021973, 'It also seems like the summation in the equation following Equation 6 could be quite large.': 1.0986123085021973, 'What is the run time of this?': 1.0986123085021973, 'The paper is also a bit schizophrenic about the extent to which the results are applicable beyond images.': 1.0986123085021973, 'The motivation for the probabilistic model is mostly in terms of images.': 1.0986123085021973, 'But in the experiments, the authors state that they do not use state-of-the-art inpainting algorithms because their method is not limited to images and they want to compare to methods that are restricted to images.': 1.0986123085021973, 'This would be more convincing if there were experiments outside the image domain.': 1.0986123085021973, 'It was also not clear to me how, if at all, the proposed network makes use of translation invariance.': 1.0986123085021973, 'It is widely assumed that much of the success of CNNs comes from their encoding of translation invariance through weight sharing.': 1.0986123085021973, 'Is such invariance built into the authors’ network?': 1.0986123085021973, 'If not, why would we expect it to work well in challenging image domains?': 1.0986123085021973, 'As a minor point, the paper is not carefully proofread.': 1.0986123085021973, 'To just give a few examples from the first page or so:': 1.0986123085021973, '“significantly lesser” -> “significantly less”': 1.0986123085021973, '“the the”': 1.0986123085021973, '“provenly” -> provably': 1.0986123085021973, 'The paper provides an interesting use of generative models to address the classification with missing data problem.': 1.0986123085021973, 'The tensorial mixture models proposed take into account the general problem of dependent samples.': 1.0986123085021973, 'This is an nice extension of current mixture models where samples are usually considered as independent.': 1.0986123085021973, 'Indeed the TMM model is reduced to the conventional latent variable models.': 1.0986123085021973, 'As much as I love the ideas behind the paper, I feel pitiful about the sloppiness of the presentation (such as missing notations) and flaws in the technical derivations.': 1.0986123085021973, 'Before going into the technical details, my high level concerns are as follows:': 1.0986123085021973, '(1) The joint density over all samples is modeled as a tensorial mixture generative model.': 1.0986123085021973, 'The interpretation of the CP decomposition or HT decomposition on the prior density tensor is not clear.': 1.0986123085021973, 'The authors have an interpretation of TMM as product of mixture models when samples are independent, however their interpretation seems flawed to me, and I will elaborate on this in the detailed technical comments below.': 1.0986123085021973, '(2) The authors employ convolution operators to compute an inner product.': 1.0986123085021973, 'It is realizable by zero padding, but the invariance structure, which is the advantage of CNN compared to feed-forward neural network, will be lost.': 1.0986123085021973, 'However, I am not sure how much this would affect the performance in practice.': 1.0986123085021973, '(3) The author could comment in the paper a little bit on the sample complexity of this method given the complexity of the model.': 1.0986123085021973, ""Because I liked the ideas of the paper so much, and the ICLR paper submitted didn't present the technical details well due to sloppiness of notations, so I read the technical details in the arXiv version the authors pointed out."": 1.0986123085021973, 'There are a few technical typos that I would like to point out (my reference to equations are to the ones in the arXiv paper).': 1.0986123085021973, '(1) The generative model as in figure (5) is flawed.': 1.0986123085021973, 'P(x_i|d_i;\\theta_{d_i}) are vectors of length s, there the product of vectors is not well defined.': 1.0986123085021973, 'It is obvious that the dimensions of the terms between two sides of the equation are not equal.': 1.0986123085021973, 'In fact, this should be a tucker decomposition instead of multiplication.': 1.0986123085021973, 'It should be P(X) = \\sum_{d1,\\ldots,d_N} P(d_1,\\ldots,d_N) (P(x_1|d_1;theta_{d_1},P(x_2|d_2;theta_{d_2},\\ldots,P(x_N|d_N;theta_{d_N}), which means a sum of multi-linear operation on tensor P(d_1,\\ldots,d_N), and each mode is projected onto P(x_i|d_i;theta_{d_i}.': 1.0975464582443237, ""(2) I suspect the special case for diagonal Gaussian Mixture Models has some typos as I couldn't derive the third last equation on page 6."": 1.0984249114990234, ""But it might be just I didn't understand this example."": 1.0935972929000854, '(3) The claim that TMM reduces to product of mixture model is not accurate.': 1.0986095666885376, 'The first equation on page 7 is only right when ""sum of product"" operation is equal to ""product of sum"" operation.': 1.0923166275024414, ""Similarly, in equation (6), the second equality doesn't hold unless in some special cases."": 1.0980952978134155, 'However, this is not true.': 1.0986121892929077, 'This might be just a typo, but it is good if the authors could fix this.': 1.0986123085021973, 'I also suspect that if the authors correct this typo,the performance on MNIST might be improved.': 1.0986123085021973, 'Overall, I like the ideas behind this paper very much.': 1.0986123085021973, 'I suggest the authors fix the technical typos if the paper is accepted.': 1.0986123085021973}"
56,https://openreview.net/forum?id=BJlxmAKlg,"{'The paper proposes an architecture called ReasoNet that reason over the relation.': 1.098596215248108, 'The paper addresses important tasks but there are many other related works.': 1.0764358043670654, 'The comparison to other methods are not comprehensive.': 1.0986123085021973, 'The Graph Reachability dataset is not a good example to use.': 1.0984408855438232, 'The paper aims to consolidate some recent literature in simple types of ""reading comprehension"" tasks involving matching questions to answers to be found in a passage, and then to explore the types of structure learned by these models and propose modifications.': 1.0986123085021973, 'These reading comprehension datasets such as CNN/Daily Mail are on the simpler side because they do not generally involve chains of reasoning over multiple pieces of supporting evidence as can be found in datasets like MCTest.': 1.0986123085021973, 'Many models have been proposed for this task, and the paper breaks down these models into ""aggregation readers"" and ""explicit reference readers.""': 1.0986123085021973, 'The authors show that the aggregation readers organize their hidden states into a predicate structure which allows them to mimic the explicit reference readers.': 1.0986123085021973, 'The authors then experiment with adding linguistic features, including reference features, to the existing models to improve performance.': 1.0986123085021973, 'I appreciate the re-naming and re-writing of the paper to make it more clear that the aggregation readers are specifically learning a predicate structure, as well as the inclusion of results about dimensionality of the symbol space.': 1.0198673009872437, 'Further, I think the effort to organize and categorize several different reading comprehension models into broader classes is useful, as the field has been producing many such models and the landscape is unclear.': 1.0986123085021973, 'The concerns with this paper are that the predicate structure demonstrated is fairly simple, and it is not clear that it provides insight towards the development of better models in the future, since the ""explicit reference readers"" need not learn it, and the CNN/Daily Mail dataset has very little headroom left as demonstrated by Chen et al. 2016.': 1.0986123085021973, 'The desire for ""dramatic improvements in performance"" mentioned in the discussion section probably cannot be achieved on these datasets.': 1.0986123085021973, 'More complex datasets would probably involve multi-hop inference which this paper does not discuss.': 1.0986123085021973, 'Further, the message of the paper is a bit scattered and hard to parse, and could benefit from a bit more focus.': 1.0985983610153198, 'I think that with the explosion of various competing neural network models for NLP tasks, contributions like this one which attempt to organize and analyze the landscape are valuable, but that this paper might be better suited for an NLP conference or journal such as TACL.': 1.0986108779907227, 'This paper proposes a new architecture for document comprehension.': 1.0986123085021973, 'The main addition to the model, as claimed by the authors, is that the model is able to adaptively determine how many inference ‘hops’ is required in order to solve a particular problem.': 1.098594307899475, 'This is in contrast to previous work, where the number of hops is fixed.': 0.7675495147705078, 'Overall, the change of adding a termination gate is rather modest, and it leads to rather small gains on the tested CNN/ Daily Mail dataset (is it possible to include significance here?).': 1.098442554473877, 'Actually, I’m not sure why having an adaptive number of hops would be better performance-wise than using the maximum number of hops – unless I missed it the authors don’t argue this point well (other than saying it mimics humans).': 1.0986123085021973, 'Does the model forget some things if it performs too many hops?': 1.0986113548278809, 'The authors do say:': 1.097941279411316, '“The results suggest that the termination gate variable in the ReasoNet is helpful when training with sophisticated examples, and makes models converge faster”': 1.0986123085021973, 'but don’t elaborate much beyond this.': 1.0986109972000122, 'I could see for example an argument being made that it reduces the amount of computation required per question.': 1.0986123085021973, 'The authors do show results comparing the model without a termination gate on the Graph Reachability dataset, and the full model does seem to perform quite a bit better, but I would like this to also be done on the CNN/ Daily Mail datasets, and for there to be more insights into why the performance is improved vs. the ReasoNet-Last model.': 1.0986123085021973, 'One of the contributions I like most from this paper is not the actual model, but the Graph Reachability dataset.': 0.4754459857940674, 'It is designed to test the reasoning abilities of the ReasoNet model in more detail.': 1.0986121892929077, 'One of the benefits is that the inference procedure necessary to solve the task is very clear, as opposed to the CNN/ Daily Mail dataset, thus it is easier to see what the model is actually doing.': 1.03907310962677, 'I would like to see future models also tested on this dataset.': 0.8264849781990051, 'Overall, I think this is a borderline paper.': 1.0783592462539673, 'Other remarks:': 1.0986123085021973, 'Note that learning a baseline for REINFORCE has previously been studied, see: https://arxiv.org/pdf/1606.01541v4.pdf (although the authors mention only it briefly in the paper)': 1.0949400663375854, '“ReasoNets are devised to mimic the inference process of human readers.”': 1.0868089199066162, 'I think this is too strong a claim (that humans use the same kind of ‘iterative hop’ method for answering questions), unless it is supported by actual analysis of humans – I think the similarities to humans are more at a surface level.': 1.0986037254333496, 'Also, I think it’s unnecessary to actually understanding the model.': 1.097368836402893, 'I would change ‘mimic’ to ‘inspired by’, or something along those lines.': 1.0986123085021973, 'EDIT: I thank the authors for taking the time to reply, and clearing some things up with regards to the idea of multiple hops.': 1.0977580547332764, 'I am keeping my score the same for the following reason: in my opinion, accepted papers involving new model architectures should either consist of (1) a small adjustment that leads to a large improvement, or (2) a very innovative idea that leads to comparable performance (or better), but provides many new insights about the problem or can be transferred to many different domains.': 1.0980303287506104, 'This paper seems to be a rather small adjustment (allowing multiple hops) which results in small improvements on CNN/Daily Mail (which as Reviewer 3 points out has little headroom).': 1.0372600555419922, 'I think this paper would be suitable for a conference such as EMNLP.': 1.0274378061294556}"
57,https://openreview.net/forum?id=BJm4T4Kgx,"{'This paper has two main contributions:': 1.0986123085021973, '(1) Applying adversarial training to imagenet, a larger dataset than previously considered': 1.0986123085021973, '(2) Comparing different adversarial training approaches, focusing importantly on the transferability of different methods.': 1.0986123085021973, 'The authors also uncover and explain the label leaking effect which is an important contribution.': 1.0986123085021973, 'This paper is clear, well written and does a good job of assessing and comparing adversarial training methods and understanding their relation to one another.': 1.0986123085021973, 'A wide range of empirical results are shown which helps elucidate the adversarial training procedure.': 1.0986123085021973, 'This paper makes an important contribution towards understand adversarial training and believe ICLR is an appropriate venue for this work.': 1.0986123085021973, 'This paper investigate the phenomenon of the adversarial examples and the adversarial training on the dataset of ImageNet.': 1.0986123085021973, 'While the final conclusions are still vague, this paper raises several noteworthy finding from its experiments.': 1.0986123085021973, 'The paper is well written and easy to follow.': 1.0986123085021973, 'Although I still have some concerns about the paper (see the comments below), this paper has good contributions and worth to publish.': 1.0986123085021973, 'Pros:': 1.0986123085021973, 'For the first time in the literature, this paper proposed the concept of ‘label leaking’.': 1.0986123085021973, 'Although its effect only becomes significant when the dataset is large, it should be carefully handled in the future research works along this line.': 1.0986123085021973, ""Using the ratio of 'clean accuracy' over ‘adversarial accuracy’ as the measure of robust is more reasonable compared to the existing works in the literature."": 1.0986123085021973, 'Cons:': 1.0986123085021973, 'Although the conclusions of the paper are based on the experiments on ImageNet, the title of the paper seems a little misleading.': 1.0986123085021973, 'I consider Section 4 as the main contribution of the paper.': 1.0986123085021973, 'Note that Section 4.3 and Section 4.4 are not specific to large-scale dataset, thus emphasizing the ‘large-scale’ in the title and in the introduction seems improper.': 1.0986123085021973, 'Basically all the conclusions of the paper are made based on observing the experimental results.': 1.0986123085021973, 'Further tests should have been performed to verify these hypotheses.': 1.0986123085021973, 'Without that, the conclusions of the paper seems rushy.': 1.0986123085021973, 'For example, one dataset of imageNet can not infer the conclusions for all large-scale datasets.': 1.0986123085021973, 'This paper is a well written paper.': 1.0986123085021973, 'This paper can be divided into 2 parts:': 1.0986123085021973, '1.Adversary training on ImageNet': 1.0986123085021973, '2.Empirical study of label leak, single/multiple step attack, transferability and importance of model capacity': 1.0986123085021973, 'For part [1], I don’t think training without clean example will not make reasonable ImageNet level model.': 1.0986123085021973, ""Ian’s experiment in “Explaining and Harnessing Adversarial Examples” didn't use BatchNorm, which may be important for training large scale model."": 1.0986123085021973, 'This part looks like an extension to Ian’s work with Inception-V3 model.': 1.0986123085021973, 'I suggest to add an experiment of training without clean samples.': 1.0986123085021973, 'For part [2], The experiments cover most variables in adversary training, yet lack technical depth.': 1.0986123085021973, 'The depth, model capacity experiments can be explained by regularizer effect of adv training;  Label leaking is novel; In transferability experiment with FGSM, if we do careful observe on some special MNIST FGSM example, we can find augmentation effect on numbers, which makes grey part on image to make the number look more like the other numbers.': 1.0580908060073853, 'Although this effect is hard to be observed with complex data such as CIFAR-10 or ImageNet, they may be related to the authors\' observation ""FGSM examples are most transferable"".': 1.0046579837799072, 'In this part the authors raise many interesting problems or guess, but lack theoretical explanations.': 0.41642776131629944, 'Overall I think these empirical observations are useful for future work.': 1.0572441816329956}"
58,https://openreview.net/forum?id=BJmCKBqgl,"{'Dyvedeep presents three approximation techniques for deep vision models aimed at improving inference speed.': 1.0986123085021973, 'The techniques are novel as far as I know.': 1.0986123085021973, 'The paper is clear, the results are plausible.': 1.0986123085021973, 'The evaluation of the proposed techniques is does not make a compelling case that someone interested in faster inference would ultimately be well-served by a solution involving the proposed methods.': 1.0986123085021973, 'The authors delineate ""static"" acceleration techniques (e.g. reduced bit-width, weight pruning) from ""dynamic"" acceleration techniques which are changes to the inference algorithm itself.': 1.0986123085021973, 'The delineation would be fine if the use of each family of techniques were independent of the other, but this is not the case.': 1.0986123085021973, 'For example, the use of SPET would, I think, conflict with the use of factored weight matrices (I recall this from http://papers.nips.cc/paper/5025-predicting-parameters-in-deep-learning.pdf, but I suspect there may be more recent work).': 1.0984079837799072, 'For this reason, a comparison between SPET and factored weight matrices would strengthen the case that SPET is a relevant innovation.': 1.0986123085021973, 'In favor of the factored-matrix approach, there would I think be fewer hyperparameters and the computations would make more-efficient use of blocked linear algebra routines': 1.0986123085021973, 'the case for the superiority of SPET might be difficult to make.': 1.0986123085021973, 'The authors also do not address their choice of the Xeon for benchmarking, when the use cases they identify in the introduction include ""low power"" and ""deeply embedded"" applications.': 1.0986123085021973, 'In these sorts of applications, a mobile GPU would be used, not a Xeon.': 1.0986123085021973, 'A GPU implementation of a convnet works differently than a CPU implementation in ways that might reduce or eliminate the advantage of the acceleration techniques put forward in this paper.': 1.0986123085021973, 'This work proposes a number of approximations for speeding up feed-forward network computations at inference time.': 1.0986123085021973, 'Unlike much of the previous work in this area which tries to compress a large network, the authors propose algorithms that decide whether to approximate computations for each particular input example.': 1.098312497138977, 'Speeding up inference is an important problem and this work takes a novel approach.': 1.0986123085021973, 'The presentation is exceptionally clear, the diagrams are very beautiful, the ideas are interesting, and the experiments are good.': 1.0986123085021973, 'This is a high-quality paper.': 1.0986123085021973, 'I especially enjoyed the description of the different methods proposed  (SPET, SDSS, SFMA) to exploit patterns in the classifer.': 1.0986123085021973, 'My main concern is that the significance of this work is limited because of the additional complexity and computational costs of using these approximations.': 1.0986123085021973, 'In the experiments, the DyVEDeep approach was compared to serial implementations of four large classification models': 1.0986123085021973, 'inference in these models is order of magnitudes faster on systems that support parallelization.': 1.0986123085021973, 'I assume that DyVEDeep has little-to-no performance advantage on a system that allows parallelization, and so anyone looking to speed up their inference on a serial system would want to see a comparison between this approach and the model-compression approaches.': 1.0986123085021973, ""Thus, I am not sure how much of an impact this approach can have in it's current state."": 1.0986123085021973, 'Suggestions:': 1.0986123085021973, 'I wondered what (if any) bounds could be made on the approximation errors of the proposed methods?': 1.0986123085021973, 'The authors describe a series of techniques which can be used to reduce the total amount of computation that needs to be performed in Deep Neural Networks.': 1.0986123085021973, 'The authors propose to selectively identify how important a certain set of computations is to the final DNN output, and to use this information to selectively skip certain computations in the network.': 1.0986123085021973, 'As deep learning technologies become increasingly widespread on mobile devices, techniques which enable efficient inference on such devices are becoming increasingly important for practical applications.': 1.0986123085021973, 'The paper is generally well-written and clear to follow.': 1.0986123085021973, 'I had two main comments that concern the experimental design, and the relationship to previous work:': 0.8119819164276123, '1. In the context of deployment on mobile devices, computational costs in terms of both system memory as well as processing are important consideration. While the proposed techniques do improve computational costs, they don’t reduce model size in terms of total number of parameters. Also, the gains obtained using the proposed method appear to be similar to other works that do allow for improvements in terms of both memory and computation (see, e.g., (Han et al., 2015)). It would have been interesting if the authors had reported results when the proposed techniques were applied to models that have been compressed in size as well.': 1.097573161125183, 'S. Han, H. Mao, and W. J. Dally.': 1.0986123085021973, '""Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding.""': 1.0986123085021973, 'arXiv prepring arXiv:1510.00149 (2015).': 1.0986123085021973, '2. The SDSS technique in the paper appears to be very similar to the “Perforated CNN” technique proposed by Figurnov et al. (2015). In that work, as in the authors work, CNN activations are approximated by interpolating responses from neighbors. The authors should comment on the similarity and differences between the proposed method and the referenced work.': 1.0986123085021973, 'Figurnov, Michael, Dmitry Vetrov, and Pushmeet Kohli.': 1.0986123085021973, '""Perforatedcnns: Acceleration through elimination of redundant convolutions.""': 1.0986123085021973, 'arXiv preprint  arXiv:1504.08362 (2015).': 1.0986123085021973, 'Other minor comments appear below:': 1.0986123085021973, '3. A clarification question: In comparing the proposed methods to the baseline, in Section 4, the authors mention that they used their own custom implementation. However, do the baselines use the same custom implementation, or do they used the optimized BLAS libraries?': 1.0986123085021973, '4. The authors should also consider citing the following additional references:': 1.0986123085021973, '* S. Tan and K. C. Sim, ""Towards implicit complexity control using variable-depth deep neural networks for automatic speech recognition,"" 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Shanghai, 2016, pp.': 1.0986123085021973, '5965-5969.': 1.0986123085021973, '* Graves, Alex.': 1.0986123085021973, '""Adaptive Computation Time for Recurrent Neural Networks.""': 1.0986123085021973, 'arXiv preprint arXiv:1603.08983 (2016).': 1.0986123085021973, '5. Please explain what the Y-axis in Figure 7 represents in the text.': 1.0986123085021973, '6. Typographical Error: Last paragraph of Section 2: “... are qualitatively different the aforementioned ...” → “... are qualitatively different from the aforementioned ...”': 1.0986123085021973}"
59,https://openreview.net/forum?id=BJrFC6ceg,"{'It is refreshing that OpenAI has taken the time to resurrect classic heuristics like down-sampling and dropout into PixelCNN.': 1.3862943649291992, 'Some sort of AR technique like PixelCNN probably holds the missing keys needed to eventually have decent originally-created images from CIFAR10 or other real-life data-sets.': 1.3862943649291992, 'So any engineering streamlining, as in this paper, is welcome to the general public, especially when helping to avoid expensive clusters of GPUs, only DeepMind can afford.': 1.3862943649291992, 'In this sense, OpenAI is fulfilling its mission and we are all very grateful!': 1.3862942457199097, 'Thus the paper is a welcome addition and we hope it finds its way into what appears to be an experimental CS conference anyway.': 1.3862942457199097, 'On a more conceptual level, our hope is that OpenAI, with so talented a team, will stop competing in these contrived contests to improve by basis points certain obscure log-likelihoods and instead focus on the bigger picture problems.': 1.3862943649291992, 'Why for example, almost two years later, the class-conditional CIFAR10 samples, as on the left of Figure 4 in this paper (column 8 - class of horses), are still inferior to, say, the samples on Figure 4 of reference [2]?': 1.3862943649291992, ""Forgive the pun, but aren't we beating a dead horse here?"": 0.47030189633369446, 'Yes, resolution and sharpness have improved, due to good engineering but nobody in the general public will take these samples seriously!': 1.3862937688827515, 'Despite the claims put forward by some on the DeepMind team, PixelCNN is not a ""fully-generative"" neural net (as rigorously defined in section 3 of reference': 1.3839638233184814, '[1]), but merely a perturbative net, in the vain of the Boltzmann machine.': 1.3862943649291992, 'After the Procrustean experience of lost decades on Boltzmann machines, the time perhaps has come to think more about the fundamentals and less about the heuristics?': 1.38613760471344, '[1] https://arxiv.org/pdf/1508.06585v5.pdf': 1.3862943649291992, '[2] https://arxiv.org/pdf/1511.02841v3.pdf': 1.3862943649291992, 'Apologies for the late submission of this review, and thank you for the author’s responses to earlier questions.': 1.3862943649291992, 'This submission proposes an improved implementation of the PixelCNN generative model.': 1.3862943649291992, 'Most of the improvements are small and can be considered as specific technical details such as the use of dropout and skip connections, while others are slightly more substantial such as the use of a different likelihood model and multiscale analysis.': 1.3747451305389404, 'The submission demonstrates state-of-the-art likelihood results on CIFAR-10.': 1.3862943649291992, 'My summary of the main contribution:': 1.3862943649291992, 'Autoregressive-type models - of which PixelCNN is an example - are a nice class of models as their likelihood can be evaluated in closed form.': 1.1186412572860718, 'A main differentiator for this type of models is how the conditional likelihood of one pixel conditioned on its causal neighbourhood is modelled:': 1.3862354755401611, 'In one line of work such as (Theis et al, 2012 MCGSM, Theis et al 2015 Spatial LSTM)': 1.3862931728363037, 'the conditional distribution is modelled as a continuous density over real numbers.': 1.3766696453094482, 'This approach has limitations: We know that in observed data pixel intensities are quantized to a discrete integer representation so a discrete distribution could give better likelihoods.': 1.3862943649291992, 'Furthermore these continuous distributions have a tail and assign some probability mass outside the valid range of pixel intensities, which may hurt the likelihood.': 1.3862943649291992, 'In more recent work by van den Oord and colleagues the conditional likelihood is modelled as an arbitrary discrete distribution over the 256 possible values for pixel intensities.': 1.3862943649291992, 'This does not suffer from the limitations of continuous likelihoods, but it also seems wasteful and is not very data efficient.': 1.3862943649291992, 'The authors propose something in the middle by keeping the discretized nature of the conditional likelihood, but restricting the discrete distribution to ones whose CDF that can be modeled as a linear combination of sigmoids.': 1.3862943649291992, 'This approach makes sense to me, and is new in a way, but it doesn’t appear to be very revolutionary or significant to me.': 1.3800994157791138, 'The second somewhat significant modification is the use of downsampling and multiscale modelling (as opposed to dilated convolutions).': 1.3862943649291992, 'The main motivation for the authors to do this is saving computation time while keeping the multiscale flexibility of the model.': 1.3592880964279175, 'The authors also introduce shortcut connections to compensate for the potential loss of information as they perform downsampling.': 1.278518795967102, 'Again, I feel that this modification not particularly revolutionary.': 1.3779088258743286, 'Multiscale image analysis with autoregressive generative models has been done for example in (Theis et al, 2012) and several other papers.': 1.3855080604553223, 'Overall I felt that this submission falls short on presenting substantially new ideas, and reads more like documentation for a particular implementation of an existing idea.': 1.3824126720428467, '# Review': 1.3862943649291992, 'This paper proposes five modifications to improve PixelCNN, a generative model with tractable likelihood.': 1.385920524597168, 'The authors empirically showed the impact of each of their proposed modifications using a series of ablation experiments.': 1.3862943649291992, 'They also reported a new state-of-the-art result on CIFAR-10.': 1.3826782703399658, 'Improving generative models, especially for images, is an active research area and this paper definitely contributes to it.': 1.3862943649291992, '# Pros': 0.7063008546829224, 'The authors motivate each modification well they proposed.': 1.3158180713653564, 'They also used ablation experiments to show each of them is important.': 1.3862897157669067, 'The authors use a discretized mixture of logistic distributions to model the conditional distribution of a sub-pixel instead of a 256-way softmax.': 1.3862923383712769, 'This allows to have a lower output dimension and to be better suited at learning ordinal relationships between sub-pixel values.': 1.3862943649291992, 'The authors also mentioned it speeded up training time (less computation) as well as the convergence during the optimization of the model (as shown in Fig.6).': 1.3862943649291992, 'The authors make an interesting remark about how the dependencies between the color channels of a pixel are likely to be relatively simple and do not require a deep network to model.': 1.3862937688827515, ""This allows them to have a simplified architecture where you don't have to separate out all feature maps in 3 groups depending on whether or not they can see the R/G/B sub-pixel of the current location."": 1.3861923217773438, '# Cons': 0.7699565291404724, 'It is not clear to me what the predictive distribution for the green channel (and the blue) looks like.': 1.3862262964248657, 'More precisely, how are the means of the mixture components linearly depending on the value of the red sub-pixel?': 1.3862800598144531, 'I would have liked to see the equations for them.': 1.386225700378418, '# Minor Comments': 1.309906005859375, 'In Fig.2 it is written ""Sequence of 6 layers"" but in the text (Section 2.4) it says 6 blocks of 5 ResNet layers.': 1.3690444231033325, 'What is the remaining layer?': 1.3853459358215332, 'In Fig.2 what does the first ""green square -> blue square"" which isn\'t in the white rectangle represents?': 1.3827461004257202, 'Is there any reason why the mixture indicator is shared across all three channels?': 1.3861689567565918, 'Summary:': 1.3862943649291992, 'This paper on autoregressive generative models explores various extensions of PixelCNNs.': 1.3852065801620483, 'The proposed changes are to replace the softmax function with a logistic mixture model, to use dropout for regularization, to use downsampling to increase receptive field size, and the introduction of particular skip connections.': 1.3862942457199097, 'The authors find that this allows the PixelCNN to outperform a PixelRNN on CIFAR-10, the previous state-of-the-art model.': 0.6275806427001953, 'The authors further explore the performance of PixelCNNs with smaller receptive field sizes.': 1.000377893447876, 'Review:': 1.3862926959991455, 'This is a useful contribution towards better tractable image models.': 1.3840246200561523, 'In particular, autoregressive models can be quite slow at test time, and the more efficient architectures described here should help with that.': 1.3862882852554321, 'My main criticism regards the severe neglect of related work.': 1.3226782083511353, 'Mixture models have been used a lot in autoregressive image modeling, including for multivariate conditional densities and including downsampling to increase receptive field size, albeit in a different manner:': 0.7240595817565918, 'Domke (2008), Hosseini et al. (2010), Theis et al. (2012), Uria et al. (2013), Theis et al. (2015).': 0.7846112847328186, 'Note that the logistic distribution is a special case of the Gaussian scale mixture (West, 1978).': 1.30695641040802, 'The main difference seems to be the integration of the density to model integers.': 1.3848072290420532, 'While this is clearly a good idea and the right way forward, the authors claim but do not support that not doing this has “proved to be a problem for earlier models based on continuous distributions”.': 1.3861149549484253, 'Please elaborate, add a reference, or ideally report the performance achieved by PixelCNN++ without integration (and instead adding uniform noise to make the variables continuous).': 1.1729823350906372, '60,000 images are not a lot in a high-dimensional space.': 1.3862943649291992, 'While I can see the usefulness of regularization for specialized content – and this can serve as a good example to demonstrate the usefulness of dropout – why not use “80 million tiny images” (superset of CIFAR-10) for natural images?': 1.3862943649291992, 'Semi-supervised learning should be fairly trivial here (because the model’s likelihood is tractable), so this data could even be used in the class-conditional case.': 1.3862756490707397, 'It would be interesting to know how fast the different models are at test time (i.e., when generating images).': 1.3862943649291992}"
60,https://openreview.net/forum?id=BJtNZAFgg,"{'This paper provides an interesting idea, which extends GAN by taking into account bidirectional network.': 1.0986123085021973, 'Totally, the paper is well-written, and easy to follow what is contribution of this paper.': 1.0986119508743286, 'From the theoretical parts, the proposed method, BiGAN, inherits similar properties in GAN.': 1.0985875129699707, 'The experimental results show that BiGAN is competitive with other methods.': 1.0571101903915405, 'A drawback would a non-convex optimization problem in BiGAN, this paper is still suitable to be accepted in my opinion.': 1.0986078977584839, 'This is a parallel work with ALI.': 1.09812331199646, 'The idea is using auto encoder to provide extra information for discriminator.': 1.0986123085021973, 'This approach seems is promising from reported result.': 1.0986108779907227, 'For feature learning part of BiGAN, there still is a lot of space to improve, compare to standard supervised convnet.': 1.0986123085021973, 'The authors extend GANs by an inference path from the data space to the latent space and a discriminator that operates on the joint latend/data space.': 1.0986123085021973, 'They show that the theoretical properties of GANs still hold for BiGAN and evaluate the features learned unsupervised in the inference path with respect to performance on supervised tasks after retraining deeper layers.': 1.0986088514328003, 'I see one structural issue with this paper: Given that, as stated in the abstract, the main purpose of the paper is to learn unsupervised features (and not to improve GANs), the paper might spent too much space on detailing the relationship to GANs and all the theoretical properties.': 1.0982182025909424, 'It is not clear whether they actually would help with the goal of learning good features.': 1.0986123085021973, 'While reading the paper, I actually totally forgot about the unsupervised features until they reappeared on page 6.': 1.0986123085021973, 'I think it would be helpful if the text of the paper would be more aligned with this main story.': 1.0986121892929077, 'Still, the BiGAN framework is an elegant and compelling extension to GANs.': 1.098503589630127, 'However, it is not obvious how much the theoretical properties help us as the model is clearly not fully converged.': 1.0986123085021973, 'To me, especially Figure 4 seems to suggest that G(E(x)) might be doing not much more than some kind of nearest neighbour retrival (and indeed one criticism for GANs has always been that they might just memorize some samples).': 1.0986123085021973, 'By the way, it would be very interesting to know how well the discriminator actually performs after training.': 0.8123148083686829, 'Coming back to the goal of learning powerful features: The method does not reach state-of-the-art performance on most evaluated tasks (Table 2 and 3) but performs competitive and it would be interesting to see how much this improves if the BiGAN training (and the convolutional architecture used) would be improved.': 1.098608374595642, 'The paper is very well written and provides most necessary details, although some more details on the training (learning rates, initialization) would be helpful for reproducing the results.': 1.065760612487793, 'Overall I think the paper provides a very interesting framework for further research, even though the results presented here are not too impressive both with respect to the feature evaluation (and the GAN learning).': 1.0986058712005615, 'Minor: It might be helpful to highlight the best performance numbers in Tables 2 and 3.': 1.0979832410812378}"
61,https://openreview.net/forum?id=BJuysoFeg,"{'This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain.': 1.0986003875732422, 'Pros:': 1.0986123085021973, 'The method is very simple and easy to understand and apply.': 1.0986121892929077, 'The experiments demonstrate that the method compares favorably with existing methods on standard domain adaptation tasks.': 1.0976368188858032, 'The analysis in section 4.3.2 shows that a very small number of target domain samples are needed for adaptation of the network.': 0.7098873257637024, 'Cons:': 1.0986095666885376, 'There is little novelty': 1.0442966222763062, 'the method is arguably too simple to be called a “method.”': 1.0986123085021973, 'Rather, it’s the most straightforward/intuitive approach when using a network with batch normalization for domain adaptation.': 1.0388344526290894, 'The alternative': 1.0986123085021973, 'using the BN statistics from the source domain for target domain examples': 1.098333477973938, 'is less natural, to me.': 0.904539167881012, '(I guess this alternative is what’s done in the Inception BN results in Table 1-2?)': 1.0986123085021973, 'The analysis in section 4.3.1 is superfluous except as a sanity check': 1.0986123085021973, 'KL divergence between the distributions should be 0 when each distribution is shifted/scaled to N(0,1) by BN.': 1.0986123085021973, 'Section 3.3: it’s not clear to me what point is being made here.': 1.0986123085021973, 'Overall, there’s not much novelty here, but it’s hard to argue that simplicity is a bad thing when the method is clearly competitive with or outperforming prior work on the standard benchmarks (in a domain adaptation tradition that started with “Frustratingly Easy Domain Adaptation”).': 1.0986123085021973, 'If accepted, Sections 4.3.1 and 3.3 should be removed or rewritten for clarity for a final version.': 1.0986123085021973, 'Update: I thank the authors for their comments.': 1.0986123085021973, ""I still think that the method needs more experimental evaluation: for now, it's restricted to the settings in which one can use pre-trained ImageNet model, but it's also important to show the effectiveness in scenarios where one needs to train everything from scratch."": 1.0986123085021973, 'I would love to see a fair comparison of the state-of-the-art methods on toy datasets (e.g. see (Bousmalis et al., 2016), (Ganin & Lempitsky, 2015)).': 1.0986123085021973, ""In my opinion, it's a crucial point that doesn't allow me to increase the rating."": 1.0986123085021973, 'This paper proposes a simple trick turning batch normalization into a domain adaptation technique.': 1.0986123085021973, 'The authors show that per-batch means and variances normally computed as a part of the BN procedure are sufficient to discriminate the domain.': 1.0986123085021973, 'This observation leads to an idea that adaptation for the target domain can be performed by replacing population statistics computed on the source dataset by the corresponding statistics from the target dataset.': 1.0986123085021973, 'Overall, I think the paper is more suitable for a workshop track rather than for the main conference track.': 1.0986123085021973, 'My main concerns are the following:': 1.0986123085021973, '1. Although the main idea is very simple, it feels like the paper is composed in such a way to make the main contribution less obvious (e.g. the idea could have been described in the abstract but the authors avoided doing so).': 0.7744957208633423, '2. (This one is from the pre-review questions) The authors are using much stronger base CNN which may account for the bulk of the reported improvement. In order to prove the effectiveness of the trick, the authors would need to conduct a fair comparison against competing methods. It would be highly desirable to conduct this comparison also in the case of a model trained from scratch (as opposed to reusing ImageNet-trained networks).': 0.8462038636207581, 'Overall I think this is an interesting paper which shows empirical performance improvement over baselines.': 1.0986099243164062, 'However, my main concern with the paper is regarding its technical depth, as the gist of the paper can be summarized as the following: instead of keeping the batch norm mean and bias estimation over the whole model, estimate them on a per-domain basis.': 1.0986123085021973, 'I am not sure if this is novel, as this is a natural extension of the original batch normalization paper.': 1.0986123085021973, 'Overall I think this paper is more fit as a short workshop presentation rather than a full conference paper.': 1.0986123085021973, 'Detailed comments:': 1.0986123085021973, 'Section 3.1: I respectfully disagree that the core idea of BN is to align the distribution of training data.': 1.0986123085021973, 'It does this as a side effect, but the major purpose of BN is to properly control the scale of the gradient so we can train very deep models without the problem of vanishing gradients.': 1.0986123085021973, 'It is plausible that intermediate features from different datasets naturally show as different groups in a t-SNE embedding.': 1.0986123085021973, 'This is not the particular feature of batch normalization: visualizing a set of intermediate features with AlexNet and one gets the same results.': 1.0986123085021973, 'So the premise in section 3.1 is not accurate.': 1.0986123085021973, 'Section 3.3: I have the same concern as the other reviewer.': 1.0986123085021973, 'It seems to be quite detatched from the general idea of AdaBN.': 1.0986123085021973, 'Equation 2 presents an obvious argument that the combined BN-fully_connected layer forms a linear transform, which is true in the original BN case and in this case as well.': 1.0986123085021973, 'I do not think it adds much theoretical depth to the paper.': 1.0986123085021973, '(In general the novelty of this paper seems low)': 1.0986123085021973, 'Experiments:': 1.0986123085021973, 'section 4.3.1 is not an accurate measure of the ""effectiveness"" of the proposed method, but a verification of a simple fact: previously, we normalize the source domain features into a Gaussian distribution.': 1.0848733186721802, 'the proposed method is explicitly normalizing the target domain features into the same Gaussian distribution as well.': 1.098610520362854, 'So, it is obvious that the KL divergence between these two distributions are closer - in fact, one is *explicitly* making them close.': 0.6893299221992493, 'However, this does not directly correlate to the effectiveness of the final classification performance.': 1.0985970497131348, 'section 4.3.2: the sensitivity analysis is a very interesting read, as it suggests that only a very few number of images are needed to account for the domain shift in the AdaBN parameter estimation.': 0.5130286812782288, 'This seems to suggest that a single ""whitening"" operation is already good enough to offset the domain bias (in both cases shown, a single batch is sufficient to recover about 80% of the performance gain, although I cannot get data for even smaller number of examples from the figure).': 0.5535511374473572, 'It would thus be useful to have a comparison between these approaches, and also a detailed analysis of the effect from each layer of the model - the current analysis seems a bit thin.': 1.0514237880706787}"
62,https://openreview.net/forum?id=BJwFrvOeg,"{'This paper proposes to incorporate knowledge base facts into language modeling, thus at each time step, a word is either generated from the full vocabulary or relevant KB entities.': 1.0986069440841675, 'The authors demonstrate the effectiveness on a new generated dataset WikiFacts which aligns Wikipedia articles with Freebase facts.': 1.0985548496246338, 'The authors also suggest a modified perplexity metric which penalizes the likelihood of unknown words.': 1.0986119508743286, 'At a high level, I do like the motivation of this paper': 1.0278995037078857, 'named entity words are usually important for downstream tasks, but difficult to learn solely based on statistical co-occurrences.': 1.0984398126602173, 'The facts encoded in KB could be a great supply for this.': 0.8874180912971497, 'However, I find it difficult to follow the details of the paper (mainly Section 3) and think the paper writing needs to be much improved.': 1.0986121892929077, 'I cannot find where  f_{symbkey} / f_{voca} / f_{copy} are defined': 1.0986123085021973, 'w^v, w^s are confusing.': 1.0985939502716064, 'e_k seems to be the average of all previous fact embeddings?': 1.0496702194213867, 'It is necessary to make it clear enough.': 1.0031723976135254, '(h_t, c_t) = f_LSTM(x_{t−1}, h_{t−1})  c_t is not used?': 0.8536038994789124, 'The notion of “fact embeddings” is also not that clear (I understand that they are taken as the concatenation of relation and entity (object) entities in the end).': 1.096239686012268, 'For the anchor / “topic-itself” facts, do you learn the embedding for the special relations and use the entity embeddings from TransE?': 1.0980035066604614, 'On generating words from KB entities (fact description), it sounds a bit strange to me to generate a symbol position first.': 1.0986119508743286, 'Most entities are multiple words, and it is necessary to keep that order.': 1.0945802927017212, 'Also it might be helpful to incorporate some prior information, for example, it is common to only mention “Obama” for the entity “Barack Obama”?': 1.0724587440490723, 'The paper proposes an evolution upon traditional Recurrent Language Models to give the capability to deal with unknown words.': 1.0986123085021973, 'It is done by pairing the traditional RNNLM with a module operating on a KB and able to copy from KB facts to generate unseen words.': 1.098610520362854, 'It is shown to be efficient and much better than plain RNNLM on a new dataset.': 1.0986100435256958, 'The writing could be improved.': 1.0982611179351807, 'The beginning of Section 3 in particular is hard to parse.': 1.0986121892929077, 'There have been similar efforts recently (like ""Pointer Sentinel Mixture Models"" by Merity et al.)': 1.09854257106781, 'that attempt to overcome limitations of RNNLMs with unknown words; but they usually do it by adding a mechanism to copy from a longer past history.': 0.680259644985199, 'The proposal of the current paper is different and more interesting to me in that it try to bring knowledge from another source (KB) to the language model.': 1.0986123085021973, 'This is harder because one needs to leverage the large scale of the KB to do so.': 1.0986120700836182, 'Being able to train that conveniently is nice.': 1.098537564277649, 'The architecture appears sound, but the writing makes it hard to fully understand completely so I can not give a higher rating.': 1.0986119508743286, 'Other comments:': 1.0977863073349, '* How to cope with the dependency on the KB?': 1.0986123085021973, 'Freebase is not updated anymore so it is likely that a lot of the new unseen words in the making are not going to be in Freebase.': 1.098583698272705, '* What is the performance on standard benchmarks like Penn Tree Bank?': 1.098610281944275, '* How long is it to train compare to a standard RNNLM?': 1.0986101627349854, '* What is the importance of the knowledge context ?': 1.0986120700836182, '* How is initialized the fact embedding  for the first word?': 1.0986123085021973, '*': 1.0986123085021973, 'When a word from a fact description has been chosen as prediction (copied), how is it encoded in the generation history for following predictions if it has no embedding (unknown word)?': 0.37798184156417847, 'In other words, what happens if ""Michelle"" in the example of Section 3.1 is not in the embedding dictionary, when one wants to predict the next word?': 1.0986027717590332, 'This paper addresses the practical problem of generating rare or unseen words in the context of language modeling.': 1.0986121892929077, 'Since language follows a Zipf’s law, most approaches limit the vocabulary (because of computation reasons) and hence rare words are often mapped to a UNK token.': 1.0986120700836182, 'Rare words are especially important in context of applications such as question answering.': 1.0986123085021973, 'MT etc.': 1.086574912071228, 'This paper proposes a language modeling technique which incorporates facts from knowledge bases (KBs) and thus has the ability to generate (potentially unseen) words from KBs.': 1.0986121892929077, 'This paper also releases a dataset by aligning words with Freebase facts and corresponding Wikipedia descriptions.': 1.098374366760254, 'The model first selects a KB fact based on the previously generated words and facts.': 1.0940557718276978, 'Based on the selected fact, it then predicts whether to generate a word based on the vocabulary or to output a symbolic word from the KB.': 1.0986000299453735, 'For the latter, the model is trained to predict the position of the word from the fact description.': 1.0986073017120361, 'Overall the paper could use some rewriting especially the notations in section 3.': 1.0984247922897339, 'The experiments are well executed and they definitely get good results.': 1.0920939445495605, 'The heat maps at the end are very insightful.': 1.0986030101776123, 'Comments': 1.098604679107666, 'This contributions of this paper would be much stronger if it showed improvements in a practical applications such as Question Answering (although the paper clearly mentions that this technique could be applied to improve QA)': 1.098607063293457, ""In section 3, it is unclear why the authors refer the entity as a ‘topic'."": 1.098210096359253, 'This makes the text a little confusing since a topic can also be associated with something abstract, but in this case the topic is always a freebase entity.': 0.5916495323181152, 'Is it really necessary to predict a fact at every step before generating a word.': 1.098610281944275, 'In other words, how many distinct facts on average does the model choose to generate a sentence.': 1.0985924005508423, 'Intuitively a natural language sentence would be describe few facts about an entity.': 1.097678303718567, 'If the fact generation step could be avoided (by adding a latent variable which decides if the fact should be generated or not), the model will also be faster.': 1.0985677242279053, 'In equation 2, the model has to make a hard decision to choose the fact.': 1.0787792205810547, 'For this to be end to end trained, every word needs to be annotated with a corresponding fact which might not be always a realistic scenario.': 1.0682790279388428, 'For e.g., in domains such as social media text.': 1.0594127178192139, 'Learning position embeddings for copying knowledge words seems a little counter-intuitive.': 1.0493881702423096, 'Does the sequence of knowledge words follow any particular structure like word O_2 is always the last name (e.g. Obama).': 1.0986119508743286, ""It would also be nice to compare to char-level LM's which inherently solves the unknown token problem."": 1.0979363918304443}"
63,https://openreview.net/forum?id=BJxhLAuxg,"{'This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction.': 1.0986123085021973, 'In Atari game playing scenarios, the authors show that this model can successfully predict both reward and next frames.': 1.0986123085021973, 'Pros:': 1.098568081855774, 'Paper is well written and easy to follow.': 1.092720866203308, 'Model is clear to understand.': 0.47181329131126404, 'Cons:': 1.098068118095398, 'The model is incrementally different than the baseline.': 1.0984374284744263, 'The authors state that their purpose is to establish a pre-condition, which they achieve.': 1.0986117124557495, 'But this makes the paper quite limited in scope.': 1.0986086130142212, 'This paper reads like the start of a really good long paper, or a good short paper.': 1.0986121892929077, 'Following through on the future work proposed by the authors would make a great paper.': 1.0986123085021973, 'As it stands, the paper is a bit thin on new contributions.': 1.0986123085021973, 'The paper extends a recently proposed video frame prediction method with reward prediction in order to learn the unknown system dynamics and reward structure of an environment.': 1.0986123085021973, 'The method is tested on several Atari games and is able to predict the reward quite well within a range of about 50 steps.': 1.098611831665039, 'The paper is very well written, focussed and is quite clear about its contribution to the literature.': 1.0973354578018188, 'The experiments and methods are sound.': 1.0985389947891235, 'However, the results are not really surprising given that the system state and the reward are linked deterministically in Atari games.': 1.0986007452011108, 'In other words, we can always decode the reward from a network that successfully encodes future system states in its latent representation.': 1.0986123085021973, 'The contribution of the paper is therefore minor.': 1.0986123085021973, 'The paper would be much stronger if the authors could include experiments on the two future work directions they suggest in the conclusions: augmenting training with artificial samples and adding Monte-Carlo tree search.': 1.0986123085021973, 'The suggestions might decrease the number of real-world training samples and increase performance, both of which would be very interesting and impactful.': 1.0986123085021973, 'The topic of the paper, model-based RL with a learned model, is important and timely.': 1.0986080169677734, 'The paper is well written.': 0.6444482803344727, 'I feel that the presented results are too incremental.': 1.098576545715332, 'Augmenting the frame prediction network with another head that predicts the reward is a very sensible thing to do.': 1.0986109972000122, 'However neither the methodology not the results are novel / surprising, given that the original method of [Oh et al. 2015] already learns to successfully increment score counters in predicted frames in many games.': 1.0986123085021973, 'I’m very much looking forward to seeing the results of applying the learned joint model of frames and rewards to model-based RL as proposed by the authors.': 1.0986123085021973}"
64,https://openreview.net/forum?id=Bk0FWVcgx,"{'This is an incremental result (several related results that the authors of the paper mentioned here were already published).': 1.0986123085021973, 'The authors claim that they can get rid of the technical assumptions from the previous papers but the results they propose are significantly weaker and also quite technical.': 1.0986120700836182, 'The main theoretical result - Theorem 2.4 is not convincing at all.': 1.0950825214385986, 'Furthermore, the paper is badly written.': 1.0985969305038452, 'No theoretical intuition is given, the experimental section is weak and in some places the formatting is wrong.': 1.0986123085021973, 'This paper studies the energy landscape of the loss function in neural networks.': 0.40755367279052734, 'It is generally clearly written and nicely provides intuitions for the results.': 1.0985264778137207, 'One main contribution is to show that the level sets of the loss becomes connected as the network is increasingly overparameterized.': 1.0984461307525635, 'It also quantifies, in a way, the degree of disconnectedness possible in terms of the increase in loss that one must allow to find a connected path.': 1.0986123085021973, 'It would seem that this might have some implications for the likelihood of escaping local minima with stochastic gradient descent.': 1.0986123085021973, 'The paper also presents a simple algorithm for finding geodesic paths between two networks such that the loss is decreasing along the path.': 1.0307468175888062, 'Using this they show that the loss seems to become more nonconvex when the loss is smaller.': 1.0986123085021973, 'This is also quite interesting.': 1.0986051559448242, 'The work does have some significant limitations, which is not surprising given the difficulty of fully analyzing the network loss function.': 1.0982509851455688, 'However, the authors are quite clear about these limitations, which especially include not yet analyzing deep networks and analyzing only the oracle loss, and not the empirical loss.': 1.0986095666885376, 'I would have also appreciated a little more practical discussion of the bound in Theorem 2.4.': 1.0986123085021973, 'It is hard to tell whether this bound is tight enough to be practically relevant.': 1.0986123085021973, 'This work contributes to understanding the landscape of deep networks in terms of its topology and geometry.': 1.0986123085021973, 'The paper analyzes the former theoretically, and studies the latter empirically.': 0.7197222709655762, 'Although the provided contributions are very specific (ReLU nets with single hidden layer, and a heuristic to calculate the normalized geodesic), the results are original and of interest.': 1.0986123085021973, 'Thus, they could potentially be used as stepping stones for deeper developments in this area.': 1.09861159324646, 'Pros:': 1.0986123085021973, '1. Providing new theory about existence of ""poor"" local minima for ReLU networks with a hidden unit that relies on input distribution properties as well as the size of the hidden layer.': 0.7744751572608948, '2. Coming up with a heuristic algorithm to compute the normalized geodesic between two solution points. The latter reflects how curved the path between the two is.': 0.6411198377609253, 'Cons:': 0.6227701306343079, 'The results are very specific in both topology and geometry analysis.': 1.0986123085021973, '1. The analysis is performed only over a ""single"" hidden layer ReLU network. Given the importance of depth in deep architectures, this result cannot really explain the kinds of architectures we are interested in practically.': 0.6727617383003235, '2. The normalized geodesic criterion is somewhat limited in representing how easy it is to connect two equally good points. For example, there might exist a straight line between the two (which is considered as easy by the geodesic criterion), but this line might be going through a very narrow valley, challenging gradient based optimization algorithms (and thus extremely difficult to navigate in practice). In addition, the proposed algorithm for computing the normalized geodesic is a greedy heuristic, which as far as I can tell, makes it difficult to know how we can trust in the estimated geodesics obtained by this algorithm.': 0.7165310978889465, 'With all cons said, I stress that I understand both problems tackled in the paper are challenging, and thus I find the contributions valuable and interesting.': 1.0986111164093018}"
65,https://openreview.net/forum?id=Bk0MRI5lg,"{'The proposed regularizer seems to be a particular combination of existing methods.': 1.0951931476593018, 'Though the implied connection between nonlinearities and stochastic regularizers is intriguing, in my opinion the empirical performance does not exceed the performance achieved by similar methods by a large enough margin to arrive at a meaningful conclusion.': 1.0986123085021973, 'The method proposed essential trains neural networks without a traditional nonlinearity, using multiplicative gating by the CDF of a Gaussian evaluated at the preactivation; this is motivated as a relaxation of a probit-Bernoulli stochastic gate.': 1.0984117984771729, 'Experiments are performed with both.': 1.0986123085021973, 'The work is somewhat novel and interesting.': 0.6525476574897766, 'Little is said about why this is preferable to other similar parameterizations of the same (sigmoidal?': 1.0974565744400024, 'softsign?': 1.0986123085021973, 'etc.)': 1.0986123085021973, 'It would be stronger with more empirical interrogation of why this works and exploration of the nearby conceptual space.': 1.0985075235366821, ""The CIFAR results look okay by today's standards but the MNIST results are quite bad, neural nets were doing better than 1.5% a decade ago and the SOI map results (and the ReLU baseline) are above 2%."": 1.0985791683197021, ""(TIMIT results on frame classification also aren't that interesting without evaluating word error rate within a speech pipeline, but this is a minor point.)"": 0.3331957757472992, 'The idea put forth that SOI map networks without additional nonlinearities are comparable to linear functions is rather misleading as they are, in expectation, nonlinear functions of their input.': 1.0986119508743286, 'Varying an input example by multiplying or adding a constant will not be linearly reflected in the expected output of the network.': 1.0747047662734985, 'In this sense they are more nonlinear than ReLU networks which are at least locally linear.': 0.9665320515632629, 'The plots are very difficult to read in grayscale,': 1.098480463027954, 'Approaches like adaptive dropout also have the binary mask as a function of input to a neuron very similar to the proposed approach.': 1.0986123085021973, 'It is not clear, even from the new draft, how the proposed approach differs to Adaptive dropout in terms of functionality.': 1.0986123085021973, 'The experimental validation is also not extensive since comparison to SOTA is not included.': 1.0986030101776123}"
66,https://openreview.net/forum?id=Bk2TqVcxe,"{'+ Understanding relations between objects is an important task in domains like vision, language and robotics.': 1.0986121892929077, 'However, models trained on real-life datasets can often exploit simple object properties (not relation-based) to identify relations (eg: animals of bigger size are typically predators and small-size animals are preys).': 1.0986123085021973, 'Such models can predict relations without necessarily understanding them.': 1.0986123085021973, 'Given the difficulty of the task, a controlled setting is required to investigate if neural networks can be designed to actually understand pairwise object relations.': 1.0222517251968384, 'The current paper takes a significant step in answering this question through a controlled dataset.': 1.0985418558120728, 'Also, multiple experiments are presented to validate the ""relation learning"" ability of proposed Relation Networks (RN).': 0.4213801324367523, '+': 1.0986123085021973, 'The dataset proposed in the paper ensures that relation classification models can succeed only by learning the relations between objects and not by exploiting ""predator-prey"" like object properties.': 1.0986073017120361, 'The paper presents very thorough experiments to validate the claim that ""RNs"" truly learn the relation between objects.': 0.8925244808197021, '1. In particular, the ability of the RN to force a simple linear layer to disentangle scene description from VAE latent space and permuted description is very interesting. This clearly demonstrates that the RN learns object relations.': 1.0948317050933838, '2. The one-shot experiments again demonstrate this ability in a convincing manner. This requires the model to understand relations in each run, represent them through an abstract label and assign the label to future samples from the relationship graph.': 1.090271234512329, 'Some suggestions:': 1.0986123085021973, 'Is g_{\\psi}(.)': 1.0986123085021973, 'permutation invariant as well.': 1.0986123085021973, 'Since it works on pairs of objects, how did you ensure that the MLP is invariant to the order of the objects in the pair?': 1.0846177339553833, 'The RNs need to operate over pairs of objects in order to identify pairwise interactions.': 1.0985760688781738, 'However, in practical applications there are more complicated group interactions.': 1.0986123085021973, '(eg. ternary interaction: ""person"" riding a ""bike"" wears ""helmet"").': 1.0986089706420898, 'Would this require g(.)': 1.0986123085021973, 'of RN to not just operate on pairs but on every possible subset of objects in the scene?': 1.0986121892929077, 'More generally, is such a pairwise edge-based approach scalable to larger number of objects?': 1.0238349437713623, 'The authors mention that "" a deep network with a sufficiently large number of parameters and a large enough training set should be capable of matching the performance of a RN"".': 0.5060991048812866, 'This is an interesting point, and could be true in practice.': 1.0986123085021973, 'Have the authors investigated this effect by trying to identify the minimum model capacity and/or training examples required by a MLP to match the performance of RN for the provided setup?': 0.7031528353691101, 'This would help in quantifying the significance of RN for practical applications with limited examples.': 1.0986123085021973, 'In other words, the task in Sec. 5.1 could benefit from another plot: the performance of MLP and RN at different amounts of training samples.': 0.8085686564445496, 'While the simulation setup in the current paper is a great first-step towards analyzing the ""relation-learning"" ability of RNs, it is still not clear if this would transfer to real-life datasets.': 1.0658620595932007, 'I strongly encourage the authors to experiment on real-life datasets like Coco, visual genome or HICO as stated in the pre-review stage.': 1.098610281944275, 'Minor: Some terminologies in the paper such as ""objects"" and ""scene descriptions"" used to refer to abstract entities can be misleading for readers from the object detection domain in computer vision. This could be clarified early on in the introduction.': 1.096718430519104, 'Minor: Some results like Fig. 8 which shows the ability of RN to generalize to unseen categories are quite interesting and could be moved to the main draft for completeness.': 1.0951118469238281, 'The paper proposes a network which is capable of understanding relationships between objects in a scene.': 1.0986123085021973, 'This ability of the RN is thoroughly investigated through a series of experiments on a controlled dataset.': 1.0986087322235107, 'While, the model is currently evaluated only on a simulated dataset, the results are quite promising and could translate to real-life datasets as well.': 1.0985854864120483, 'This paper proposes relation networks in order to model the pairwise interactions between objects in a visual scene.': 1.0986108779907227, 'The model is very straight forward, first an MLP (with shared weights) is applied to each pair of objects.': 1.0986121892929077, 'Finally a prediction is created by an MLP which operates by summing non-linear functions of these pairs of objects.': 1.0986123085021973, 'Experimental evaluation is done in a synthetic dataset that is generated to fit the architecture hand-crafted in this paper.': 1.0986123085021973, 'The title of the paper claims much more than the paper delivers.': 1.0660560131072998, 'Discovering objects and their relations is a very important task.': 1.0986106395721436, 'However, this paper does not discover objects or their relations, instead, each objects is represented with hand coded ground truth attributes, and only a small set of trivial relationships are ""discovered"", e.g., relative position.': 1.0986123085021973, 'Discovering objects and their relationships has been tackled for several decades in computer vision (CV).': 1.0986104011535645, 'The paper does not cite or compare to any technique in this body of literature.': 1.0981810092926025, 'This is typically refer to as ""contextual models"".': 0.5263484716415405, 'Can the proposed architecture help object detection and/or scene classification?': 1.0986123085021973, 'would it work in the presence of noise (e.g, missing detections, non accurate detection estimates, complex texture)?': 1.0986120700836182, 'would it work when the attributes of objects are estimated from real images?': 1.0986123085021973, ""I'll be more convinced if experiments where done in real scenes."": 1.0986053943634033, 'In the case of indoor scenes, datasets such as NYUv2, Sun-RGB-D, SceneNN, Chen et al CVPR 14 (text-to-image-correference) could be used.': 1.0963302850723267, 'In outdoor scenes, KITTI and the relationships between cars, pedestrians and cyclist could also serve as benchmark.': 1.0986018180847168, 'Without showing real scenes, this paper tackles a too toy problem with a very simple model which does not go much further than current context models, which model pairwise relationships between objects (with MRFs, with deep nets, etc).': 1.098611831665039, 'This paper proposes a relation network (RN) to model relations between input entities such as objects.': 1.0956966876983643, 'The relation network is built in two stages.': 1.0985759496688843, 'First a lower-level structure analyzes a pair of input entities.': 0.3335846960544586, 'All pairs of input entities are fed to this structure.': 0.4210796058177948, 'Next, the output of this lower-level structure is aggregated across all input pairs via a simple sum.': 1.0560158491134644, 'This is used as the input to a higher-level structure.': 0.9756644368171692, 'In the basic version, these two structures are each multi-layer perceptrons (MLPs).': 0.5410012006759644, 'Overall, this is an interesting approach to understanding relations among entities.': 0.5826449990272522, 'The core idea is clear and well-motivated': 1.0986121892929077, 'pooling techniques that induce invariance can be used to learn relations.': 1.0986089706420898, 'The idea builds on pooling structures (e.g. spatial/temporal average/max pooling) to focus on pairwise relations.': 1.0986123085021973, 'The current pairwise approach could potentially be extended to higher-order interactions, modulo scaling issues.': 1.0986123085021973, 'Experiments on scene descriptions and images verify the efficacy of relation networks.': 1.0986123085021973, 'The MLP baselines used are incapable of modeling the structured dependencies present in these tasks.': 1.0986123085021973, 'It would be interesting to know if pooling operators (e.g. across-object max pooling in an MLP) or data augmentation via permutation would be effective for training MLPs at these tasks.': 1.0986123085021973, 'Regardless, the model proposed here is novel and effective at handling relations and shows promise for higher-level reasoning tasks.': 1.0986123085021973}"
67,https://openreview.net/forum?id=Bk3F5Y9lx,"{'This paper is refreshing and elegant in its handling of ""over-sampling"" in VAE.': 1.3862943649291992, 'Problem is that good reconstruction requires more nodes in the latent layers of the VAE.': 1.3862943649291992, 'Not all of them can or should be sampled from at the ""creative"" regime of the VAE.': 1.3862943649291992, 'Which ones to choose?': 1.3754644393920898, 'The paper offers and sensible solution.': 1.3862518072128296, 'Problem is that real-life data-sets like CIFAR have not being tried, so the reader is hard-pressed to choose between many other, just as natural, solutions.': 1.3862943649291992, 'One can e.g. run in parallel a classifier and let it choose the best epitome, in the spirit of spatial transformers, ACE, reference [1].': 1.386293888092041, 'The list can go on.': 1.3862943649291992, 'We hope that the paper finds its way to the conference because it addresses an important problem in an elegant way, and papers like this are few and far between!': 1.3862899541854858, 'On a secondary note, regarding terminology: Pls avoid using ""the KL term"" as in section 2.1, there are so many ""KL terms"" related to VAE-s, it ultimately gets out of control.': 1.3862943649291992, '""Generative error"" is a more descriptive term, because minimizing it is indispensable for the generative qualities of the net.': 1.1674017906188965, 'The variational error for example is also a ""KL term"" (equation (3.4) in reference [1]), as is the upper bound commonly used in VAE-s (your formula (5) and its equivalent - the KL expression as in formula (3.8) in reference [1]).': 1.3862884044647217, 'The latter expression is frequently used and is handy for, say, importance sampling, as in reference [2].': 1.386290192604065, '[1] https://arxiv.org/pdf/1508.06585v5.pdf': 1.3476293087005615, '[2] https://arxiv.org/pdf/1509.00519.pdf': 1.364704966545105, 'This paper replaces the Gaussian prior often used in a VAE with a group sparse prior.': 1.3835610151290894, 'They modify the approximate posterior function so that it also generates group sparse samples.': 1.3862943649291992, 'The development of novel forms for the generative model and inference process in VAEs is an active and important area of research.': 1.382394552230835, ""I don't believe the specific choice of prior proposed in this paper is very well motivated however."": 1.3862943649291992, 'I believe several of the conceptual claims are incorrect.': 1.3862911462783813, 'The experimental results are unconvincing, and I suspect compare log likelihoods in bits against competing algorithms in nats.': 1.3862943649291992, 'Some more detailed comments:': 1.3862943649291992, 'In Table 1, the log likelihoods reported for competing techniques are all in nats.': 1.3862930536270142, 'The reported log likelihood of cVAE using 10K samples is not only higher than the likelihood of true data samples, but is also higher than the log likelihood that can be achieved by fitting a 10K k-means mixture model to the data (eg as done in ""A note on the evaluation of generative models"").': 1.386291742324829, 'It should nearly impossible to outperform a 10K k-means mixture on Parzen estimation, which makes me extremely skeptical of these eVAE results.': 1.3862923383712769, 'However, if you assume that the eVAE log likelihood is actually in bits, and multiply it by log 2 to convert to nats, then it corresponds to a totally believable log likelihood.': 1.3862943649291992, 'Note that some Parzen window implementations report log likelihood in bits.': 1.3862943649291992, 'Is this experiment comparing log likelihood in bits to competing log likelihoods in nats?': 1.3862943649291992, '(also, label units': 1.3862943649291992, 'eg bits or nats': 1.3862943649291992, 'in table)': 1.3862943649291992, 'It would be really, really, good to report and compare the variational lower bound on the log likelihood!!': 1.3862943649291992, 'Alternatively, if you are concerned your bound is loose, you can use AIS to get a more exact measure of the log likelihood.': 1.370551586151123, 'Even if the Parzen window results are correct, Parzen estimates of log likelihood are extremely poor.': 1.3862855434417725, 'They possess any drawback of log likelihood evaluation (which they approximate), and then have many additional drawbacks as well.': 0.7944883704185486, 'The MNIST sample quality does not appear to be visually competitive.': 1.181169867515564, 'Also': 1.3862943649291992, 'it appears that the images are of the probability of activation for each pixel, rather than actual samples from the model.': 1.3860180377960205, 'Samples would be more accurate, but either way make sure to describe what is shown in the figure.': 1.383386254310608, 'There are no experiments on non-toy datasets.': 1.3861443996429443, 'I am still concerned about most of the issues I raised in my questions below.': 0.715116024017334, ""Briefly, some comments on the authors' response:"": 0.7693167924880981, '1. ""minibatches are constructed to not only have a random subset of training examples but also be balanced w.r.t. to epitome assignment (Alg. 1, ln. 4).""': 0.7774986028671265, 'Nice!': 1.3862943649291992, 'This makes me feel better about why all the epitomes will be used.': 0.9267640709877014, ""2. I don't think your response addresses why C_vae would trade off between data reconstruction and being factorial. The approximate posterior is factorial by construction"": 0.9648656845092773, ""there's nothing in C_vae that can make it more or less factorial."": 0.8517271280288696, '3. ""For C_vae to have zero contribution from the KL term of a particular z_d (in other words, that unit is deactivated), it has to have all the examples in the training set be deactivated (KL term of zero) for that unit""': 0.9976755976676941, ""This isn't true."": 0.9772897958755493, 'A standard VAE can set the variance to 1 and the mean to 0 (KL term of 0) for some examples in the training set, and have non-zero KL for other training examples.': 1.2153067588806152, '4. The VAE loss is trained on a lower bound on the log likelihood, though it does have a term that looks like reconstruction error. Naively, I would imagine that if it overfits, this would correspond to data samples becoming more likely under the generative model.': 1.0831698179244995, '5/6.': 1.3862943649291992, 'See Parzen concerns above.': 1.3861889839172363, ""It's strange to train a binary model, and then treat it's probability of activation as a sample in a continuous space."": 1.3695558309555054, '6. ""we can only evaluate the model from its samples""': 1.290799617767334, ""I don't believe this is true."": 1.3810255527496338, 'You are training on a lower bound on the log likelihood, which immediately provides another method of quantitative evaluation.': 1.2023597955703735, 'Additionally, you could use techniques such as AIS to compute the exact log likelihood.': 1.3858377933502197, ""7. I don't believe Parzen window evaluation is a better measure of model quality, even in terms of sample generation, than log likelihood."": 1.3862943649291992, 'The paper presents a version of a variational autoencoder that uses a discrete latent variable that masks the activation of the latent code, making only a subset (an ""epitome"") of the latent variables active for a given sample.': 1.3862943649291992, 'The justification for this choice is that by letting different latent variables be active for different samples, the model is forced to use more of the latent code than a usual VAE.': 1.3862943649291992, ""While the problem of latent variable over pruning is important and has been highlighted in the literature before in the context of variational inference, the proposed solution doesn't seem to solve it beyond, for instance, a mixture of VAEs."": 1.3862943649291992, 'Indeed, a mixture of VAEs would have been a great baseline for the experiments in the paper, as it uses a categorical variable (the mixture component) along with multiple VAEs.': 1.3862943649291992, 'The main difference between a mixture and an epitomic VAE is the sharing of parameters between the different ""mixture components"" in the epitomic VAE case.': 1.3862943649291992, 'The experimental section presents misleading results.': 1.3862943649291992, ""1. The log-likelihood of the proposed models is evaluated with Parzen window estimator. A significantly more accurate lower bound on likelihood that is available for the VAEs is not reported. In reviewer's experience continuous MNIST likelihood of upwards of 900 nats is easy to obtain with a modestly sized VAE."": 1.222959280014038, '2. The exposition changes between dealing with binary MNIST and continuous MNIST experiments. This is confusing, because these versions of the dataset present different challenges for modeling with likelihood-based models. Continuous MNIST is harder to model with high-capacity likelihood optimizing models, because the dataset lies in a proper subspace of the 784-dimensional space (some pixels are always or almost always equal to 0), and hence probability density can be arbitrarily large on this subspace. Models that try to maximize the likelihood often exploit this option of maximizing the likelihood by concentrating the probability around the subspace at the expense of actually modeling the data. The samples of a well-tuned VAE trained on binary MNIST (or a VAE trained on continuous MNIST to which noise has been appropriately added) tend to look much better than the ones presented in experimental results.': 0.9420943856239319, '3. The claim that the VAE uses its capacity to ""overfit"" to the training data is not justified. No evidence is presented that the reconstruction likelihood on the training data is significantly higher than the reconstruction likelihood on the test data. It\'s misleading to use a technical term like ""overfitting"" to mean something else.': 0.7761802673339844, '4. The use of dropout in dropout VAE is not specified: is dropout applied to the latent variables, or to the hidden layers of the encoder/decoder? The two options will exhibit very different behaviors.': 1.1119654178619385, ""5. MNIST eVAE samples and reconstructions look more like a more diverse version of 2d VAE samples/reconstructions - they are blurry, the model doesn't encode precise position of strokes. This is consistent with an interpretation of eVAE as a kind of mixture of smaller VAEs, rather than a higher-dimensional VAE. It is misleading to claim that it outperforms a high-dimensional VAE based on this evidence."": 1.200064778327942, ""In reviewer's opinion the paper is not yet ready for publication."": 1.1038336753845215, 'A stronger baseline VAE evaluated with evidence lower bound (or another reliable method) is essential for comparing the proposed eVAE to VAEs.': 1.3862943649291992, 'This paper proposes an elegant solution to a very important problem in VAEs, namely that the model over-regularizes itself by killing off latent dimensions.': 1.3862943649291992, 'People have used annealing of the KL term and “free bits” to hack around this issue but a better solution is needed.': 1.3862943649291992, 'The offered solution is to introduce sparsity for the latent representation: for every input only a few latent distributions will be activated but across the dataset many latents can still be learned.': 1.3862943649291992, 'What I didn’t understand is why the authors need the topology in this latent representation.': 1.3862943649291992, 'Why not place a prior over arbitrary subsets of latents?': 1.3862943649291992, 'That seems to increase the representational power a lot without compromising the solution to the problem you are trying to solve.': 1.3862943649291992, 'Now the number of ways the latents can combine is no longer exponentially large, which seems a pity.': 1.3862943649291992, 'The first paragraph on p.7 is a mystery to me: “An effect of this …samples”.': 1.3862943649291992, 'How can under-utilization of model capacity lead to overfitting?': 1.3862943649291992, 'The experiments are modest but sufficient.': 1.3862943649291992, 'This paper has an interesting idea that may resolve a fundamental issue of VAEs and thus deserves a place in this conference.': 1.3862943649291992}"
68,https://openreview.net/forum?id=Bk67W4Yxl,"{'The paper tests various feedforward network architectures for supervised training to predict a human’s next move, given a board position.': 1.3862943649291992, 'It trains on human play data taken from KGX, augmenting the data by considering all 8 rotations/reflections of each board position.': 1.3862943649291992, 'The paper’s presentation is inefficient and muddled, and the results seem incremental.': 1.3862943649291992, 'Presentation:': 1.3862943649291992, 'The abstract and introduction point out that AlphaGo requires many RL iterations to train, and propose to improve this by swapping out the policy network with one that is more amenable to training.': 1.3862943649291992, 'However, the paper only presents supervised learning results, not RL.': 1.3862943649291992, 'While it’s not unreasonable to assume that a higher-capacity network that shows improvements in supervised learning will also yield dividends in RL, it’s still unsatisfying to be presented with SL improvements and be asked to assume that the RL improvements will be of a similar magnitude, whatever that may mean.': 1.3862943649291992, ""It would’ve been more convincing to train both AlphaGo and this paper's architectures on an equal number of RL self-play iterations, then have them play each other."": 1.3836339712142944, '(Both would be pre-trained using supervised training, as per the AlphaGo paper).': 0.6132888197898865, 'It is not until section 3.3 that it is clearly stated this is strictly a supervised-learning paper.': 1.376270055770874, 'This should have been put front and center in the abstract and introduction.': 1.378594994544983, 'Fully 3 pages are spent on giant but simple architecture diagrams.': 1.386269211769104, 'This is both extravagant and muddles the exposition.': 0.4354077875614166, 'It seems better to show just the architectures used in the experiments, and spend at most half a page doing so, so that they may be seen alongside one another.': 1.384317398071289, 'The results (Table 1, Figures 7 and 8) are hard to skim, as there is little information in the captions, and the graph axes are poorly labeled.': 1.1490916013717651, 'For example, I assume “Examples” in figures 7 and 8 should be “Training examples”, and the number of training examples isn’t 0-50, but some large multiple thereof.': 1.3862842321395874, 'Results:': 1.3862943649291992, 'The take-home seems to be that that deeper networks do better, and residual architectures and spatial batch normalization each improve the results in this domain, as they are known to do in others.': 1.2829318046569824, 'Furthermore, we are asked to assume that the improvements in an RL setting will be similar to the improvements in SL shown here.': 1.1673580408096313, 'These results seem too incremental to justify an ICLR publication.': 1.3862943649291992, 'This paper reports new CNN architectures for playing Go.': 1.3862943649291992, 'The results are better than previously reported, but there is no mention of computational time and efficiency, and relative metric of performance/flop or performance/flop/energy.': 1.3862943649291992, 'Overall a good paper.': 1.3862943649291992, 'The paper trains slightly different network architectures on Computer Go, and provides an analysis of the accuracy as the number of training samples increases and the network architecture differs.': 1.3862943649291992, 'The paper looks like a follow up paper of the author’s previous paper Cazenave (2016a), however, the contribution over the previous paper is not clear.': 1.3862943649291992, 'A section should be added to state what the differences are.': 1.35905122756958, 'The paper states that the improvements that are obtained in this study are because of the changes in the training set, the input features and the architecture of the network.': 1.1359094381332397, 'It is not clear what are the changes that were done to the training set and input features.': 0.966035783290863, 'How are they different than the previous work?': 1.2564289569854736, 'The paper investigates several different architectures for move prediction in computer Go.': 1.3862943649291992, 'The main innovation seems to be the use of residual networks.': 1.3862943649291992, 'The best proposed architecture outperforms previous results on KGS move prediction dataset.': 1.3862923383712769, 'The network also reached amateur 3 dan level on KGS.': 1.3862943649291992, 'I found the paper to be somewhat poorly written and lacking important details.': 1.3833022117614746, 'Here are my main concerns:': 1.386085867881775, '1) This paper references a previous paper by the author(Cazenave 2016a) as having introduced residual network architectures to computer go.': 1.0908160209655762, 'The overlap with this paper seems quite significant but I could not find it anywhere.': 1.3862943649291992, 'What exactly is new?': 1.3862709999084473, '2) The author claims the addition of batch norm to a residual architecture as the main architectural innovation, but the original ResNet paper was already using batch norm between the convolution and activation layers.': 1.110543131828308, 'Have you compared your architecture (ResNet with batch norm after ReLU) with the original ResNet architecture (batch norm before ReLU)?': 1.3862940073013306, '3) It is not at all surprising that ResNets do slightly better than vanilla CNNs on move prediction.': 0.8166823387145996, ""I don't think this alone is enough for an ICLR paper."": 1.3862940073013306, ""It would be good to see at least a comparison of several different variants of the network evaluated at actually playing Go, even if it's against other bots like GnuGo, Pachi, and Fuego."": 1.3862922191619873, '4) Are the differences between net_dark and your proposed networks after 20 iterations (Table 1) significant?': 1.3441789150238037, 'Please also see my original questions.': 1.3862906694412231}"
69,https://openreview.net/forum?id=Bk8BvDqex,"{'A well written paper and an interesting construction - I thoroughly enjoyed reading it.': 1.3862922191619873, ""I found the formalism a bit hard to follow without specific examples- that is, it wasn't clear to me at first what the specific components in figure 1A were."": 1.3862943649291992, 'What constitutes the controller, a control, the optimizer, what was being optimized, etc., in specific cases.': 1.3862942457199097, 'Algorithm boxes may have been helpful, especially in the case of your experiments.': 1.3822340965270996, 'A description of existing models that fall under your conceptual framework might help as well.': 1.3862943649291992, 'In Practical Bayesian Optimization of Machine Learning Algorithms, Snoek, Larochelle and Adams propose optimizing with respect to expected improvement per second to balance computation cost and performance loss.': 1.3862943649291992, 'It might be interesting to see how this falls into your framework.': 1.3854583501815796, 'Experimental results were presented clearly and well illustrated the usefulness of the metacontroller.': 0.29366636276245117, ""I'm curious to see the results of using more metaexperts."": 1.3860973119735718, 'Pros (quality, clarity, originality, significance:):': 1.3862943649291992, 'This paper presents a novel metacontroller optimization system that learns the best action for a one-shot learning task, but as a framework has the potential for wider application.': 1.3862943649291992, 'The metacontroller is a model-free reinforcement learning agent that selects how many optimization iterations and what function or “expert” to consult from a fixed set (such as an action-value or state transition function).': 1.3862943649291992, 'Experimental results are presented from simulation experiments where a spacecraft must fire its thruster once to reach a target location, in the presence of between 1 and 5 heavy bodies.': 1.3862943649291992, 'The metacontroller system has a similar performance loss on the one-shot learning task as an iterative (standard) optimization procedure.': 1.3862943649291992, 'However, by taking into account the computational complexity of running a classical, iterative optimization procedure as a second “resource loss” term, the metacontroller is shown to be more efficient.': 1.3862942457199097, 'Moreover, the metacontroller agent successfully selects the optimal expert to consult, rather than relying on an informed choice by a domain-expert model designer.': 1.3862943649291992, 'The experimental performance is a contribution that merits publication, and it also exhibits the use of an interaction network for learning the dynamics of a simulated physical system.': 1.3862943649291992, 'The dataset that has been developed for this task also has the potential to act as a new baseline for future work on one-shot physical control systems.': 1.3862943649291992, 'The dataset constitutes an ancillary contribution which could positively impact future research in this area.': 1.018682837486267, 'Cons:': 1.386289119720459, ""It's not clear how this approach could be applied more broadly to other types of optimization."": 1.367708683013916, 'Moreover, the REINFORCE gradient estimation method is known to suffer from very high variance, yielding poor estimates.': 1.3862143754959106, ""I'm curious what methods were used to ameliorate these problems and if any other performance tricks were necessary to train well."": 1.386175513267517, 'Content of this type this could form a useful additional appendix.': 1.1385418176651, 'A few critiques on the communication of results:': 0.7360657453536987, 'The formal explication of the paper’s content is clear, but Fig.’s': 1.386286735534668, '1A and 3 could be improved.': 1.3636456727981567, 'Fig.': 1.204930305480957, '1A is missing a clear visual demarcation of what exactly the metacontroller agent is.': 1.335829257965088, 'Have you considered a plate or bounding box around the corresponding components?': 1.3862943649291992, 'This would likely speed the uptake of the formal description.': 1.3862943649291992, 'Fig. 3 is generally clear, but the lack of x-axis tick marks on any subplots makes it more challenging than necessary to compare among the experts.': 1.3862943649291992, 'Also, the overlap among the points and confidence intervals in the upper-left subplot interferes with the quantitative meaning of those symbols.': 1.242712140083313, 'Perhaps thinner bars of different colors would help here.': 1.3726941347122192, 'Moreover, this figure lacks a legend and so the different lines are impossible to compare with each other.': 0.9613907337188721, 'Lastly, the second sentence in Appendix B. 2 is a typo and terminates without completion.': 1.0388073921203613, 'This paper introduces an approach to reinforcement learning and control wherein, rather than training a single controller to perform a task, a metacontroller with access to a base-level controller and a number of accessory « experts » is utilized.': 1.3862943649291992, 'The job of the metacontroller is to decide how many times to call the controller and the experts, and which expert to invoke at which iteration.': 1.3862943649291992, '(The controller is a bit special in that in addition to being provided the current state, it is given a summary of the history of previous calls to itself and previous experts.)': 0.9409887790679932, 'The sequence of controls and expert advice is embedded into a fixed-size vector through an LSTM.': 1.3862943649291992, 'The method is tested on an N-body  control task, where it is shown that there are benefits to multiple iterations (« pondering ») even for simple experts, and that the metacontroller can deliver accuracy and computational cost benefits over fixed-iteration controls.': 1.3862943649291992, 'The paper is in general well written, and reasonably easy to follow.': 1.384879231452942, 'As the authors note, the topic of metareasoning has been studied to some extent in AI, but its use as a differentiable and fully trainable component within an RL system appears new.': 1.3862943649291992, 'At this stage, it is difficult to evaluate the impact of this kind of approach: the overall model architecture is intriguing and probably merits publication, but whether and how this will scale to other domains remains the subject of future work.': 1.3862943649291992, 'The experimental validation is interesting and well carried out, but remains of limited scope.': 1.386291265487671, 'Moreover, given such a complex architecture, there should be a discussion of the training difficulties and convergence issues, if any.': 1.3862452507019043, 'Here are a few specific comments, questions and suggestions:': 0.7472938299179077, '1) in Figure 1A, the meaning of the graphical language should be explained.': 1.1399205923080444, 'For instance, there are arrows of different thickness and line style — do these mean different things?': 1.3830665349960327, '2) in Figure 3, the caption should better explain the contents of the figure.': 1.2697882652282715, 'For example, what do the colours of the different lines refer to?': 1.3822996616363525, 'Also, in the top row, there are dots and error bars that are given, but this is explained only in the « bottom row » part.': 1.3487963676452637, 'This makes understanding this figure difficult.': 0.9094577431678772, '3) in Figure 4, the shaded area represents a 95% confidence interval on the regression line; in addition, it would be helpful to give a standard error on the regression slope (to verify that it excludes zero, i.e. the slope is significant), as well as a fraction of explained variance (R^2).': 0.19459441304206848, '4) in Figure 5, the fraction of samples using the MLP expert does not appear to decrease monotonically with the increasing cost of the MLP expert (i.e. the bottom left part of the right plot, with a few red-shaded boxes).': 0.411451131105423, 'Why is that?': 1.3862943649291992, 'Is there lots of variance in these fractions from experiment to experiment?': 1.3862943649291992, '5) the supplementary materials are very helpful.': 1.3862943649291992, 'Thank you for all these details.': 1.3862930536270142, 'Thank you for an interesting read on an approach to choose computational models based on kind of examples given.': 1.3862943649291992, 'Pros': 1.3862943649291992, 'As an idea, using a meta controller to decide the computational model and the number of steps to reach the conclusion is keeping in line with solving an important practical issue of increased computational times of a simple example.': 1.3862943649291992, 'The approach seems similar to an ensemble learning construct.': 1.3862943649291992, 'But instead of random experts and a fixed computational complexity during testing time the architecture is designed to estimate hyper-parameters like number of ponder steps which gives this approach a distinct advantage.': 1.3862943649291992, 'Cons': 1.3862943649291992, 'Even though the metacontroller is designed to choose the best amongst the given experts, its complete capability has not been explored yet.': 1.3862943649291992, 'It would be interesting to see the architecture handle more than 2 experts.': 1.3862943649291992}"
70,https://openreview.net/forum?id=Bk8N0RLxx,"{'This paper conducts a comprehensive series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation.': 1.2754449844360352, 'A range of techniques are investigated, ranging from very simple methods such as word co-occurences, to the relatively complex use of SVMs.': 1.3862943649291992, 'The experiments are solid, comprehensive and very useful in practical terms.': 1.386100172996521, 'It is good to see that the best vocabulary selection method is very effective at achieving a very high proportion of the coverage of the full-vocabulary model (fig 3).': 1.386293888092041, 'However, I feel that the experiments in section 4.3 (vocabulary selection during training) was rather limited in their scope - I would have liked to see more experiments here.': 1.3862942457199097, 'A major criticism I have with this paper is that there is little novelty here.': 1.3196643590927124, 'The techniques are mostly standard methods and rather simple, and in particular, there it seems that there is not much additional material beyond the work of Mi et al (2016).': 1.3862756490707397, 'So although the work is solid, the lack of originality lets it down.': 1.2888544797897339, 'Minor comments: in 2.1, the word co-occurence measure - was any smoothing used to make this measure more robust to low counts?': 1.3862943649291992, 'This paper compares several strategies for guessing a short list of vocabulary for the target language in neural machine translation.': 1.3862942457199097, 'The primary findings are that word alignment dictionaries work better than a variety of other techniques.': 1.3840371370315552, 'My take on this paper is that to have a significant impact, it needs to make the case for why one might want vocabulary rather than characters or sub word units like BPE.': 1.3862941265106201, 'I think there are likely many very good reasons to do this that could be argued for (synthesize morphology, deal with transliteration, etc), but most of these would suggest some particular models and experiments, which are of course not in this paper.': 1.386293888092041, 'As it is, I think this paper is a useful but minor contribution that shows that word alignment is a good way of getting short lists, but it does not strongly make the case that we should abandon work in other directions.': 1.3862943649291992, 'Minor comments:': 1.3862396478652954, 'In addition to the SVM approach for modeling vocabulary, the discriminative word lexicon of Mauser et al. (2009) and the neural version of Ha et al. (2014) are also worth mentioning.': 1.3862932920455933, 'It would be useful to know what the coverage rate of the actual full vocabulary would be (rather than the 100k “full vocabulary”).': 1.3862428665161133, 'Since presumably this technique could be used to work with much larger vocabularies.': 1.386236548423767, 'When reducing the vocabulary size for training, the Mi et al. (2016) technique of taking the union of all the vocabularies in a mini batch seems like a rather strange objective.': 1.3862943649291992, 'If the vocabulary of a single sentence is used, the probabilistic semantics of the translation model can still be preserved since p(e | f, vocab(f))': 0.9242512583732605, '= p(e | f)': 1.0342180728912354, 'if p(vocab(f) | f)': 1.1677261590957642, '= 1, i.e., is deterministic, which it is here.': 1.3779499530792236, 'Whereas the objective is no longer a sensible probability model in the mini batch vocabulary case.': 1.3862943649291992, 'Thus, while it may be a bit more difficult to implement, it seems like it would at least be a sensible comparison to make.': 1.3862943649291992, 'In this paper, the authors present several strategies to select a small subset of target vocabulary to work with per source sentence, which results in significant speedup.': 1.171250820159912, 'The results are convincing and I think this paper offers practical values to general seq2seq approaches to language tasks.': 0.8955174684524536, 'However, there is little novelty in this work: the authors further mostly extend the work of (Mi et al., 2016) with more vocabulary selection strategies and thorough experiments.': 1.2035369873046875, 'This paper will fit better in an NLP venue.': 1.3862943649291992, 'This paper evaluates several strategies to reduce output vocabulary size in order to speed up NMT decoding and training.': 1.3862943649291992, 'It could be quite useful to practitioners, although the main contributions of the paper seem somewhat orthogonal to representation learning and neural networks, and I am not sure ICLR is the ideal venue for this work.': 1.3862943649291992, 'Do the reported decoding times take into account the vocabulary reduction step?': 1.3862943649291992, 'Aside from machine translation, might there be applications to other settings such as language modeling, where large vocabulary is also a scalability challenge?': 1.3862943649291992, 'The proposed methods are helpful because of the difficulties induced by using a word-level model.': 1.3862943649291992, 'But (at least in my opinion) starting from a character or even lower-level abstraction seems to be the obvious solution to the huge vocabulary problem.': 1.3862943649291992}"
71,https://openreview.net/forum?id=Bk8aOm9xl,"{'The authors present a novel approach to surprise-based intrinsic motivation in deep reinforcement learning.': 0.726589560508728, 'The authors clearly explain the difference from other recent approaches to intrinsic motivation and back up their method with results from a broad class of discrete and continuous action domains.': 1.0986123085021973, ""They present two tractable approximations to their framework - one which ignores the stochasticity of the true environmental dynamics, and one which approximates the rate of information gain (somewhat similar to Schmidhuber's formal theory of creativity, fun and intrinsic motivation)."": 1.0986123085021973, 'The results of this exploration bonus when added to TRPO are generally better than standard TRPO.': 1.0986119508743286, 'However, I would have appreciated a more thorough comparison against other recent work on intrinsic motivation.': 1.0986123085021973, ""For instance, Bellemare et al 2016 recently achieved significant performance gains on challenging Atari games like Montezuma's Revenge by combining DQN with an exploration bonus, however Montezuma's Revenge is not presented as an experiment here."": 1.0982921123504639, 'Such comparisons would significantly improve the strength of the paper.': 1.0986123085021973, 'This paper provides a surprise-based intrinsic reward method for reinforcement learning, along with two practical algorithms for estimating those rewards.': 1.0984116792678833, 'The ideas are similar to previous work in intrinsic motivation (including VIME and other work in intrinsic motivation).': 1.0986084938049316, 'As a positive, the methods are simple to implement, and provide benefits on a number of tasks.': 1.0985978841781616, 'However, they are almost always outmatched by VIME, and not one of their proposed method is consistently the best of those proposed (perhaps the most consistent is the surprisal, which is unfortunately not asymptotically equal to the true reward).': 1.0986123085021973, 'The authors claim massive speed up, but the numerical measurements show that VIME is slower to initialize but not significantly slower per iteration otherwise (perhaps a big O analysis would clarify the claims).': 1.0986121892929077, ""Overall it's a decent, simple technique, perhaps slightly incremental on previous state of the art."": 1.0986121892929077, 'This paper explores the topic of intrinsic motivation in the context of deep RL.': 1.0835782289505005, 'It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game “venture”, maybe).': 1.0986117124557495, 'Novelty: none of the proposed types of intrinsic motivation are novel, and it’s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012).': 0.39273756742477417, 'Potential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited': 1.0986123085021973, 'I would encourage the authors to also discuss the limitations.': 1.0986111164093018, 'For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration.': 1.0986123085021973, 'So other forms of learning progress or surprise derived from the agent’s competence instead might be more promising in the long run?': 1.0986123085021973, 'See also Srivastava et al 2012 for further thoughts.': 1.0986123085021973, 'Computation time: I find the paper’s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost?': 1.0986123085021973, 'So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?': 1.0986123085021973}"
72,https://openreview.net/forum?id=BkCPyXm1l,"{'The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction.': 1.0986123085021973, 'Very similar method was proposed in Section 6 from (Hinton et al. 2016, Distilling the Knowledge in a Neural Network).': 1.0986123085021973, 'Pros:': 1.0986123085021973, '+ Comprehensive analysis on the co-label similarity.': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'Weak baselines.': 1.0986123085021973, 'I am not sure the authors have found the best hyper-parameters in their experiments.': 1.0986123085021973, 'I just trained a 5 layer fully connected MNIST model with 512 hidden units without any regularizer and achieved 0.986 acc.': 1.0986123085021973, 'using Adam and He initialization, where the paper reported 0.981 for such architecture.': 1.0986123085021973, 'The authors failed to bring the novel idea.': 1.0986123085021973, 'It is very similar to (Hinton et al. 2016).': 1.0986123085021973, 'This is probably not enough for ICLR.': 1.0986123085021973, 'Inspired by the analysis on the effect of the co-label similarity (Hinton et al., 2015), this paper proposes a soft-target regularization that iteratively trains the network using weighted average of the exponential moving average of past labels and hard labels as target argument of loss.': 1.0986123085021973, 'They claim that this prevents the disappearing of co-label similarity after early training and  yields a competitive regularization to dropout without sacrificing network capacity.': 1.0986123085021973, 'In order to make a fair comparison to dropout,  the dropout should be tuned carefully.': 1.0986123085021973, 'Showing that it performs better than dropout regularization for some particular values of dropout (Table 2) does not demonstrate a convincing advantage.': 1.0986123085021973, 'It is possible that dropout performs better after a reasonable tuning with cross-validation.': 1.0986123085021973, 'The baseline architectures used in the experiments do not belong the recent state of art methods thus yielding significantly lower accuracy.': 1.0986123085021973, 'It seems also that experiment setup does not involve any data augmentation, the results can also change with augmentation.': 1.0986123085021973, 'It is not clear why number of epochs are set to a small number like 100 without putting some convergence tests..': 1.0986123085021973, 'Therefore the significance of the method is not convincingly demonstrated in empirical study.': 1.0986123085021973, 'Co-label similarities could be calculated using softmax results at final layer rather than using predicted labels.': 1.0986123085021973, 'The advantage over dropout is not clear in Figure 4, the dropout is set to 0.2 without any cross-validation.': 1.0986123085021973, 'Regularizing by enforcing the training steps to keep co-label similarities is interesting idea but not very novel and the results are not significant.': 1.0986123085021973, 'Pros :': 1.0986123085021973, 'provides an investigation of regularization on co-label similarity during training': 1.0986123085021973, 'The empirical results do not support the intuitive claims regarding proposed procedure': 1.0986123085021973, 'Iterative version can be unstable in practice': 1.0986123085021973, 'This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself.': 1.0986123085021973, 'In this sense it is similar in spirit to scheduled sampling (Bengio et al) and SEARN (Daume et al) DAgger (Ross et al) which consider a ""roll-in"" mixture of the target and model distributions during training.': 1.0986123085021973, 'It was clarified in the pre-review questions that these targets are generated on-line rather than from a lagged distribution, which I think makes the algorithm pseudocode somewhat misleading if I understand it correctly.': 1.0986123085021973, 'This is an incremental improvement on the idea of label softening/smoothing that has recently been revived, and so the novelty is not that high.': 1.0986123085021973, ""The author points out that co-label similarity is better preserved by this method but it doesn't follow that this is causal re: regularization; a natural baseline would be a fixed, soft label distribution, as well as one where the softening/temperature of the label distribution is gradually reduced (as one would expect for this method to do as the model gets closer and closer to reproducing the target distribution)."": 1.085695505142212, ""It's an interesting and somewhat appealing idea but the case is not clearly made that this is all that useful."": 0.4189211130142212, 'The dropout baselines for MNIST seem quite far from results already in the literature (Srivastava et al 2014 achieves 1.06% with a 3x1024 MLP with dropout and a simple max norm constraint; the dropout baselines here fail to break 1.3% which is rather high by contemporary standards on the permutation-invariant task), and results for CIFAR10 are quite far from the current state of the art, making it difficult to judge the contribution in light of other innovations.': 0.49040037393569946, 'The largest benchmark considered is SVHN where the reported accuracies are quite bad indeed; SOTA for single net performance has been less than half the reported error rates for 3-4 years now.': 0.5995778441429138, ""It's unclear what conclusions can be drawn about how this would help (or even hurt) in a better-tuned setting."": 0.6495397090911865, 'I have remaining reservations about data hygiene, namely reporting minimum test loss/maximum test accuracy rather than an unbiased method for model selection (minimum validation set error, for example).': 1.0967050790786743, 'Relatedly, the regularization potential of early stopping on a validation set is not considered.': 0.753626823425293, 'See, e.g. the protocol in Goodfellow et al (2013).': 1.09861159324646}"
73,https://openreview.net/forum?id=BkGakb9lx,"{'This paper addresses the problem of decoding barcode-like markers depicted in an image.': 1.0764873027801514, 'The main insight is to train a CNN from generated data produced from a GAN.': 1.0986121892929077, 'The GAN is trained using unlabeled images, and leverages a ""3D model"" that undergoes learnt image transformations (e.g., blur, lighting, background).': 1.0986123085021973, 'The parameters for the image transformations are trained such that it confuses a GAN discriminator.': 1.09756338596344, 'A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images.': 1.0983966588974, 'The proposed method out-performs both baselines on decoding the barcode markers.': 1.0984379053115845, 'The proposed GAN architecture could potentially be interesting.': 1.090622067451477, 'However, I won’t champion the paper as the evaluation could be improved.': 1.0864256620407104, 'A critical missing baseline is a comparison against a generic GAN.': 1.0980043411254883, 'Without this it’s hard to judge the benefit of the more structured GAN.': 1.0652825832366943, 'Also, it would be worth seeing the result when one combines generated and real images for the final task.': 1.0986123085021973, 'A couple of references that are relevant to this work (for object detection using rendered views of 3D shapes):': 1.0986123085021973, '[A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015.': 0.38179588317871094, '[B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views.': 0.4652763903141022, 'Francisco Massa, Bryan C. Russell, Mathieu Aubry.': 0.7435770034790039, 'CVPR 2016.': 1.0985995531082153, 'The problem domain (decoding barcode markers on bees) is limited.': 0.42104747891426086, 'It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed.': 1.0986067056655884, 'I found the writing to be somewhat vague throughout.': 0.7276172041893005, 'For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper.': 1.094957709312439, 'Minor comments:': 1.0985971689224243, 'Fig 3 - Are these really renders from a 3D model?': 1.066641092300415, 'The images look like 2D images, perhaps spatially warped via a homography.': 1.0983163118362427, 'Page 3: ""chapter"" => ""section"".': 0.22950172424316406, 'In Table 2, what is the loss used for the DCNN?': 0.9212017059326172, 'Fig 9 (a) -': 1.0975505113601685, 'The last four images look like they have strange artifacts.': 1.098492980003357, 'Can you explain these?': 1.042851209640503, 'The paper proposes an approach to generating synthetic training data for deep networks, based on rendering 3D models and learning additional transformations with adversarial training.': 1.0986123085021973, 'The approach is applied to generating barcode-like markers used for honeybee identification.': 1.0985664129257202, 'The authors demonstrate that a classifier trained on synthetic data generated with the proposed approach outperforms both training on (limited) real data and training on data with hand-designed augmentations.': 1.0986123085021973, 'The topic of the paper — using machine learning (in particular, adversarial training) for generating realistic synthetic training data — is very interesting and important.': 1.0986123085021973, 'The proposed method looks reasonable, and the paper is written well.': 1.0986008644104004, 'The downside is that experiments are limited to a fairly simple and not-widely-known domain of honeybee marker classification.': 1.0986123085021973, 'While I am sure this is an important task by itself, in order to demonstrate general applicability of the method and to allow comparison with existing techniques, experiments on some standard and/or realistic datasets would be very helpful.': 1.0986123085021973, 'Overall, I recommend acceptance, but encourage the authors to perform experiments on more datasets.': 1.0984375476837158, 'I appreciate that the authors added a baseline with manually designed transformations.': 1.098585605621338, 'This strengthens the paper.': 1.0986123085021973, 'As Reviewer3 points out, it would be interesting to analyze if restricting GAN to a fixed set of transformations is necessary here, and which transformations are most important.': 1.0986123085021973, 'Perhaps this would provide some guidelines for designing sets of transformations for more complicated scenarios.': 1.0986123085021973, 'The authors should tone down their claims such as “Our method is an improvement over previous work  <...>': 1.0986123085021973, 'Whereas previous work relied on real data for training using pre-trained models or mixing real and generated data, we were able to train a DCNN from scratch with generated data that performed well when tested on real data.': 1.0985897779464722, '“.': 1.0986123085021973, 'This is not a fair comparison: the domain studied by authors in this work is much simpler than what was studied in these previous works, so this comparison is not appropriate.': 1.0986121892929077, 'The submission proposes an interesting way to match synthetic data to real data in a GAN type architecture.': 1.0959253311157227, 'The main novelty are parametric modules that emulate different transformations and artefact that allow to match the natural appearance.': 1.0986123085021973, 'several points were raised during the discussion:': 1.0986123085021973, '1. the proposed method is more model driven that previous GAN models. But does it pay off? how would a traditional GAN approach perform? The mentioned effects like blur, lighting and background could also potentially be modelled by upsamling network that directly predicts the image. I would assume that blur and lighting can be modelled by convolutions. transformations to some extend by convolutions - or spatial transformer networks.': 0.2457321733236313, 'The answers of the authors only partially addresses the point.': 0.5268537402153015, 'The key proposal of the submission seems parameterised modules that can be trained to match the real data distribution.': 1.0986119508743286, 'but it remains unclear why not a more generic parameterisation can also do the job.': 1.0986123085021973, 'E.g. a neural network - as done in regular GANs.': 0.2688697874546051, 'The benefit of introducing a stronger model is unclear.': 1.0986086130142212, 'Using a render engine to generate the initial sample appearance if of limited novelty.': 1.0986123085021973, '2. how does it compare to traditional data augmentation techniques, e.g. noise, dropout, transformations. you are linking to keras code - where data augmentation is readily available and could be tested (ImageDataGenerator)': 0.7360577583312988, 'The authors reply that plenty of such augmentation was used and more details are going to be provided in the appendix.': 1.0985376834869385, 'it would have been appreciated if such information was directly included in the revision - so that the procedure could be directly checked.': 0.842062771320343, 'right now - this remains a point of uncertainty.': 1.0897245407104492, '3. How do the different stages (\\phis) effect performance? which are the most important ones?': 0.4607245624065399, 'The authors do evaluate the effect of hand tuning the transformation stages vs. learning them.': 0.8792243003845215, 'it would be great to also include results of including/excluding stages completely - and also reporting how much the initial jittering of the data helps.': 1.0984010696411133, 'While there is an interesting idea of (limited) novelty to the paper, there are some concerns about evalations and comparisons as outlined above.': 1.0982532501220703, 'In addition, only success on a single dataset/task is shown.': 1.0986123085021973, 'Yet the task is interesting and seems challenging.': 1.0986123085021973, 'Overall, this remains makes only a weak recommendation for acceptance.': 1.0986123085021973}"
74,https://openreview.net/forum?id=BkIqod5ll,"{'Previous literature uses data-derived adjacency matrix A to obtain neighbors to use as foundation of graph convolution.': 1.0986123085021973, 'They propose extending the set of neighbors by additionally including nodes reachable by i<=k steps in this graph.': 1.0986123085021973, 'This introduces an extra tunable parameter k, so it needs some justification over the previous k=1 solution.': 1.0986123085021973, 'In one experiment provided (Merk), using k=1 worked better.': 1.0986123085021973, ""They don't specify which k that used, just that it was big enough for their to be p=5 nodes obtained as neighbors."": 1.0986123085021973, 'In the second experiment (MNIST), they used k=1 for their experiments, which is what previous work (Coats & Ng 2011) proposed as well.': 1.0985692739486694, 'A compelling experiment would compare to k=1 and show that using k>1 gives improvement strong enough to justify an extra hyper-parameter.': 1.0986123085021973, 'Update: I thank the authors for their comments!': 1.0860425233840942, 'After reading them, I decided to increase the rating.': 1.098602056503296, 'This paper proposes a variant of the convolution operation suitable for a broad class of graph structures.': 1.0413089990615845, 'For each node in the graph, a set of neighbours is devised by means of random walk (the neighbours are ordered by the expected number of visits).': 1.0986123085021973, 'As a result, the graph is transformed into a feature matrix resembling MATLAB’s/Caffe’s im2col output.': 1.0986123085021973, 'The convolution itself becomes a matrix multiplication.': 0.30919432640075684, 'Although the proposed convolution variant seems reasonable, I’m not convinced by the empirical evaluation.': 1.0971318483352661, 'The MNIST experiment looks especially suspicious.': 1.0986123085021973, 'I don’t think that this dataset is appropriate for the demonstration purposes in this case.': 1.0986104011535645, 'In order to make their method applicable to the data, the authors remove important structural information (relative locations of pixels) thus artificially increasing the difficulty of the task.': 1.0986123085021973, 'At the same time, they are comparing their approach with regular CNNs and conclude that the former performs poorly (and does not even reach an acceptable accuracy for the particular dataset).': 1.0986123085021973, 'I guess, to justify the presence of MNIST (or similar datasets) in the experimental section, the authors should modify their method to incorporate additional graph structure (e.g. relative locations of nodes) in cases when the relation between nodes cannot be fully described by a similarity matrix.': 1.0986121892929077, 'I believe, in its current form, the paper is not yet ready for publication but may be later resubmitted to a workshop or another conference after the concern above is addressed.': 1.0986121892929077, 'This work proposes a convolutional architecture for any graph-like input data (where the structure is example-dependent), or more generally, any data where the input dimensions that are related by a similarity matrix.': 1.0986111164093018, 'If instead each input example is associated with a transition matrix, then a random walk algorithm is used generate a similarity matrix.': 0.42089614272117615, 'Developing convolutional or recurrent architectures for graph-like data is an important problem because we would like to develop neural networks that can handle inputs such as molecule structures or social networks.': 1.0986123085021973, ""However, I don't think this work contributes anything significant to the work that has already been done in this area."": 0.8974311351776123, 'The two main proposals I see in this paper are:': 0.4385860860347748, '1) For data associated with a transition matrix, this paper proposes that the transition matrix be converted to a similarity matrix.': 1.0986123085021973, 'This seems obvious.': 1.0986123085021973, '2) For data associated with a similarity matrix, the k nearest neighbors of each node are computed and supply the context information for that node.': 1.0986123085021973, 'This also seems obvious.': 1.0986123085021973, 'Perhaps I have misunderstood the contribution, but the presentation also lacks clarity, and I cannot recommend this paper for publication.': 1.0986123085021973, 'Specific Comments:': 1.0986123085021973, '1) On page 4: ""An interesting attribute of this convolution, as compared to other convolutions on graphs is that, it preserves locality while still being applicable over different graphs with different structures.""': 1.0986123085021973, 'This is false; the other proposed architectures can be applied to inputs with different structures (e.g. Duvenaud et.': 1.0986123085021973, 'al., Lusci et.': 1.0986123085021973, 'al. for NN architectures on molecules specifically).': 1.0986123085021973}"
75,https://openreview.net/forum?id=BkJsCIcgl,"{'The paper proposes an approach to learning models that are good for planning problems, using deep netowork architectures.': 1.0982742309570312, 'The key idea is to ensure that models are self-consistent and accurately predict the future.': 1.0985900163650513, 'The problem of learning good planning models (as opposed to simply good predictive models is really crucial and attempts so far have failed.': 1.0985366106033325, 'This paper is conceptually interesting and provides a valuable perspective on how to achieve this goal.': 1.0986078977584839, 'Its incorporation of key RL concepts (like discounting and eligibility traces) and the flexibility to learn these is very appealing.': 1.0986123085021973, 'Hence, I think it should be accepted.': 1.0986123085021973, 'This being said, I think the paper does not quite live up to its claims.': 1.0986121892929077, 'Here are some aspects that need to be addressed (in order of importance):': 1.0986123085021973, '1. Relationship to past work: the proposed representation seems essentially a non-linear implementation of the Horde architecture. It is also very similar in spirit to predictive state representations. Yet these connections are almost not discussed at all. The related work paragraph is very brief and needs expansion to situate the work in the context of other predictive modelling attempts that both were designed to be used for planning and (in the case of PSRs) were in fact successsfully used in planning tasks. Some newer work on learning action-conditional models in Atari games are also not discussed. Situating the paper better in the context of existing model learning would also help understand easier both the motivations and the novel contributions of the work (otherwise, the reader is left to try and elucidate this for themselves, and may come to the wrong conclusion).': 1.0986123085021973, '2. The paper needs to provide some insight about the necessity of the recurrent core of the architecture. The ideas are presented nicely in general fashion, yet the proposed impolementation is quite specific and ""bulky"" (very high number of parameters). Is this really necessary in all tasks? Can one implement the basic ideas outside of the particular architecture proposed? Can we use feedforward approximations or is the recurrent part somehow necessary? At the very least the paper should expand the discussion on this topic, if not provide some empirical evidence.': 1.033892273902893, ""3. The experiments are very restricted in their setup: iid data drawn from fixed distributions, correct targets. So, the proposed approach seems like an overkill for these particular tasks. There is an indirect attempt to provide evidence the learned models would be useful for planning, but no direct measurement to support this'd claim (no use of the models in planning). Compared to the original Horde paper, fewer predictions are learned, and these are more similar to each other. While I sympathize with the desire to go in steps, I think the paper stops short of where it should. At the very least, doing prediction in the context of an actual RL prediction task, with non-iid inputs, should be included in the paper. This should only require minor modifications to the experiments (same task, just different data). Ideally, in the case of the mazes, the learned models should be used in some form of simplified planning to learn paths. This would align the experiments much better with the claims in the presentation of the architecture."": 0.9784808158874512, 'I think there may be a nice paper to made from this, but as it is, it should not be accepted.': 1.0986109972000122, 'The authors describe a new architecture for regression, inspired by techniques for estimating the value function of an Markov reward process.': 1.0986123085021973, 'The connection is interesting, and there is certainly merit in the idea.': 1.0986021757125854, 'However, the writing is confusing,  and as far as I can tell, the experiments and discussion are inadequate.': 1.0986123085021973, 'It is quite possible that I am misunderstanding some things, so I am not putting high confidence.': 1.0986123085021973, ""Because of all the discussion of MRP's and the background that inspired the model, it is difficult to see that the authors are in a pure, i.i.d. regression setting, where they sample inputs i.i.d."": 1.0986123085021973, '(with deterministic outputs given the input) from a distribution, and try to match  a parameterized function to the input output pairs.': 1.0986123085021973, 'Because they are in this setting, there is a lot lacking from the experiments.': 1.0986123085021973, 'For example, they report l2 loss on the maze problem; but not ""percent correct""; indeed, it looks like the deep net with skips goes to about .001 average l2 loss on the 0-1 output maze problem.': 1.0986123085021973, 'This is an issue because because it suggests that by simply thresholding the outputs, you could get nearly perfect results, which would point to a model specification error of the baseline.': 1.0986123085021973, 'Are there sigmoids at the end of the baseline plain deep network?': 1.0986123085021973, 'Note that the proposed models do have sigmoids in the outputs in the multiplicative weightings.': 1.0986123085021973, 'How do the number of parameters of the proposed network compare to the baselines?': 1.0986123085021973, 'Is the better performance (and again, better is really marginal if I am understanding the way loss is measured) simply an issue of modeling power (perhaps because of the multiplicative connections of the proposed model vs. the baseline)?': 1.0986123085021973, 'Because the input is taken i.i.d and the test distribution exactly matches the train, this is an important part of the discussion.': 1.0986123085021973, 'Moreover, there do not seem to be experiments where the size of the training set is fixed- the axis in the graphs is number of samples seen, which is tied to the number of optimization steps.': 1.0986123085021973, 'Thus there is no testing of over-fitting.': 1.0986123085021973, 'Why not try the model on more standard regression problems (as at heart, the paper seems to be about a new convnet architecture for regression)?': 1.0986123085021973, 'Show imagenet or cifar accuracies, for example.': 1.0986123085021973, 'If  the proposed model does worse there, try to explain/understand what it is about the reported tasks that favor the proposed model?': 1.0986123085021973, '**********************************************************************************': 1.0986123085021973, 'edited with increased confidence in post review discussions': 1.0986123085021973, 'This work proposes a computational structure of function approximator with a strong prior: it is optimized to act as an abstract MRP, capable of learning its own internal state, model, and notion of time-step.': 1.0986123085021973, 'Thanks to the incorporation of a \\lambda-return style return estimation, it can effectively adapt its own ""thinking-depth"" on the current input, thus performing some sort of soft iterative inference.': 1.0986123085021973, 'Such a prior, maintained by strong regularization, helps perform better than similar baselines or some prediction tasks that require some form of sequential reasoning.': 1.0986123085021973, 'The proposed idea is novel, and a very interesting take on forcing internal models upon function approximators which begs for future work.': 1.0986123085021973, 'The experimental methodology is complete, showcases the potential of the approach, and nicely analyses the iterative/adaptative thinking depth learned by the model.': 1.0986123085021973, 'As pointed out by my previous comments, the paper reads well but utilizes language that may confuse a reader unfamiliar with the subject.': 1.0986123085021973, 'I think some rewording could be done without having much impact on the depth of the paper.': 1.0986123085021973, 'In particular, introducing the method as a regularized model pushed to act like an MRP, rather than an actual MRP performing some abstract reasoning, may help confused readers such as myself.': 1.0986123085021973}"
76,https://openreview.net/forum?id=BkLhzHtlg,"{'While my above review title is too verbose, it would be a more accurate title for the paper than the current one (an overall better title would probably be somewhere in between).': 1.0986123085021973, 'The overall approach is interesting: all three of the key techniques (aux.': 1.0986123085021973, 'tasks, skip/diagonal connections, and the use of internal labels for the kind of data available) make a lot of sense.': 1.0983186960220337, 'I found some of the results hard to understand/interpret.': 1.0986098051071167, 'Some of the explanation in the discussion below has been helpful (e.g. see my earlier questions about Fig 4 and 5); the paper would benefit from including more such explanations.': 1.0986123085021973, 'It may be worthwhile very briefly mentioning the relationship of ""diagonal"" connections to other emerging terms for similar ideas (e.g. skip connections, etc).': 1.0986123085021973, '""Skip"" seems to me to be accurate regardless of how you draw the network, whereas ""diagonal"" only makes sense for certain visual layouts.': 0.4430920481681824, 'In response to comment in the discussion below: ""leading to less over-segmentation of action bouts"" (and corresponding discussion in section 5.1 of the paper): I would be like to have a bit more about this in the paper.': 1.0986123085021973, 'I have assumed that ""per-bout"" refers to ""per-action event"", but now I am not certain that I have understood this correctly (i.e. can a ""bout"" last for a few minutes?): given the readership, I think it would not be inappropriate to define some of these things explicitly.': 1.0986121892929077, 'In response to comment about fly behaviours that last minutes vs milliseconds: This is interesting, and I would be curious to know how classification accuracy relates to the time-scale of the behaviour (e.g. are most of the mistakes on long-term behaviours?': 1.0986123085021973, 'i realize that this would only tell part of the story, e.g. if you have a behaviour that has both a long-term duration, but that also has very different short-term characteristics than many other behaviours, it should be easy to classify accuractely despite being ""long-term"").': 0.9414670467376709, ""If easy to investigate this, I would add a comment about it; if this is hard to investigate, it's probably not worth it at this point, although it's something you might want to look at in future."": 1.0985606908798218, 'In response to comment about scaling to human behavior: I agree that in principle, adding conv layers directly above the sensory input would be the right thing to try, but seriously: there is usually a pretty big gap between what ""should"" work and what actually works, as I am sure the authors are aware.': 1.0986121892929077, '(Indeed, I am sure the authors have a much more experiential and detailed understanding of the limitations of their work than I do).': 0.6687536239624023, 'What I see presented is a nice system that has been demonstrated to handle spatiotemporal trajectories.': 0.4850841462612152, 'The claims made should correspond to this.': 1.0986121892929077, 'I would consider adjusting my rating to a 7 depending on future revisions.': 1.0974658727645874, 'The paper presents a method for joint motion prediction and activity classification from sequences with two different applications: motion of fruit flies and online handwriting recognition.': 1.0986123085021973, 'The method uses a classical encoder-decoder pipeline, with skip connections allowing direct communication between the encoder and the decoder on respective levels of abstraction.': 1.0986123085021973, 'Motion is discretized and predicted using classification.': 1.0986123085021973, 'The model is trained on classification loss combined with a loss on motion prediction.': 1.0986123085021973, 'The goal is to leverage latter loss in a semi-supervised setting from parts of the data which do not contain action labels.': 1.0986123085021973, 'The idea of leveraging predictions to train feature representations for discrimination is not new.': 1.0986123085021973, 'However, the paper presents a couple of interesting ideas, partially inspired from other work in other areas.': 1.0986123085021973, 'My biggest concern is with the experimental evaluation.': 1.0986123085021973, 'The experimental section contains a large amount of figures, which visualize what the model has learned in a qualitative way.': 1.0986123085021973, 'However, quantitative evaluation is rarer.': 1.0986123085021973, 'On the fly application, the authors compare the classification performance with another method previously published by the first author.': 1.0986123085021973, 'Again on the fly application, the performance gain on motion prediction in figure 5c looks small compared to the baseline.': 1.0986123085021973, 'I am not sure it is significant.': 1.0986123085021973, 'I did not see any recognition results on the handwriting application.': 1.0986123085021973, 'Has this part not been evaluated?': 1.0986123085021973, 'Figure 5a is difficult to understand and to interpret.': 1.0986123085021973, 'The term ""BesNet"" is used here without any introduction.': 1.0986123085021973, 'Figure 4 seems to tell multiple and different stories.': 1.0986086130142212, ""I'd suggest splitting it into at least two different figures."": 1.088269829750061, 'This paper proposes a recurrent architecture for simultaneously predicting motion and action states of agents.': 1.0986123085021973, 'The paper is well written, clear in its presentation and backed up by good experiments.': 1.0911128520965576, 'They demonstrate that by forcing the network to predict motion has beneficial consequences on the classification of actions states,': 1.0986123085021973, 'allowing more accurate classification with less training data.': 0.6465769410133362, 'They also show how the information learned by the network is interpretable and organised in a hierarchy.': 1.098611831665039, 'Weaknesses:': 1.09861159324646, 'a critical discussion on the interplay between motion an behaviour that is needed to experience the benefits of their proposed model is missing from the paper.': 0.993422269821167, 'moreover, a discussion on how this approach could scale to more challenging scenarios ""involving animals"" and visual input for instance and more general ""behaviours"" is also missing;': 1.0985735654830933, 'The criticism here is pointed at the fact that the title/abstract claim general behaviour modelling, whilst the experiments are focused on two very specific and relatively simple scenarios,': 1.0986123085021973, 'making the original claim a little bit far fetched unless its backed up by additional evidence.': 1.0907258987426758, 'Using ""Insects"", or ""fruit flies"" would be more appropriate than ""animals"".': 1.0986123085021973}"
77,https://openreview.net/forum?id=BkSmc8qll,"{'The authors proposed a dynamic neural Turing machine (D-NTM) model that overcomes the rigid location-based memory access used in the original NTM model.': 0.9629145860671997, 'The paper has two main contributions: 1) introducing a learnable addressing to NTM.': 1.0986123085021973, '2) curriculum learning using hybrid discrete and continuous attention.': 1.0986123085021973, 'The proposed model was empirically evaluated on Facebook bAbI task and has shown improvement over the original NTM.': 1.0986123085021973, 'Pros:': 1.0986123085021973, '+ Comprehensive comparisons of feed-forward controllers v.s. recurrent controllers': 1.0986123085021973, '+': 1.0986123085021973, 'Encouraging results on the curriculum learning on hybrid discrete and continuous attentions': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'Very weak NTM baseline (due to some hyper-parameter engineering?) in Table 1, 31% err.': 1.0986123085021973, 'comparing to the NTM 20% err.': 1.0986123085021973, 'reported in Table 1 in(Graves et al, 2016, Hybrid computing using a neural network with dynamic external memory).': 1.0986123085021973, 'In fact, the NTM baseline in (Graves et al 2016) is better than the proposed D-NTM with GRU controller.': 1.0945762395858765, 'Maybe it is worthwhile to reproduce their results using the hyper-parameter setting in their Table2 which could potentially lead to better D-NTM performance?': 0.40611281991004944, 'Section 3 of the paper is hard to follow.': 1.0986123085021973, 'The overall clarity of the paper needs improvement.': 1.0986123085021973, 'This paper introduces a variant of the neural Turing machine (NTM, Graves et al. 2014) where key and values are stored.': 1.0986123085021973, 'They try both continuous and discrete mechanisms to control the memory.': 1.0986123085021973, 'The model is quite complicated and seem to require a lot of tricks to work.': 1.0986123085021973, 'Overall it seems that more than 10 different terms appear in the cost function and many different hacks are required to learn the model.': 1.0986123085021973, 'It is hard to understand the justification for all of these tricks and sophisticated choices.': 1.0986123085021973, 'There is no code available nor plan to release it (afaik).': 1.0986123085021973, 'The model is evaluated on a set of toy problems (the “babi task”) and achieves performance that are only slightly above those of a vanilla LSTM but are much worse than the different memory augmented models proposed in the last few years.': 0.5818829536437988, 'In terms of writing, the description of the model is quite hard to follow, describing different blocks independently, optimization tricks and regularization.': 1.0546990633010864, 'The equations are hard to read, using non standard notation (e.g., “softplus”), overloading notations (w_t, b…), or write similar equations in different ways (for example, eq (8-9) compared to (10-11).': 1.0848468542099, 'Why are two equations in scalar and the other in vectors?': 1.097969651222229, 'Why is there an arrow instead of an equal?…).': 1.094283938407898, 'Overall it is very hard to put together all the pieces of this model(s), there is no code available and I’m afraid there is not enough details to be able to reproduce their numbers.': 1.0986123085021973, 'Finally, the performance on the bAbI tasks are quite poor compared to other memory augmented models.': 1.0986123085021973, 'The paper extends the NTM by a trainable memory addressing scheme.': 1.0986123085021973, 'The paper also investigates both continuous/differentiable as well as discrete/non-differentiable addressing mechanisms.': 1.0986123085021973, '* Extension to NTM with trainable addressing.': 1.0986123085021973, '* Experiments with discrete addressing.': 1.0986123085021973, '* Experiments on bAbI QA tasks.': 1.0986123085021973, '* Big gap to MemN2N and DMN+ in performance.': 1.0986123085021973, '* Code not available.': 1.0986123085021973, '* There could be more experiments on other real-world tasks.': 1.0986123085021973}"
78,https://openreview.net/forum?id=BkSqjHqxg,"{'This paper proposes an unsupervised graph embedding learning method based on random walk and skip-thought model.': 1.3862943649291992, 'They show promising results compared to several competitors on four chemical compound datasets.': 1.3862943649291992, 'Strength:': 1.386290431022644, '1, The idea of learning the graph embedding by applying skip-thought model to random walk sequences is interesting.': 0.6281982660293579, '2, The paper is well organized.': 0.694791853427887, 'Weakness:': 1.0964221954345703, '1, As the current datasets are small (e.g., the average number of nodes per graph is around 30), it would be great to explore larger graph datasets to further investigate the method.': 0.7552160620689392, '2, Comparisons with recent work like LINE and node2vec are missing.': 0.6786012053489685, 'You can compare them easily by applying the same aggregation strategy to their node embeddings.': 1.3862943649291992, 'Detailed Questions:': 1.3843563795089722, '1, The description about how to split the random walk sequence into 3 sub-sequences is missing.': 0.5501480102539062, 'Also, the line “l_min >= (n_k - 1), … >= l_max” in section 2.2.2 is a mistake.': 1.3862943649291992, '2, Can you provide the standard deviations of the 5-fold cross validation in Table 2?': 0.5072001218795776, 'I’m curious about how stable the algorithm is.': 1.3842167854309082, 'This paper studies the graph embedding problem by using the encoder-decoder method.': 1.3862943649291992, 'The experimental study on real network data sets show the features extracted by the proposed model is good for classification.': 1.3862924575805664, 'Strong points of this paper:': 1.386017084121704, '1. The idea of using the methods from natural language processing to graph mining is quite interesting.': 0.3609681725502014, '2. The organization of the paper is clear': 0.5265747308731079, 'Weak points of this paper:': 1.3862943649291992, '1. Comparisons with state-of-art methods (Graph Kernels) is missing.': 1.0986124277114868, '2. The problem is not well motivated, are there any application of this. What is the different from the graph kernel methods? The comparison with graph kernel is missing.': 0.47031477093696594, '3. Need more experiment to demonstrate the power of their feature extraction methods. (Clustering, Search, Prediction etc.)': 0.702160120010376, '4. Presentation of the paper is weak. There are lots of typos and unclear statements.': 1.3851603269577026, ""5. The author mentioned about the graph kernel things, but in the experiment they didn't compare them. Also, only compare the classification accuracy by using the proposed method is not enough."": 0.2882688343524933, 'Authors take the skip-graph architecture (Kiros 2015) and apply it to classifying labeled graphs (molecular graphs).': 1.3862943649291992, 'They do it by creating many sentences by walking the graph randomly, and asking the model to predict previous part and next part from the middle part.': 1.3852351903915405, 'Activations of the decoder part of this model on a walk generated from a new graph are used as features for a binary classifier use to predict whether the molecule has anti-cancer properties.': 1.3862937688827515, 'Paper is well written, except that evaluation section is missing details of how the embedding is used for actual classification (ie, what classifier is used)': 1.3859320878982544, ""Unfortunately I'm not familiar with the dataset and how hard it is to achieve the results they demonstrate, that would be the important factor to weight on the papers acceptance."": 1.3862942457199097, 'The paper presents a method to learn graph embeddings in a unsupervised way using random walks.': 1.3862941265106201, 'It is well written and the execution appears quite accurate.': 1.3862910270690918, 'The area of learning whole graph representations does not seem to be very well explored in general, and the proposed approach enjoys having very few competitors.': 1.3862943649291992, 'In a nutshell, the idea is to linearize the graph using random walks and to compute the embedding of the central segment of each walk using the skip-thought criterion.': 1.3862941265106201, 'Being not an expert in biology, I can not comment whether or not this makes sense, but the gains reported in Table 2 are quite significant.': 1.3862942457199097, 'An anonymous public comment compared this work to a number of others in which the problem of learning representations of nodes is considered.': 1.3862941265106201, 'While this is arguably a different goal, one natural baseline would be to pool these representations using mean- or max- pooling.': 1.3862943649291992, 'It would very interesting to do such a comparison, especially given that the considered approach heavily relies on pooling (see Figure 3(c))': 1.3862943649291992, 'To sum up, I think it is a nice paper, and with more baselines I would be ready to further increase the numerical score.': 1.3862942457199097}"
79,https://openreview.net/forum?id=BkUDvt5gg,"{'There have been numerous works on learning from raw waveforms and training letter-based CTC networks for speech recognition, however, there are very few works on combining both of them with purely ConvNet as it is done in this paper.': 0.6931471824645996, 'It is interesting to see results on a large scale corpus such as Librispeech that is used in this paper, though some baseline results from hybrid NN/HMM systems should be provided.': 0.6931471228599548, 'To readers, it is unclear how this system is close to state-of-the-art only from Table 2.': 0.6931464672088623, 'The key contribution of this paper may be the end-to-end sequence training criterion for their CTC variant (where the blank symbol is dropped), which may be viewed as sequence training of CTC as H. Sak, et al.': 0.6931471824645996, '""Learning acoustic frame labeling for speech recognition with recurrent neural networks"", 2015.': 0.6931471824645996, 'However, instead of generating the denominator lattices using a frame-level trained CTC model first, this paper directly compute the sequence-level loss by considering all the competing hypothesis in the normalizer.': 0.6931471824645996, 'Therefore, the model is trained end-to-end.': 0.6929613947868347, ""From this perspective, it is closely related to D. Povey's LF-MMI for sequence-training of HMMs."": 0.6930626034736633, 'As another reviewer has pointed out, references and discussions on that should be provided.': 0.6931260228157043, ""This approach should be more expensive than frame-level training of CTCs, however, from Table 1, the authors' implementation is much faster."": 0.6931471824645996, 'Did the systems there use the same sampling rate?': 0.6931437253952026, 'You said at the end of 2.2 that the step size for your model is 20ms.': 0.5275932550430298, ""Is it also the same for Baidu's CTC system."": 0.10242550075054169, 'Also, have you tried increasing the step size, e.g. to 30ms or 40ms, as people have found that it may work (equally) better, while significantly cut down the computational cost.': 0.6931471228599548, 'This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped.': 0.6931470036506653, 'Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting.': 0.693139910697937, 'The approach is evaluated on the LibriSpeech task.': 0.6464639902114868, 'The authors claim that their approach is competitive.': 0.6931471824645996, 'They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing.': 0.6931471824645996, 'Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see.': 0.6931471824645996, ""I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea."": 0.6931078433990479, ""As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations."": 0.6930453777313232, 'You are using quite a huge analysis window (nearly 2s).': 0.6931471824645996, 'Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting.': 0.6931471824645996, 'The submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable.': 0.6927607655525208, 'Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption.': 0.3372354805469513, 'Prior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced.': 0.6765615344047546, 'What do you mean by transition ""scalars""?': 0.6931471824645996, 'I do not repeat further comments here, which were already given in the pre-review period.': 0.6931471824645996, 'Minor comments:': 0.6931471824645996, '- Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly': 0.6931471824645996, 'End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check)': 0.6931471824645996, '- Sec. 2.3: Bayse -> Bayes': 0.6931471824645996, '- definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper).': 0.6931471824645996, '- line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural)': 0.6931471824645996, '- Sec. 2.4, first line: threholding -> thresholding (spell check..)': 0.6931471824645996, '- Figure 4: mention the corpus used here - dev?': 0.6931471824645996}"
80,https://openreview.net/forum?id=BkV4VS9ll,"{'The authors have put forward a sincere effort to investigate the ""fundamental nature of learning representations in neural networks"", a topic of great interest and importance to our field.': 1.0986123085021973, 'They propose to do this via a few simplistic pruning algorithms, to essentially monitor performance decay as a function of unit pruning.': 1.0986123085021973, ""This is an interesting idea and one that could potentially be instructive, though in total I don't think that has been achieved here."": 1.0986123085021973, 'First, I find the introduction of pruning lengthy and not particularly novel or surprising.': 1.0986123085021973, 'For example, Fig 1 is not necessary, nor is most of the preamble section 3.3.0.': 1.0986117124557495, 'The pruning algorithms themselves are sensible (though overly simplistic) approaches, which of course would not matter if they were effective in addressing the question.': 1.0986121892929077, 'However, in looking for contributions this paper makes, an interesting, pithy, or novel take on pruning is not one of them, in my opinion.': 1.0986121892929077, 'Second, and most relevant to my overall rating, Section 4 does not get deeper than scratching the surface.': 1.0986123085021973, 'The figures do not offer much beyond the expected decay in performance as a percentage of neurons removed or gain value.': 1.0986123085021973, 'The experiments themselves are not particularly deep, covering a toy problem and MNIST, which does not convince me that I can draw lessons to the broader story of neural networks more generally.': 1.0986123085021973, 'Third, there is no essential algorithmic, architectural, or mathematical insight, which I expect out of all but the most heavily experimental papers.': 1.0986123085021973, 'I did enjoy reading some of the introductions and background, in particular that of reminding readers of popular papers from the late 1980s and early 1990s.': 1.0986123085021973, 'The idea of the proposal is straight forward: remove neurons based on the estimated change in the loss function from the packpropagation estimate with either first or second order backpropagation.': 1.0986123085021973, 'The results are as expected that the first order method is worse then the second order method which in turn is worse than the brute force method.': 1.0986104011535645, 'However, there are many reasons why I think that this work is not appropriate for ICLR.': 1.0859521627426147, 'For one, there is now a much stronger comprehension of weight decay algorithms and their relation to Bayesian priors which has not been mentioned at all.': 1.0986123085021973, 'I would think that any work in this regime would require at least some comments about this.': 1.0986123085021973, 'Furthermore, there are many statements in the text that are not necessarily true, in particular in light of deep networks with modern regularization methods.': 1.0986123085021973, 'For example, the authors state that the most accurate method is what they call brute-force.': 1.0986123085021973, 'However, this assumes that the effects of each neurons are independent which might not be the case.': 1.0986121892929077, 'So the serial order of removal is not necessarily the best.': 1.0986121892929077, 'I also still think that this paper is unnecessarily long and the idea and the results could have been delivered in a much compressed way.': 1.0986123085021973, 'I also don’t think just writing a Q&A section is not enough, and the points should be included in the paper.': 1.0985777378082275, 'The paper introduces a new pruning method for neural networks based on the second-order Taylor expansion and compares the results against a first-order method and brute-force pruning.': 1.0666699409484863, 'It performs experiments of the three methods on several toy examples - including a two-layer network on MNIST - and shows that the second-order method behaves much worse then the brute-force baseline.': 1.0975738763809204, 'In addition, from the success of the brute-force pruning the authors conclude that the hypothesis of Mozer et al - that neurons either contribute to performance or cancel out the effect of other neurons - is probably correct.': 1.0960520505905151, 'The authors put in considerable effort to explain all details of the paper clearly and at length, so the content of the paper is accessible even to people novel to pruning methods.': 0.46666139364242554, 'Additionally, the authors have very carefully answered all questions that were coming up through the pre-review and have been very responsive.': 0.4576784372329712, 'My major criticism is that the paper lacks focus, does not have a concrete conclusion and does not explain what it adds to the literature.': 1.060633659362793, 'To make this apparent, I here summarise each paragraph of the conclusion section:': 1.0737751722335815, 'Paragraph 1: We do not benchmark / Pruning methods do not fare well against brute-force baseline /': 1.0984618663787842, 'Some evidence for hypothesis of Mozer & Smolensky, but further investigation needed': 0.8350248336791992, 'Paragraph 2: Introduced 2nd order Taylor method / Does not fare well against baseline': 1.0983128547668457, 'Paragraph 3: Re-training may help but is not fair': 1.0980528593063354, 'Paragraph 4: Brute-force can prune 40-70% in shallow networks': 1.0898452997207642, 'Paragraph 5: Brute-force less effective in deep networks': 1.0986123085021973, 'Paragraph 6: Not all neurons contribute equally to performance of network': 1.0986123085021973, 'The title of the paper and answers of the authors to the pre-review questions seemed to strongly suggest that the paper is not about the new second-order method, is not about benchmarking pruning algorithms but is instead about the learnt representations.': 1.0986123085021973, 'But only two or three sentences in the conclusion, and no sentence in the part on results in the abstract, even refers to neural representations.': 1.0986123085021973, 'In an answer to the pre-review questions the authors stated:': 1.0986123085021973, '> Furthermore, we do not have to accept the conclusion that re-training is a necessary part of pruning because a brute force search reveals that neurons can in fact be': 1.0725737810134888, '> pruned from trained networks in a piecemeal fashion with no retraining and minimal adverse effect on the overall performance of the network.': 1.0986123085021973, 'This would be': 1.0986123085021973, '> impossible if neurons did not belong to the distinct classes we describe.""': 1.0986123085021973, 'But this can already be concluded from the 2nd order method, which has a similar characteristic and is based on other 2nd order methods (not shown here).': 0.9360974431037903, 'What is the motivation to introduce a new 2nd order method here?': 0.6329643130302429, 'In addition, some other minor conclusions about representations - in particular the cancellation effect - might be based on side-effects of the greedy serial pruning method.': 0.45136794447898865, 'Optimally, one would need to consider all the different ways of pruning (which, of course, scales exponentially with the number of neurons and is computationally infeasible).': 0.4325181841850281, 'Notably, the authors do consider this limitation in the context of conventional pruning methods in the conclusions: ""Third, we assumed that pruning could be done in a serial fashion': 0.1309622824192047, '[...].': 0.9971098899841309, 'We found that all of these assumptions are deeply flawed in the sense that the true relevance of a neuron can only be partially approximated [...] at certain stages of the pruning process"".': 1.015510082244873, 'But the brute-force pruning process is also serial - why is that not a problem?': 0.22541944682598114, 'All in all it is unclear to me what the paper adds: there are little conclusions regarding the learnt representations nor is there sufficient benchmarking against state-of-the-art pruning methods.': 0.7422636151313782, 'I would suggest to focus the paper in the following way: first, use a state-of-the-art pruning method from the literature (that works without re-training) or do not use any other pruning methods besides brute-force (depending on whether you want to compare pruning methods against brute-force, or want to learn something about the learnt representations).': 1.039965271949768, 'In this way you need to write little about this second-order tuning methods, and readers are not so easily confused about the purpose of this paper (plus it will be considerably shorter!).': 0.8065326809883118, 'Then concentrate on 2-layer MNIST and a deeper CIFAR10 network.': 0.8618206977844238, 'Further focus the paper by adding an itemised list of the exact contributions that you make, and streamline the paper accordingly.': 0.7147447466850281, 'These measures could strongly boost the impact of your work but will require a major revision.': 1.0646641254425049, 'PS: I think the confusion starts with the following sentence in the abstract: ""In this work we set out to test several long-held hypothesis about neural network learning representations and numerical approaches to pruning.""': 0.8815625905990601, 'Both aspects are pretty orthogonal, but are completely mixed up in the paper.': 0.6762304902076721}"
81,https://openreview.net/forum?id=BkVsEMYel,"{'This paper investigates the fact why deep networks perform well in practice and how modifying the geometry of pooling can make the polynomially': 0.6076958775520325, 'sized deep network to provide a function with exponentially high separation rank (for certain partitioning.)': 1.0986080169677734, ""In the authors' previous works, they showed the superiority of deep networks over shallows when the activation function is ReLu and the pooling is max/mean pooling but in the current paper there is no activation function after conv and the pooling is just a multiplication of the node values."": 1.098575472831726, ""Although for the experimental results they've considered both scenarios."": 1.078028917312622, 'Actually, the general reasoning for this problem is hard, therefore, this drawback is not significant and the current contribution adds a reasonable amount of knowledge to the literature.': 0.5506472587585449, 'This paper studies the convolutional arithmetic circuits and shows how this model can address the inductive biases and how pooling can adjust these biases.': 0.8641878366470337, 'This interesting contribution gives an intuition about how deep network can capture the correlation between the input variables when its size is polynomial but and correlation is exponential.': 0.39700520038604736, 'It worth to note that although the authors tried to express their notation and definitions carefully where they were very successful, it would be helpful if they elaborate a bit more on their definitions, expressions, and conclusions in the sense to make them more accessible.': 1.098507285118103, 'The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network.': 1.0978283882141113, 'As in most attempts  in this direction in the literature, the ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match the type of CNNs used in practice: for instance the Relu non-linearity is replaced with a product of linear functions (or a sum of logs).': 1.0986123085021973, 'While the paper is very technical to read, every concept is clearly stated and mathematical terminology properly introduced.': 1.0986123085021973, 'Still, I think some the authors could make some effort to make the key concepts more accessible, and give a more intuitive understanding of what the separation rank means rather before piling up different mathematical interpretation.': 1.0986123085021973, 'My SVM-era algebra is quite rusted, and I am not familiar with the separation rank framework: it would have been much easier for me to first fully understand a simple and gentle case (shallow network in section 5.3), than the general deep case.': 0.4333331882953644, 'To summarize my understanding of the key theorem 1 result:': 1.0986123085021973, 'The upper bound of the separation rank is used to show that in the shallow case, this rank grows AT MOST linearly with the network size (as measured by the only hidden layer).': 1.0915627479553223, 'So exponential network sizes are caused by this rank needing to grow exponentially, as required by the partition.': 1.0986121892929077, 'In the deep case, one also uses the case that the upper bound is linear in the size of the network (as measured by the last hidden layer), however, this situation is caused by the selection of a partition (I^low, J^high), and the maximal rank induced by this partition is only linear anyway, hence the network size can remain linear.': 1.0975160598754883, 'If tried my best to summarize the key point of this paper and still probably failed at it, which shows how complex is this notion of partition rank, and that its linear growth with network size can either be a good or bad thing depending on the setting.': 1.097810983657837, 'Hopefully, someone will come one day with an explanation that holds in a single slide.': 1.0986123085021973, 'While this is worth publishing as conference paper in its present form, I have two suggestions that, IMHO, would make this work more significant:': 1.0986123085021973, 'On the theory side, we are still very far from the completeness of the PAC bound papers of the ""shallow era"".': 1.0986123085021973, 'In particular, the non-probabilistic lower and upper bound in theorem 1 are probably loose, and there is no PAC-like theory to tell us which one to use and what is the predicted impact on performance (not just the intuition).': 1.0978612899780273, 'Also, in the prediction of the inductive bias, the other half is missing.': 1.0986123085021973, 'This paper attempts to predict the maximal representation capacity of a DNN under bounded network size constraints, but one of the reason why this size has to be bounded is overfitting (justified by PAC or VC-dim like bounds).': 1.0985965728759766, 'If we consider the expected risk as  the sum of the empirical risk and the structural risk, this paper only seems to address fully the empirical risk minimization part, freezing the structural risk.': 0.9983379244804382, 'On the practice side, an issue is that experiments in this paper mostly confirm what is obvious through intuition, or some simpler form of reasonings.': 1.0842903852462769, 'For instance to use convolutions that join pixels which are symmetrical in images to detect symmetry.': 1.0607194900512695, 'Basic hand-crafted pattern detectors, as they have been used in computer vision for decades, would just do the job.': 1.0985844135284424, ""What would be a great motivation for using this framework is if it answered questions that simple human intuition cannot, and for which we are still in the dark: one example I could think of in the recent use of gated convolutions 'a trous' for 1D speech signal, popularized in Google WaveNet (https://deepmind.com/blog/wavenet-generative-model-raw-audio/)."": 1.0985924005508423, 'Note that 1D inputs would also simplify the notation!': 1.09861159324646, 'This paper addresses the question of which functions are well suited to deep networks, as opposed to shallow networks.': 1.098609209060669, 'The basic intuition is convincing and fairly straightforward.': 1.0966854095458984, 'Pooling operations bring together information.': 1.0986123085021973, 'When information is correlated, it can be more efficiently used if the geometry of pooling regions matches the correlations so that it can be brought together more efficiently.': 1.0986123085021973, 'Shallow networks without layers of localized pooling lack this mechanism to combine correlated information efficiently.': 1.0986123085021973, 'The theoretical results are focused on convolutional arithmetic circuits, building on prior theoretical results of the authors.': 1.0978327989578247, 'The results make use of the interesting technical notion of separability, which in some sense measures the degree to which a function can be represented as the composition of independent functions.': 0.5141626596450806, 'Because separability is measured relative to a partition of the input, it is an appropriate mechanism for measuring the complexity of functions relative to a particular geometry of pooling operations.': 1.094114065170288, 'Many of the technical notions are pretty intuitive, although the tensor analysis is pretty terse and not easy to follow without knowledge of the authors’ prior work.': 1.097853660583496, 'In some sense the comparison between deep and shallow networks is somewhat misleading, since the shallow networks lack a hierarchical pooling structure.': 1.0986123085021973, 'For example, a shallow convolutional network with RELU and max pooling does not really make sense, since the max occurs over the whole image.': 1.098563551902771, 'So it seems that the paper is really more of an analysis of the effect of pooling vs. not having pooling.': 1.0986123085021973, 'For example, it is not clear that a deep CNN without pooling would be any more efficient than a shallow network, from this work.': 1.0986123085021973, 'It is not clear how much the theoretical results depend on the use of a model with product pooling, and how they might be extended to the more common max pooling.': 1.0985742807388306, 'Even if theoretical results are difficult to derive in this case, simple illustrative examples might be helpful.': 1.0986123085021973, 'In fact, if the authors prepare a longer version of the paper for a journal I think the results could be made more intuitive if they could add a simple toy example of a function that can be efficiently represented with a convolutional arithmetic circuit when the pooling structure fits the correlations, and perhaps showing also how this could be represented with a convolutional network with RELU and max pooling.': 1.0985761880874634, 'I would also appreciate a more explicit discussion of how the depth of a deep network affects the separability of functions that can be represented.': 1.0261443853378296, 'A shallow network doesn’t have local pooling, so the difference between deep and shallow if perhaps mostly one of pooling vs. not pooling.': 1.0659199953079224, 'However, practitioners find that very deep networks seem to be more effective than “deep” networks with only a few convolutional layers and pooling.': 1.0058152675628662, 'The paper does not explicitly discuss whether their results provide insight into this behavior.': 1.0980098247528076, 'Overall, I think that the paper attacks an important problem in an interesting way.': 0.633545458316803, 'It is not so convincing that this really gets to the heart of why depth is so important, because of the theoretical limitation to arithmetic circuits, and because the comparison is to shallow networks that are without localized pooling.': 0.4940466284751892}"
82,https://openreview.net/forum?id=BkXMikqxx,"{'This paper explores the use of Open Bigrams as a target representation of words, for application to handwriting image recognition.': 1.09522545337677, 'Pros:': 1.0986123085021973, 'The use of OBs is novel and interesting.': 1.0742590427398682, 'Clearly written and explained.': 1.0983046293258667, 'Cons:': 1.0986123085021973, 'No comparison to previous state of the art, only with author-generated results.': 1.0941885709762573, 'More ablation studies needed': 1.0986123085021973, ""i.e. fill in Table3 with rnn0,1 rnn0,1,2 rnn0,1' etc etc."": 0.45389941334724426, ""It is not clear where the performance is coming from, as it seems that it is single character modelling (0) and word endings (') that are actually beneficial."": 1.09861159324646, 'While the use of Open bigrams is novel, there are works which use bag of bigrams and ngrams as models which are not really compared to or explored.': 1.0969185829162598, 'E.g. https://arxiv.org/abs/1406.2227': 1.098606824874878, '[1] and https://arxiv.org/abs/1412.5903 [2].': 1.0985866785049438, 'Both use bag of ngrams models and achieve state of the art results, so it would be interesting to see whether open bigrams in the same experimental setup as [1] would yield better results.': 1.0983974933624268, 'Why not use a graph-based decoder like in Fig 2 b?': 1.0986123085021973, 'Overall an interesting paper but the lack of comparisons and benchmarks makes it difficult to assess the reality of the contributions.': 1.0986114740371704, 'This paper uses an LSTM model to predict what it calls ""open bigrams"" (bigrams of characters that may or may not have letters inbetween) from handwriting data.': 0.539338231086731, 'These open bigrams are subsequently used to predict the written word in a decoding step.': 1.0906591415405273, 'The experiments indicate that the system does slightly better than a baseline model that uses Viterbi decoding.': 1.0698137283325195, 'I have some major concerns about this paper:': 1.0986089706420898, 'I find the ""cortical inspired"" claim troublesome.': 0.9135253429412842, 'If anything, it is psychology/cognitive science inspired, in the sense that open bigrams appear to help for word recognition (Touzet et al. 2014).': 0.9501650929450989, 'But the implied cortical characteristics, implicitly referred to e.g. by pointing to analogies between deep neural nets for object recognition and in that case the visual cortex, is unfounded.': 0.10374074429273605, 'Is there any direct evidence from neuroscience that open-bigrams constitute a wholly separate layer in the cortex for a handwriting recognition task?': 0.9626813530921936, 'Dehaene\'s work is a proposal, so you\'ll need to describe more ""findings in cognitive neurosciences [sic] research on reading"" (p. 8) to substantiate those claims.': 1.0985673666000366, 'I am further worried by the fact that the authors seem to think that ""deep neural networks are based on a series of about five pairs of neurons [sic] layers"".': 0.46368733048439026, ""Unless I misunderstand something, you are specifically referring to Krizhevsky's AlexNet here (which you should probably have cited there)?"": 0.4994739592075348, ""I hope you don't mean to imply that all deep neural nets need five layers."": 1.0986101627349854, 'It is also not true that ten is ""quite close to the number of layers of an efficient deep NN""': 1.0986121892929077, 'what network?': 1.0986123085021973, 'what task?': 1.0986123085021973, 'etc.': 1.0986123085021973, 'The model is not clearly explained.': 1.0986123085021973, 'There is a short paragraph in Appendix A.3.': 1.0986123085021973, 'that roughly describes the setup, but this does not include e.g. the objective function, or answer why the network output is only considered each two consecutive time steps, rather than at each time step (or so it seems?).': 1.0986123085021973, 'This is probably because the paper argues that it ""is focused on the decoder"" (p. 6), rather than on the whole problem.': 1.0986123085021973, ""I find this problematic, because in that case we're effectively measuring how easy it is to reconstruct a word from its open bigrams, which has very little to do with handwriting recognition (it could have been evaluated on any text corpus)."": 1.0986123085021973, 'In fact, as the example on page 4 shows, handwriting is not necessary to illustrate the open bigram hypothesis.': 1.0986123085021973, 'Which leads me to wonder why these particular tasks were chosen, if we are only interested in the decoding mechanism?': 1.0986123085021973, 'The comparison is not really fair.': 1.0986123085021973, 'The Viterbi decoder only has access to unigrams, as far as I can tell.': 1.0986123085021973, 'The only model that does better than that baseline has access to a lot more information, and does not do that much better.': 1.0986123085021973, 'Did the Viterbi model have access to the word boundary information (at one point rather confusingly called ""extremities"") that pushed the open bigram model over the edge in terms of performance?': 1.0986123085021973, ""Why is there no comparison to e.g. rnn_0,1' (unigram+bigram+boundary markers)?"": 1.0986123085021973, 'The dataset also appears to be biased in favor of the proposed approach (longer words, only ).': 1.0986123085021973, 'I am not convinced that this paper really shows that open bigrams help.': 1.0986123085021973, 'I very much like the idea of the paper, but I am simply not convinced by its claims.': 1.0523951053619385, 'Minor points:': 1.098568081855774, 'There are quite a few typos.': 1.098611831665039, 'Just a sample: ""independant"" (Fig.1), ""we evaluate an handwritten"", "", hand written words [..], an the results"", ""their approach include"", ""the letter bigrams of a word w is"", ""for the two considered database""': 1.0978432893753052, ""Wouldn't it be easy to add how many times a bigram occurs, which would improve the decoding process?"": 1.0984007120132446, 'You can just normalize over the full counts instead of the binary occurrence counts.': 0.517198383808136, 'The results in Table 5 are the same (but different precision) as the results in Table 2, except that edit distance and SER are added, this is confusing.': 0.8461991548538208, 'This submission investigates the usability of cortical-inspired distant bigram representations for handwritten word recognition.': 1.0986123085021973, 'Instead of generating neural network based posterior features for character (optionally in local context), sets posterior for character bigrams of different length are used to represent words.': 1.0986123085021973, 'The aim here is to investigate the viability of this approach and to compare to the standard approach.': 1.097222924232483, 'Overall, the submission is well written, although information is missing w.r.t.': 1.098319172859192, 'to the comparison between the proposed approach and the standard approach, see below.': 1.0986123085021973, 'It would be desirable to see the model complexity of all the different models used here, i.e. the number of parameters used.': 1.0986076593399048, 'Language models are not used here.': 1.0986123085021973, 'Since the different models utilize different levels of context, language models can be expected to have a different effect on the different approaches.': 1.0986123085021973, 'Therefore I suggest to include the use of language models into the evaluation.': 1.0986123085021973, 'For your comparative experiments you use only 70% of the data by choosing longer words only.': 1.0986123085021973, 'On the other hand, it is well known that the shorter words are more prone to result in misrecognitions.': 1.0986123085021973, 'The question remains, if this choice is advantageous for one of the tasks, or not - corresponding quantitative results should be provided to be able to better evaluate the effect of using this constrained corpus.': 1.0986123085021973, 'Without clarification of this I would not readily agree that the error rates are competitive or better than the standard approach, as stated at the end of Sec. 5.': 1.0986123085021973, 'I do see the motivation for introducing open-bigrams in an unordered way due to the corresponding evidence from cognitive research.': 1.0986123085021973, 'However, decision theoretically I wonder, why the order should be given up, if the underlying sequential classification problem clearly is of a monotonous nature.': 1.0986123085021973, 'It would be interesting to see an experiment, where only the use of the order is varied, to differentiate the effect of the order from the effect of other aspects of the approach.': 1.098597526550293, 'End of page 1: ""whole language method"" - please explain what is meant by this.': 1.0982271432876587, 'Page 6: define your notation for rnn_d(x,t).': 1.0986123085021973, 'The number of target for the RNNs modeling order 0 (unigrams effectively) and the RNNs modeling order 1 and larger are very much different.': 1.0985502004623413, 'Therefore the precision and recall numbers in Table 2 do not seem to be readily comparable between order 0 and orders >=1.': 1.0986119508743286, 'At least, the column for order 0 should be visually separated to highlight this.': 1.098611831665039, 'Minor comments: a spell check is recommended': 1.0673375129699707, 'p. 2: state-of-art -> state-of-the-art': 1.096516489982605, 'p. 2: predict character sequence -> predict a character sequence': 1.0827542543411255, 'p. 3, top: Their approach include -> Their approach includes': 1.0949925184249878, 'p. 3, top: an handwritten -> a handwritten': 1.094325304031372, 'p. 3, bottom: consituent -> constituent': 1.0975210666656494, 'p. 4, top: in classical approach -> in the classical approach': 1.096472144126892, 'p. 4, top: transformed in a vector -> transformed into a vector': 1.0822020769119263, 'p. 5: were build -> were built': 1.0986123085021973, 'References: first authors name written wrongly: Thodore Bluche -> Theodore Bluche': 1.0986123085021973}"
83,https://openreview.net/forum?id=Bk_zTU5eg,"{'This paper theoretically justified a faster convergence (in terms of average gradient norm attained after processing a fixed number of samples) of using small mini-batches for SGD or ASGD with smaller number of learners.': 1.098611831665039, 'This indicates that there is an inherent inefficiency in the speed-up obtained with parallelizing gradient descent methods by taking advantage of hardware.': 1.0986123085021973, 'This paper looks good overall and makes some connection between algorithm design and hardware properties.': 1.0985450744628906, 'My main concern is that Lemma 1 looks incorrect to me.': 1.0986123085021973, 'The factor D_f / S should be D_f/ (S*M) for me.': 1.098604440689087, 'Please clarify this and check the subsequent theorem.': 1.0986123085021973, 'This paper shows that when a larger mini-batch is used (in the serial setting), the number of samples needed to be processed for the same convergence guarantee is larger.': 1.0986123085021973, 'A similar behavior is discussed for using multiple learners in asynchronous SGD.': 1.097415566444397, 'This behavior has been known in convex optimization (e.g., ""Better Mini-Batch Algorithms via Accelerated Gradient Methods"", NIPS 2011).': 1.0955119132995605, 'There, the convergence is of the form O(1/\\sqrt{bT}+1/T), and so using bT samples only lead to \\sqrt{b} time improvement.': 1.0986121892929077, 'This paper extends a similar result to the nonconvex case (but the underlying mathematics is mainly from [Ghadimi & Lan, 2013]).': 1.0986123085021973, 'However, this behavior is also known and indeed has been summarized in the deep learning textbook (chapter 8).': 1.0986123085021973, 'Hence, the novelty is limited.': 0.3684248626232147, 'The theoretical results in this paper suggest that it is best not to use mini-batch.': 1.0984351634979248, 'However, using mini-batch is often beneficial in practice.': 1.0982781648635864, 'As discussed in the deep learning textbook, using mini-batch allows using a larger learning rate (note that this paper assumes the same learning rate for both mini-batch sizes).': 1.0986123085021973, 'Moreover, multicore architectures allows parallel execution of mini-batch almost for free.': 0.7981882095336914, 'Hence, the practical significance of the results is also limited.': 0.958789050579071, 'Other comments:': 1.0986123085021973, ""Equation (4): since the same number of samples (S) is processed and S=MK, where M is the mini-batch size and K is the number of mini-batches processed (as mentioned in the first paragraph of section 2), when two different mini-batch sizes are considered (M_l and M_h), their K's should differ."": 1.0985296964645386, 'However, the same K is used on the LHS of (4).': 1.0517473220825195, 'Figures 1 and 2: As convergence speed is of main interest, why not show the training objective instead?': 1.098602294921875, 'This paper addresses the problem of the influence of mini-batch size on the SGD convergence in a general non-convex setting.': 1.0986123085021973, 'The results are then translated to analyze the influence of the number of learners on ASGD.': 1.098573923110962, 'I find the problem addressed in the paper relevant and the theoretical part clearly written.': 1.098608374595642, 'The experimental evaluation is somehow limited though.': 1.0984091758728027, 'I would like to see experiments on more data sets and more architectures, as well as richer evaluation, e.g. N=16 is a fairly small experiment.': 1.0984834432601929, 'It would also enhance the paper if the experiments were showing a similar behavior of other popular methods like momentum SGD or maybe EASGD (the latter in distributed setting).': 1.0986121892929077, 'I understand the last evaluation does not directly lie in the scope of the paper, though adding these few experiments do not require much additional work and should be done.': 1.0986123085021973}"
84,https://openreview.net/forum?id=Bkab5dqxe,"{'Summary': 1.0986123085021973, '===': 1.0986123085021973, 'This paper proposes the Neural Physics Engine (NPE), a network architecture': 1.0986123085021973, 'which simulates object interactions.': 1.0986123085021973, 'While NPE decides to explicitly represent': 1.0986123085021973, 'objects (rather than video frames), it incorporates knowledge of physics': 1.0986123085021973, 'almost exclusively through training data.': 1.0986123085021973, 'It is tested in a toy domain with': 1.0986123085021973, 'bouncing 2d balls.': 1.0986123085021973, 'The proposed architecture processes each object in a scene one at a time.': 1.0986123085021973, 'Pairs of objects are embedded in a common space where the effect of the': 1.0986123085021973, 'objects on each other can be represented.': 1.0986123085021973, 'These embeddings are summed': 1.0986123085021973, ""and combined with the focus object's state to predict the focus object's"": 1.0986123085021973, 'change in velocity.': 1.0986123085021973, 'Alternative baselines are presented which either': 1.0986123085021973, 'forego the pairwise embedding for a single object embedding or': 1.0986123085021973, ""encode a focus object's neighbors in a sequence of LSTM states."": 1.0986123085021973, 'NPE outperforms the baselines dramatically, showing the importance of': 1.0986123085021973, 'architecture choices in learning to do this object based simulation.': 1.0986123085021973, 'The model is tested in multiple ways.': 1.0986123085021973, 'Ability to predict object trajectory': 1.0986123085021973, 'over long time spans is measured.': 1.0986123085021973, 'Generalization to different numbers of objects': 1.0986123085021973, 'is measured.': 1.0986123085021973, 'Generalization to slightly altered environments (difference': 1.0986123085021973, 'shaped walls) is measured.': 1.0986123085021973, 'Finally, the NPE is also trained to predict': 1.0986123085021973, 'object mass using only interactions with other objects, where it also': 1.0986123085021973, 'outperforms baselines.': 1.0986123085021973, 'Comments': 1.0986123085021973, '* I have one more clarifying question.': 1.0986123085021973, 'Are the inputs to the blue box in': 1.0761675834655762, 'figure 3 (b)/(c) the concatenation of the summed embeddings and state vector': 0.10833863168954849, 'of object 3?': 0.9696797728538513, 'Or is the input to the blue module some other combination of the': 0.448529452085495, 'two vectors?': 1.0986123085021973, '* Section 2.1 begins with ""First, because physics does not': 0.5821859836578369, 'change across inertial frames, it suffices to separately predict the future state of each object conditioned': 1.0986123085021973, 'on the past states of itself and the other objects in its neighborhood, similar to Fragkiadaki': 1.0986123085021973, 'et al. (2015).""': 1.0986123085021973, 'I think this is an argument to forego the visual representation used by previous': 1.0986123085021973, 'work in favor of an object only representation.': 1.0986123085021973, 'This would be more clear if there': 1.0986123085021973, 'were contrast with a visual representation.': 1.0986123085021973, '*': 1.0986123085021973, 'As addressed in the paper, this approach is novel, though less so after taking': 1.0986123085021973, 'into consideration the concurrent work of Battaglia et.': 1.0986123085021973, 'al. in NIPS 2016 titled': 1.0986123085021973, '""Interaction Networks for Learning about Objects, Relations and Physics.""': 1.0986123085021973, 'This work offers a different network architecture and set of experiments, as': 1.0986123085021973, 'well as great presentation, but the use of an object based representation': 1.0986123085021973, 'for learning to predict physical behavior is shared.': 1.0986123085021973, 'Overall Evaluation': 1.0986123085021973, 'This paper was a pleasure to read and provided many experiments that offered': 1.0986123085021973, 'clear and interesting conclusions.': 1.0986123085021973, 'It offers a novel approach (though': 1.0986123085021973, 'less so compared to the concurrent work of Battaglia et.': 1.0986123085021973, 'al. 2016)': 1.0986123085021973, 'which': 1.0986123085021973, 'represents a significant step forward in the current investigation of': 1.0986123085021973, 'intuitive physics.': 1.0986123085021973, 'Paper proposes a neural physics engine (NPE).': 1.0986123085021973, 'NPE provides a factorization of physical scene into composable object-based representations.': 0.6199402213096619, 'NPE predicts a future state of the given object as a function composition of the pairwise interactions between itself and near-by objects.': 1.0986123085021973, 'This has a nice physical interpretation of forces being additive.': 1.0986123085021973, 'In the paper NPE is investigated in the context of 2D worlds with balls and obstacles.': 1.0986089706420898, 'Overall the approach is interesting and has an interesting flavor of combining neural networks with basic properties of physics.': 1.0986123085021973, 'Overall, it seems like it may lead to interesting and significant follow up work in the field.': 1.0986123085021973, 'The concerns with the paper is mainly with evaluation, which in places appears to be weak (see below).': 1.0986123085021973, '> Significance & Originality:': 1.0986117124557495, 'The approach is interesting.': 1.0986123085021973, 'While other methods have tried to build models that can deal with physical predictions, the idea of summing over pair-wise terms, to the best of my knowledge, is novel and much more in-line with the underlying principles of mechanics.': 1.0986123085021973, 'As such, while relatively simple, it seems to be an important contribution.': 1.0986120700836182, '> Clarity:': 1.0986123085021973, 'The paper is generally well written.': 1.0985867977142334, 'However, large portion of the early introduction is rather abstract and it is difficult to parse until one gets to 5th paragraph.': 1.0986123085021973, 'I would suggest editing the early part of introduction to include more specifics about the approach or even examples ... to make text more tangible.': 1.0986123085021973, '> Experiments': 1.0986123085021973, 'Generally there are two issues with experiments in my opinion: (1) the added indirect comparison with Fragkiadaki et al (2015) does not appears to be quantitatively flattering with respect to the proposed approach, and (2) quantitative experiments on the role the size of the mask has on performance should really be added.': 1.0986123085021973, 'Authors mention that they observe that mask is helpful, but it is not clear how helpful or how sensitive the overall performance is to this parameter.': 1.0986123085021973, 'This experiment should really be added.': 1.0986123085021973, 'I do feel that despite few mentioned shortcomings that would make the paper stronger, this is an interesting paper and should be published.': 1.0986123085021973, '- summary': 1.0986123085021973, 'The paper proposes a differntiable Neural Physics Engine (NPE).': 1.098413109779358, 'The NPE consists of an encoder and a decoder function.': 0.9070239067077637, 'The NPE takes as input the state of pairs of objects (within a neighbourhood of a focus object) at two previous time-steps in a scene.': 1.0986121892929077, 'The encoder function summarizes the interaction of each pair of objects.': 1.0962085723876953, 'The decoder then outputs the change in velocity of the focus object at the next time step.': 0.023862212896347046, 'The NPE is evaluated on various environments containing bouncing balls.': 0.298737108707428, 'novelty': 1.0986123085021973, 'The differentiable NPE is a novel concept.': 0.4073163866996765, 'However, concurrently Battaglia et al. (NIPS 2016) proposes a very similar model.': 1.0986058712005615, 'Just as this work, Battaglia et al. (NIPS 2016) consider a model which consists of a encoder function (relation-centric) which encodes the interaction among a focus object and other objects in the scene and a decoder (relation-centric) function which considers the cumulative (encoded) effect of object interactions on the focus object and predicts effect of the interactions.': 1.0986089706420898, 'Aspects like only considering objects interactions within a neighbourhood (versus the complete object interaction graph in Battaglia et al.) based on euclideian distance  are novel to this work.': 1.0986121892929077, 'However, the advantages (if any) of NPE versus the model of Battaglia et al. are not clear.': 1.0985352993011475, 'Moreover, it is not clear how this neighbourhood thresholding scene would preform in case of n-ball systems, where gravitational forces of massive objects can be felt over large distances.': 1.0986123085021973, 'citations': 1.0986123085021973, 'This work includes all relevant citations.': 1.0986123085021973, 'clarity': 1.0985791683197021, 'The article is well written and easy to understand.': 1.0986123085021973, 'experiments': 1.0986121892929077, 'Battaglia et al. evaluates on wider variety senerios compared to this work (e.g. n-bodies under gravitation, falling strings).': 1.0986123085021973, 'Such experiments demonstrate the ability of the models to generalize.': 0.8160864114761353, 'However, this work does include more in-depth experiments in case of bouncing balls compared to Battaglia et al.': 1.098607063293457, '(e.g. mass estimation and varying world configurations with obstacles in the bouncing balls senerio).': 0.4809512197971344, 'Moreover, an extensive comparison to Fragkiadaki et al. (2015) (in the bouncing balls senerios) is missing.': 1.096544623374939, 'The authors (referring to answer to question 4) do point out to comaprable numbers in both works, but the experimental settings are different.': 1.0986123085021973, 'Comparison in a billiard table senerio like that Fragkiadaki et al. (2015) where a initial force is applied to a ball, would have been enlightening.': 1.0953744649887085, 'The authors only evaluate the error in velocity in the bouncing balls senerios.': 1.0986114740371704, 'We understand that this model predicts only the velocity (refer to answer of question 2).': 0.466016948223114, 'Error analysis also with respect to ground truth ball position would be more enlightening.': 1.0971380472183228, 'As small errors in velocity can quickly lead to entirely different scene configuration.': 0.9382205009460449, 'conclusion / recommendation': 1.0986123085021973, 'The main issue with this work is the unclear novelty with respect to work of Battaglia et al.': 1.098564863204956, ""at NIPS'16."": 1.0986123085021973, 'A quantitative and qualitative comparison with Battaglia et al. is lacking.': 0.7654832005500793, 'But the authors state that their work was developed independently.': 0.4708499610424042, 'Differentiable physics engines like NPE or that of Battaglia et al. (NIPS 2016) requires generation of an extensive amount of synthetic data to learn about the physics of a certain senerio.': 1.0949324369430542, 'Moreover, extensive retraining is required to adapt to new sceneries (e.g. bouncing balls to n-body systems).': 0.7814579606056213, 'Any practical advantage versus generating new code for a physics engine is not clear.': 1.045802116394043, 'Other ""bottom-up"" approaches like that of  Fragkiadaki et al. (2015) couple vision along with learning dynamics.': 0.4063226580619812, 'However, they require very few input parameters (position, mass, current velocity, world configuration), as approximate parameter estimation can be done from the visual component.': 0.7628156542778015, 'Such approaches could be potentially more useful of a robot in ""common-sense"" everyday tasks (e.g. manipulation).': 1.0320614576339722, 'Thus, overall potential applications of a differentiable physics engine like NPE is unclear.': 1.0984512567520142}"
85,https://openreview.net/forum?id=BkbY4psgg,"{'This paper argues that being able to handle recursion is very important for neural programming architectures — that handling recursion allows for strong generalization to out of domain test cases and learning from smaller amounts of training data.': 1.0986123085021973, 'Most of the paper is a riff on the Reed & de Freitas paper on Neural Programmer Interpreters from ICLR 2016 which learns from program traces — this paper trains NPI models on traces that have recursive calls.': 1.0986117124557495, 'The authors show how to verify correctness by evaluating the learned program on only a small set of base cases and reduction rules and impressively, show that the NPI architecture is able to perfectly infer Bubblesort and the Tower of Hanoi problems.': 1.0986123085021973, 'What I like is that the idea is super simple and as the authors even mention, the only change is to the execution traces that the training pipeline gets to see.': 1.0986123085021973, 'I’m actually not sure what the right take-away is — does this mean that we have effectively solved the neural programming problem when the execution traces are available?': 1.0986123085021973, '(and was the problem too easy to begin with?).': 1.0974938869476318, 'For example, a larger input domain (as one of the reviewers also mentions) is MNIST digits and we can imagine a problem where the NPI must infer how to sort MNIST digits from highest to lowest.': 1.0986123085021973, 'In this setting, having execution traces would effectively decouple the problem of recognizing the digits from that of inferring the program logic — and so the problem would be no harder than learning to recognize MNIST digits and learning to bubble sort from symbols.': 1.0986121892929077, 'What is a problem where we have access to execution traces but cannot infer it using the proposed method?': 1.0985358953475952, 'This is a very interesting and fairly easy to read paper.': 1.0985897779464722, 'The authors present a small, yet nifty approach to make Neural Programming Interpreters significantly more powerful.': 1.098608136177063, 'By allowing recursion, NPI generalizes better from fewer execution traces.': 1.0986123085021973, ""It's an interesting example of how a small but non-trivial extension can make a machine learning method significantly more practical."": 1.0986123085021973, 'I also appreciate that the same notation was used in this paper and the original Deepmind paper.': 1.0986123085021973, 'As a non-expert on this topic, it was easy to read the original paper in tandem.': 1.0986120700836182, 'My one point of critique is that the generalization proves are a bit vague.': 1.0986123085021973, 'For the numerical examples in the paper, you can iterate over all possible execution paths until the next recursive call.': 1.0986113548278809, 'However, how would this approach generalize a continuous input space (e.g. the 3D car example in the original paper).': 1.0986123085021973, 'It seems that a prove of generalization will still be intractable in the continuous case?': 1.0986123085021973, 'Are you planning on releasing the source code?': 1.0986123085021973, 'This paper improves significantly upon the original NPI work, showing that the model generalizes far better when trained on traces in recursive form.': 1.0986123085021973, 'The authors show better sample complexity and generalization results for addition and bubblesort programs, and add two new and more interesting tasks - topological sort and quicksort (added based on reviewer discussion).': 1.0986123085021973, 'Furthermore, they actually *prove* that the algorithms learned by the model generalize perfectly, which to my knowledge is the first time this has been done in neural program induction.': 1.0986123085021973}"
86,https://openreview.net/forum?id=Bkbc-Vqeg,"{'This paper is a follow-up on the NIPS 2016 paper ""Unsupervised learning of spoken language with visual context"", and does exactly what that paper proposes in its future work section: ""to perform acoustic segmentation and clustering, effectively learning a lexicon of word-like units"" using the embeddings that their system learns.': 1.0986123085021973, 'The analysis is very interesting and I really like where the authors are going with this.': 1.0986123085021973, 'My main concern is novelty.': 1.09773588180542, 'It feels like this work is a rather trivial follow-up on an existing model, which is fine, but then the analysis should be more satisfying: currently, it feels like the authors are just illustrating some of the things that the NIPS model (with some minor improvements) learns.': 1.0986123085021973, 'For a more interesting analysis, I would have liked things like a comparison of different segmentation approaches (both in audio and in images), i.e., suppose we have access to the perfect segmentation in both modalities, what happens?': 1.0986123085021973, 'It would also be interesting to look at what is learned with the grounded representation, and evaluate e.g. on multi-modal semantics tasks.': 1.0986123085021973, 'Apart from that, the paper is well written': 1.0986123085021973, 'and I really like this research direction.': 0.6556777954101562, 'It is very important to analyze what models learn, and this is a good example of the types of questions one should ask.': 1.0986123085021973, 'I am afraid, however, that the model is not novel enough, nor the questions deep enough, to make this paper better than borderline for ICLR.': 1.0985891819000244, 'This work proposes a joint classification of images and audio captions for the task of word like discovery of acoustic units that correlate to semantically visual objects.': 1.0986123085021973, 'The general this is a very interesting direction of research as it allows for a richer representation of data: regularizing visual signal with audio and visa versa.': 1.0986123085021973, 'This allows for training of visual models from video, etc.': 1.0986123085021973, ""A major concern is the amount of novelty between this work and the author's previous publication at NIPs 2016."": 1.0986088514328003, 'The authors claim a more sophisticated architecture and indeed show an improvement in recall.': 1.0986123085021973, 'However, the improvements are marginal, and the added complexity to the architecture is a bit ad hoc.': 1.0986123085021973, 'Clustering and grouping in section 4, is hacky.': 1.0986123085021973, 'Instead of gridding the image, the authors could actually use an object detector (SSD, Yolo, FasterRCNN, etc.) to estimate accurate object proposals; rather than using k-means, a spectral clustering approach would alleviate the gaussian assumption of the distributions.': 1.0986123085021973, 'In assigning visual hypotheses with acoustic segments, some form of bi-partite matching should be used.': 1.0986123085021973, 'Overall, I really like this direction of research, and encourage the authors to continue developing algorithms that can train from such multimodal datasets.': 1.0986123085021973, ""However, the work isn't quite novel enough from NIPs 2016."": 1.0986123085021973, 'CONTRIBUTIONS': 1.0986123085021973, 'This paper introduces a method for learning semantic ""word-like"" units jointly from audio and visual data.': 1.0986123085021973, 'The authors use a multimodal neural network architecture which accepts both image and audio (as spectrograms) inputs.': 1.0986123085021973, 'Joint training allows one to embed both image and spoken language captions into a shared representation space.': 1.0718516111373901, 'Audio-visual groundings are generated by measuring affinity between image patches and audio clips.': 1.0986123085021973, 'This allows the model to relate specific visual regions to specific audio segments.': 1.0986123085021973, 'Experiments cover image search (audio to image) and annotation (image to audio) tasks and acoustic word discovery.': 1.0986123085021973, 'NOVELTY+SIGNIFICANCE': 1.0985982418060303, 'As correctly mentioned in Section 1.2, the computer vision and natural language communities have studied multimodal learning for use in image captioning and retrieval.': 1.0986123085021973, 'With regards to multimodal learning, this paper offers incremental advancements since it primarily uses a novel combination of input modalities (audio and images).': 1.0986123085021973, 'However, bidirectional image/audio retrieval has already been explored by the authors in prior work (Harwath et al, NIPS 2016).': 1.0986123085021973, 'Apart from minor differences in data and CNN architecture, the training procedure in this submission is identical to this prior work.': 1.0986123085021973, 'The novelty in this submission is therefore the procedure for using the trained model for associating image regions with audio subsequences.': 1.0986123085021973, 'The methods employed for this association are relatively straightforward combination of standard techniques with limited novelty.': 1.0986123085021973, 'The trained model is used to compute alignment scores between densely sampled image regions and audio subsequences; from these alignment scores a number of heuristics are applied to associate clusters of image regions with clusters of audio subsequences.': 1.0986123085021973, 'MISSING CITATION': 1.0986123085021973, 'There is a lot of work in this area spanning computer vision, natural language, and speech recognition.': 1.0986123085021973, 'One key missing reference:': 1.0986123085021973, 'Ngiam, et al.': 1.0986123085021973, '""Multimodal deep learning.""': 1.0986123085021973, 'ICML 2011': 1.0986123085021973, 'POSITIVE POINTS': 1.0986123085021973, 'Using more data and an improved CNN architecture, this paper improves on prior work for bidirectional image/audio retrieval': 1.0986123085021973, 'The presented method performs efficient acoustic pattern discovery': 1.0986123085021973, 'The audio-visual grounding combined with the image and acoustic cluster analysis is successful at discovering audio-visual cluster pairs': 1.0986123085021973, 'NEGATIVE POINTS': 1.0986123085021973, 'Limited novelty, especially compared with Harwath et al, NIPS 2016': 1.0986123085021973, 'Although it gives good results, the clustering method has limited novelty and feels heuristic': 1.0986123085021973, 'The proposed method includes many hyperparameters (patch size, acoustic duration, VAD threshold, IoU threshold, number of k-means clusters, etc) and there is no discussion of how these were set or the sensitivity of the method to these choices': 1.0986123085021973}"
87,https://openreview.net/forum?id=BkdpaH9ll,"{'CONTRIBUTIONS': 0.9062407612800598, 'This paper extends end-to-end CNN+RNN image captioning model with auxiliary attribute predictions.': 0.6430923938751221, 'It proposes five variants of network architectures, which take the attribute/image features in alternating orders or at every timestamp.': 1.0986123085021973, 'The attributes (i.e., 1,000 most common words on COCO) are obtained by attribute classifiers trained by a multiple instance learning approach.': 1.0986123085021973, 'The experiment results indicated that having these attributes as input improves captioning performance on standard metrics, including BLEU, METEOR, ROUGE-L, CIDEr-D.': 0.3727349042892456, 'NOVELTY + SIGNIFICANCE': 1.0986123085021973, 'All five variants of the network architectures, shown in Fig.': 1.0659154653549194, '1, have followed a standard seq-to-seq LSTM model.': 0.24148748815059662, 'The differences between these variants come from two aspects: 1. the order of image/attribute inputs; 2. whether to input attribute/image features at each time step.': 0.4568631947040558, 'No architectural changes have been added to the proposed model over a standard seq-to-seq model.': 1.0976868867874146, 'The proposed approach achieves decent performance improvement over previous work, but the technical novelty of this work is significantly limited by existing work that used similar ideas.': 0.8005543351173401, 'In particular, Fang et al. 2015 and You et al. 2016 have both used attributes for image captioning.': 0.2356206625699997, 'This work has used the same multiple instance learning procedure as Fang et al. 2015 to train visual detectors for common words and used detector outputs as conditional inputs to a language model.': 1.0587284564971924, 'In addition, the idea of using image feature as input to every RNN timestamp has been widely explored, for instance, in Donahue et al. 2015.': 0.6925969123840332, 'The authors did not offer a clear explanation about the technical contribution of this work over these existing approaches.': 1.0964921712875366, 'CLARITY': 1.0986123085021973, 'First, it is not clear to me which image features have been used by the baseline methods.': 1.0986120700836182, 'As the baselines may rely on different image representations, the experiments would not offer a completely fair comparison.': 1.0263439416885376, 'For example, the results of the attention-based models in Table 1 are directly copied from Xu et al., 2015, which were reported with Oxford VGG features, instead of GoogLeNet used by LSTM-A. Even if the baselines do not use the same types of features, it should at least be explicitly mentioned.': 0.32966411113739014, 'Besides, the fact that the results of Table 1 and Table 2 are reported with different features (GoogLeNet v.s. ResNet) are not described clearly in the paper.': 1.000341773033142, '“We select 1,000 most common words on COCO…” How would this approach guarantee that the selected attributes have clear semantic meanings, as many words among the top 1000 would be stop words, abstract nouns, non-visual verbs, etc.?': 0.607913076877594, 'It would be interesting to perform some quantitative analysis to see whether these 1000-dimensional attribute vectors actually carry semantic meanings, or merely capture biases of word distributions from the ground-truth captions.': 0.39983001351356506, 'One possible way to justify the importance of semantic attributes is to experiment with ground-truth attributes or attribute classifiers trained on annotated sources such as COCO-Attribute (Patterson et al. 2016) or Visual Genome (Krishna et al. 2016).': 0.28514328598976135, 'EXPERIMENTS': 1.0974819660186768, 'It would be interesting to analyze how the behavior of the seq-to-seq model changes with respect to the additional attribute inputs.': 0.4274739623069763, 'My hypothesis is that the model’s word choices will shift towards words with high scores.': 1.0447601079940796, 'This experiments will help us understand how the model can take advantage of the auxiliary attribute inputs.': 0.9636818766593933, 'Since the attributes are selected as the most frequent words from COCO, it is likely for the model to overfit the metrics, such as BLEU, that rely on word matching.': 0.3719246983528137, 'On the other hand, it has been shown that these automatic caption evaluation metrics do not necessarily correlate with human judgment.': 1.0357059240341187, 'Therefore, I think it is necessary to conduct a human study to convince the readers the quality improvement of the proposed model is not caused by overfitting to metrics.': 0.9068650603294373, 'SUMMARY': 0.8183125257492065, 'This paper demonstrates that image captioning can be improved by having attributes as auxiliary inputs.': 1.0087529420852661, 'However, the model has minor novelty given existing work that has explored similar ideas.': 1.0986123085021973, 'Besides, more analysis is necessary to demonstrate the semantic meanings of attributes.': 1.0986123085021973, 'A human study is recommended to justify the actual performance improvement.': 0.9644014835357666, 'Given these points to be improved, I would recommend rejecting this paper in its current form.': 0.7999395728111267, 'The authors take a standard image captioning system and, in addition to the image, also condition the caption on a fixed vector of attributes.': 1.0986123085021973, 'These attributes are the top 1,000 most common words trained with MIL as seen in Fang et al. 2015.': 1.0937846899032593, 'The authors investigate 5 ways of plugging in the attributes vector for each image.': 1.0837491750717163, 'More or less all of them turn out to work comparably (24 - 25.2 METEOR), but also a good amount better than a standard image captioning model (25.2 > 23.1 METEOR).': 1.0986117124557495, 'At the time this approach was state of the art on the MS COCO leaderboard.': 0.5283510088920593, 'I am conflicted judging these kinds of application-heavy papers.': 1.0845606327056885, 'It is clear that the technical execution is done relatively well, but there is little to take away or learn.': 1.0986123085021973, 'I find it interesting and slightly surprising that you can boost image captioning results by appending these automatically-extracted attributes, but on the topic of how many people would find this relevant or how much additional work it can inspire I am not so sure.': 1.0983498096466064, 'The paper evaluates different variants to include attributes for caption generation.': 1.0986123085021973, 'Attributes are automatically learned from descriptions as in [Fang et al., 2015].': 0.9459136128425598, 'Strength:': 0.6234821081161499, '1. The paper discusses and evaluates different variants to incorporate attribute and image representation in a recurrent network.': 0.50951087474823, '2. The paper reports improvements over state-of-the-art on the large-scale MS COCO image caption dataset.': 0.9349137544631958, 'Weaknesses:': 1.0986123085021973, '1. The technical novelty of the paper is limited; it is mainly a comparison of different variants to include attributes.': 0.8827699422836304, '2. While the exact way how attributes are used is different to prior work, the presented variants are not especially exiting.': 0.7199441194534302, '3. The reported metrics are known to not always correlate very well with human judgments.': 0.41279563307762146, '3.1. It would be good to additionally also report the SPICE (Anderson et al ECCV 2016) metric, which has shown good correlation w.r.t. human judgments.': 0.3839128613471985, '3.2. In contrast to the arguments of the authors during the discussion period, I believe a human evaluation is feasible at least on a subset of the test set. In my experience, most authors will share their generated sentences if they did not publish them. Also, a comparison between the different variants should be possible.': 0.8325592875480652, '4. Qualitative results:': 0.9592627882957458, '4.1. How do the different variants A1 to A5 compare? Is there a difference in the sentence between the different approaches?': 0.3133784234523773, 'Other (minor/discussion points)': 1.0986123085021973, 'Page 8: “is benefited” -> benefits': 1.0712549686431885, 'Summary:': 1.0986117124557495, 'While the paper presents a valuable experimental evaluation with convincing results, the contribution w.r.t.': 0.4772716462612152, 'to novelty and approach is limited.': 0.547906219959259}"
88,https://openreview.net/forum?id=Bkepl7cee,"{'The paper deals with a very important issue of vanishing gradients and the quest for a perfect activation function.': 1.1019935607910156, 'Proposed is an approach of learning the activation functions during the training process.': 1.3862943649291992, 'I find this research very interesting, but I am concerned that the paper is a bit premature.': 1.3756701946258545, 'There is a long experimental section, but I am not sure what the conclusion is.': 1.3862841129302979, 'The authors appear to be somewhat confused themselves.': 1.3862943649291992, 'The amount of ""maybe"" ""could mean"", ""perhaps"" etc. statements in the paper is exceptionally high.': 1.3860911130905151, 'For this paper to be accepted it needs a bold statement about the performance, with a solid evidence.': 0.9828031659126282, 'In my opinion, that is lacking as of now.': 1.3862943649291992, 'This approach is either a breakthrough or a dud, and after reading the paper I am not convinced which case it is.': 1.2932555675506592, 'The theoretical section could be made a little clearer.': 1.3862943649291992, 'Finally, how is the performance affected.': 1.3862943649291992, 'The huge advantage if ReLU is in the fact that the formula is so simple and thus not costly to evaluate.': 1.3862906694412231, 'How do PELU-s compare.': 1.3862943649291992, 'This paper presents a new non-linear function for CNN and deep neural networks.': 1.3862943649291992, 'The new non-linearity reports some gains on most datasets of interest, and can be used in production networks with minimal increase in computation.': 1.3862943649291992, 'This paper proposes a modification of the ELU activation function for neural networks, by parameterizing it with 2 trainable parameters per layer.': 1.3354313373565674, 'This parameter is proposed to more effectively counter vanishing gradients.': 1.3862943649291992, ""My main concern regarding this paper is related to the authors' claims about the effectiveness of PELU."": 1.3862940073013306, 'The analysis in Sections 2 and 3 discusses how PELU might improve training by combating gradient propagation issues.': 1.3862801790237427, 'This by itself does not imply that improved generalization will result, only that models may be easier to train.': 1.3862520456314087, 'However, the experiments all seek to demonstrate improved generalization performance.': 1.3862943649291992, 'But this could in principle be due to a better inductive bias, and have nothing to do with the optimization analysis.': 0.7292604446411133, 'None of the experiments are designed to directly support the stated theoretical advantage of PELU compared to ELU in optimizing models.': 1.386209487915039, 'In the response to the pre-review question, the authors state that the claims in Section 2 and 3.3 are meant to apply to generalization performance.': 0.9841228723526001, 'I fail to see how this is true for most claims, except the flexibility claim.': 1.3842053413391113, 'As the authors agree, better training may or may not lead to better out-of-sample performance.': 0.9872930645942688, 'I can only agree that having flexibility can sometimes help the network adapt its inductive bias to the problem (instead of overfitting), but this is a much weaker claim compared to the mathematical justifications for improved optimization.': 1.3862943649291992, 'On selection of learning hyperparameters:': 1.3862943649291992, 'The authors state in the discussion on OpenReview that the learning rates selected were favorable to ReLU, and not PELU.': 1.3862937688827515, 'However, this does not guarantee that they were not unfavorable to ELU.': 1.3862437009811401, 'It raises the question: can a regime be constructed where ELU has better performance than PELU?': 1.3805582523345947, 'If so, how can we draw the conclusion that PELU is better?': 1.3862661123275757, 'Overall, I am not yet convinced by the experimental setup and the match between theory and experiments in this paper.': 1.3862943649291992, 'Authors present a parameterized variant of ELU and show that the proposed function helps to deal with vanishing gradients in deep networks in a way better than existing non-linearities.': 1.3862942457199097, 'They present both a theoretical analysis and practical validation for presented approach.': 1.3862943649291992, 'Interesting observations on statistics of the PELU parameters are reported.': 1.3862943649291992, 'Perhaps explanation for the observed evolution of parameters can help better understand the non-linearity.': 1.3862942457199097, 'It is hard to evaluate the experimental validation presented given the difference in number of parameters compared to other approaches.': 1.3862935304641724}"
89,https://openreview.net/forum?id=BkfiXiUlg,"{'This paper introduces a novel hierarchical memory architecture for neural networks, based on a binary tree with leaves corresponding to memory cells.': 1.0986123085021973, 'This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks such as sorting from pure input-output examples and dealing with longer sequences.': 1.0986121892929077, 'The idea of the paper is novel and well-presented, and the memory structure seems reasonable to have advantages in practice.': 1.0986123085021973, 'However, the main weakness of the paper is the experiments.': 1.0985773801803589, 'There is no experimental comparison with other external memory-based approaches (e.g. those discussed in Related Work), or experimental analysis of computational efficiency given overhead costs (beyond just computational complexity) despite that being one of the main advantages.': 1.0986123085021973, 'Furthermore, the experimental setups are relatively weak, all on artificial tasks with moderate increases in sequence length.': 1.0986123085021973, 'Improving on these would greatly strengthen the paper, as the core idea is interesting.': 1.0986121892929077, 'This paper proposes to use a hierarchical softmax to speed up attention based memory addressing in memory augmented network (e.g. NTM, memNN…).': 1.0986123085021973, 'The model build a hierarchical softmax on top of the input sequence then at each time step SEARCH for the most relevant input to predict the next output (this search is discrete), and use its corresponding embedding to update the state of an LSTM that will then produce the output.': 1.0986123085021973, 'Finally the embedding of the used input is update by a WRITE function (an LSTM working that takes hidden state of the other LSTM as an input).': 1.0986123085021973, 'The model has a discrete component (the SEARCH) and is thus trained with REINFORCE.': 1.098165512084961, 'In the experimental section they test their approach on several algorithmic tasks such as search, sort...': 1.0986123085021973, 'The main advantage of replacing the full softmax by a hierarchical softmax is that during inference, the complexity goes from O(N) to O(log(N)).': 1.0986123085021973, 'It would be great to see if the gain in complexity allows to tackle problem which are a few orders of magnitude bigger than the one addressed with full softmax.': 1.0986123085021973, 'However the authors only test on toy sequences up to 32 tokens, which is quite small.': 1.0953028202056885, 'The model requires a relatively complex search mechanism that can only be trained with REINFORCE.': 1.098611831665039, 'While this seems to work on problems with relatively small and simple sequences, it would be great to see how performance changes with the size of the problem.': 1.0969196557998657, 'Overall, while the idea of replacing the softmax in the attention mechanism by a hierachical softmax is appealing, this work is not quite convincing yet.': 1.0986117124557495, 'Their approach is not very natural, may be hard to train and may not be that simple to scale.': 1.098508596420288, 'The experiment section is very weak.': 1.098588466644287, 'The authors introduce a new memory model which allows memory access in O(log n) time.': 1.0986123085021973, 'Pros:': 1.09861159324646, '*': 1.0986123085021973, 'The paper is well written and everything is clear.': 1.0986123085021973, ""* It's a new model and I'm not aware of a similar model."": 0.8454229831695557, ""* It's clear that memory access time is an issue for longer sequences and it is clear how this model solves this problem."": 0.64346843957901, 'Cons:': 1.0986123085021973, 'The motivation for O(log n) access time is to be able to use the model on very long sequences.': 1.0986123085021973, 'While it is clear from the definition that the computation time is low because of its design, it is not clear that the model will really generalize well to very long sequences.': 1.0986123085021973, 'The model was also not tested on any real-world task.': 1.0986123085021973, 'I think such experiments should be added to show whether the model really works on long sequences and real-world tasks, otherwise it is not clear if this is a useful model.': 1.0986123085021973}"
90,https://openreview.net/forum?id=Bkfwyw5xg,"{'This paper analyzes dependency trees vs standard window contexts for word vector learning.': 1.0986123085021973, ""While that's a good goal I believe the paper falls short of a thorough analysis of the subject matter."": 1.0986050367355347, 'It does not analyze Glove like objective functions which often work better than the algorithms used here.': 1.0976951122283936, ""It doesn't compare in absolute terms to other published vectors or models."": 1.0173468589782715, ""It fails to gain any particularly interesting insights that will modify other people's work."": 1.0985941886901855, 'It fails to push the state of the art or make available new resources for people.': 1.0986123085021973, 'This paper evaluates how different context types affect the quality of word embeddings on a plethora of benchmarks.': 1.0986120700836182, 'I am ambivalent about this paper.': 1.0625723600387573, 'On one hand, it continues an important line of work in decoupling various parameters from the embedding algorithms (this time focusing on context); on the other hand, I am not sure I understand what the conclusion from these experiments is.': 1.0986119508743286, 'There does not appear to be a significant and consistent advantage to any one context type.': 1.0983883142471313, 'Why is this?': 1.0324710607528687, 'Are the benchmarks sensitive enough to detect these differences, if they exist?': 1.0986123085021973, 'While I am OK with this paper being accepted, I would rather see a more elaborate version of it, which tries to answer these more fundamental questions.': 1.0986123085021973, ""This paper investigates the issue of whether and how to use syntactic dependencies in unsupervised word representation learning models like CBOW or Skip-Gram, with a focus one the issue of bound (word+dependency type, 'She-nsubj') vs. unbound (word alone, 'She') representations for context at training time."": 1.0986123085021973, 'The empirical results are extremely mixed, and no specific novel method consistently outperforms existing methods.': 1.098606824874878, 'The paper is systematic and I have no major concerns about its soundness.': 1.098358392715454, ""However, I don't think that this paper is of broad interest to the ICLR community."": 1.0976498126983643, 'The paper is focused on a fairly narrow detail of representation learning that is entirely specific to NLP, and its results are primarily negative.': 1.09861159324646, 'A short paper at an ACL conference would be a more reasonable target.': 0.9053953289985657}"
91,https://openreview.net/forum?id=BkjLkSqxg,"{'The authors present a well thought out and constructed system for performing lipreading.': 1.0986123085021973, 'The primary novelty is the end-to-end nature of the system for lipreading, with the sentence-level prediction also differentiating this with prior work.': 1.0986123085021973, 'The described neural network architecture contains convolutional and recurrent layers with a CTC sequence loss at the end, and beam search decoding with an LM is done to obtain best results.': 1.0986123085021973, 'Performance is evaluated on the GRID dataset, with some saliency map and confusion matrix analysis provided as well.': 1.0986123085021973, 'Overall, the work seems of high quality and clearly written with detailed explanations.': 1.0986019372940063, 'The final results and analysis appear good as well.': 1.0986123085021973, 'One gripe is that that the novelty lies in the choice of application domain as opposed to the methods.': 0.9771923422813416, 'Lack of word-level comparisons also makes it difficult to determine the importance of using sentence-level information vs. choices in model architecture/decoding, and finally, the GRID dataset itself appears limited with the grammar and use of a n-gram dictionary.': 1.0986123085021973, ""Clearly the system is well engineered and final results impress, though it's unclear how much broader insight the results yield."": 1.0986114740371704, '- Proven again that end to end training with deep networks gives large': 0.49020057916641235, 'gains over traditional hybrid systems with hand crafted features.': 1.0976401567459106, 'The results': 1.0690245628356934, 'are very nice for the small vocabulary grammar task defined by the GRID corpus.': 0.9414251446723938, 'The engineering here is clearly very good, will be interesting to see the performance on large vocabulary LM tasks.': 1.0986123085021973, 'Comparison to human lip reading performance for conversational speech will be very interesting here.': 0.47449004650115967, 'Traditional AV-ASR systems which apply weighted audio/visual posterior fusion reduce to pure lip reading when all the weight is on the visual, there are many curves showing performance of this channel in low audio SNR conditions for both grammar and LM tasks.': 1.0986123085021973, 'Traditional hybrid approaches to AV-ASR are also sentence level sequence trained with fMPE/MPE/MMI etc. objectives (see old references), so we cannot say here that this is the first sentence-level objective for lipreading model (analogous to saying there was no sequence training in hybrid LVCSR ASR systems before CTC).': 1.0978641510009766, ""UPDATE:  I have read the authors' responses."": 0.7248718738555908, 'I did not read the social media comments about this paper prior to reviewing it.': 1.0986123085021973, ""I appreciate the authors' updates in response to the reviewer comments."": 1.0985530614852905, 'Overall, however, my review stands.': 1.0986120700836182, 'The authors have taken a task that had not yet been addressed with a straightforward modern deep learning approach, and addressed it with such an approach.': 1.0986121892929077, ""I assume that if we pick up any task that hasn't been worked on for a while, and give it a solid deep learning treatment, we will do well."": 1.0986123085021973, 'I do not see such papers as a contribution to ICLR, unless they also provide new insights, analysis, or surprising results (which, to my mind, this paper does not).': 1.098603367805481, 'This is a general point and the program chairs may disagree with it, of course.': 0.4467483162879944, 'I have removed my recommendation that this be accepted as a workshop paper, as I have since noticed that the workshop track this year has a different focus.': 1.095740556716919, '************************': 1.0986123085021973, 'ORIGINAL REVIEW:': 1.098499059677124, 'The authors show that an appropriately engineered LSTM+CNN+CTC network does an excellent job of lipreading on the GRID corpus.': 0.7780224680900574, 'This is a nice result to know about': 0.9352333545684814, 'yet another example of a really nice result that one can get the first time one applies such methods to an old task': 0.4730302095413208, 'and all of the work that went into getting it looks solid (and likely involved some significant engineering effort).': 1.015655755996704, 'However, this in itself is not sufficiently novel for publication at ICLR.': 1.0716301202774048, 'The paper also needs to be revised to better represent prior work, and ideally remove some of the vague motivational language.': 1.0914009809494019, 'Some specifics on what I think needs to be revised:': 1.0924021005630493, 'First, the claim of being the first to do sentence-level lipreading.': 0.9697092175483704, 'As mentioned in a pre-review comment, this is not true.': 1.0128324031829834, 'The paper should be revised to discuss the prior work on this task (even though much of it used data that is not public).': 1.0905218124389648, 'Ideally the title should also be changed in light of this.': 0.24937240779399872, 'The comparison with human lipreaders needs to be qualified a bit.': 0.5680472254753113, 'This task is presumably very unnatural for humans because of the unusual grammar, so perhaps what you are showing is that a machine can better take into account the strong contraints.': 0.8317846059799194, 'This is great, but not a general statement about LipNet vs. humans.': 1.0340936183929443, 'The paper contains some unnecessary motivational platitudes.': 1.0986123085021973, 'We do not need to invoke Easton and Basala 1982 to motivate modeling context in a linguistic sequence prediction task, and prior work using older sequence models (e.g. HMMs) for lipreading has modeled context as well.': 0.8534700274467468, 'The McGurk effect does not show that lipreading plays a crucial role in human communication.': 0.7148417234420776, 'It is worth noting that even without the spatial convolution, your Baseline-2D already does extremely well.': 1.0580732822418213, 'So I am not sure about the ""importance of spatiotemporal feature extraction"" as stated in the conclusion.': 1.0155811309814453, 'Some more minor comments, typos, etc.:': 1.0819389820098877, 'citations for LSTMs, CTC, etc. should be provided the first time they are mentioned.': 0.3030608594417572, 'I did not quite follow the justification for upsampling.': 0.6229109764099121, 'what is meant by ""lip-rounding vowels""?': 1.088618516921997, 'They seem to include almost all English vowels.': 0.8522977232933044, 'Did you consider keeping the vowel visemes V1-V4 separate rather than collapsing them into one?': 1.0983037948608398, ""Since you list Neti et al.'s full viseme set, it is worth mentioning why you modified it."": 1.0905039310455322, '""Given that the speakers are British, the confusion between /aa/ and /ay/...""': 0.013811183162033558, 'I am not sure what this has to do with British speakers, as the relationship between these vowels exists in other English dialects as well (e.g. American).': 1.098414421081543, 'The discussion about confusions within bilabial stops and within alveolar stops is a bit mismatched with the actual confusion data in Fig.': 1.0986055135726929, '3(b,c).': 0.8585063219070435, 'For example, there does not seem to be any confusion between /m/ and /b/ or between /m/ and /p/.': 1.0423868894577026, '""lipreading actuations"":  I am not sure what ""actuations"" means in this context': 1.0922657251358032, '""palato-alvealoar""': 0.39691510796546936, '> ""palato-alveolar""': 0.7623029351234436, '""Articulatorily alveolar""': 0.7703810334205627, '> ""Alveolar""?': 1.037079930305481}"
92,https://openreview.net/forum?id=BkmM8Dceg,"{'The paper shows how group convolutions (for two dimensional commutative groups) can be performed by standard CNNs if the input is warped using a fixed warp.': 1.0986123085021973, 'The idea is practical and seems to work well.': 0.82404625415802, 'The paper is well written.': 1.0986123085021973, 'I agree with reviewer 1 that the ‘theorems’ do not deserve to be labelled as such.': 1.0986108779907227, 'Theorem 1 is equivalent to the second equation from this section of the wikipedia page on convolution: https://en.wikipedia.org/wiki/Convolution#Convolutions_on_groups.': 1.0769237279891968, 'The existence of Haar measure (from which commutation of Lg and convolution follows immediately) is a well known and elementary theorem in harmonic analysis, which would be treated in the first few pages of any textbook on the subject.': 1.0986119508743286, 'It also immediately clear that any commutative group has an additive parameterization (theorem 2).': 1.0986123085021973, 'The fact that the paper does not present new deep mathematical results is not a significant weakness in my opinion, but the derivations should not be camouflaged as such.': 1.0984176397323608, 'The claim that previous methods that use group convolutions are slow and that the presented approach has better computational complexity is not supported by empirical evidence, and the theoretical analysis is still a bit misleading.': 1.0986117124557495, 'For example, the authors write “Unfortunately these approaches do not possess the same memory and speed benefits that CNNs enjoy.': 1.098520278930664, 'The reason is that, ultimately, they have to enumerate all possible transformations”.': 1.0986106395721436, 'The presented method also has to enumerate all transformations (in a limited range, on a discretized grid), and this is feasible only because the group is only 2 dimensional (and indeed this is also true for standard CNNs which enumerate translations).': 1.080998182296753, 'As noted in my pre-review question, I believe the computational complexity analysis is not entirely correct, and assume the authors will correct this.': 1.098608374595642, 'Equation 4 is presented as a new invention, but this has been used in previous works and is well known in mathematics, so a citation should be added.': 1.0986113548278809, 'The main advantage of the presented method over several earlier methods is that it is very simple to implement, and can re-use highly optimized convolution routines.': 1.092542290687561, 'As I understand it, Dieleman et al. and Cohen & Welling also use standard convolutions (after a fixed filter / feature map warp), but these papers only consider discrete groups.': 0.8497441411018372, 'So it seems like this paper occupies a unique place in the space of equivariant convolutional networks:': 0.392672598361969, 'non-commutative (-), low-dimensional (-), continuous (+) groups, simple (+) and efficient (+) algorithm.': 0.7693836688995361, 'Given the proximity though, a more thorough and balanced appraisal of the merits and demerits, as well as the novelty, relative to each of the previous works would be useful.': 0.6013672351837158, 'Provided that these issues are cleared up, I would recommend the paper for publication.': 1.0985430479049683, 'The paper suggests method to make the convolution invariant to other type of spatial transformations besides translation.': 1.0986123085021973, 'It is interesting that the convolution can be invariant to different variation types by simple warping of the input image, which is related to arbitrary data perturbation in image recognition.': 1.0986123085021973, 'Experiment section needs more description and technique implementation details.': 1.098559021949768, 'The relationship of theorems to Lie group is not defined properly.': 1.0986123085021973, 'It need more detailed comparison of method with available methods in the literature.': 1.0773676633834839, 'This paper deals with convolution along Lie groups.': 1.0986123085021973, 'Pros:': 1.0986123085021973, 'Good numerical results': 1.0986123085021973, 'Interesting applications linked to convolutions on Lie groups': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'Sometimes, the writing is not clear/poor.': 1.0986123085021973, 'The theorems 1 and 2 are in fact direct applications of Lie group theory, but this is not made explicit.': 1.0986123085021973, 'I do not detail more my review, as I believe my questions/answers complete it fairly.': 1.0986123085021973, 'However, if the authors require it, I can be more specific.': 1.0986123085021973, 'I think the writing should be improved a bit.': 1.0986123085021973}"
93,https://openreview.net/forum?id=Bkp_y7qxe,"{'This paper implements the method of Jonschkowski & Brock to learn a low-dimensional state representation represented as the last layer of a neural network.': 1.0986123085021973, 'The experiments apply the method for learning a one-dimensional state representation of a simulated robot’s head position from synthetic images.': 1.0986123085021973, 'Learning state representations is an active and useful area of research for learning representations in interactive domains such as robotics.': 1.0984814167022705, 'However, there seems to be no novelty in the method, over Jonschkowki & Brock.': 1.0986123085021973, 'The primary contribution is the experimental evaluation performed on one task, where the paper evaluates the correlation between the learned state representation and the ideal state representation for the task (which is the robot’s head position).': 1.0986123085021973, 'As acknowledged by the authors, the experiments are very preliminary, only showing one simple task with a one-dimensional learned representation and a two-dimensional discrete action space.': 1.0986089706420898, 'To make the experiments compelling, there need to be comparisons to prior methods such as Lange et al. ’12, Watter et al. NIPS ’15, and Finn et al.': 1.0984559059143066, 'ICRA ’16 which also learn state representations from raw images.': 1.0986123085021973, 'PCA on the images would also be a useful comparison, especially for simple tasks.': 1.0896251201629639, 'Without these comparisons, it is impossible to evaluate the effectiveness of the method.': 0.5387862920761108, 'Lastly, as mentioned in the pre-review questions, the related work should include a discussion of other state representation learning methods such as Watter et al. NIPS ’15, Finn et al. ICRA ’16, and van Hoof et al.': 1.0986013412475586, 'IROS ’16.': 1.0986123085021973, 'In summary, this paper lacks novelty and significance, as the paper implements an existing method and demonstrates results on only one simple task.': 0.4763929545879364, 'Without comparisons, the results are impossible to interpret.': 0.985862672328949, 'More challenging tasks and experimental comparisons would significantly improve the paper.': 1.0133861303329468, 'Additionally, this paper does not introduce any novel contributions to state representation learning for solving challenges in this domain.': 1.042410969734192, 'One pro is that the paper is generally written clearly.': 1.083656907081604, 'The paper proposes to use the representation learning approach of [Jonschkowski & Brock, 2015] with a deep network as function approximator.': 1.0986123085021973, 'The general task and approach are interesting, but contribution of this work is limited, and experimental evaluation is absolutely unsatisfactory, so the paper cannot be accepted for publications.': 1.0986123085021973, 'The approach is tested on a simple synthetic task with very small training and test sets and very little variation in the data.': 1.0986121892929077, 'The authors admitted themselves that the results are preliminary.': 1.0985509157180786, 'The proposed method is not compared with existing approaches or simple hand-crafted baselines.': 1.0986123085021973, 'It is impossible to judge if the proposed method is useful and/or performs well compared to existing approaches.': 1.0930644273757935, 'This makes the paper unfit for publication.': 1.0871851444244385, 'With proper experiments, and if the method works in interesting realistic scenarios, this could become a good paper.': 1.0986123085021973, 'This paper proposed to use unsupervised learning to learn features in a reinforcement learning setting.': 1.02717924118042, 'It is unclear what ""unsupervised"" means here since the ""causality prior"" uses reward signals for training.': 1.0986123085021973, 'This is reinforcement learning, not unsupervised learning.': 1.0791981220245361, 'The experiments are also very premature.': 1.071923851966858, 'The task is as simple as moving the head of the robot left or right.': 1.0976293087005615, 'There is also no comparison to baselines.': 1.0986123085021973, 'In conclusions section, the authors claim the proposed method can be used for transfer learning without experiments to backup the claim.': 1.0986123085021973, 'Overall this paper is confusing and premature.': 1.0986123085021973}"
94,https://openreview.net/forum?id=Bks8cPcxe,"{'This paper presents and evaluates a Scala-based deep learning framework called “DeepDSL,” describing the language’s syntactic and performance benefits with respect to existing frameworks.': 1.0986123085021973, 'Pros:': 1.0986123085021973, 'The use of Scala is unique among deep learning frameworks, to my knowledge, making this framework interesting for Scala users.': 1.0986123085021973, 'The fact that Scala compiles to Java and therefore cross-platform support comes for free is also nice.': 1.0986123085021973, 'The ability to inspect memory information as shown in Figure 3 is interesting and potentially useful for large networks or situations where memory is limited.': 1.0986123085021973, 'DeepDSL compares favorably with existing frameworks in terms of memory use and speed for many common convolutional network architectures.': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'There appears to be special privileged handling of parameters, gradients, and updates in the compilation process itself (as in Caffe), rather than having gradients/updates as a normal part of the full user-defined computation graph (as in Theano + TensorFlow).': 1.0918458700180054, 'This makes certain applications, such as RNNs (which require parameter sharing) and GANs (which require gradients wrt multiple objectives), impossible to implement in DeepDSL without further extension of the underlying API.': 1.093942642211914, '(Note: I might be wrong about this': 1.0881750583648682, 'and please correct me if I am': 1.0986123085021973, 'but all the examples in the paper are nets trained by gradient descent on a single objective, and do not share parameters or access gradients directly.)': 0.9899681806564331, 'The paper repeatedly refers to line counts from the verbose Protobuf-based low-level representation of networks in Caffe to demonstrate the compactness of its own syntax.': 1.0977070331573486, 'This is misleading as Caffe has officially supported a compact network definition style called “NetSpec” for years': 1.0986027717590332, 'see a ~15 line definition of AlexNet': 1.0986123085021973, '[1].': 1.0986123085021973, 'Given that, Protobuf is essentially an intermediate representation for Caffe (as with TensorFlow), which happens to have a human-readable text format.': 0.5548341274261475, 'DeepDSL is not especially novel when compared with existing frameworks, which is not a problem in and of itself, but some statements misleadingly or incorrectly oversell the novelty of the framework.': 0.5555493235588074, 'Some examples:': 1.0986121892929077, '“This separation between network definition and training is an unique advantage of DeepDSL comparing to other tools.”': 0.9800506234169006, 'This separation is not unique': 1.0045883655548096, 'it’s certainly a feature of Caffe where the network definition is its own file, and can be attained in TensorFlow as well (though it’s not the default workflow there).': 0.5175175666809082, '“The difference [between our framework and Theano, TensorFlow, etc.] is that we do not model deep networks as ‘networks’ but as abstract ‘functions’.”': 1.0924324989318848, 'There is no notion of a “network” in Theano or TensorFlow (not sure about the others) either': 1.0631285905838013, 'there are only functions, like in DeepDSL.': 0.6576635837554932, 'I asked about this statement, and the response didn’t convince me otherwise.': 1.0544698238372803, 'The counterexample given was that in TensorFlow the number of input channels needs to be specified separately for each convolution.': 1.0778050422668457, 'This is only true using the low-level API and can easily be worked around with higher-level wrappers like TensorFlow Slim': 1.0976821184158325, 'e.g., see the definition of AlexNet': 0.6441944241523743, '[2].': 0.8160823583602905, 'It may be true that DeepDSL is more “batteries included” for writing compact network definitions than these other frameworks, but the paper’s claims seem to go beyond this.': 0.5044313073158264, 'Overall, the DeepDSL framework seems to have real value in its use of Scala and its memory/speed efficiency as demonstrated by the experiments, but the current version of the paper contains statements that overclaim novelty in ways that are misleading and unfair to existing frameworks.': 0.4218241572380066, 'I will consider upgrading my rating if these statements are removed or amended to be more technically precise.': 1.0986123085021973, '[1] https://github.com/BVLC/caffe/blob/master/examples/pycaffe/caffenet.py#L24': 1.0986121892929077, '[2] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/alexnet.py#L92': 1.0986123085021973, '=====================': 1.0986123085021973, 'Update: the authors have revised their paper to address the concerns that I considered grounds for rejection in my review.': 1.0986123085021973, ""I've upgraded my rating from 5 (below threshold) to 7 (good paper, accept)."": 1.0986123085021973, 'The paper presents DeepDSL, a ""domain specific language (DSL) embedded in': 1.0986123085021973, 'Scala, that compiles deep networks written in DeepDSL to Java source code"".': 1.0986123085021973, 'It': 1.0986123085021973, 'introduces its syntax and the key concepts which differentiate it': 1.0986123085021973, 'from other existing frameworks, such as Torch7, Theano, Caffe, TensorFlow,': 1.0986123085021973, 'CNTK, Chainer and MXNet.': 1.0986123085021973, 'It also benchmarks speed and memory usage against': 1.0986123085021973, 'TensorFlow and Caffe on a variety of convolutional neural network architectures.': 1.0986123085021973, 'The paper is clear and well written and it does a good job of presenting DeepDSL': 1.0986123085021973, 'in the context of existing deep learning frameworks.': 1.0986123085021973, ""However, I don't think ICLR is the right venue for this type of work."": 1.0986123085021973, 'Some of': 1.0986123085021973, 'the ideas it presents are interesting, but overall the paper lacks novelty and': 1.0986123085021973, 'potential impact and stays firmly within the realm of deep learning framework': 1.0986123085021973, ""whitepapers such as [1,2,3,4], which to my knowledge don't have a precedent of"": 1.0986123085021973, 'being accepted at venues like ICLR.': 1.0986123085021973, '[1]: Bergstra, James, et al.': 1.0986123085021973, '""Theano: A CPU and GPU math compiler in Python.""': 1.0986123085021973, 'Proc.': 1.0986123085021973, '9th Python in Science Conf. 2010.': 1.0986123085021973, '[2]: Bastien, Frédéric, et al.': 1.0986123085021973, '""Theano: new features and speed improvements.""': 1.0986123085021973, 'arXiv preprint arXiv:1211.5590 (2012).': 1.0986123085021973, '[3]: Abadi, Martın, et al.': 1.0986123085021973, '""Tensorflow: Large-scale machine learning on': 1.0986123085021973, 'heterogeneous distributed systems."" arXiv preprint arXiv:1603.04467 (2016).': 1.0986123085021973, '[4]: The Theano Development Team et al.': 1.0986123085021973, '""Theano: A Python framework for fast': 1.0986123085021973, 'computation of mathematical expressions.""': 1.0986123085021973, 'arXiv preprint arXiv:1605.02688': 1.0986123085021973, '(2016).': 1.0986123085021973, 'UPDATE:': 1.0986123085021973, ""The rating has been revised to a 6 following the authors' reply."": 1.0986123085021973, 'This paper presents a domain specific language for the specification of deep learning models.': 1.0986123085021973, 'The intermediate representation offers many possibilities for optimization and to focus on speed or runtime.': 1.0986123085021973, 'The paper is well-written and makes conclusive statements and comparisons.': 1.0986123085021973, 'The experiments cover five fundamentally differenct CNN architectures, each evaluated for two batch sizes.': 1.0986123085021973, 'They include the two competing frameworks Tensorflow and Caffe and show convincing performance.': 1.0986123085021973, 'Overall, the paper is well-written and structured.': 1.0986123085021973}"
95,https://openreview.net/forum?id=Bkul3t9ee,"{'The paper tries to present a first step towards solving the difficult problem of ""learning from limited number of demonstrations"".': 1.0986123085021973, 'The paper tries to present 3 contributions towards this effort:': 1.0986123085021973, '1. unsupervised segmentation of videos to identify intermediate steps in a process': 1.0986123085021973, '2. define reward function based on feature selection for each sub-task': 1.0986123085021973, 'Pros:': 1.0986123085021973, '+': 1.0986123085021973, 'The paper is a first attempt to solve a very challenging problem, where a robot is taught real-world tasks with very few visual demonstrations and without further retraining.': 1.0986123085021973, 'The method is well motivated and tries to transfer the priors learned from object classification task (through deep network features) to address the problem of limited training examples.': 1.0986123085021973, 'As demonstrated in Fig.': 1.0982253551483154, '3, the reward functions could be more interpretable and correlate with transitions between subtasks.': 1.0977907180786133, '+ Breaking a video into subtasks helps a video demonstration-based method achieve comparable performance with a method which requires full instrumentation for complex real-world tasks like door opening.': 1.0972810983657837, 'Cons:': 1.0986123085021973, '1. Unsupervised video segmentation can serve as a good starting point to identify subtasks. However, there are multiple prior works in this domain which need to be referenced and compared with. Particularly, video shot detection and shot segmentation works try to identify abrupt change in video to break it into visually diverse shots. These methods could be easily augmented with CNN-features.': 1.0290257930755615, '(Note that there are multiple papers in this domain, eg. refer to survey in Yuan et al.': 1.0979435443878174, 'Trans.': 1.0986123085021973, 'on Circuits and Systems for video tech.': 1.0986123085021973, '2007)': 1.0986123085021973, ""2. The authors claim that they did not find it necessary to identify commonalities across demonstrations. This limits the scope of the problem drastically and requires the demonstrations to follow very specific set of constraints. Again, it is to be noted that there is past literature (video co-segmentation, eg. Tang et al. ECCV'14) which uses these commonalities to perform unsupervised video segmentation."": 1.041366457939148, '3. The unsupervised temporal video segmentation approach in the paper is only compared to a very simple random baseline for a few sample videos. However, given the large amount of literature in this domain, it is difficult to judge the novelty and significance of the proposed approach from these experiments.': 0.4810566306114197, '4. The authors hypothesize that ""sparse independent features exists which can discriminate a wide range of unseen inputs"" and encode this intuition through the feature selection strategy. Again, the validity of the hypothesis is not experimentally well demonstrated. For instance, comparison to a simple linear classifier for subtasks would have been useful.': 1.0565885305404663, 'Overall, the paper presents a simple approach based on the idea that recognizing sub-goals in an unsupervised fashion would help in learning from few visual demonstrations.': 1.0986123085021973, 'This is well motivated as a first-step towards a difficult task.': 1.0842204093933105, 'However, the methods and claims presented in the paper need to be analyzed and compared with better baselines.': 0.8990566730499268, 'This paper proposes a novel method to learn vision feature as intermediate rewards to guide the robot training in the real world.': 1.0986123085021973, 'Since there are only a few sequences of human demonstrations, the paper first segments the sequences into fragments so that the features are roughly invariant on the corresponding fragments across sequences, then clusters and finds most discriminative features on those fragments, and uses them as the reward function.': 1.0986121892929077, 'The features are from pre-trained deep models.': 1.0986123085021973, 'The idea is simple and seems quite effective in picking the right reward functions.': 0.47347113490104675, 'Fig. 6 is a good comparison (although it could be better with error bars).': 1.0986123085021973, 'However, some baselines are not strong, in particular vision related baselines.': 1.098113775253296, 'For example, the random reward (""simply outputs true or false"") in Tbl. 2 seems quite arbitrary and may not serve as a good baseline (but its performance is still not that bad, surprisingly.).': 1.0986109972000122, 'A better baseline would be to use random/simpler feature extraction on the image, e.g., binning features and simply picking the most frequent ones, which might not be as discriminative as the proposed feature.': 1.0986123085021973, 'I wonder whether a simpler vision-based approach would lead to a similarly performed reward function.': 1.0986123085021973, 'If so, then these delicate steps (segment, etc) altogether.': 0.8535431623458862, 'The paper explores a simple approach to learning reward functions for reinforcement learning from visual observations of expert trajectories for cases were only little training data is available.': 0.5812960267066956, 'To obtain descriptive rewards even under such challenging conditions the method re-uses a pre-trained neural network as feature extractor (this is similar to a large body of work on task transfer with neural nets in the area of computer vision) and represents the reward function as a weighted distance to features for automatically extracted ""key-frames"" of the provided expert trajectories.': 1.0986095666885376, 'The paper is well written and explains all involved concepts clearly while also embedding the presented approach in the literature on inverse reinforcement learning (IRL).': 0.5640265941619873, 'The resulting algorithm is appealing due to its simplicity and could prove useful for many real world robotic applications.': 1.0986123085021973, 'I have three main issues with the paper in its current form, if these can be addressed I believe the paper would be significantly strengthened:': 1.0986123085021973, '1)': 1.0986123085021973, 'Although the recursive splitting approach for extracting the ""key-frames"" seems reasonable and the feature selection is well motivated I am missing two baselines in the experiments:': 1.0986123085021973, '- what happens if the feature selection is disabled and the distance between all features is used ? will this immediately break the procedure ? If not, what is the trade-off here ?': 1.085224986076355, '- an even simpler baseline than what is proposed in the paper would be the following procedure: simply use all frames of the recorded trajectories, calculate the distance to them in feature space and weights them according to their time as in the approach proposed in the paper. How well would that work ?': 1.0914939641952515, '2) I understand the desire to combine the extracted reward function with a simple RL method but believe the used simple controller could potentially introduce a significant bias in the experiments since it requires initialization from an expert trajectory.': 1.036137580871582, 'As a direct consequence of this initialization the RL procedure is already started close to a good solution and the extracted reward function is potentially only queried in a small region around what was observed in the initial set of images (perhaps with the exception of the human demonstrations).': 1.0985987186431885, 'Without an additional experiment it is thus unclear how well the presented approach will work in combination with other RL methods for training the controller.': 1.098610758781433, '3) I understand that the low number of available images excludes training a deep neural net directly for the task at hand but one has to wonder how other baselines would do.': 1.0850417613983154, 'What happens if one uses a random projection of the images to form a feature vector?': 1.0440351963043213, 'How well would a distance measure using raw images (e.g. L2 norm of image differences) or a distance measure based on the first principal components work?': 1.0984972715377808, 'It seems that occlusions etc. would exclude them from working well but without empirical evidence it is hard to confirm this.': 1.0940377712249756, 'Minor issues:': 1.0986019372940063, 'Page 1: ""make use of ideas about imitation"" reads a bit awkwardly': 1.0479345321655273, 'Page 3: ""We use the Inception network pre-trained ImageNet"" -> pre-trained for ImageNet classification': 1.083320140838623, 'Page 4: the definition of the transition function for the stochastic case seems broken': 0.9775343537330627, 'Page 6: ""efficient enough to evaluate"" a bit strangely written sentence': 0.7477355599403381, 'Additional comments rather than real issues:': 1.0986123085021973, 'The paper is mainly of empirical nature, little actual learning is performed to obtain the reward function and no theoretical advances are needed.': 1.0985865592956543, 'This is not necessarily bad but makes the empirical evaluation all the more important.': 0.5981917977333069, 'While I liked the clear exposition the approach': 1.0975074768066406, 'in the end': 1.0986123085021973, 'boils down to computing quadratic distances to features of pre-extracted ""key-frames"", it is nice that you make a connection to standard IRL approaches in Section 2.1 but one could argue that this derivation is not strictly necessary.': 0.9435306787490845}"
96,https://openreview.net/forum?id=By14kuqxx,"{'The paper presents a hardware accelerator architecture for deep neural network inference, and a simulated performance evaluation thereof.': 0.0758410096168518, 'The central idea of the proposed architecture (PRA) revolves around the fact that the regular (parallel) MACC operations waste a considerable amount of area/power to perform multiplications with zero bits.': 1.0933923721313477, 'Since in the DNN scenario, one of the multiplicands (the weight) is known in advance, the multiplications by the zero digits can be eliminated without affecting the calculation and lowest non-zero bits can be further dropped at the expense of precision.': 1.098220944404602, 'The paper proposes an architecture exploiting this simple idea implementing bit-serial evaluation of multiplications with throughput depending on the number of non-zero bits in each weight.': 1.0986120700836182, 'While the idea is in general healthy, it is limited to fixed point arithmetics.': 1.098283290863037, 'Nowadays, DNNs trained on regular graphics hardware have been shown to work well in floating point down to single (32bit) and even half-precision (16bit) in many cases with little or no additional adjustments.': 1.0986123085021973, 'However, this is generally not true for 16bit (not mentioning 8bit) fixed point.': 1.0986123085021973, 'Since it is not trivial to quantize a network to 16 or 8 bits using standard learning, recent efforts have shown successful incorporation of quantization into the training process.': 1.0983818769454956, 'One of the extreme cases showed quanitzation to 1bit weights with negligible loss in performance (arXiv:1602.02830).': 1.0986099243164062, '1-bit DNNs involve no multiplication at all; moreover, the proposed multiplier-dependent representation of multiplication discussed in the present paper can be implemented as a 1-bit DNN.': 0.8779059052467346, 'I think it would be very helpful if the authors could address the advantages their architecture brings to the evaluation of 1-bit DNNs.': 1.098551869392395, 'To summarize, I believe that immediately useful hardware DNN accelerators still need to operate in floating point (a good example are Movidius chips': 1.0859935283660889, 'nowadays Intel).': 1.0986123085021973, 'Fixed point architectures promise additional efficiency and are important in low-power applications, but they depend very much on what has been done at training.': 0.9137752652168274, 'In view of this': 1.0986123085021973, 'and this is my own extreme opinion': 1.0986109972000122, 'it makes sense to build an architecture for 1-bit DNNs.': 1.0331461429595947, 'I have the impression that the proposed architecture could be very suitable for this, but the devil is in the details and currently evidence is missing to make such claims.': 1.0500845909118652, 'Interesting and timely paper.': 0.8670548796653748, 'Lots of new neural network accelerators popping up.': 1.0986123085021973, ""I'm not an expert in this domain and to familiarize myself with the topic, I browsed through related work and skimmed the DaDianNao paper."": 1.0975598096847534, 'My main question is about the choice of technology.': 1.0781229734420776, 'What struck me is that your paper contains very few implementation details except for the technology (PRA 65nm vs DaDianNao 28nm).': 1.0986123085021973, 'Combined with the fact that the main improvement of your work appears to be performance rather than energy efficiency, I was wondering about the maximum clock estimated frequency of the PRA implementation due to the added complexity?': 1.0986123085021973, 'Based on the explanation in the methodology section, I assume that the performance comparison is based on number of clock cycles.': 1.0986121892929077, 'Do you have any numbers/estimates about the performance in practices (taking into account clock frequency)?': 1.0986123085021973, 'An interesting idea, and seems reasonably justified and well-explored in the paper, though this reviewer is no expert in this area, and not familiar with the prior work.': 1.0986123085021973, 'Paper is fairly clear.': 1.098454236984253, 'Performance evaluation (in simulation) is on a reasonable range of recent image conv-nets, and seems thorough enough.': 1.0985406637191772, 'Rather specialized application area may have limited appeal to ICLR audience.': 1.0011003017425537, '(hence the ""below threshold rating"", I don\'t have any fundamental structural / methodological criticism for this paper.)': 0.9948282241821289, 'Improve your bibliography citation style - differentiate between parenthetical citations and inline citations where only the date is in parentheses.': 1.0944976806640625}"
97,https://openreview.net/forum?id=By1snw5gl,"{'The paper proposes a new second-order method L-SR1 to train deep neural networks.': 1.0986117124557495, 'It is claimed that the method addresses two important optimization problems in this setting: poor conditioning of the Hessian and proliferation of saddle points.': 1.0986028909683228, 'The method can be viewed as a concatenation of SR1 algorithm of Nocedal & Wright (2006) and limited-memory representations Byrd et al. (1994).': 1.0966126918792725, 'First of all, I am missing a more formal, theoretical argument in this work (in general providing more intuition would be helpful too), which instead is provided in the works of Dauphin (2014) or Martens.': 1.0984022617340088, 'The experimental section in not very convincing considering that the performance in terms of the wall-clock time is not reported and the advantage over some competitor methods is not very strong even in terms of epochs.': 1.0986123085021973, 'I understand that the authors are optimizing their implementation still, but the question is: considering the experiments are not convincing, why would anybody bother to implement L-SR1 to train their deep models?': 1.098611831665039, 'The work is not ready to be published.': 1.098556399345398, 'L-SR1 seems to have O(mn) time complexity.': 1.0899828672409058, 'I miss this information in your paper.': 1.0986123085021973, 'Your experimental results suggest that L-SR1 does not outperform Adadelta (I suppose the same for Adam).': 0.9477488398551941, 'Given the time complexity of L-SR1, the x-axis showing time would suggest that L-SR1 is much (say, m times) slower.': 1.0986123085021973, '""The memory size of 2 had the lowest minimum test loss over 90"" suggests that the main driven force of L-SR1': 0.6987965106964111, 'was its momentum, i.e., the second-order information was rather useless.': 0.3821200728416443, 'It is an interesting idea to go after saddle points in the optimization with an SR1 update and a good start in experiments, but missing important comparisons to recent 2nd order optimizations such as Adam, other Hessian free methods (Martens 2012), Pearlmutter fast exact multiplication by the Hessian.': 1.0986123085021973, 'From the mnist/cifar curves it is not really showing an advantage to AdaDelta/Nag (although this is stated), and much more experimentation is needed to make a claim about mini-batch insensitivity to performance, can you show error rates on a larger scale task?': 1.0986123085021973}"
98,https://openreview.net/forum?id=By5e2L9gl,"{'This paper presents a novel layer-wise optimization approach for learning CNN with piecewise linear nonlinearities.': 1.0986121892929077, 'The proposed approach trains piecewise linear CNNs layer by layer and reduces the sub-problem into latent structured SVM, which has been well-studied in the literature.': 1.0965296030044556, 'In addition, the paper presents improvements of the BCFW algorithm used in the inner procedure.': 1.098605990409851, 'Overall, this paper is interesting.': 0.5349007844924927, 'However, unfortunately, the experiment is not convincing.': 0.46693912148475647, 'Pros:': 1.0777215957641602, 'To my best knowledge, the proposed approach is novel, and the authors provide nice theoretical analysis.': 1.0979948043823242, 'The paper is well-written and easy to follow.': 1.098597764968872, 'Cons:': 1.0587913990020752, 'Although the proposed approach can be applied in general structured prediction problem, the experiments only conduct on a simple multi-class classification task.': 1.0986121892929077, 'This makes this work less compelling.': 0.6761895418167114, ""The test accuracy performance on CIFAR-10 reported in the paper doesn't look right."": 1.0954426527023315, 'The accuracy of the best model reported in this paper is 70.2% while existing work often reports 90+%.': 1.0985828638076782, 'For example, https://arxiv.org/pdf/1412.6806.pdf showed an accuracy of 91% without data augmentation.': 1.0981110334396362, 'Also, CIFAR-10 is a relatively small dataset,': 1.0264832973480225, 'Other comments:': 0.44231581687927246, 'If I understand correctly, BCFW only guarantees monotonically increasing in the dual objective and does not have guarantees on the primal objective.': 1.0980162620544434, 'Especially, in practice, the inner optimization process often stops pretty early (i.e., stops when duality gap is still large).': 1.0018383264541626, 'Therefore, when putting them together, the CCCP procedure may not monotonically decrease as the inner procedure is only solved approximately.': 1.096957802772522, 'The authors should add this note when they discuss the properties of their algorithm.': 1.0986121892929077, 'A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems.': 1.0986121892929077, 'Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed.': 1.0986121892929077, 'Summary:': 1.0986121892929077, '———': 1.0976749658584595, 'I think the discussed insights are very interesting but not presented convincingly.': 1.098611831665039, 'Firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended.': 1.0986123085021973, 'In summary, I think the paper requires some more attention to form a compelling story.': 1.0986123085021973, 'Quality: I think some of the techniques could be described more carefully to better convey the intuition.': 1.098583698272705, 'At times apples are compared to oranges, e.g., back propagation is contrasted with CCCP.': 1.0962709188461304, 'Clarity: Some of the derivations and intuitions could be explained in more detail.': 1.0986123085021973, 'Originality: The suggested idea is reasonable albeit heuristics are required.': 1.0986123085021973, 'Significance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.': 1.0986123085021973, 'Details:': 1.0986123085021973, '————': 1.0986123085021973, '1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation.': 1.0264238119125366, '2. In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks (https://openreview.net/pdf?id=ROVmA279BsvnM0J1IpNn) ICLR Workshop Track 2016; which should probably be mentioned.': 0.5998879671096802, '3. I think the comparison between backprop and the discussed CCCP approach is not really appropriate. Note that backprop is a mechanism to compute gradients and is not at all related to any optimization technique/algorithm. This means that backprop combined with Armijo line-search would for example result in convergence guarantees. Hence a statement like `one of the main advantages of our approach compared to back propagation and its variants, which fail to provide similar guarantees on the value of the objective function from one iteration to the next’ is according to my opinion superficial.': 0.8519642353057861, '4. More justification regarding the argument that search for the step-size is a disadvantage seems necessary. There is evidence that noise introduced by mini-batches and inaccurate/no line search is actually beneficial. In contrast the proposed CCCP procedure may converge pre-maturely to a local optimum close to the initialization. Since mini-batches are used no guarantees are available in any case. Hence I think additional evidence for the fact that such convergence guarantees don’t result in premature stopping seems necessary.': 0.9325690865516663, '5. The observation 1 required to convert the layer wise procedure into an SVM seems rather ad-hoc. Note that you can easily construct examples where Eq. (9) and Eq. (7) differ significantly.': 0.5444799661636353, '6. I personally think the experimental evaluation is conducted on examples that are too small and some results are obvious. E.g., LW-SVM always improves over the solution of the SGD algorithm. If the authors were to use backtracking line search they could also improve upon LW-SVM.': 0.8163149356842041, 'This paper proposes a new approaches for optimizing the objective of CNNs.': 1.0891669988632202, 'The proposed method uses a lay-wise optimization, i.e. at each step, it optimizes the parameters in one layer of CNN while fixing the parameters in other layers.': 1.0986114740371704, 'The key insight of this paper is that, for a large class of CNNs, the optimization problem at a particular can be formulated as optimizing a piecewise linear (PL) function.': 1.0986040830612183, 'This PL function optimization happens to be the optimization problem commonly encountered in latent structural SVM.': 1.0938714742660522, 'This connection allows this paper to borrows ideas from the latent structural SVM literature, in particular concave-convex procedure, to learn the parameters of CNNs.': 1.098594307899475, 'Overall, the paper is well-written.': 1.0886627435684204, 'Traditional, CNNs and structural SVMs are almost two separate research communties.': 0.5069706439971924, 'The connection of CNNs to latent structural SVM is interesting, and might bridge the gap and facilitate the transferring of ideas between these two camps.': 1.0986123085021973, 'Of course, the proposed method also has some limitations.': 0.999391496181488, '1) It is limited to layer-wise optimization.': 0.46530619263648987, 'Nowadays layer-wise optimization is essentially a coordinate descent algorithm and is not really a competitive strategy in learning CNNs.': 1.0986109972000122, 'When you choose layer-wise optimization, you already lose something in terms of optimizing the objective (since you are using coordinate descent, instead of gradient descent).': 1.0883491039276123, 'Of course, you also gain something since now you can guarantee that each coordinate descent step always improve the objective.': 1.0001020431518555, 'It is not clear to me how the loss/gain balances each other.': 1.0874464511871338, '2)': 1.0986123085021973, 'This paper focues on improving the optimization of CNN objective.': 1.0980802774429321, 'However, we all know that a better objective does not necessarily correspond to a good model (e.g. due to overfitting).': 0.7811741828918457, 'Although the SGD with backprop in standard CNN learning does not always improve the solution of the objective (unlike the proposed method in this paper), but to me, this might be a good thing since it can prevent overfitting (the goal of learning is not to get better solution for the optimization problem in the first place': 1.0949304103851318, 'the optimization problem is merely a proxy to learn a model with good generalization ability).': 1.0986123085021973, 'The experiment is a bit weak.': 1.0986123085021973, '1) Only CIFAR10 is used.': 1.0986123085021973, ""This is a very small dataset by today's standard, while CNNs are typically used in large-scale datasets, such as ImageNet."": 1.0986123085021973, 'It is not clear whether the conclusions of this paper still hold when applied on ImageNet.': 1.0986123085021973, 'This paper only compares with a crippled variant of SGD (without batch normalization, dropout, etc).': 1.0986123085021973, 'Although this paper mentions that the reason is that it wants to focus on optimization.': 1.0986123085021973, 'But I mentioned earlier, SGD is not designed to purely obtain the best solution that optimizes the objective, the goal of SGD is to reasonably optimize the objective, while preventing overfitting.': 1.0986123085021973, 'So the comparison to SGD purely in terms of the optimization is that meaningful in the first place.': 1.0986123085021973}"
99,https://openreview.net/forum?id=ByBwSPcex,"{'In this paper, the authors build music-theoretical structure directly into a music generating LSTM.': 1.0986123085021973, 'Even though such simple rules should be learnable from data, this surely is a neat idea for the limited-data regime.': 1.0986123085021973, 'Further, it seems like a desirable model when thinking about musicians, for instance, wanting to train on own (and thus limited) source material.': 1.0986123085021973, 'They consider a dataset of 100 hours of midi and add multiple priors, drawn from basic music theory.': 1.0986123085021973, 'The priors as such are OK, but some of them seem rather heuristic and should, in my opinion, be learned from data and it should be discussed how the performance changes if you remove them.': 1.0986123085021973, 'Further, the authors evaluate their study on an artificial benchmark, consisting of a behavioral experiment where 27 subjects judge songs generated by Magenta and their approach in a questionable side-by-side evaluation.': 1.0986123085021973, 'Using this as a performance criterion is problematic, as no details about the subjects are given away and no attempt is made to assess the statistical significance of such results, let alone discussing the difficulty of pairing the songs.': 1.0986123085021973, 'Further, I assume there are standard behavioral batteries, concerned with assessing music preferences that should have been used or at least addressed.': 1.0986123085021973, 'Introducing the neural Karaoke and dancing is fun, but does not have much scientific value at this point, as it does not seem to work in a meaningful way.': 1.0986123085021973, 'I would recommend to either improve results of the latter drastically or add it as an extra to a blog post and remove it from the paper.': 1.0986123085021973, 'It is good that the authors make an attempt to encode general prior knowledge into their architecture, but I am not convinced by the results and the heuristic choices being made.': 1.0986123085021973, 'Further, it is still not 100% clear to me how the weighted probability distribution is constructed for the scales and how strong the prior it effectively incorporates is.': 1.0986123085021973, 'If it is very strong, it is not surprising to me, that the songs sound relatively coherent, as in scale playing with rejected outliers have to sound somewhat coherent.': 1.0899474620819092, 'I am not familiar enough with the Magenta baseline system and it is problematic that the baseline is not explained well.': 0.6176477670669556, 'If the baseline does not take explicit scale priors into account, it does make sense that it sounds less coherent by definition.': 1.098324179649353, 'This has to be discussed and the effect of the introduced priors has to be evaluated.': 1.0897216796875, 'Finally, the question remains if this will generalize to datasets with more than 4 dominant scales and why the authors chose their thresholds the way they did.': 1.0949581861495972, 'Did the model perform worse if one chooses to include more scales?': 1.0984598398208618, 'How do you know, that the heavy tail of such a distribution is not desirable and important for natural sounding music, did you investigate this?': 0.7347394227981567, 'The multitrack idea is great.': 1.0986123085021973, 'However, I am not convinced it works in this case.': 1.0986119508743286, 'The results sound more like assigning a couple of notes in the melodies to rhythmic sounds, but they do not interact with the melody, they just move along, as if they were part of the melody.': 1.075081467628479, 'This is not how rhythm works in music in most cases.': 1.0986119508743286, 'Pro:': 1.0986123085021973, '+ Incorporating general musical knowledge into the learned network is a good idea and non-trivial.': 1.0984587669372559, '+ Idea to introduce a behavioral measure for the quality of the generated samples is useful, as music is very subjective.': 1.098578691482544, '+': 1.0986123085021973, 'The multitrack idea seems useful and a clear step beyond Magenta as far as I understand.': 1.0980192422866821, 'Con:': 1.0986123085021973, 'However, the multitrack part of the architecture does not seem to work properly, rhythm does not seem to behave differently from melodic movement.': 0.5937874913215637, 'The music excerpts sound very simplistic and similar.': 1.0986123085021973, 'The pair-wise evaluation metric with 27 supposedly ""random"" subjects is not very meaningful and very likely, not significant.': 1.0986037254333496, 'Evaluation of generative models is difficult, but the authors could have done better.': 1.0986123085021973, 'The 4 bins of the random scale variable seem ad-hoc.': 1.0986111164093018, 'I have to emphasize that I like the ideas introduced in this paper, but I am not convinced by the way they are presented and evaluated.': 1.0985409021377563, 'I would like to suggest this paper for workshop publication.': 1.0986123085021973, 'The paper presents a recurrent neural network (RNN) for generating pop music.': 1.0985950231552124, 'The model is trained on 100 hours of user composed pop songs and video game music and the resulting music is evaluated in user studies against songs produced by the Magenta framework.': 1.0986123085021973, 'Overall, I find the paper to be well written and clear.': 0.7457471489906311, 'I appreciate the review early on of music theory concepts.': 1.0985759496688843, 'I think the paper provides a reasonable support for the connection between how pop music is composed and the hierarchical model for generating melody accompanied with chords and drums.': 1.0923216342926025, 'With some post-processing, the model appears to generate pleasant sounding music as judged by users and from a personal perspective of listening to the examples available on the web.': 1.0986052751541138, 'While the generated examples on the web sound pleasant, they also sound quite similar and make it hard to judge what the model has learned.': 1.0986111164093018, 'There are some open questions regarding evaluation of the model.': 1.0478289127349854, 'The paper would benefit from improvements in both user and metric evaluations.': 1.0986123085021973, '*  The Magenta system serves as a lower baseline for evaluation.': 1.0986123085021973, 'The study would benefit from an upper baseline by also evaluating against human composed songs.': 1.0986123085021973, 'This would help contextualize the findings for both this and future work.': 1.0986123085021973, '*  The user study could be improved by examining other dimensions of appeal, perhaps to gauge diversity through ""interestingness"" over a collection of samples.': 1.0986123085021973, '*  I think a paired/side-by-side design for the user study seems limited (examples on http://www.cs.toronto.edu/songfrompi/eval/eval.html).': 1.0986123085021973, 'A simpler design with rating one sample at a time may have been more appropriate because there is no natural way to pair the songs.': 1.0986123085021973, ""The examples from each system used in the experiment should be provided with labels or an answer key so that readers can compare the merits of each of the systems' compositions themselves."": 1.0986123085021973, '*  The authors propose specific metrics for insight into the diversity (longest subsequence and number of repeats).': 1.0986123085021973, 'These would be more meaningful with some context, e.g. by comparison with baseline Magenta samples and the training data (as an upper baseline).': 1.0986123085021973, '*  Details of the baseline Magenta system would also benefit the paper.': 1.0986123085021973, '*  No guidance is provided on how to judge the applications of neural singing, dancing and karaoke.': 1.0986123085021973, 'The paper describes a recurrent neural network model for generating pop music in the symbolic domain (i.e. MIDI).': 1.0692003965377808, 'The layers of the model each generate part of the output, with the first few layers responsible for generating the melody, and further layers generating drums and chords conditioned on the generated melody.': 1.09861159324646, 'The authors argue that this matches how pop music is usually composed.': 1.0986123085021973, 'The model is trained on 100+ hours of pop music in MIDI format.': 1.098586916923523, 'The resulting generated music is compared against that produced by another system using human evaluation, which is probably the only way in which such a system can be fairly evaluated.': 1.0986123085021973, 'I appreciate that the authors went through the trouble of setting up these experiments.': 1.0960004329681396, 'The RNNs generating the different outputs (i.e. key, duration, chord, melody) are trained in sequence, conditioned on the output of the previous step(s).I found the text a bit confusing at times as it initially seems to describe a single end-to-end trained model (even if this is never stated explicitly).': 1.0986123085021973, 'It only becomes clear later on that the layers are trained in sequence, with additional supervision provided at each stage.': 1.0986123085021973, ""This may simply be a personal bias because recent work on hierarchical RNNs that I've read has focused on end-to-end training, but nevertheless it might be useful to mention this more clearly from the get-go."": 0.41527584195137024, 'The post-processing of model samples described in the 2nd paragraph of Section 4.5 seems to affect results quite dramatically (based on the results in Table 1).': 1.0986123085021973, 'It seems equally applicable to the outputs of the Magenta system, so it might be interesting to compare this version to Magenta as well, to get an idea of how much it contributes to the improvement over the Magenta system.': 1.0986120700836182, 'It would be somewhat disappointing if it ends up accounting for most of the gain.': 1.0986123085021973, 'I am still unconvinced by the experiment described in the last paragraph of Section 5, where subsequences of generated music fragments are searched for in the training data.': 1.098592758178711, 'While I agree with the authors that minor differences in note choice can have profound effects on how the melody is perceived, I still think this is not particularly convincing, and I think drawing the unambiguous conclusion that ""our model is able to generate new music"" from this experiment is a bit premature.': 1.0984456539154053, 'The additional applications described in Section 6 feel a bit like an afterthought and the datasets used are probably too small for the results to be meaningful.': 1.0986123085021973, 'Instead I would have preferred to read about how to reduce the importance of prior knowledge in the design of the model.': 1.0986123085021973, 'Considering the venue this work was submitted to, moving towards a more ""end-to-end learning"" approach (rather than incorporating even more prior knowledge, as the conclusion seems to imply) seems like an interesting direction for future research.': 1.0986123085021973, ""Minor remark: giving the formulas for LSTM is probably a bit of a waste of space, especially if you're not explaining the semantics. A reference is sufficient, and in fact adding a reference to the original LSTM paper is probably a good idea regardless."": 1.0986123085021973}"
100,https://openreview.net/forum?id=ByC7ww9le,"{'The contribution of this paper can be summarized as:': 1.0986123085021973, '1, A TransGaussian model (in a similar idea of TransE) which models the subject / object embeddings in a parameterization of Gaussian distribution.': 1.0986123085021973, 'The model can be naturally adapted to path queries like the formulation of (Guu et al, 2015).': 1.0986123085021973, '2. Along with the entity / relation representations trained by TransGaussian, an LSTM + attention model is built on natural language questions, aiming at learning a distribution (not normalized though) over relations for question answering.': 1.0986123085021973, '3. Experiments on a generated WorldCup2014 dataset, focusing on path queries and conjunctive queries.': 1.0986123085021973, 'Overall, I think the Gaussian parameterization exhibits some nice properties, and could be suitable to KB completion and question answering.': 1.0986123085021973, 'However, some details and the main experimental results are not convincing enough to me.': 1.0986123085021973, 'The paper writing also needs to be improved.': 1.0986123085021973, 'More comments below:': 1.0986123085021973, '[Major comments]': 1.0986123085021973, 'My main concern is that that evaluation results are NOT strong.': 1.0986123085021973, 'Either knowledge base completion or KB-based question answering, there are many existing and competitive benchmarks (e.g., FB15k / WebQuestions).': 1.0986123085021973, 'Experimenting with such a tiny WordCup2014 dataset is not convincing.': 1.0515724420547485, 'Moreover, the questions are just generated by a few templates, which is far from NL questions.': 0.42403727769851685, 'I am not even not sure why we need to apply an LSTM in such scenario.': 1.0970380306243896, 'The paper would be much stronger if you can demonstrate its effectiveness on the above benchmarks.': 1.0985203981399536, 'Conjunctive queries:  the current model assumes that all the detected entities in the question could be aligned to one or more relations and we can take conjunctions in the end.': 1.0986101627349854, 'This assumption might be not always correct, so it is more necessary to justify this on real QA datasets.': 1.0873451232910156, 'The model is named as  “Gaussian attention” and I kind of think it is not very closely related to well-known attention mechanism, but more related to KB embedding literature.': 1.0125014781951904, '[Minor comments]': 1.0986123085021973, 'I find Figure 2 a bit confusing.': 1.0986123085021973, 'The first row of orange blocks denote KB relations, and the second row of those denote every single word of the NL question.': 0.504676878452301, 'Maybe make it clearer?': 1.0986123085021973, 'Besides “entity recognition”, usually we still need an “entity linker” component which links the text mention to the KB entity.': 1.057978868484497, 'This paper presents extensions to previous work using embeddings for modeling Knowledge Bases and performing Q&A on them, centered around the use of multivariate gaussian likelihood instead of inner products to score attention.': 1.0986123085021973, 'This is supposed to allow more control on the attention by dealing with its spread.': 1.0986123085021973, 'This is a dense paper centered around a quite complicated model.': 1.098611831665039, 'With the supplementary material, this makes a 16p paper.': 1.0986123085021973, 'It might be clearer to make 2 separate papers: one on KB completion and another one on Q&A.': 1.0986123085021973, 'I like the idea of controlling the spread of the attention.': 1.0986123085021973, 'This makes sense.': 1.0986123085021973, 'However, I do not feel that this paper is convincing enough to justify its use compared to usual inner products.': 1.0986123085021973, 'For several reasons:': 1.0986123085021973, 'These should be more ablation experiments to separate the different pieces of the model and study their influence separately.': 1.0986123085021973, 'The only interesting point in that sense is Table 8 in Appendix B.': 1.0986123085021973, 'We need more of this.': 1.0986123085021973, 'In particular, a canonical experiments comparing Gaussian interaction vs inner product would be very useful.': 1.0986123085021973, 'Experiments on existing benchmarks (for KB completion, or QA) would help.': 1.0986123085021973, 'I agree with the authors that it is difficult to find the perfect benchmark, so it is a good idea to propose a new one (WorldCup2014).': 1.0986123085021973, 'But this should come in addition to experiments on existing data.': 1.0986123085021973, 'Table 11 of Appendix C (page 16) that compares TransE and TransGaussian for the task of link prediction on WordNet can be seen as fixing the two points above (simple setting on existing benchmark).': 1.0986123085021973, 'Unfortunately, TransGaussian does not perform well compared to simpler TransE. This, along with the poor results of TransGaussian (SINGLE) of Table 2, indicate that training TransGaussian seems pretty complex, and hence question the actual validity of this architecture.': 1.098591685295105, 'SUMMARY.': 1.0986123085021973, 'The paper propose a new scoring function for knowledge base embedding.': 1.0986123085021973, 'The scoring function called TransGaussian is an novel take on (or a generalization of) the well-known TransE scoring function.': 1.0986002683639526, 'The proposed function is tested on two tasks knowledge-base completion and question answering.': 0.8862552642822266, 'OVERALL JUDGMENT': 1.089734435081482, 'While I think this proposed work is very interesting and it is an idea worth to explore further, the presentation and the experimental section of the paper have some problems.': 1.0986117124557495, 'Regarding the presentation, as far as I understand this is not an attention model as intended standardly in the literature.': 1.0986123085021973, 'Plus, it has hardly anything to share with memory networks/neural Turing machines, the parallel that the authors try to make is not very convincing.': 1.0986123085021973, 'Regarding the experimental section, for a fair comparison the authors should test their model on standard benchmarks, reporting state-of-the-art models.': 1.0986123085021973, 'Finally, the paper lack of discussion of results and insights on the behavior of the proposed model.': 1.0986123085021973, 'DETAILED COMMENTS': 1.0986123085021973, 'In section 2.2 when the authors calculate \\mu_{context} do not they loose the order of relations?': 1.0986123085021973, 'And if it is so, does it make any sense?': 1.0986123085021973}"
101,https://openreview.net/forum?id=ByEPMj5el,"{'This paper examines computational creativity from a machine learning perspective.': 0.8834009170532227, ""Creativity is defined as a model's ability to generate new types of objects unseen during training."": 1.3862040042877197, ""The authors argue that likelihood training and evaluation are by construction ill-suited for out-of-class generation and propose a new evaluation framework which relies on the use of held-out classes of objects to measure a model's ability to generate new and interesting object types."": 1.3862943649291992, ""I am not very familiar with the literature on computational creativity research, so I can't judge on how well this work has been put into the context of existing work."": 1.3862875699996948, 'From a machine learning perspective, I find the ideas presented in this paper new, interesting and thought-provoking.': 1.38474702835083, 'As I understand, the hypothesis is that the ability of a model to generate new and interesting types we *do not* know about correlates with its ability to generate new and interesting types we *do* know about, and the latter is a good proxy for the former.': 1.3861817121505737, 'The extent to which this is true depends on the bias introduced by model selection.': 1.3827775716781616, 'Just like when measuring generalization performance, one should be careful not to reuse the same held-out classes for model selection and for evaluation.': 1.3862935304641724, 'Nevertheless, I appreciate the effort that has been made to formalize the notion of computational creativity within the machine learning framework.': 1.3862665891647339, 'I view it as an important first step in that direction, and I think it deserves its place at ICLR, especially given that the paper is well-written and approachable for machine learning researchers.': 0.7686409950256348, 'First, the bad:': 1.3695350885391235, 'This paper is frustratingly written.': 1.3861353397369385, 'The grammar is fine, but:': 0.6911345720291138, '- The first four pages are completely theoretical and difficult to follow without any concrete examples. These sections would benefit greatly from a common example woven through the different aspects of the theoretical discussion.': 0.7548438906669617, '- The ordering of the exposition is also frustrating. I found myself constantly having to refer ahead to figures and back to details that were important but seemingly presented out of order. Perhaps a reordering of some details could fix this. Recommendation: give the most naturally ordered oral presentation of the work and then order the paper similarly.': 0.5393672585487366, 'Finally, the description of the experiments is cursory, and I found myself wondering whether the details omitted were important or not.': 1.381882905960083, 'Including experimental details in a supplementary section could help assuage these fears.': 1.3862524032592773, 'The good:': 1.3862943649291992, 'What the paper does well is to gather together past work on novelty generation and propose a unified framework in which to evaluate past and future models.': 1.3862943649291992, 'This is done by repurposing existing generative model evaluation metrics for the task of evaluating novelty.': 1.3862943649291992, 'The experiments are basic, but even the basic experiments go beyond previous work in this area (to this reviewer’s knowledge).': 1.3862943649291992, 'Overall I recommend the paper be accepted, but I strongly recommend rewriting some components to make it more digestible.': 1.3862943649291992, 'As with other novelty papers, it would be read thoroughly by the interested few, but it is likely to fight an uphill battle against the majority of readers outside the sub-sub-field of novelty generation; for this reason the theory should be made even more intuitive and clear and the experiments and results even more accessible.': 1.3862943649291992, 'The authors proposed an way to measure the generation of out-of-distribution novelty.': 1.384946346282959, 'Their methods implied, if a model trained on MNIST digits could generate some samples are more like letters judged by anther model trained both on MNIST and letters,  the model trained on MNIST could be seen as having  the ability to generate novel samples.': 1.386175513267517, 'Some empirical experiments were reported.': 1.3862943649291992, 'The novelty is hard to define.': 0.8919327855110168, 'The proposed metric is also problematic.': 1.3846733570098877, 'A naive combination of MNIST and letters dataset do not represent the natural distribution of handwritten digits and letters.': 1.3795021772384644, 'IT means that the model trained on the combination could not properly distinguished digits and letters.': 1.386183261871338, 'The proposed out-of-class count and out-of-class max are thus pointless.': 1.0972505807876587, 'For the ""novel"" samples in Fig. 3,  they are clearly digits.': 0.6791180372238159, 'I guess they quantize the samples to binary.': 1.3862941265106201, 'If they would quantize the samples to 8 bit, the resulting images would look even more like digits.': 1.3862888813018799, 'This paper proposed a quantitative metric for evaluating out-of-class novelty of samples from generative models.': 1.3862942457199097, 'The authors evaluated the proposed metric on over 1000 models with different hyperparameters and performed human subject study on a subset of them.': 1.3862943649291992, 'The authors mentioned difficulties in human subject studies, but did not provide details of their own setting.': 1.3862941265106201, 'An ""in-house"" annotation tool was used but it\'s unclear how many subjects were involved, who they are, and how many samples were presented to each subject.': 1.3862943649291992, ""I'm worried about the diversity in the subjects because there may be too few subjects who are shown too many samples and/or are experts in this field."": 1.3862943649291992, 'This paper aims at proposing a general metric for novelty but the experiments only used one setting, namely generating Arabic digits and English letters.': 1.3862943649291992, 'There is insufficient evidence to prove the generality of the proposed metric.': 1.3521664142608643, 'Moreover, defining English letters as ""novel"" compared to Arabic digits is questionable.': 1.3657549619674683, 'What if the model generates Arabic or Indian letters?': 1.380505084991455, 'Can a human who has never seen Arabic handwriting tell it from random doodle?': 0.6821901202201843, 'What makes English letters more ""novel"" than random doodle?': 1.386284589767456, 'In my opinion these questions are best answered through large scale human subject study on tasks that has clear real world meanings.': 1.3862943649291992, 'For example, do you prefer painting A (generated) or B (painted by artist).': 1.3862943649291992}"
102,https://openreview.net/forum?id=ByG4hz5le,"{'This paper presents a model for video captioning with both soft and hard attention, using a C3D network for the encoder and a RNN for the decoder.': 1.0983237028121948, 'Experiments are presented on YouTube2Text, M-VAD, and MSR-VTT.': 0.4009610116481781, 'While the ideas of image captioning with soft and hard attention, and video captioning with soft attention, have already been demonstrated in previous work, the main contribution here is the specific architecture and attention over different layers of the CNN.': 1.0983997583389282, 'The work is well presented and the experiments clearly show the benefit of attention over multiple layers.': 1.0986123085021973, 'However, in light of previous work in captioning, the contribution and resulting insights is too incremental for a conference paper at ICLR.': 1.098602533340454, 'Further experiments and analysis of the main contribution would strengthen the paper, but I would recommend resubmission to a more suitable venue.': 1.0986121892929077, 'The authors apply the image captioning architecture of Xu et al. 2015 to video captioning.': 1.0986123085021973, 'The model is extended to have attention over multiple layers of the ConvNet instead of just a single layer.': 1.0928192138671875, 'Experiments on YouTube2Text, M-VAD and MSR-VTT show that this works better than only using one of the layers at a time.': 0.7894493341445923, 'I think this is solid work on the level of a well-executed course project or a workshop paper.': 1.0986123085021973, 'The model makes sense, it is adequately described, and the experiments show that attending over multiple layers works better than attending over any one layer in isolation.': 1.0979875326156616, ""Unfortunately, I don't think there is enough to get excited about here from a technical perspective and it's not clear what value the paper brings to the community."": 1.0986123085021973, ""Other aspects of the paper, such as including the hard attention component, don't seem to add to the paper but take up space."": 1.0986121892929077, 'If the authors want to contribute a detailed, focused exploration of multi-level features this could become a more valuable paper, but in that case I would expect a much more thorough exploration of the choices and tradeoffs of different schemes without too many spurious aspects such as video features, hard attention, etc.': 1.0986123085021973, '1) Summary': 1.0986123085021973, 'This paper proposes a video captioning model based on a 3D (space+time) convnet (C3D) encoder and a LSTM decoder.': 1.098244547843933, 'The authors investigate the benefits of using attention mechanisms operating both at the spatio-temporal and layer (feature abstraction) levels.': 1.0986119508743286, '2) Contributions': 1.027359127998352, '+ Well motivated and implemented attention mechanism to handle the different shapes of C3D feature maps (along space, time, and feature dimensions).': 0.10971105098724365, '+ Convincing quantitative and qualitative experiments on three challenging datasets (Youtube2Text, M-VAD, MSR-VTT) showing clearly the benefit of the proposed attention mechanisms.': 0.5751157402992249, '+ Interesting comparison of soft vs hard attention showing a slight performance advantage for the (simpler) soft attention mechanism in this case.': 0.6248641014099121, '3) Suggestions for improvement': 1.076398491859436, 'Hypercolumns comparison:': 1.0986123085021973, 'As mentioned during pre-review questions, it would be interesting to compare to the hypercolumns of https://arxiv.org/abs/1411.5752, as they are an alternative to the proposed attention mechanisms, with the same purpose of leveraging different feature abstraction levels.': 1.0986123085021973, 'Minor clarifications in the text and figures as agreed with the authors in our pre-review discussions.': 1.098611831665039, '4) Conclusion': 1.096351981163025, 'Although the novelty with existing video captioning approaches is limited, the paper is relevant to ICLR, as the proposed simple but efficient implementation and benefits of spatio-temporal + feature abstraction attention are clearly validated in this work.': 1.098611831665039}"
103,https://openreview.net/forum?id=ByG8A7cee,"{'This paper presents a new type of language model that treats entity references as latent variables.': 1.0986123085021973, 'The paper is structured as three specialized models for three applications: dialog generation with references to database entries, recipe generation with references to ingredients, and text generation with coreference mentions.': 1.0986123085021973, 'Despite some opaqueness in details that I will discuss later, the paper does a great job making the main idea coming through, which I think is quite interesting and definitely worth pursuing further.': 1.0986123085021973, 'But it seems the paper was rushed into the deadline, as there are a few major weaknesses.': 1.0986123085021973, 'The first major weakness is that the claimed latent variables are hardly latent in the actual empirical evaluation.': 1.0986123085021973, 'As clarified by the authors via pre-review QAs, all mentions were assumed to be given to all model variants, and so, it would seem like an over-claim to call these variables as latent when they are in fact treated as observed variables.': 1.0986123085021973, 'Is it because the models with latent variables were too difficult to train right?': 1.0986123085021973, 'A related problem is the use of perplexity as an evaluation measure when comparing reference-aware language models to vanilla language models.': 1.0986123085021973, 'Essentially the authors are comparing two language models defined over different event space, which is not a fair comparison.': 1.0986123085021973, 'Because mentions were assumed to be given for the reference-aware language models, and because of the fact that mention generators are designed similar to a pointer network, the probability scores over mentions will naturally be higher, compared to the regular language model that needs to consider a much bigger vocabulary set.': 1.0986123085021973, 'The effect is analogous to comparing language models with aggressive UNK (and a small vocabulary set) to a language models with no UNK (and a much larger vocabulary set).': 1.0986123085021973, 'To mitigate this problem, the authors need to perform one of the following additional evaluations: either assuming no mention boundaries and marginalizing over all possibilities (treating latent variables as truly latent), or showing other types of evaluation beyond perplexity, for example, BLEU, METEOR, human evaluation etc on the corresponding generation task.': 1.0986123085021973, 'The other major weakness is writing in terms of technical accuracy and completeness.': 1.0986117124557495, 'I found many details opaque and confusing even after QAs.': 1.0974540710449219, 'I wonder if the main challenge that hinders the quality of writing has something to do with having three very specialized models in one paper, each having a lot of details to be worked out, which may have not been extremely important for the main story of the paper, but nonetheless not negligible in order to understand what is going on with the paper.': 1.0694661140441895, 'Perhaps the authors can restructure the paper so that the most important details are clearly worked out in the main body of the paper, especially in terms of latent variable handling — how to make mention detection and conference resolution truly latent, and if and when entity update helps, which in the current version is not elaborated at all, as it is mentioned only very briefly for the third application (coreference resolution) without any empirical comparisons to motivate the update operation.': 1.098587155342102, 'This paper explores 3 language modeling applications with an explicit modeling of reference expressions: dialog, receipt generation and coreferences.': 1.0986123085021973, 'While these are important tasks for NLP and the authors have done a number of experiments, the paper is limited for a few reasons:': 1.0986114740371704, '1. This paper is not clearly written and is pretty hard to follow some details. In particular,  there are many obvious math errors, such as missing the marginalization sum in Eq (1), and P(z_{i,v}...) = 1 (should be 0 here) on page 5, pointer switch section.': 0.8677741885185242, '2. The major novelty seems to be the 2-dimensional attention from the table and the pointer to the 2-D table. These are more of a customization of existing work to a particular task with 2-D tables as a part of the input to seq2seq model with both attentions and pointer networks.': 0.8307607173919678, '3. The empirical results are not very conclusive yet, limited by either the relatively small data size, or the lack of well-established baseline for some new applications (e.g., the recipe generation task).': 0.7899277806282043, 'Overall, this paper, as it is for now, is more suitable for a workshop rather than for the main conference.': 1.0985960960388184, 'This paper introduces pointer-network neural networks, which are applied to referring expressions in three small-scale language modeling tasks: dialogue modeling, recipe modeling and news article modeling.': 1.0986121892929077, 'When conditioned on the co-reference chain, the proposed models outperform standard sequence-to-sequence models with attention.': 1.0986121892929077, 'The proposed models are essentially variants of pointer networks with copy mechanisms (Gulcehre et al., 2016; Gu et al., 2016; Ling et al., 2016), which have been modified to take into account reference chains.': 1.0258625745773315, 'As such, the main architectural novelty lies in 1) restricting the pointer mechanism to focus on co-referenced entities, 2) applying pointer mechanism to 2D arrays (tables), and 3) training with supervised alignments.': 1.0986123085021973, 'Although useful in practice, these are minor contributions from an architectural perspective.': 1.0986123085021973, 'The empirical contributions are centred around measuring perplexity on the three language modeling tasks.': 1.0985795259475708, 'Measuring perplexity is typical for standard language modeling tasks, but is really an unreliable proxy for dialogue modeling and recipe generation performance.': 1.0986109972000122, 'In addition to this, both the dialogue and recipe tasks are tiny compared to standard language modeling tasks.': 1.0959558486938477, 'This makes it difficult to evaluate the impact of the dialogue and recipe modeling results.': 0.8675852417945862, 'For example, if one was to bootstrap from a larger corpus, it seems likely that a standard sequence-to-sequence model with attention would yield performance comparable to the proposed models (with enough data, the attention mechanism could learn to align referring entities by itself).': 1.0986071825027466, 'The language modeling task on news article (Gigaword) seems to yield the most conclusive results.': 1.0986123085021973, 'However, the dataset for this task is non-standard and results are provided for only a single baseline.': 1.0986123085021973, 'Overall, this limits the conclusions we can draw from the empirical experiments.': 1.0986123085021973, 'Finally, the paper itself contains many errors, including mathematical errors, grammatical errors and typos:': 1.0986123085021973, 'Eq. (1) is missing a sum over .': 1.0986123085021973, '""into the a decoder LSTM"" -> ""into the decoder LSTM""': 1.0986123085021973, '""denoted as his"" -> ""denoted as""': 1.0986123085021973, '""Surprising,"" -> ""Surprisingly,""': 1.0986123085021973, '""torkens"" -> ""tokens""': 1.0986123085021973, '""if follows that the next token"" -> ""the next token""': 1.0986123085021973, 'In the ""COREFERENCE BASED LANGUAGE MODEL"" sub-section, what does  denote?': 1.0986123085021973, 'In the sentence: ""The attribute of each column is denoted as cs_cp(z_{i,v} |s_{i,v})': 1.0986123085021973, '= 1p(z_{i,v} |s_{i,v})': 1.0986123085021973, '= 0$.': 1.0986123085021973, 'In the ""Table Pointer"" paragraph, I assume you mean outer-product instead of cross-product?': 1.0986123085021973, ""Otherwise, I don't see how the equations add up."": 1.0986123085021973, 'Other comments:': 1.0986123085021973, 'For the ""Attention based decoder"", is the attention computed using the word embeddings themselves or the hidden states of the sentence encoder?': 1.0986123085021973, 'Also, it applied only to the previous turn of the dialogue or to the entire dialogue history?': 1.0986123085021973, 'Please clarify this.': 1.0986123085021973, 'What\'s the advantage of using an ""Entity state update"" rule, compared to a pointer network or copy network, which you used in the dialogue and recipe tasks?': 1.0986123085021973, 'Please elaborate on this.': 1.0986123085021973, 'In the Related Work section, the following sentence is not quite accurate: ""For the task oriented dialogues, most of them embed the seq2seq model in traditional dialogue systems while our model queries the database directly.': 1.0986119508743286, '"".': 1.0986123085021973, 'There are task-oriented dialogue models which do query databases during natural language generation.': 1.0986123085021973, 'See, for example, ""A Network-based End-to-End Trainable Task-oriented Dialogue System"" by Wen et al.': 1.0986123085021973}"
104,https://openreview.net/forum?id=ByIAPUcee,"{'This paper focusses on attention for neural language modeling and has two major contributions:': 1.0986123085021973, '1. Authors propose to use separate key, value, and predict vectors for attention mechanism instead of a single vector doing all the 3 functions. This is an interesting extension to standard attention mechanism which can be used in other applications as well.': 0.5496375560760498, '2. Authors report that very short attention span is sufficient for language models (which is not very surprising) and propose an n-gram RNN which exploits this fact.': 1.0986123085021973, 'The paper has novel models for neural language modeling and some interesting messages.': 1.0986123085021973, 'Authors have done a thorough experimental analysis of the proposed ideas on a language modeling task and CBT task.': 1.0986123085021973, 'I am convinced with authors’ responses for my pre-review questions.': 1.0986123085021973, 'Minor comment: Ba et al., Reed & de Freitas, and Gulcehre et al. should be added to the related work section as well.': 1.0985727310180664, 'This paper explores a variety of memory augmented architectures (key, key-value, key-predict-value) and additionally simpler near memory-less RNN architectures.': 1.0986123085021973, 'Using an attention model that has access to the various decompositions is an interesting idea and one worth future explorations, potentially in different tasks where this type of model could excel even more.': 1.0986123085021973, 'The results over the Wikipedia corpus are interesting and feature a wide variety of different model types.': 1.0986123085021973, 'This is where the models suggested in the paper are strongest.': 1.0986123085021973, 'The same models run over the CBT dataset show a comparable but less convincing demonstration of the variations between the models.': 1.0986123085021973, 'The authors also released their Wikipedia corpus already.': 1.0986123085021973, 'Having inspected it I consider it a positive and interesting contribution.': 1.0986123085021973, 'I still believe that, if a model was found that could better handle longer term dependencies, it would do better on this Wikipedia dataset, but at least within the realm of what .': 1.0986123085021973, 'As an example, the first article in train.txt is about a person named ""George Abbot"", yet ""Abbot"" isn\'t mentioned again until the next sentence 40 tokens later, and then the next ""Abbot"" is 15 tokens from there.': 1.0986123085021973, 'Most gaps between occurrences of ""Abbot"" are dozens of timesteps.': 1.0986123085021973, 'Performing an analysis based upon easily accessed information, such as when the same token reappears again or average sentence length, may be useful as an approximation for the length that an attention window may prefer.': 1.0986123085021973, 'This is a well explained paper that raises interesting questions regarding the spans used in existing language modeling approaches and serves as a potential springboard for future directions.': 1.0986123085021973, 'The paper presents an investigation of various neural language models designed to query context information from their recent history using an attention mechanism.': 1.0986123085021973, 'The authors propose to separate the attended vectors into key, value and prediction parts.': 1.0940494537353516, 'The results suggest that this helps performance.': 1.028326153755188, 'The authors also found that a simple model which which concatenates recent activation vectors performs at a similar level as the more complicated attention-based models.': 1.0986123085021973, 'The experimental methodology seems sound in general.': 1.04791259765625, 'I do have some issues with the way the dimensionality of the vectors involved in the attention-mechanism is chosen.': 0.9327811598777771, 'While it’s good that the hidden layer sizes are adapted to ensure similar numbers of trainable parameters for all the models, this doesn’t control for the fact that key/value/prediction vectors of a higher dimensionality may simply work better regardless of whether their dimensions are dedicated to one particular task or used together.': 1.0986123085021973, 'This separation clearly saves parameters but there could also be benefits of having some overlap of information assuming that vectors that lead to similar predictions may also be required in similar contexts for example.': 1.0986123085021973, 'Some tasks may also require more dimensions than others and the explicit separation prevents the model from discovering and exploiting this.': 1.0986123085021973, 'While memory augmented RNNs and RNNs with attention mechanisms are not new, some of these architectures had not yet been applied to language modeling.': 1.098375678062439, 'Similarly (and as acknowledged by the authors), the strategy of separating key and value functionality has been proposed before, but not in the context of natural language modeling.': 1.0986123085021973, 'I’m not sure about the novelty of the proposed n-gram RNN because I recall seeing similar architectures before but I understand that novelty was not the point of that architecture as it mainly serves as a proof of the lack of ability of the more complicated architectures to do better.': 1.0986052751541138, 'In that sense I do consider it an inventive baseline that could be used in future work to test the ability of other models that claim to exploit long-term dependencies.': 1.0974076986312866, 'The exact computation of the representation h_t was initially not that clear to me (the terms hidden and output can be ambiguous at times) but besides this, the paper is quite clear and generally well-written.': 1.0966777801513672, 'The results in this paper are important because they show that learning long-term dependencies is not a solved problem by any means.': 1.0986123085021973, 'The authors provide a very nice comparison to prior results and the fact that their n-gram RNN is often at least competitive with far more complicated approaches is a clear indication that some of those methods may not capture as much context information as previously thought.': 1.0948967933654785, 'The success of the separation of key/value/prediction functionality in attention-based system is also noteworthy although I think this is something that needs to be investigated more thoroughly (i.e., with more control for hyperparameter choices).': 1.0985878705978394, 'Pros:': 1.0986123085021973, 'Impressive and also interesting results.': 1.0986123085021973, 'Good comparison with earlier work.': 1.0986123085021973, 'The n-gram RNN is an interesting baseline.': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'The relation between the attention-mechanism type and the number of hidden units weakens the claim that the key/value/prediction separation is the reason for the increase in performance somewhat.': 1.0986123085021973, 'The model descriptions are not entirely clear.': 1.0986123085021973, 'I would have liked to have seen what happens when the attention is applied to a much larger context size.': 1.0986123085021973}"
105,https://openreview.net/forum?id=ByOK0rwlx,"{'This paper addresses to reduce test-time computational load of DNNs.': 1.0979386568069458, 'Another factorization approach is proposed and shows good results.': 1.0986123085021973, 'The comparison to the other methods is not comprehensive, the paper provides good insights.': 1.0977550745010376, 'I do need to see the results in a clear table.': 1.0986123085021973, 'Original results and results when compression is applied for all the tasks.': 1.0293009281158447, 'In any case, i would like to see the results when the compression is applied to state of the art nets where the float representation is important.': 1.098594307899475, 'For instance a network with 0.5% - 0.8% in MNIST.': 1.0970509052276611, 'A Imagenet lower that 5% - 10%.': 1.0976426601409912, 'Some of this results are feasible with float representation but probably imposible for restricted representations.': 1.0986123085021973, 'This paper explores a new quantization method for both the weights and the activations that does not need re-training.': 1.0986111164093018, 'In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x.': 1.0985840559005737, 'The paper is very well written and clearly exposes the details of the methodology and the results.': 1.0919164419174194, 'My major criticisms are three-fold: for one, the results are not compared to one of the many other pruning methods that are described in section 1.1, and as such the performance of the method is difficult to judge from the paper alone.': 1.0986123085021973, 'Second, there have been several other compression schemes involving pruning, re-training and vector-quantization': 1.0986123085021973, '[e.g. 1, 2, 3] that seem to achieve much higher accuracies, compression ratios and speed-ups.': 0.6105464696884155, 'Hence, for the practical application of running such networks on low-power, low-memory devices, other methods seem to be much more suited.': 1.0986119508743286, 'The advantage of the given method - other then possibly reducing the time it takes to compress the network - is thus unclear.': 1.09861159324646, 'In particular, taking a pre-trained network as a starting point for a quantized model that is subsequently fine-tuned might not take much longer to process then the method given here (but maybe the authors can quantify this?).': 1.0986123085021973, 'Finally, much of the speed-up and memory reduction in the VGG-model seems to arise from the three fully-connected layers, in particular the last one.': 1.0986123085021973, 'The speed-up in the convolutional layers is comparably small, making me wonder how well the method would work in all-convolutional networks such as the Inception architecture.': 1.0986123085021973, '[1] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding, https://arxiv.org/abs/1510.00149': 1.0986123085021973, '[2] Compressing Deep Convolutional Networks using Vector Quantization, https://arxiv.org/abs/1412.6115': 1.0986123085021973, '[3] XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks, https://arxiv.org/abs/1603.05279': 1.0986123085021973}"
106,https://openreview.net/forum?id=ByOvsIqeg,"{'The author proposed a simple but yet effective technique in order to regularized neural networks.': 1.0986123085021973, 'The results obtained are quite good and the technique shows to be effective when it it applied even on state of the art topologies, that is welcome because some regularization techniques used to be applied in easy task or on a initial configuration which results are still far from the best known results.': 1.0986123085021973, 'The paper proposes a new regulariser for CNNs that penalises positive correlations between feature weights, but does not affect negative correlations.': 1.0986121892929077, 'An alternative version which penalises all correlations regardless of sign is also considered.': 1.0985987186431885, 'The paper refers to these as ""local"" and ""global"" respectively, which I find a bit confusing as these are very general terms that can mean a plethora of things.': 1.0986084938049316, 'The experimental validation is quite rigorous.': 1.0814335346221924, 'Several experiments are conducted on benchmark datasets (MNIST, CIFAR-10, CIFAR-100, SVHN) and improvements are demonstrated in most cases.': 0.5022801756858826, 'While these improvements may seem modest, the baselines are already very competitive as the authors pointed out.': 1.0813982486724854, 'In some cases it does raise some questions about statistical significance though.': 0.4768579304218292, 'More results with the global regulariser (i.e. not just on MNIST) would have been interesting, as the main novelty in the paper seems to be leaving the negative correlations alone, so it would be interesting to see exactly how much of a difference this makes.': 1.0982111692428589, 'One of my main concerns is ambiguity stemming from the fact that the paper sometimes discusses activations and sometimes filter weights, but refers to both as ""features"".': 1.0986123085021973, 'However, the authors have already said they will address this.': 1.098258137702942, 'The paper somewhat ignores interactions with the choice of nonlinearity, which seems like it could be very important; especially because the goal is to obtain feature activations that are uncorrelated, and this is done only by applying a penalty to the weights (i.e. in a data-agnostic way and also ignoring any nonlinearity).': 1.0985908508300781, 'I believe the authors already mentioned in their responses to reviewer questions that this would be addressed, but I think this important': 0.9502288699150085, 'and it definitely needs to be discussed.': 0.41588878631591797, 'In response to the authors\' answer to my question about the role of biases: as they point out, it is perfectly possible to combine their proposed technique with the ""multi-bias"" approach, but this was not really my point.': 1.0973402261734009, 'Rather, the latter is an example that challenges the idea that features should not be positively correlated / redundant, which seems to be the assumption that this work is built upon.': 1.0131261348724365, ""My current intuition is that it's okay to have correlated features, as long as you're not wasting model capacity on them."": 1.0502594709396362, 'This is the case for ""multi-bias"", seeing as the weights are shared across sets of correlated features.': 0.9807627201080322, ""The dichotomy between regularisation methods that reduce capacity and those that don't which is described in the introduction seems a bit arbitrary to me, especially considering that weight decay is counted among the former and the proposed method is counted among the latter."": 0.7431251406669617, 'I think this very much depends on ones definition of model capacity (clearly weight decay does not actually reduce the number of parameters in a model).': 1.0766717195510864, 'Overall, the work is perhaps a bit incremental, but it seems to be well-executed.': 0.9953781366348267, ""The results are convincing, even if they aren't particularly ground-breaking."": 1.098527431488037, 'Encouraging orthogonality in weight features has been reported useful for deep networks in many previous works.': 1.0986114740371704, 'The authors present a explicit regularization cost to achieve de-correlation among weight features in a layer and encourage orthogonality.': 1.0986123085021973, 'Further, they also show why and how negative correlations can and should be avoided for better de-correlation.': 1.0939823389053345, 'Orthogonal weight features achieve better generalization in case of large number of trainable parameters and less training data, which usually results in over-fitting.': 1.0986123085021973, 'As also mentioned by the authors biases help in de-correlation of feature responses even in the presence of correlated features (weights).': 1.0985885858535767, 'Regularization techniques like OrthoReg can be more helpful in training deeper and leaner networks, where the representational capacity of each layer is low, and also generalize better.': 0.43476030230522156, 'Although the improvement in performances is not significant the direction of research and the observations made are promising.': 1.0918041467666626}"
107,https://openreview.net/forum?id=ByQPVFull,"{'This paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group.': 1.0986121892929077, 'The technique is applied in the setting of image classification with “privileged information” in the form of foreground segmentation masks, where the model is trained to learn orthogonal groups of foreground and background features using the correlation penalty and an additional “background suppression” term.': 1.098611831665039, 'Pros:': 1.0986123085021973, 'Proposes a “group-wise model diversity” loss term which is novel, to my knowledge.': 0.22842341661453247, 'The use of foreground segmentation masks to improve image classification is also novel.': 1.0986121892929077, 'The method is evaluated on two standard and relatively large-scale vision datasets: ImageNet and PASCAL VOC 2012.': 1.0985462665557861, 'Cons:': 1.097835659980774, 'The evaluation is lacking.': 1.0980092287063599, 'There should be a baseline that leaves out the background suppression term, so readers know how much that term is contributing to the performance vs. the group orthogonal term.': 1.0986123085021973, 'The use of the background suppression term is also confusing to me': 1.0986003875732422, 'it seems redundant, as the group orthogonality term should already serve to suppress the use of background features by the foreground feature extractor.': 1.084800362586975, 'It would be nice to see the results with “Incomplete Privileged Information” on the full ImageNet dataset (rather than just 10% of it) with the privileged information included for the 10% of images where it’s available.': 0.8742897510528564, 'This would verify that the method and use of segmentation masks remains useful even in the regime of more labeled classification data.': 1.0986055135726929, 'The presentation overall is a bit confusing and difficult to follow, for me.': 1.0655263662338257, 'For example, Section 4.2 is titled “A Unified Architecture: GoCNN”, yet it is not an overview of the method as a whole, but a list of specific implementation details (even the very first sentence).': 1.0965306758880615, 'Minor: calling eq 3 a “regression loss” and writing “||0 - x||” rather than just “||x||” is not necessary and makes understanding more difficult': 1.0986075401306152, 'I’ve never seen a norm regularization term written this way or described as a “regression to 0”.': 1.0986123085021973, 'Minor: in fig. 1 I think the FG and BG suppression labels are swapped: e.g., the “suppress foreground” mask has 1s in the FG and 0s in the BG (which would suppress the BG, not the FG).': 1.0982940196990967, 'An additional question: why are the results in Table 4 with 100% privileged information different from those in Table 1-2?  Are these not the same setting?': 1.0983234643936157, 'The ideas presented in this paper are novel and show some promise, but are currently not sufficiently ablated for readers to understand what aspects of the method are important.': 1.0810233354568481, 'Besides additional experiments, the paper could also use some reorganization and revision for clarity.': 1.0985932350158691, '===============': 1.0962300300598145, 'Edit (1/29/17): after considering the latest revisions': 1.0978561639785767, ""particularly the full ImageNet evaluation results reported in Table 5 demonstrating that the background segmentation 'privileged information' is beneficial even with the full labeled ImageNet dataset"": 0.17828351259231567, ""I've upgraded my rating from 4 to 6."": 0.0339602530002594, '(I\'ll reiterate a very minor point about Figure 1 though: I still think the ""0"" and ""1"" labels in the top part of the figures should be swapped to match the other labels.': 1.0222800970077515, 'e.g., the topmost path in figure 1a, with the text ""suppress foreground"", currently has 0 in the background and 1 in the foreground, when one would want the reverse of this to suppress the foreground.)': 0.9284306764602661, 'This paper proposes a modification to ConvNet training so that the feature activations before the linear classifier are divided into groups such that all pairs of features across all pairs of groups are encouraged to have low statistical correlation.': 1.0986123085021973, 'Instead of discovering the groups automatically, the work proposes to use supervision, which they call privileged information, to assign features to groups in a hand-coded fashion.': 1.0986123085021973, 'The developed method is applied to image classification.': 1.0986100435256958, 'The paper is clear and easy to follow': 1.0986123085021973, 'The experimental results seem to show some benefit from the proposed approach': 1.0986123085021973, '(1) The paper proposes one core idea (group orthogonality w/ privileged information), but then introduces background feature suppression without much motivation and without careful experimentation': 1.0986123085021973, '(2) No comparison with an ensemble': 1.0635000467300415, '(3) Full experiments on ImageNet under the ""partial privileged information"" setting would be more impactful': 1.0985926389694214, 'This paper is promising and I would be willing to accept an improved version.': 1.0986123085021973, 'However, the current version lacks focus and clean experiments.': 1.0986123085021973, 'First, the abstract and intro focus on the need to replace ensembles with a single model that has diverse (ensemble like) features.': 1.0986123085021973, 'The hope is that such a model will have the same boost in accuracy, while requiring fewer FLOPs and less memory.': 1.0986026525497437, 'Based on this introduction, I expect the rest of the paper to focus on this point.': 1.0986063480377197, 'But it does not; there are no experimental results on ensembles and no experimental evidence that the proposed approach in able to avoid the speed and memory cost of ensembles while also retaining the accuracy benefit.': 1.0986121892929077, 'Second, the technical contribution of the paper is presented as group orthogonality (GO).': 1.098563551902771, 'However, in Sec 4.1 the idea of background feature suppression is introduced.': 1.0986123085021973, 'While some motivation for it is given, the motivation does not tie into GO.': 0.9727633595466614, 'GO does not require bg suppression and the introduction of it seems ad hoc.': 1.0986073017120361, 'Moreover, the experiments never decouple GO and bg suppression, so we are unable to understand how GO works on its own.': 1.09860360622406, 'This is a critical experimental flaw in my reading.': 1.0986123085021973, 'Minor suggestions / comments:': 1.0986121892929077, 'The equation in definition 2 has an incorrect normalizing factor (1/c^(k)^2)': 1.098541021347046, 'Figure 1 seems to have incorrect mask placements.': 1.0986117124557495, 'The top mask is one that will mask out the background and only allow the fg to pass': 1.0986123085021973, 'The starting point of this work is the understanding that by having decorrelated neurons (e.g. neurons that only fire on background, or only on foreground regions) one provides independent pieces of information to the subsequent decisions.': 1.0986123085021973, 'As such one gives ""complementary viewpoints"" of the input to the subsequent layers, which can be thought of as performing ensembling/expert combination within the model, rather than using an ensemble of networks.': 1.0986123085021973, 'For this, the authors propose a sensible method to decorrelate the activations of intermediate neurons, with the aim of delivering complementary inputs to the final classification layers: they split intermediate neurons to a ""foreground"" and a ""background"" subset, and append side-losses that force them to be zero on background and foreground pixels respectively.': 1.0986123085021973, 'They demonstrate that this can improve classification on a mid-scale classification example (a fraction of imagenet, and a ResNet with 18, rather than 150 layers), when compared to a ""vanilla"" baseline that does not use these losses.': 1.0986123085021973, 'I enjoyed reading the paper because the idea is simple, smart, and seems to be effective.': 1.0985527038574219, 'But there are a few concerns;': 1.0986123085021973, 'firstly, the way of doing this seems very particular to vision.': 1.0294297933578491, 'In vision one knows that masking the features (during both training and testing) helps, e.g. https://arxiv.org/abs/1412.1283': 1.0986121892929077, 'To be fair, this is not truly the same thing as what the authors are doing, because in the reference above the masking is computed  during both training and testing, while here it is used as a method of decorrelating neurons at training time.': 1.0986120700836182, 'But I understand that to the broader iclr community this may seem as ""yet another vision-specific trick"", while to the vision community one would ask why not just use the mask during both training and testing, since one can compute it in the first place.': 1.0986123085021973, 'More importantly, the evaluation is quite limited; the authors use only one network (18 rather than 150 layers) and only part of imagenet for testing.': 1.0982831716537476, 'They do get a substantial boost, but it is not clear if this will transfer to more data/layers.': 1.0986123085021973, 'The authors could at least have also tried CIFAR-10/100.': 1.0986123085021973, 'I would expect to see some more results during the rebuttal period.': 1.0986123085021973}"
108,https://openreview.net/forum?id=ByToKu9ll,"{'This paper performs a series of experiments to systematically evaluate the robustness of several defense methods, including RAD, AEC and its improved version etc..': 1.0986123085021973, 'It provides interesting observations.': 1.0984482765197754, ""Overall, RAD and distillation have the best performances, but none of the methods can really resist the 'additional' attack from cg or adam."": 1.0986121892929077, 'Since it is an experimental paper, my main concern is about its clarity.': 1.0986123085021973, 'See the comments below for details.': 1.098595380783081, 'Pros:': 1.09861159324646, '1. This paper provides a good comparison of the performances for the selected methods.': 1.0986069440841675, ""2. Section 3.3 (the 'additional' attack) is a interesting investigation. Although the final result about the defense methods is negative, its results are still inspiring."": 0.8026704788208008, '3. Overall, this paper provides interesting and inspiring experimental results about the selected methods.': 1.0240941047668457, 'Cons:': 1.0986123085021973, '1. There are several other methods in the literature that are missing from the paper. For example the defense methods and the attack methods in the papers [1,2].': 0.8602736592292786, '2. Although a long list of experimental results are provided in the paper, many details are skipped. For example, details of the experiments that generate the results in Table 5.': 0.7813980579376221, '3. Without further explanations and analyses about the experimental results, the contribution of the paper seems limited.': 1.0868315696716309, '4. This paper proposed an improved version of the AEC algorithm. But its experimental results seems not promising.': 0.40973830223083496, 'Minor comments:': 1.0986123085021973, 'Page 3: Equation (3) is also non-convex.': 1.0985921621322632, 'So the non-convexity of Equation (2) should not be the motivation of Equation (3).': 1.0985815525054932, '[1] https://arxiv.org/abs/1507.00677': 0.8491612672805786, '[2] https://arxiv.org/abs/1511.03034': 1.027093768119812, 'The paper compares several defense mechanisms against adversarial attacks: retraining, two kinds of autoencoders and distillation with the conclusion that the retraining methodology proposed by Li et al. works best of those approaches.': 1.0986086130142212, 'The paper documents a series of experiments on making models robust against adversarial examples.': 1.0415549278259277, 'The methods proposed here are not all too original, RAD was proposed by Li et al, distillation was proposed in Goodfellow et al\'s ""Explaining and harnessing adversarial examples"", stacked autoencoders were proposed by Szegedy et al\'s ""Intriguing Properties of Neural Networks"".': 1.0986112356185913, 'The most original part of the paper is the improved version of autoencoders proposed in this paper.': 0.9952622652053833, 'The paper establishes experimental evidence that the RAD framework provides the best defense mechanism against adversarial attacks which makes the introduction of the improved autoencoder mechanism less appealing.': 1.098609209060669, 'Although the paper establishes interesting measurement points and therefore it has the potential for being cited as a reference, its relative lack of originality decreases its significance.': 1.092244267463684, 'I reviewed the manuscript as of December 6th.': 1.098547339439392, 'The authors perform a systematic investigation of various retraining methods for making a classification network robust to adversarial examples.': 1.0986123085021973, 'The authors achieve lower error rates using their RAD and IAEC methods perform better then previously introduced distillation methods for retraining networks to be robust to adversarial examples.': 1.0986123085021973, 'This method suggests a promising direction for building a defense for adversarial examples.': 1.0986120700836182, 'Major Comments:': 1.0986123085021973, 'I find the paper to not be lacking in exposition and clarity.': 1.0929549932479858, 'The paper has a laundry list of related results (page 2) but no clear message.': 1.0986121892929077, 'I *think* one larger point is the superior performance of their retraining techniques but it is not clear how well these techniques perform compared to other retraining techniques, nor are the details of the retraining techniques clear.': 1.0986123085021973, 'The paper requires more discussion and a clear exposition about the methods the authors introduced (i.e. RAD, IAEC).': 1.0985972881317139, 'What follow are some more detailed comments along this theme of improving the exposition and clarity:': 0.6854674220085144, 'The authors should provide more details about how they constructed the auto-encoder in the IAEC method (diagram?).': 1.098608374595642, 'The same needs to be said for the RAD method.': 1.098250389099121, 'The authors point to a previous workshop submission (https://arxiv.org/abs/1604.02606) but the authors need more discussion about what this method entails and how it compares to other retraining methods (e.g. [1,2]) since this is the first peer-review of this work.': 1.0986123085021973, ""Section 3.2 indicates that the authors are concerned with 'cross-model' efficiency but it is not clear from the text what this means."": 1.0986123085021973, 'Are the author exploring the phenomenon of retraining off one algorithm and then evaluating adversarial images derived on another?': 1.0986123085021973, 'Or, are the authors examining how examples derived from one instance of a trained model may fool or trick a second instance of a model?': 1.0986123085021973, 'The latter point is quite important because this points towards examples and retraining procedures that can generalize across the class of all models.': 1.0972812175750732, 'How do RAD compare with basic retraining methods described in [1, 2]?': 1.0986123085021973, 'Since the main contribution of this paper seems to be evaluating the efficacy of RAD, AEC and IAEC, I would suggest that the authors provide more discussion and exposition.': 1.0977842807769775, ""Why are the authors measuring 'recall' (https://en.wikipedia.org/wiki/Precision_and_recall) in Section 3.1?"": 0.6123465299606323, 'What does recall mean in this context?': 1.0927013158798218, 'I would expect the authors to measure something more like the error rate of the classifier after employing the retraining procedure.': 1.0913527011871338, 'This needs to be clarified in the manuscript.': 1.0986123085021973, '[1] https://arxiv.org/abs/1412.6572': 1.0986123085021973, '[2] https://arxiv.org/abs/1611.01236': 1.0986123085021973}"
109,https://openreview.net/forum?id=ByW2Avqgg,"{'This paper proposes to use a causality score to weight a sparsity regularizer.': 1.0986117124557495, 'In that way, selected variables trade off between being causal and discriminative.': 1.0986123085021973, 'The framework is primarily evaluated on a proprietary health dataset.': 1.0977636575698853, 'While the dataset does give a good motivation to the problem setting, the paper falls a bit short for ICLR due to the lack of additional controlled experiments, relatively straightforward methodology (given the approach of Chalupka et al., arXiv Preprint, 2016, which is a more interesting paper from a technical perspective), and paucity of theoretical motivation.': 1.0986121892929077, 'At the core of this paper, the approach is effectively to weight a sparsity regularizer so that ""causal"" variables (as determined by a separate objective) are more likely to be selected.': 1.0986123085021973, 'This is generally a good idea, but we do not get a proper validation of this from the experiments as ground truth is absent.': 1.0986121892929077, 'A theorem on identifiability of causal+discriminative variables from a data sample combined with adequate synthetic experiments would have probably been sufficient, for example, to push the paper towards accept from a technical perspective, but as it is, it is lacking in insight and reproducibility.': 1.0986123085021973, 'The authors extend their method of causal discovery (Chalupka et al 2016) to include assumptions about sparsity via regularization.': 0.4551475942134857, 'They apply this extension to an interesting private dataset from Sutter Health.': 1.0986030101776123, 'While an interesting direction, I found the presentation somewhat confused, the methodological novelty smaller than the bulk of ICLR works, and the central results (or perhaps data; see below) inadequate to address questions of causality.': 0.40757066011428833, 'First, I found the presentation somewhat unclear.': 1.098592758178711, 'The paper at some points seems to be entirely focused on healthcare data, at other points it uses it as a motivating example, and at other points it is neglected.': 1.0630661249160767, ""Also, algorithm 1 seems unreferenced, and I'm not entirely sure why it is needed."": 0.9292963743209839, 'Figure 2 is not needed for this community.': 0.854572594165802, 'The key methodological advance in this work appears in section 2.1 (Causal regularizer), but it is introduced amidst toy examples and without clear terminology or standard methodological assumptions/build-up.': 0.72080397605896, 'In Section 3.1 (bottom of first paragraph), key data and results seem to be relegated to the appendices.': 0.4199539124965668, 'Thus overall the paper read rather haphazardly.': 1.0822287797927856, 'Finally, there seems to be an assumption throughout of fairly intimate familiarity with the Cholupka preprint, which i think should be avoided.': 0.8394914269447327, 'This paper should stand alone.': 1.0821349620819092, ""Second, while the technical contributions/novelty are not a focus of the paper's presentation, I am concerned by the lack of methodological advance."": 0.440570592880249, ""Essentially a regularization objective is added to the previous method, which of itself is not a bad idea, but I can't point to a technical novelty in the paper that the community can not do without."": 0.40589427947998047, ""Third, fundamentally i don't see how the experiments address the central question of causality; they show regularization behaving as expected (or rather, influencing weights as expected), but I don't think we really have any meaningful quantitative evidence that causality has been learned."": 0.548401415348053, 'This was briefly discussed (see ""ground truth causality?"" and the response below).': 0.45534324645996094, ""I appreciate the technical challenges/impossibility of having such a dataset, but if that's the case, then I think this work is premature, since there is no way to really validate."": 0.9134716391563416, ""Overall it's clearly a sincere effort, but I found it wanting in terms of a few critical areas."": 1.0068488121032715, 'The present submission discusses a ""causal regularizer"", which promotes the use of causal dependencies (X -> Y, where X is a feature of the learning problem, and Y is the target variable) in predictive models.': 1.0986123085021973, 'Similarly, such causal regularizer penalizes the use of non-causal dependencies, which can arise due to reverse causation (Y -> X) or confounding (X <- Z -> Y, where Z is a hidden confounder).': 1.0985966920852661, '+': 1.0986123085021973, 'Overall, this submission tackles one of the most important problems in machine learning, which is to build causal models.': 1.0985709428787231, 'The paper discusses and addresses this issue effectively when applied to a dataset in heart disease.': 1.0985549688339233, 'In their experiments, the authors correctly identify some of the common causes of heart disease by virtue of their causal regularizer.': 1.0986123085021973, 'The authors do not discuss the robustness of their approach with respect to choice of hyper-parameters (both describing the neural network architecture and the generative model that synthesizes artificial causal data).': 1.0986123085021973, 'This seems like a crucial issue, in particular when dealing with medical data.': 1.095397710800171, 'The conclusions of the experimental evaluation should be discussed in greater length.': 1.081929326057434, 'On the one hand, Figure 4.a shows that there are no differences between L1 and causal regularization in terms of predictive performance, but it is difficult to conclude if this result is statistically significant without access to error-bars.': 1.0986018180847168, 'On the other hand, Table 3 describes the qualitative differences between L1 and causal regularization.': 1.0986123085021973, 'However, this table is hard to read: How were the 30 rows selected?': 1.0986123085021973, 'What does the red highlighting mean?': 1.098417043685913, 'Are these red rows some true causal features that were missed?': 0.8314850330352783, 'If so, this is related to precision.': 1.0936083793640137, 'What about recall?': 1.0986123085021973, 'Did the causal regularization pick up many non-causal features as causal?': 1.0986123085021973, 'Regarding causal classifiers, this paper should do a much better job at reviewing previous work.': 1.0925240516662598, 'For instance, the paper ""Towards a Learning Theory of Cause-Effect Inference"" from Lopez-Paz et al. is missing from the references.': 1.0954875946044922, 'However, this prior work studies many of the aspects that are hinted as novel in this submission.': 1.098495364189148, 'In particular, the prior work of Lopez-Paz 1) introduces the concept of Mother distribution (referred as Nature hyper-prior in this submission) which explicitly factorizes the distribution over causes and mechanisms, 2) circumvented intractable likelihoods by synthesizing and training on causal data, 3) tackled the confounding case (compare Figure 1 of this submission and Appendix C of Lopez-Paz), and 4) dealt with discrete data seamlessly (such as the ChaLearn data from Section 5.3 in Lopez-Paz).': 0.9982714653015137, 'On a positive note, this is a well-written paper that addresses the important, under-appreciated problem of incorporating causal reasoning into machine learning.': 0.8413383960723877, 'On a negative note, the novelty of the technical contributions is modest and the qualitative evaluation of the results could be greatly extended.': 1.0986123085021973, 'In short, I am leaning slightly towards acceptance.': 1.0986123085021973}"
110,https://openreview.net/forum?id=ByZvfijeg,"{'The authors of the paper explore the idea of incorporating skip connections *over time* for RNNs.': 1.098611831665039, 'Even though the basic idea is not particularly innovative, a few proposals on how to merge that information into the current hidden state with different pooling functions are evaluated.': 1.0986123085021973, 'The different models are compared on two popular text benchmarks.': 1.0983604192733765, 'Some points.': 0.8379992246627808, '1)': 1.0986123085021973, 'The experiments feature only NLP and only prediction tasks.': 0.8969419598579407, 'It would have been nice to see the models in other domains, i.e. modelling a conditional distribution p(y|x), not only p(x).': 1.0986121892929077, 'Further, sensory input data such as audio or video would have given further insight.': 1.0986123085021973, '2) As pointed out by other reviewers, it does not feel as if the comparisons to other models are fair.': 1.09673011302948, 'SOTA on NLP changes quickly and it is hard to place the experiments in the complete picture.': 1.0885945558547974, '3) It is claimed that this helps long-term prediction.': 1.0929806232452393, 'I think the paper lacks a corresponding analysis, as pointed out in an earlier question of mine.': 1.0986113548278809, '4)  It is claimed that LSTM trains slow and is hard to scale.': 0.4485757350921631, 'For one does this not match my personal experience.': 1.0986123085021973, 'Then, the prevalence of LSTM systems in production systems (e.g. Google, Baidu, Microsoft, …) clearly speaks against this.': 1.09861159324646, 'I like the basic idea of the paper, but the points above make me think it is not ready for publication.': 1.0986043214797974, 'I think the backbone of the paper is interesting and could lead to something potentially quite useful.': 1.0986123085021973, 'I like the idea of connecting signal processing with recurrent network and then using tools from one setting in the other.': 1.0986123085021973, 'However, while the work has nuggets of very interesting observations, I feel they can be put together in a better way.': 1.0986123085021973, ""I think the writeup and everything can be improved and I urge the authors to strive for this if the paper doesn't go through."": 1.0986123085021973, 'I think some of the ideas of how to connect to the past are interesting, it would be nice to have more experiments or to try to understand better why this connections help and how.': 1.0986119508743286, 'This paper proposes an idea of looking n-steps backward when modelling sequences with RNNs.': 1.0949900150299072, 'The proposed RNN does not only use the previous hidden state (t-1) but also looks further back ( (t - k) steps, where k=1,2,3,4 ).': 1.09861159324646, 'The paper also proposes a few different ways to aggregate multiple hidden states from the past.': 1.0986123085021973, 'The reviewer can see few issues with this paper.': 0.9304769039154053, 'Firstly, the writing of this paper requires improvement.': 1.0500637292861938, 'The introduction and abstract are wasting too much space just to explain unrelated facts or to describe already well-known things in the literature.': 1.0985641479492188, 'Some of the statements written in the paper are misleading.': 0.20612703263759613, 'For instance, it explains, “Among various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long term dependency in sequential data using a simple mechanism of recurrent feedback” and then it says RNNs cannot actually capture long-term dependencies that well.': 1.0986123085021973, 'RNNs are appealing in the first place because they can handle variable length sequences and can model temporal relationships between each symbol in a sequence.': 1.0690946578979492, 'The criticism against LSTMs is hard to accept when it says: LSTMs are slow and because of the slowness, they are hard to scale at larger tasks.': 1.0929614305496216, 'But we all know that some companies are already using gigantic seq2seq models for their production (LSTMs are used as building blocks in their systems).': 1.0986123085021973, 'This indicates that the LSTMs can be practically used in a very large-scale setting.': 0.5733340382575989, 'Secondly, the idea proposed in the paper is incremental and not new to the field.': 1.0972613096237183, 'There are other previous works that propose to use direct connections to the previous hidden states [1].': 0.456013023853302, 'However, the previous works do not use aggregation of multiple number of previous hidden states.': 1.092829942703247, 'Most importantly, the paper fails to deliver a proper analysis on whether its main contribution is actually helpful to improve the problem posed in the paper.': 1.0985586643218994, 'The new architecture is said that it handles the long-term dependencies better, however, there is no rigorous proof or intuitive design in the architecture that help us to understand why it should work better.': 0.7913956642150879, 'By the design of the architecture, and speaking in very high-level, it seems like the model maybe helpful to mitigate the vanishing gradients issue by a linear factor.': 0.26159191131591797, 'It is always a good practice to have at least one page to analyze the empirical findings in the paper.': 0.9010776281356812, 'Thirdly, the baseline models used in this paper are very weak.': 0.5439367294311523, 'Their are plenty of other models that are trained and tested on word-level language modelling task using Penn Treebank corpus, but the paper only contains a few of outdated models.': 0.9167830348014832, 'I cannot fully agree on the statement “To the best of our knowledge, this is the best performance on PTB under the same training condition”, these days, RNN-based methods usually score below 80 in terms of the test perplexity, which are far lower than 100 achieved in this paper.': 0.8788677453994751, '[1] Zhang et al., “Architectural Complexity Measures of Recurrent Neural Networks”, NIPS’16': 0.5423378944396973}"
111,https://openreview.net/forum?id=BybtVK9lg,"{'The authors propose NVI for LDA variants.': 1.3862943649291992, 'The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI.': 1.3862570524215698, 'The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?)': 1.14987313747406, 'In general, I like the direction of this paper and NVI looks promising for LDA.': 1.3862943649291992, 'The experimental results however confound model vs inference which makes it hard to understand the significance of the results.': 1.3862940073013306, ""Furthermore, the authors don't discuss hyper-parameter selection which is known to significantly impact performance of topic models."": 0.7078068256378174, 'This makes it hard to understand when the proposed method can be expected to work.': 1.3862943649291992, 'Can you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters?': 1.3725489377975464, 'Figure 1: Is this prior or posterior?': 1.3862943649291992, 'The text talks about sparsity whereas the y-axis reads ""log p(topic proportions)"" which is a bit confusing.': 1.3813586235046387, 'Section 3.2: it is not clear what you mean by unimodal in softmax basis.': 1.3862943649291992, 'Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal.': 1.3775982856750488, ""Isn't the softmax basis still multimodal?"": 1.3862943649291992, 'None of the numbers include error bars.': 1.3862943649291992, 'Are the results statistically significant?': 1.3862943649291992, 'Minor comments:': 1.3862943649291992, 'Last term in equation (3) is not ""error""; reconstruction accuracy or negative reconstruction error perhaps?': 1.3862812519073486, 'The idea of using an inference network is much older, cf.': 1.3862943649291992, 'Helmholtz machine.': 1.3862943649291992, 'This paper proposes the use of neural variational inference method for topic models.': 1.3862943649291992, 'The paper shows a nice trick to approximate Dirichlet prior using softmax basis with a Gaussian and then the model is trained to maximize the variational lower bound.': 1.1039392948150635, 'Also, the authors study a better way to alleviate the component collapsing issue, which has been problematic for continuous latent variables that follow Gaussian distribution.': 1.385292887687683, 'The results look promising and the experimental protocol sounds fine.': 1.34013032913208, 'Please add citation to [1] or [2] for neural variational inference, and [2] for VAE.': 1.3862943649291992, 'A typo in “This approximation to the Dirichlet prior p(θ|α) is results in the distribution”, it should be “This approximation to the Dirichlet prior p(θ|α) results in the distribution”': 1.2991061210632324, 'In table 2, it is written that DMFVI was trained more than 24hrs but failed to deliver any result, but why not wait until the end and report the numbers?': 1.386240005493164, 'In table 3, why are the perplexities of LDA-Collapsed Gibbs and NVDM are lower while the proposed models (ProdLDA) generates more coherent topics?': 1.3854644298553467, 'What is your intuition on this?': 0.8996303081512451, 'How does the training speed (until the convergence) differs by using different learning-rate and momentum scheduling approaches shown as in figure 1?': 1.3862943649291992, 'It may be also interesting to add some more analysis on the latent variables z (component collapsing and etc., although your results indirectly show that the learning-rate and momentum scheduling trick removes this issue).': 1.3862940073013306, 'Overall, the paper clearly proposes its main idea, explain why it is good to use NVI, and its experimental results support the original claim.': 1.2930607795715332, 'It explains well what are the challenges and demonstrate their solutions.': 0.6914256811141968, '[1] Minh et al., Neural Variational Inference and Learning in Belief Networks, ICML’14': 0.7520895004272461, '[2] Rezende et al., Stochastic Backpropagation and Approximate Inference in Deep Generative Models, ICML’14': 1.2159126996994019, 'This is an interesting paper on a VAE framework for topic models.': 1.3862923383712769, 'The main idea is to train a recognition model for the inference phase which, because of so called “amortized inference” can be much faster than normal inference where inference must be run iteratively for every document.': 1.1212037801742554, 'Some comments:': 1.386277437210083, 'Eqn 5: I find the notation p(theta(h)|alpha) awkward.': 1.3862730264663696, 'Why not P(h|alpha) ?': 1.3022522926330566, 'The generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space.': 1.3861695528030396, 'However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes.': 1.3786020278930664, 'This seems undesirable.': 1.3849300146102905, 'Maybe they should normalize the input to the recognition network?': 0.9138070344924927, 'The ProdLDA model might well be equivalent to exponential family PCA or some variant thereof: http://jmlr.csail.mit.edu/proceedings/papers/v9/li10b/li10b.pdf': 1.383395791053772, 'Section 4.1: error in the equation.': 1.3862943649291992, 'The last term should be': 1.3861758708953857, 'Prod_i exp(delta*_r_i) * exp((1-delta)*s_i).': 1.3862943649291992, 'Last paragraph 4.1.': 1.3862943649291992, 'The increment relative to NVDM seems small: approximating the Dirichlet with a Gaussian and high momentum training.': 1.3862943649291992, 'While these aspects may be important in practice they are somewhat incremental.': 1.3862943649291992, 'I couldn’t find the size of the vocabularies of the datasets in the paper.': 1.3862943649291992, 'Does this method work well for very high dimensional sparse document representations?': 1.3862943649291992, 'The comment on page 8 that the method is very sensitive to optimization tricks like very high momentum in ADAM and batch normalization is a bit worrying to me.': 1.3862943649291992, 'In the end, it’s a useful paper to read, but it’s not going to be the highlight of the conference.': 1.3427261114120483, 'The relative increment is somewhat small and seems to heavily rely optimization tricks.': 1.3699586391448975, ""The comparison to NVDM looks unfair since the user introduces a couples tricks (Dirichlet prior, batch normalisation, high momentum training, etc.) which NVDM doesn't use."": 1.3862943649291992, 'A more convincing experimental design is to explore the effect of each trick separately in neural variational inference.': 1.3862943649291992}"
112,https://openreview.net/forum?id=BycCx8qex,"{'The paper proposes a new neural architecture, called DRAGNN, for the transition-based framework.': 1.0986123085021973, 'A DRAGNN uses TBRUs which are neural units to compute hidden activations for the current state of a transition-based system.': 1.0986123085021973, 'The paper proves that DRAGNNs can cover a wide range of transition-based methods in the literature.': 1.0986123085021973, 'In addition, one can easily implement multitask learning systems with DRAGNNs.': 1.0986123085021973, 'The experimental results shows that using DRAGNNs the authors built (near) state-of-the-art systems for 2 tasks: parsing and summarization.': 1.0986123085021973, 'The paper contains two major parts: DRAGNN and demonstrations of its usages.': 1.0986123085021973, 'Regarding to the first part, the proposed DRAGNN is a neat tool for building any transition-based systems.': 1.0986123085021973, 'However, it is difficult to say whether the DRAGNN is novel.': 1.0986123085021973, ""Transition-based framework is already well defined and there's a huge trend in NLP using neural networks to implement transition-based systems."": 1.0986123085021973, 'In my opinion, the difference between the Stack-LSTM (Dyer et al., 2015) and DRAGNN is slight.': 1.0986123085021973, 'Of course, the DRAGNN is a powerful architecture but the contribution here should be considered mainly in terms of software engineering.': 0.6863119602203369, 'In the second part, the authors used DRAGNN to implement new transition-based systems for different (multi-)tasks.': 1.0986121892929077, 'The implementations are neat, confirming that DRAGNN is a powerful architecture, especially for multitask learning.': 1.0984714031219482, 'However, we should bear in mind that the solutions employed are already there in the literature, thus making difficult to judge the novelty of this part w.r.t.': 1.098543643951416, 'the theme of the conference.': 1.0986123085021973, 'The authors present a general framework for defining a wide variety of recurrent neural network architectures, including seq2seq models, tree-structured models, attention, and a new family of dynamically connected architectures.': 1.0986123085021973, 'The framework defines a new, general-purpose recurrent unit called the TBRU, which takes a transition system, defining and constraining its inputs and outputs, and input function which defines the mapping between raw inputs and fixed-width vector representations, and recurrence function that defines the inputs to each recurrent step as a function of the current state, and an RNN cell that computes the output from the input (fixed and recurrent).': 1.0986121892929077, 'Many example instantiations of this framework are provided, including sequential tagging RNNs, Google’s Parsey McParseface parser, encoder/decoder networks, tree LSTMs and less familiar examples that demonstrate the power this framework.': 1.0986106395721436, 'The most interesting contribution of this work is the ease by which it can be used to incorporate dynamic recurrent connections through the definition of the transition system.': 1.0986119508743286, 'In particular, this paper explores the application of these dynamic connections to syntactic dependency parsing, both as a standalone task, and by multitasking parsing with extractive summarization, using the same compositional phrase representations as features for the parser and summarization (previous work used discrete parse features), which is particularly simple/elegant in this framework.': 1.0986123085021973, 'In experimental results, the authors demonstrate that such multitasking leads to more accurate summarization models, and using the framework to incorporate more structure into existing parsing models also leads to increased accuracy with no big-oh efficiency loss (compared with e.g. attention).': 1.0986123085021973, 'The “raison d’etre,” in particular the example, perhaps described even more thoroughly/explicitly, should be made as clear as possible as soon as possible.': 1.0986123085021973, 'This is the most important contribution, but it gets lost in the description and presentation as a framework — emphasizing that attention, seq2seq, etc can be represented in the framework is distracting and makes it seem less novel than it is.': 1.0986123085021973, 'AnonReviewer6 clearly missed this point, as did I in my first pass over the paper.': 1.0986123085021973, 'To get this idea across and to emphasize the benefits of this representation, I’d love to see more detailed analysis of these representations and their importance to achieving your experimental results.': 1.0986123085021973, 'I think it would also be helpful to emphasize the difference between a stack LSTM and Example 6.': 1.0986123085021973, 'Overall I think this paper presents a valuable contribution, though the exposition could be improved and analysis of experimental results expanded.': 1.0986123085021973, 'Overall, this is a nice paper.': 1.0986123085021973, 'Developing a unifying framework for these newer': 1.0986123085021973, 'neural models is a worthwhile endeavor.': 1.0986123085021973, ""However, it's unclear if the DRAGNN framework (in its current form) is a"": 1.0986123085021973, 'significant standalone contribution.': 1.0986123085021973, 'The main idea is straightforward: use a': 1.0986123085021973, 'transition system to unroll a computation graph.': 1.0986123085021973, 'When you implement models in': 1.0986123085021973, 'this way you can reuse code because modules can be mixed and matched.': 1.0986123085021973, 'This is': 1.0986123085021973, 'nice, but (in my opinion) is just good software engineering, not machine': 1.0986123085021973, 'learning research.': 1.0986123085021973, 'Moreover, there appears to be little incentive to use DRAGNN, as there are no': 1.0986123085021973, ""'free things' (benefits) that you get by using the framework."": 1.0986123085021973, 'For example:': 1.0986123085021973, 'If you write your neuralnet in an automatic differentiation library (e.g.,': 1.0668655633926392, ""tensorflow or dynet) you get gradients for 'free'."": 1.0279865264892578, ""In the VW framework, there are efficiency tricks that 'the credit assignment"": 1.092284917831421, ""compiler' provides for you, which would be tedious to implement on your"": 1.0672887563705444, 'own.': 1.094740390777588, 'There is also a variety of algorithms for training the model in a': 0.860219419002533, 'principled way (i.e., without exposure bias).': 0.43430009484291077, ""I don't feel that my question about the limitations of the framework has been"": 0.6240365505218506, 'satisfactorily addressed.': 1.0984972715377808, 'Let me ask it in a different way: Can you give me': 1.085484266281128, ""examples of a few models that I can't (nicely) express in the DRAGNN framework?"": 0.8659883737564087, 'What if I wanted to implement https://openreview.net/pdf?id=HkE0Nvqlg or': 1.0331934690475464, 'http://www.cs.jhu.edu/~jason/papers/rastogi+al.naacl16.pdf?': 0.09091334789991379, 'Can I implement the': 1.098595142364502, 'dynamic programming components as transition units and (importantly) would it be': 1.0387496948242188, 'efficient?': 1.0930094718933105, 'disagree that the VW framework is orthogonal, it is a *competing* way to': 0.9046364426612854, 'implement recurrent models.': 0.9735909700393677, ""The main different to me appears to be that VW's"": 1.0752010345458984, 'imperative framework is more general, but less modular.': 0.9545707106590271, 'The experimental contribution seems useful as does the emphasis on how easy it': 1.0986123085021973, 'is to incorporate multi-task learning.': 0.8837743997573853, 'Minor:': 1.0986123085021973, 'It would be useful to see actual code snippets (possibly in an': 0.5613905787467957, 'appendix).': 1.0986123085021973, 'Otherwise, its unclear how modular DRAGNN really are.': 1.0981414318084717, 'The introduction states that (unlike seq2seq+attention) inference remains': 0.7686829566955566, 'linear.': 1.096701979637146, 'Is this *necessarily* the case?': 0.9328199625015259, 'Users define a transition system that': 1.0985089540481567, 'is quadratic, just let attention be over all previous states.': 1.0985385179519653, 'I recommend that': 1.0979424715042114, 'authors rephrase statement more carefully.': 1.0986123085021973, 'It seems strange to use A() as in ""actions"", then use d as ""decision"" for its': 1.0986123085021973, 'elements.': 1.0986123085021973, 'I recommend adding i as an argument to the definition of the recurrence': 1.0986123085021973, ""function r(s) to make it clear that it's the subset of previous states at time"": 1.0986123085021973, 'i, otherwise it looks like an undefined variable.': 1.0986123085021973, 'A nice terse option is to': 1.0986123085021973, 'write r(s_i).': 1.0986123085021973, 'Real numbers should be \\mathbb{R} not \\mathcal{R}.': 1.0986123085021973, ""It's more conventional to use t for a time-step instead of i."": 1.0986123085021973, 'Example 2: ""52 feature embeddings"" -> did you mean ""52-DIMENSIONAL feature': 1.0986123085021973, 'embeddings""?': 1.0986123085021973}"
113,https://openreview.net/forum?id=BydARw9ex,"{'This paper performs a very important service: exploring in a clear and systematic way the performance and trainability characteristics of a set of neural network architectures': 1.0986086130142212, 'in particular, the basic RNN motifs that have recently been popular.': 1.0986123085021973, 'Pros:': 1.0986123085021973, ""* This paper addresses an important question I and many others would have liked to know the answer to but didn't have the computational resources to thoroughly attack it."": 1.0986084938049316, ""This is a nice use of Google's resources to help the community."": 0.7811921834945679, '*': 1.0986123085021973, 'The work appears to have been done carefully so that the results can be believed.': 1.0979511737823486, 'The basic answer arrived at (that, in the ""typical training environment"" LSTMs are reliable but basically GRUs are the answer) seems fairly decisive and practically useful.': 1.0985864400863647, 'Of course the real answer is more complicated than my little summary here, but the subtleties are discussed nicely in the paper.': 0.5247523188591003, 'The insistence on a strong distinction between capacity and trainability helps nicely clear up a misconception about the reasons why gated architectures work.': 1.0986101627349854, ""In sum, they're much more easily trainable but somewhat lower capacity than vanilla RNNs, and in hard tasks, the benefits of better trainability far outweigh the costs of mildly lower capacity."": 1.0986120700836182, '* The point about the near-equivalence of capacity at equal numbers of parameters is very useful.': 1.0986123085021973, 'The paper makes it clear the importance of HP tuning, something that has sometimes gotten lost in the vast flow of papers about new architectures.': 1.0986108779907227, ""The idea of quantifying the fraction of infeasible parameters (e.g. those that diverge) is nice, because it's a practical problem that everyone working with these networks has but often isn't addressed."": 1.0986099243164062, '* The paper text is very clearly written.': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'The work on the UGRNNs and the +RNNs seems a bit preliminary.': 0.5239781141281128, 'I don\'t think that the authors have clearly shown that the +RNN should be ""recommended"" with the same generality as the GRU.': 0.5483004450798035, ""I'd at the least want some better statistics on the significance of differences between +RNN and GRU performances quantifying the results in Figure 4 (8-layer panel)."": 0.4274103343486786, 'In a way the high standards for declaring an architecture useful that are set in the paper make the UGRNNs and +RNN contributions seem less important.': 0.3465820848941803, ""I don't really mind having them in the paper though."": 0.27447715401649475, 'I guess the point of this paper is not really to be novel in the first place': 1.0805302858352661, ""which is totally fine with me, though I don't know what the ICLR area chairs will think."": 1.095989465713501, 'The paper gives short shrift to the details of the HP algorithm itself.': 0.49723249673843384, 'They do say:': 1.0986123085021973, '""Our setting of the tuner’s internal parameters was such that it uses Batched GP Bandits with an expected improvement acquisition function and a': 1.0986011028289795, 'Matern 5/2 Kernel with feature scaling and automatic relevance determination performed by optimizing over kernel HPs""': 1.0676809549331665, 'and give some good references, but I expect that actually trying to replicate this involves a lot of missing details.': 1.0986123085021973, '* I found some of the figures a bit hard to read at first, esp.': 1.0986123085021973, 'Fig 4, mostly due to the panels being small, having a lot of details, and bad choices for visual cleanliness.': 1.0986123085021973, 'The neuroscience reference (""4.7 bits per synapse"") seems a little bit of a throw-away to me, because the connection between these results and the experimental neuroscience is very tenuous, or at any rate, not well explained.': 1.0986123085021973, ""I guess it's just in the discussion, but it seems gratuitous."": 1.0986123085021973, 'Maybe it should couched in slightly less strong terms (nothing is really strongly shown to be ""in agreement"" here between computational architectures and neuroscience, but perhaps they could say something like': 1.0986117124557495, '""We wonder if it is anything other than coincidence that our 5 bits result is numerically similar to the 4.7 bits measurement from neuroscience."")': 1.0986117124557495, 'CONTRIBUTIONS': 1.0986104011535645, 'Large-scale experiments are used to measure the capacity and trainability of different RNN architectures.': 1.0973079204559326, 'Capacity experiments suggest that across all architectures, RNNs can store between three and six bits of information per parameter, with ungated RNNs having the highest per-parameter capacity.': 1.0985296964645386, 'All architectures are able to store approximately one floating point number per hidden unit.': 1.0985808372497559, 'Trainability experiments show that ungated architectures (RNN, IRNN) are much harder to train than gated architectures (GRU, LSTM, UGRNN, +RNN).': 0.06183759868144989, 'The paper also proposes two novel RNN architectures (UGRNN and +RNN); experiments suggest that the UGRNN has similar per-parameter capacity as the ungated RNN but is much easier to train, and that deep (8-layer) +RNN models are easier to train than existing architectures.': 1.0911433696746826, 'CLARITY': 1.0986123085021973, 'The paper is well-written and easy to follow.': 1.0352026224136353, 'NOVELTY': 1.0986106395721436, 'This paper is the first to my knowledge to empirically measure the number of bits of information that can be stored per learnable parameter.': 1.0366952419281006, 'The idea of measuring network capacity by finding the dataset size and other hyperparameters that maximizes mutual information is a particularly novel experimental setup.': 1.0986120700836182, 'The proposed UGRNN is similar but not identical to the minimal gated unit proposed by Zhou et al, “Minimal Gated Unit for Recurrent Neural Networks”, International Journal of Automation and Computing, 2016.': 0.4194892644882202, 'SIGNIFICANCE': 1.0985959768295288, 'I have mixed feelings about the significance of this paper.': 0.22567637264728546, 'I found the experiments interesting, but I don’t feel that they reveal anything particularly surprising or unexpected about recurrent networks; it is hard to see how any of the experimental results will change the way either that I think about RNNs, or the way that I will use them in my own future work.': 1.0025666952133179, 'On the other hand it is valuable to see intuitive results about RNNs confirmed by rigorous experiments, especially since few researchers have the computational resources to perform such large-scale experiments.': 0.3267894387245178, 'The capacity experiments (both per-parameter capacity and per-unit capacity) essentially force the network to model random data.': 1.0982410907745361, 'For most applications of RNNs, however, we do not expect them to work with random data; instead when applied in machine translation or language modeling or image captioning or any number of real-world tasks, we hope that RNNs can learn to model data that is anything but random.': 1.098597526550293, 'It is not clear to me that an architecture’s ability to model random data should be beneficial in modeling real-world data; indeed, the experiments in Section 2.1 show that architectures vary in their capacity to model random data, but the text8 experiments in Section 3 show that these same architectures do not significantly vary in their capacity to model real-world data.': 1.0528818368911743, 'I do not think that the experimental results in the paper are sufficient to prove the significance of the proposed UGRNN and +RNN architectures.': 0.9998496770858765, 'It is interesting that the UGRNN can achieve comparable bits per parameter as the ungated RNN and that the deep +RNNs are more easily trainable than other architectures, but the only experiments on a real-world task (language modeling on text8) do not show these architectures to be significantly better than GRU or LSTM.': 0.4913874566555023, 'SUMMARY': 1.0986123085021973, 'I wish that the experiments had revealed more surprising insights about RNNs, though there is certainly value in experimentally verifying intuitive results.': 1.0982190370559692, 'The proposed UGRNN and +RNN architectures show some promising results on synthetic tasks, but I wish that they showed more convincing performance on real-world tasks.': 1.09831702709198, 'Overall I think that the good outweighs the bad, and that the ideas of this paper are of value to the community.': 1.0983638763427734, 'PROS': 1.0986123085021973, 'The paper is the first of my knowledge to explicitly measure the bits per parameter that RNNs can store': 1.0986123085021973, 'The paper experimentally confirms several intuitive ideas about RNNs:': 1.0986123085021973, '- RNNs of any architecture can store about one number per hidden unit from the input': 1.0986123085021973, '- Different RNN architectures should be compared by their parameter count, not their hidden unit count': 1.0986123085021973, '- With very careful hyperparameter tuning, all RNN architectures perform about the same on text8 language modeling': 1.0985935926437378, '- Gated architectures are easier to train than non-gated RNNs': 1.098551630973816, 'CONS': 1.0986119508743286, 'Experiments do not reveal anything particularly surprising or unexpected': 1.0721261501312256, 'The UGRNN and +RNN architectures do not feel well-motivated': 0.7450554966926575, 'The utility of the UGRNN and +RNN architectures is not well-established': 0.6506519913673401, 'The authors investigate a variety of existing and two new RNN architectures to obtain more insight about the effectiveness at which these models can store task information in their parameters and activations.': 0.8568544387817383, 'The experimental setups look sound.': 1.0955164432525635, 'To generalize comparisons between different architectures it’s necessary to consider multiple tasks and control for the effect of the hyperparameters.': 0.9646210074424744, 'This work uses multiple tasks of varying complexities, principled hyperparameter tuning methodology and a number of tuning iterations that can currently only be achieved by the computational resources of some of the larger industrial research groups.': 0.6966001391410828, 'The descriptions of the models and the objective where very clear to me.': 0.41758906841278076, 'The descriptions of the experiments and presentation of the results were not always clear to me at times, even with the additional details in the appendix available.': 0.5961109399795532, 'Most of these issues can easily be resolved by editing the text.': 0.5042003989219666, 'For example, in the memory task the scaling of the inputs (and hence also outputs) is not provided so it’s hard to interpret the squared error scores in Figure 2c.': 1.0448098182678223, 'It’s not clear to me what the term ‘unrollings’ refers to in Figure 2b.': 0.7203943133354187, 'Is this a time lag with additional hidden state updates between the presentation of the input sequence and the generation of the output?': 1.0022956132888794, 'Since the perceptron capacity task is somewhat central to the paper, I think a slightly more precise description of how and when the predictions are computed would be helpful.': 0.7342824935913086, 'Due to the large number of graphs, it can be somewhat hard to find the most relevant results.': 0.4901651442050934, 'Perhaps some of the more obvious findings (like Figure 1(b-d) given Figure 1a) could move to the appendix to make space for more detailed task descriptions.': 0.42373552918434143, 'Novelty is not really the aim of this paper since it mostly investigates existing architectures.': 0.5674527287483215, 'To use the mutual information to obtain bits per parameter scores in highly non-linear parameterized functions is new to me.': 0.8029137253761292, 'The paper also proposed to new architectures that seem to have practical value.': 0.7870168089866638, 'The paper adds to the currently still somewhat neglected research effort to employ the larger computational resources we currently have towards a better understanding of architectures which were designed when such resources were not yet available.': 1.0985931158065796, 'I’d argue that the paper is original enough for that reason alone.': 1.0986117124557495, 'The paper provides some interesting new insights into the properties of RNNs.': 1.097132921218872, 'While observed before, it is interesting to see the importance of gated units for maintaining trainability of networks with many layers.': 1.0986095666885376, 'It is also interesting to see a potential new use for vanilla RNNs for simpler tasks where a high capacity per parameter may be required due to hardware constraints.': 1.0986039638519287, 'The proposed +RNN may turn out to have practical value as well and the hyperparameter robustness results shed some light on the popularity of certain architectures when limited time for HP tuning is available.': 1.0981359481811523, 'The large body of results and hyperparameter analysis should be useful to many researchers who want to use RNNs in the future.': 1.0985993146896362, 'All in all, I think this paper would make a valuable addition to the ICLR conference but would benefit from some improvements to the text.': 1.0969494581222534, '* Thorough analysis.': 1.0986123085021973, '* Seemingly proper experiments.': 1.0986014604568481, '* The way of quantifying capacity in neural networks adds to the novelty of the paper.': 1.097946286201477, 'The results have some practical value and suggest similar analysis of other architectures.': 1.0986123085021973, 'The results provide useful insights into the relative merits of different RNN architectures.': 1.0986123085021973, '* It’s hard to isolate the most important findings (some plots seem redundant).': 1.0981754064559937, '* Some relevant experimental details are missing.': 1.0986123085021973}"
114,https://openreview.net/forum?id=BydrOIcle,"{'This work introduces a novel method for training GANs by displacing simultaneous SGD, and unrolling the inner optimization in the minmax game as a computational graph.': 1.0986123085021973, 'The paper is very clearly written, and explains the justification very well.': 1.0986123085021973, 'The problem being attacked is very significant and important.': 1.0986123085021973, 'The approach is novel, however, similar ideas have been tried to solve problems unrelated to GANs.': 1.0986123085021973, 'The first quantitative experiment is section 3.3.1, where the authors attempt to find the best z which can generate training examples.': 1.0986123085021973, 'This is done by using L-BFGS on |G(z) - x|.': 1.0986123085021973, ""The claim is that if we're able to find such a z, then the generator can generate this particular training example."": 1.0986123085021973, ""It's demonstrated that 0-step GANs are not able to generate many training examples, while unrolled GANs do."": 1.0986123085021973, 'However, I find this experiment unreasonable.': 1.0986123085021973, 'Being able to find a certain z, which generates a certain sample does not guarantee that this particular mode is high probability.': 1.0986123085021973, 'In fact, an identity function can potentially beat all the GAN models in the proposed metric.': 1.0986123085021973, ""And due to Cantor's proof of equivalence between all powers of real spaces, this applies to smaller dimension of z as well."": 1.0986123085021973, 'More realistically, it should be possible to generate *any* image from a generator by finding a very specific z. That a certain z exists which can generate a sample does not prove that the generator is not missing modes.': 1.0986123085021973, 'It just proves that the generator is similar enough to an identity function to be able to generate any possible image.': 1.0986123085021973, 'This metric is thus measuring something potentially tangential to diversity or mode-dropping.': 1.0986123085021973, ""Another problem with this metric is that that showing that the optimization is not able to find a z for a specific training examples does not prove that such a z does not exist, only that it's harder to find."": 1.0986123085021973, 'So, this comparison might just be showing that unrolled GANs have a smoother function than 0-step GANs, and thus easier to optimize for z.': 1.0986123085021973, 'The second quantitative experiment considers mean pairwise distance between generated samples, and between data samples.': 1.0986123085021973, 'The first number is likely to be small in the case of a mode-dropping GAN.': 1.0986123085021973, 'The authors argue that the two numbers being closer to each other is an indication of the generated samples being as diverse as the data.': 1.0986123085021973, 'Once again, this metric is not convincing.': 1.0986123085021973, '1.': 1.0986123085021973, 'The distances are being measured in pixel-space.': 1.0986123085021973, '2.': 1.0986123085021973, 'A GAN model could be generating garbage, and yet still perform very well in this metric.': 1.0986123085021973, 'There are no other quantitative results in the paper.': 1.0986123085021973, 'Even though the method is optimizing diversity, for a sanity check, scores for quality such as Inception scores or SSL performance would have been useful.': 1.0986123085021973, 'Another metric that the authors can consider is training GAN using this approach on the tri-MNIST dataset (concatenation of 3 MNIST digits), which results in 1000 easily-identifiable modes.': 1.0986123085021973, 'Then, demonstrate that the GAN is able to generate all the 1000 modes with equal probability.': 1.0986123085021973, 'This is not a perfect metric either, but arguably much better than the metrics in this paper.': 1.0986123085021973, 'This metric is used in this ICLR submission: https://openreview.net/pdf?id=HJKkY35le': 1.0986123085021973, 'Whether this paper is accepted or not, I encourage the authors to investigate this approach further, since the method is promising and interesting.': 1.0986123085021973, '# Post-rebuttal review': 1.0965721607208252, 'The authors have incorporated changed in the paper by adding more experiments.': 1.0590499639511108, 'These experiments now demonstrate the claims of the paper better.': 0.5618702173233032, 'The paper was already well-written and introduced a novel idea and addressed an important problem.': 1.0985143184661865, 'The only thing holding this paper back was unconvincing experiments, which now has been corrected.': 1.096358299255371, 'Thus, I would increase my score by 2 points, and recommend accepting the paper.': 0.4375635087490082, 'The paper presents an approach for tackling the instability problem that is present in generative adversarial networks.': 1.0986123085021973, 'The general idea is to allow the generator to ""peek ahead"" at how the discriminator will evolve its decision boundary over-time with the premise that this information should prevent the generator from collapsing to produce only samples from a single mode of the data distribution.': 1.0986123085021973, 'This is a very well written paper that clearly motivates its attack on an important open issue.': 1.0986121892929077, 'The experiments are well carried out and strongly support the presented idea.': 0.7673799395561218, 'The pursued approach is substantially more elegant than current existing ""hacks"" that are commonly used to make GANs work in practice.': 1.0986123085021973, 'I however have three main issues that let me partly doubt the success of the method.': 1.0986123085021973, 'If these can be resolved this paper is a clear candidate for acceptance.': 1.0986123085021973, '1) I am not entirely convinced that the same effect cannot be obtained by the following procedure: simply train the discriminator for an extended number of K steps when updating the generator (say a number equivalent to the unrolling steps used in the current experiments) then, after the generator was updated undo the K updates to the discriminator and do 1 new update step instead.': 1.0007762908935547, 'I only briefly glanced at your response to Reviewer2 which seems to imply you now tried something similar to this setup by stopping gradient flow at an appropriate point (although I think this is not exactly equivalent).': 1.0986123085021973, '2) I tried to reproduce the simple MNIST example but using a fully connected network instead of an RNN generator without much success.': 1.0964844226837158, 'Even when unrolling the discriminator for 30-40 steps the generator still engages in mode seeking behavior or does not train at all.': 1.0986123085021973, 'This could either be because of a bug in my implementation or because of some peculiarities of the RNN generator or because I did not use batch normalization anywhere.': 1.0986123085021973, 'If it is one of the latter two this would entail a dependence of the proposed approach on specific forms of the discriminator and generator and should be discussed.': 1.0985819101333618, 'My code can be found here https://github.com/iclrreproducer/unrolled_gan': 1.0986123085021973, 'see the comments in the file for switching on unrolling and batch normalization in the generator.': 1.0986123085021973, 'This issue could also be addressed simply by opening up the code and setting up a fully connected network example on your side.': 1.0986080169677734, '3)': 1.0986123085021973, 'For the more complicated data distributions the method is only used in combination with many of the existing tricks for training GANs.': 1.0986123085021973, 'As a result the experiments are much less convincing.': 1.0986123085021973, 'I personally cannot see a strong difference between the samples generated by any of the CIFAR-10 models.': 1.0986123085021973, 'Why did you not try to remove batch-normalization etc. for these examples ?': 1.0986123085021973, 'Does training then again become unstable ?': 1.0986123085021973, 'UPDATE': 1.0986123085021973, 'I am leaving the initial review intact to preserve the conversation.': 1.0986123085021973, 'After the authors response all points have been resolved, including a fix to my re-implementation of the ideas presented in the paper.': 1.0975161790847778, 'As a result I have increased my score and believe the paper should be accepted.': 1.0986123085021973, 'Please also see my response to the rebuttal below.': 1.0984289646148682, 'The paper introduces a technique for stabilizing the training of Generative Adversrial Networks by unrolling the inner (discriminator) optimization in the GAN loss function several steps and optimizing the generator with respect to the final state of this optimization process.': 1.0986121892929077, 'The experimental evidence that this actually helps is very compelling: the 2d example shows a toy problem where this technique helps substantially, the LSTM MNIST generator example shows that the procedure helps with stabilizing the training of an unusual architecture of generator, and the image generation experiment, while not being definitive, is very convincing.': 1.0986123085021973, 'For future work it would be interesting to see whether a method with smaller memory requirements could be devised based on similar principles.': 1.0986123085021973, 'I strongly recommend to accept this paper.': 0.5825808048248291}"
115,https://openreview.net/forum?id=Bygq-H9eg,"{'The paper evaluates recent development in competitive ILSVRC CNN architectures from the perspective of resource utilization.': 1.0986123085021973, 'It is clear that a lot of work has been put into the evaluations.': 1.0986123085021973, 'The findings are well presented and the topic itself is important.': 1.0986123085021973, 'However, most of the results are not surprising to people working with CNNs on a regular basis.': 1.0986123085021973, 'And even if they are, I am not convinced about their practical value.': 1.0986123085021973, 'It is hard to tell what we actually learn from these findings when approaching new problems with computational constraints or when in production settings.': 1.0986123085021973, 'In my opinion, this is mainly because the paper does not discuss realistic circumstances.': 1.0986123085021973, 'Main concerns:': 1.0986123085021973, '1) The evaluation does not tell me much for realistic scenarios, that mostly involve fine-tuning networks, as ILSVRC is just a starting point in most cases.': 1.0985612869262695, 'VGG for instance really shines for fine-tuning, but it is cumbersome to train from scratch.': 1.0986123085021973, 'And VGG works well for compression, too.': 1.0986123085021973, 'So possibly it is a very good choice if these by now standard steps are taken into account.': 1.0986123085021973, 'Such questions are of high practical relevance!': 1.0986123085021973, '2) Compressed networks have a much higher acc/parameter density, so comparison how well models can be compressed is important, or at least comparing to some of the most well-known and publicly available compressed networks.': 1.0970940589904785, '3) There is no analysis on the actual topology of the networks and where the bottlenecks lie.': 1.0986123085021973, 'This would be very useful to have as well.': 1.0986123085021973, 'Minor concern:': 1.0986123085021973, '1) Why did the authors choose to use batch normalization in NiN and AlexNet?': 1.0986123085021973, 'The authors did solid work in collecting all the reported data.': 1.0986123085021973, ""However, most findings don't seem to be too surprising to me:"": 1.0986123085021973, 'Finding #1 mainly shows that all architectures and batch sizes manage to utilize the GPU fully (or to the same percentage).': 1.0947468280792236, 'Regarding Finding #2, I agree that from a linear relationship in Figure 9 you could conclude said hyperbolic relationship.': 0.9459893107414246, 'However, for this finding to be relevant, it has to hold especially for the latest generations of models.': 1.0557996034622192, 'These cluster in the upper left corner of Figure 9 and on their own do not seem to show too much of a linear behaviour.': 0.9442707300186157, 'Therefore I think there is not enough evidence to conclude asymptotic hyperbolic behaviour: For this the linear behaviour would have to be the stronger, the more models approach the upper left corner.': 1.098551869392395, 'Finding #3 seems to be a simple conclusion from finding #1: As long as slower models are better and faster models do draw the same power, finding #3 holds.': 0.45896023511886597, 'Finding #4 is again similar to finding #1: If all architectures manage to fully utilize the GPU, inference time should be proportional to the number of operations.': 1.0984318256378174, ""Maybe the most interesting finding would be that all tested models seem to use the same percentage of computational resources available on the GPU, while one might expect that more complex models don't manage to utilize as much computational resources due to inter-dependencies."": 1.0985673666000366, 'However actual GPU utilization was not evaluated and as the authors choose to use an older GPU, one would expect that all models manage to make use of all available computational power.': 1.0935237407684326, 'Additionally, I think these findings would have to be put in relation with compressing techniques or tested on actual production networks to be of more interest.': 1.097069263458252, 'A few issues with this paper:': 1.0986123085021973, ""1- I find finding #2 trivial and unworthy of mention, but the author don't seem to agree with me that it is."": 1.0986121892929077, 'See discussions.': 1.0986123085021973, ""2- Finding #1 relies on Fig #4, which appears very noisy and doesn't provide any error analysis."": 1.0982383489608765, 'It makes me question how robust this finding is.': 1.0985792875289917, ""One would have naively expected the power usage trend to mirror Fig #3, but given the level of noise, I can't convince myself whether the null hypothesis of there being no dependency between batch size and power consumption is more likely than the alternative."": 1.0543947219848633, '3- Paper is unfriendly to colorblind readers (or those with B/W printers)': 0.9875005483627319, ""Overall, this paper is a reasonable review of where we are in terms of SOTA vision architectures, but doesn't provide much new insight."": 1.0922775268554688, ""I found most interesting the clear illustration that VGG models stand out in terms of being a bad tradeoff in resource-constrained environments (too many researchers are tempted to benchmark their model compression algorithm on VGG-class models because that's always where one can show 10x improvements without doing much.)"": 1.0986123085021973}"
116,https://openreview.net/forum?id=Byiy-Pqlx,"{'The paper introduces a novel memory mechanism for NTMs based on differentiable Lie groups.': 1.3862943649291992, 'This allows to place memory elements as points on a manifold, while still allowing training with backpropagation.': 1.3862943649291992, ""It's a more general version of the NTM memory, and possibly allows for training a more efficient addressing schemes."": 1.3862943649291992, 'Pros:': 1.38583242893219, 'novel and interesting idea for memory access': 1.3862943649291992, 'nicely written': 1.3862943649291992, 'Cons:': 1.3860372304916382, 'need to manually specify the Lie group to use (it would be better if network could learn the best way of accessing memory)': 1.3569077253341675, 'not clear if this really works better than standard NTM (compared only to simplified version)': 1.0202717781066895, 'not clear if this is useful in practice (no comparison on real tasks)': 1.3855003118515015, 'The Neural Turing Machine and related “external memory models” have demonstrated an ability to learn algorithmic solutions by utilizing differentiable analogues of conventional memory structures.': 1.3862943649291992, 'In particular, the NTM, DNC and other approaches provide mechanisms for shifting a memory access head to linked memories from the current read position.': 1.386163353919983, 'The NTM, which is the most relevant to this work, uses a differentiable version of a Turing machine tape.': 1.360756754875183, 'The controller outputs a kernel which “softly” shifts the head, allowing the machine to read and write sequences.': 1.3862942457199097, 'Since this soft shift typically “smears” the focus of the head, the controller also outputs a sharpening parameter which compensates by refocusing the distribution.': 1.3862943649291992, 'The premise of this work is to notice that while the NTM emulates a differentiable version of a Turing tape, there is no particular reason that one is constrained to follow the topology of a Turing tape.': 1.3862606287002563, 'Instead they propose memory stored at a set of points on a manifold and shift actions which form a Lie group.': 1.3862733840942383, 'In this way, memory points can have have different relationships to one another, rather than being constrained to Z.': 1.3845994472503662, 'This is mathematically elegant and here they empirically test models with the shift group R^2 acting on R^2 and the rotation group acting on a sphere.': 1.3862943649291992, 'Overall, the paper is well communicated and a novel idea.': 1.3862699270248413, 'The primary limitation of this paper is its limited impact.': 1.3751182556152344, 'While this approach is certainly mathematically elegant, even likely beneficial for some specific problems where the problem structure matches the group structure, it is not clear that this significantly contributes to building models capable of more general program learning.': 1.3862686157226562, 'Instead, it is likely to make an already complex and slow model such as the NTM even slower.': 1.3862910270690918, 'In general, it would seem memory topology is problem specific and should therefore be learned rather than specified.': 1.3862922191619873, 'The baseline used for comparison is a very simple model, which does not even having the sharpening (the NTM approach to solving the problem of head distributions becoming ‘smeared’).': 1.385376214981079, 'There is also no comparison with the successor to the NTM, the DNC, which provides a more general approach to linking memories based on prior memory accesses.': 1.3279149532318115, 'Minor issues:': 1.378423810005188, 'Footnote on page 3 is misleading regarding the DNC.': 0.7296741604804993, 'While the linkage matrix explicitly excludes the identity, the controller can keep the head in the same position by gating the following of the link matrix.': 1.2581822872161865, 'Figures on page 8 are difficult to follow.': 1.386273980140686, 'The paper proposes a new memory access scheme based on Lie group actions for NTMs.': 1.3761745691299438, '* Well written': 1.3743470907211304, '* Novel addressing scheme as an extension to NTM.': 1.3862943649291992, '* Seems to work slightly better than normal NTMs.': 1.3862943649291992, '* Some interesting theory about the novel addressing scheme based on Lie groups.': 1.3862943649291992, '* In the results, the LANTM only seems to be slightly better than the normal NTM.': 1.3862943649291992, '* The result tables are a bit confusing.': 1.3862943649291992, '*': 1.3862943649291992, 'No source code available.': 1.376795768737793, ""The difference to the properties of normal NTM doesn't become too clear."": 1.3862942457199097, 'Esp it is said that LANTM are better than NTM because they are differentiable end-to-end and provide a robust relative indexing scheme but NTM are also differentiable end-to-end and also provide a robust indexing scheme.': 1.3862942457199097, '* It is said that the head is discrete in NTM': 1.3862943649291992, 'but actually it is in space R^n, i.e. it is already continuous.': 1.3862943649291992, ""It doesn't become clear what is meant here."": 1.3862943649291992, 'No tests on real-world tasks, only some toy tasks.': 1.3862943649291992, 'No comparisons to some of the other NTM extensions such as D-NTM or Sparse Access Memory (SAM) (https://arxiv.org/abs/1610.09027).': 1.3862943649291992, 'Although the motivations of other NTM extensions might be different, such comparisons still would have been interesting.': 1.3862943649291992, '*** Paper Summary ***': 1.3862943649291992, 'This paper formalizes the properties required for addressing (indexing) memory augmented neural networks as well as how to pair the addressing with read/write operation.': 1.3862943649291992, 'It then proposes a framework in which any Lie group as the addressing space.': 1.3862943649291992, 'Experiments on algorithmic tasks are reported.': 1.3862943649291992, '*** Review Summary ***': 1.3862943649291992, 'This paper brings unity and formalism in the requirement for memory addressing while maintaining differentiable memories.': 1.3862943649291992, 'Its proposal provide a generic scheme to build addressing mechanisms.': 1.3862943649291992, 'When comparing the proposed approach with key-value networks, the unbounded number of memory cells and the lack of incentive to reuse indexes might reveal impractical.': 1.3862943649291992, '*** Detailed Review ***': 1.3862943649291992, 'The paper reads well, has appropriate relevance to related work.': 1.3862943649291992, 'The unified presentation of memory augmented networks is clear and brings unity to the field.': 1.3862943649291992, 'The proposed approach is introduced clearly, is powerful and gives a tool that can be reused after reading the article.': 1.3862943649291992, 'I do not appreciate that the growing memory is not mentioned as a drawback.': 1.3862943649291992, 'It should be stressed and a discussion on the impact it has on efficiency/scalability is needed.': 1.3862943649291992}"
117,https://openreview.net/forum?id=Byj72udxe,"{'This paper proposes augmenting RNN-based language models with a pointer network in order to deal better with rare words.': 0.4238225519657135, 'The pointer network can point to words in the recent context, and hence the prediction for each time step is a mixture between the usual softmax output and the pointer distribution over the recent words.': 1.0915336608886719, 'The paper also introduces a new language modelling dataset, which overcomes some of the shortcomings of previous datasets.': 1.0749242305755615, 'The reason for the score I gave for this paper is that I find the proposed model a direct application of the previous work Gulcehre et al., which follows a similar approach but for machine translation and summarization.': 1.098207712173462, 'The main differences I find is that Gulcehre et al. use an encoder-decoder architecture, and use the attention weights of the encoder to point to locations of words in the input, while here an RNN is used and a pointer network produces a distribution over the full vocabulary (by summing the softmax probabilities of words in the recent context).': 1.0986027717590332, 'The context (query) vector for the pointing network is also different, but this is also a direct consequence of having a different application.': 0.9454883933067322, 'While the paper describes the differences between the proposed approach and Gulcehre et al.’s approach, I find some of the claims either wrong or not that significant.': 1.0986123085021973, 'For example, quoting from Section 1:': 1.0986123085021973, '“Rather than relying on the RNN hidden state to decide when to use the pointer, as in the recent work of Gulcehre et al. (2016), we allow the pointer component itself to decide when to use the softmax vocabulary through a sentinel.”': 1.0986121892929077, 'As far as I can tell, your model also uses the recent hidden state to form a query vector,  which is matched by the pointer network to previous words.': 1.0986123085021973, 'Can you please clarify what you mean here?': 1.0986123085021973, 'In addition, quoting from section 3 which describes the model of Gulcehre et al.:': 1.0986123085021973, '“Rather than constructing a mixture model as in our work, they use a switching network to decide which component to use”': 1.0986123085021973, 'This is not correct.': 1.0986123085021973, 'The model of Gulcehre is also a mixture model, where an MLP with sigmoid output (switching network) is used to form a mixture between softmax prediction and locations of the input text.': 1.0986123085021973, 'Finally, in the following quote, also from section 3:': 1.0986123085021973, '“The pointer network is not used as a source of information for the switching network as in our model.”': 1.0986123085021973, 'It is not clear what the authors mean by “source of information” here.': 1.0986123085021973, 'Is it the fact that the switching probability is part of the pointer softmax?': 1.0986123085021973, 'I am wondering how significant this difference is.': 1.0986123085021973, 'With regards to the proposed dataset, there are also other datasets typically used for language modelling, including The Hutter Prize Wikipedia (enwik8) dataset (Hutter, 2012) and e Text8 dataset (Mahoney, 2009).': 1.0986123085021973, 'Can you please comment on the differences between your dataset and those as well?': 1.0986123085021973, 'I would be happy to discuss with the authors the points I raised, and I am open to changing my vote if there is any misunderstanding on my part.': 1.0986123085021973, 'This work is basically a combined pointer network applied on language modelling.': 1.0986123085021973, 'The smart point is that this paper aims at language modelling with longer context, where a memory of seen words (especially the rare words) would be very useful for predicting the rest of the sentences.': 1.0924800634384155, 'Hence, a combination of a pointer network and a standard language model would balance the copying seen words and predicting unseen words.': 1.0986123085021973, 'Generally, such as the combined pointer networks applied in sentence compression, a vector representation of the source sequence would be used to compute the gate.': 1.0986123085021973, 'This paper, instead, introduces a sentinel vector to carry out the mixture model, which is suitable in the case of language modelling.': 1.0986123085021973, 'I would be interested in the variations of sentinel mixture implementation, though the current version has achieved very good results.': 1.0986123085021973, 'In addition, the new WikiText language modelling dataset is very interesting.': 1.0986123085021973, 'It probably can be a more standard dataset for evaluating the continuously-updated language model benchmarks than ptb dataset.': 1.0986123085021973, 'Overall, this is a well-written paper.': 1.0986123085021973, 'I recommend it to be accepted.': 1.0986123085021973, 'This work is an extension of previous works on pointer models, that mixes its outputs with standard softmax outputs.': 1.0986123085021973, 'The idea is appealing in general for context biasing and the specific approach appears quite simple.': 1.0986123085021973, 'The idea is novel to some extent, as previous paper had already tried to combine pointer-based and standard models,': 1.0986123085021973, 'but not as a mixture model, as in this paper.': 1.0986123085021973, 'The paper is clearly written and the results seem promising.': 1.0986123085021973, 'The new dataset the authors created (WikiText) also seems of high interest.': 1.0986123085021973, 'A comment regarding notation:': 1.0986123085021973, 'The symbol p_ptr is used in two different ways in eq. 3 and eq. 5. : p_ptr(w) vs. p_ptr(y_i|x_i)': 1.0986123085021973, 'This is confusing as these are two different domains: for eq 3.': 1.0986123085021973, 'the domain is a *set* of words and for eq. 5': 1.0986123085021973, 'the domain is a *list* of context words.': 1.0986123085021973, 'It would be helpful to use different symbol for the two objects.': 1.0986123085021973}"
118,https://openreview.net/forum?id=Byk-VI9eg,"{'This work brings multiple discriminators into GAN.': 1.0899871587753296, 'From the result, multiple discriminators is useful for stabilizing.': 1.0706504583358765, 'The main problem of stabilizing seems is from gradient signal from discriminator, the authors motivation is using multiple discriminators to reduce this effect.': 1.0986061096191406, 'I think this work indicates the direction is promising, however I think the authors may consider to add more result vs approach which enforce discriminator gradient, such as GAN with DAE (Improving Generative Adversarial Networks with Denoising Feature Matching), to show advantages of multiple discriminators.': 1.0986078977584839, 'The paper extends the GAN framework to accommodate multiple discriminators.': 1.0986123085021973, 'The authors motivate this from two points of view:': 1.098604679107666, '(1) Having multiple discriminators tackle the task is equivalent to optimizing the value function using random restarts, which can potentially help optimization given the nonconvexity of the value function.': 1.098610758781433, '(2) Having multiple discriminators can help overcome the optimization problems arising when a discriminator is too harsh a critic.': 1.098609209060669, 'A generator receiving signal from multiple discriminators is less likely to be receiving poor gradient signal from all discriminators.': 1.0986123085021973, ""The paper's main idea looks straightforward to implement in practice and makes for a good addition to the GAN training toolbelt."": 1.0986123085021973, 'I am not very convinced by the GAM (and by extension the GMAM) evaluation metric.': 1.0986123085021973, 'Without evidence that the GAN game is converging (even approximately), it is hard to make the case that the discriminators tell something meaningful about the generators with respect to the data distribution.': 1.0986123085021973, 'In particular, it does not inform on mode coverage or probability mass misallocation.': 1.0986123085021973, 'The learning curves (Figure 3) look more convincing to me: they provide good evidence that increasing the number of discriminators has a stabilizing effect on the learning dynamics.': 1.0986123085021973, 'However, it seems like this figure along with Figure 4 also show that the unmodified generator objective is more stable even with only one discriminator.': 1.0986123085021973, 'In that case, is it even necessary to have more than one discriminator to train the generator using an unmodified objective?': 1.0986123085021973, 'Overall, I think the ideas presented in this paper show good potential, but I would like to see an extended analysis in the line of Figures 3 and 4 for more datasets before I think it is ready for publication.': 1.0986123085021973, 'UPDATE:': 1.0986123085021973, 'The rating has been revised to a 7 following discussion with the authors.': 1.0986123085021973, 'In this interesting paper the authors explore the idea of using an ensemble of multiple discriminators in generative adversarial network training.': 1.0986123085021973, 'This comes with a number of benefits, mainly being able to use less powerful discriminators which may provide better training signal to the generator early on in training when strong discriminators might overpower the generator.': 1.0986123085021973, 'My main comment is about the way the paper is presented.': 1.0986123085021973, 'The caption of Figure 1. and Section 3.1 suggests using the best discriminator by taking the maximum over the performance of individual ensemble members.': 1.0986123085021973, 'This does not appear to be the best thing to do because we are just bound to get a training signal that is stricter than any of the individual members of the ensemble.': 1.0986123085021973, 'Then the rest of the paper explores relaxing the maximum and considers various averaging techniques to obtain a ’soft-discriminator’.': 1.0986123085021973, 'To me, this idea is far more appealing, and the results seem to support this, too.': 1.0986123085021973, 'Skimming the paper it seems as if the authors mainly advocated always using the strongest discriminator, evidenced by my premature pre-review question earlier.': 1.0986123085021973, 'Overall, I think this paper is a valuable contribution, and I think the idea of multiple discriminators is an interesting direction to pursue.': 1.0986123085021973}"
119,https://openreview.net/forum?id=BylSPv9gx,"{'The paper proposes a method for pruning weights in neural networks during training to obtain sparse solutions.': 0.11338962614536285, 'The approach is applied to an RNN-based system which is trained and evaluated on a speech recognition dataset.': 0.3766102194786072, 'The results indicate that large savings in test-time computations can be obtained without affecting the task performance too much.': 0.014026299118995667, 'In some cases the method can actually improve the evaluation performance.': 0.6931381225585938, 'The experiments are done using a state-of-the-art RNN system and the methodology of those experiments seems sound.': 0.5672848224639893, 'I like that the effect of the pruning is investigated for networks of very large sizes.': 0.6644787788391113, 'The computational gains are clearly substantial.': 0.6931471824645996, 'It is a bit unfortunate that all experiments are done using a private dataset.': 0.6931231617927551, 'Even with private training data, it would have been nice to see an evaluation on a known test set like the HUB5 for conversational speech.': 0.6931471824645996, 'It would also have been nice to see a comparison with some other pruning approaches given the similarity of the proposed method to the work by Han et al.': 0.6931468844413757, '[2] to verify the relative merit of the proposed pruning scheme.': 0.6931346654891968, 'While single-stage training looks more elegant at first sight, it may not save much time if more experiments are needed to find good hyperparameter settings for the threshold adaptation scheme.': 0.6931471824645996, 'Finally, the dense baseline would have been more convincing if it involved some model compression tricks like training on the soft targets provided by a bigger network.': 0.6931471824645996, 'Overall, the paper is easy to read.': 0.6678838133811951, 'The table and figure captions could be a bit more detailed but they are still clear enough.': 0.6900864243507385, 'The discussion of potential future speed-ups of sparse recurrent neural networks and memory savings is interesting but not specific to the proposed pruning algorithm.': 0.6931435465812683, 'The paper doesn’t motivate the details of the method very well.': 0.6893152594566345, 'It’s not clear to me why the threshold has to ramp up after a certain period time for example.': 0.693112850189209, 'If this is based on preliminary findings, the paper should mention that.': 0.5823677778244019, 'Sparse neural networks have been the subject of research for a long time and this includes recurrent neural networks (e.g., sparse recurrent weight matrices were standard for echo-state networks [1]).': 0.6885778903961182, 'The proposed method is also very similar to the work by Han et al.': 0.6262171864509583, '[2], where a threshold is used to prune weights after training, followed by a retraining phase of the remaining weights.': 0.6931471824645996, 'While I think that it is certainly more elegant to replace this three stage procedure with a single training phase, the proposed scheme still contains multiple regimes that resemble such a process by first training without pruning followed by pruning at two different rates and finally training without further pruning again.': 0.6931453347206116, 'The main novelty of the work would be the application of such a scheme to RNNs, which are typically more tricky to train than feedforward nets.': 0.6637213230133057, 'Improving scalability is an important driving force of the progress in neural network research.': 0.0047147199511528015, 'While I don’t think the paper presents much novelty in ideas or scientific insight, it does show that weight pruning can be successfully applied to large practical RNN systems without sacrificing much in performance.': 0.6931344270706177, 'The fact that this is possible with such a simple heuristic is a result worth sharing.': 0.059204623103141785, 'Pros:': 0.6931465864181519, 'The proposed method is successful at reducing the number of parameters in RNNs substantially without sacrificing too much in performance.': 0.6907492280006409, 'The experiments are done using a state-of-the-art system for a practical application.': 0.689700722694397, 'Cons:': 0.6908758282661438, 'The proposed method is very similar to earlier work and barely novel.': 0.2465788722038269, 'There is no comparison with other pruning methods.': 0.6931471824645996, 'The data is private and this prevents others from replicating the results.': 0.6931471824645996, '[1] Jaeger, H. (2001).': 0.6931471824645996, 'The “echo state” approach to analyzing and training recurrent neural networks-with an erratum note.': 0.6931470632553101, 'Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, 148, 34.': 0.6931471228599548, '[2] Han, Song, Pool, Jeff, Tran, John, and Dally, William J. Learning both weights and connections for efficient neural networks.': 0.6931218504905701, 'In Advances in Neural Information Processing Systems, 2015.': 0.6931471824645996, 'Summary: The paper presents a technique to convert a dense to sparse network for RNNs. The algorithm will increasingly set more weights to zero during the RNN training phase. This provides a RNN model with less storage requirement and higher inference rate.': 0.6931471824645996, 'Proposes a pruning method that doesn’t need re-training and doesn’t affect the training phase of RNN.': 0.6931365132331848, 'The method achieves 90% sparsity, and hence less number of parameters.': 0.11310431361198425, 'Cons & Questions:': 0.6931471824645996, 'Judiciously choosing hyper parameters for different models and different applications wouldn’t be cumbersome?': 0.693146824836731, 'In equation 1, is q the sparsity of final model?': 0.6931471824645996, 'Is there a formula to know what is sparsity, number of parameters and accuracy of final model given a set of hyper parameters, before going through training?': 0.6931471824645996, '(Questions answered)': 0.6931471824645996, 'In table3, we see a trade-off between number of units and sparsity to achieve better number of parameters or accuracy, or in table5 better speed.': 0.6931471824645996, 'Good, but where are the results for GRU sparse big?': 0.6930961012840271, 'I mean, accuracy must be similar and still get decent compression rate and speed up.': 0.6931471824645996, 'Just like RNN Sparse medium compared with RNN Dense.': 0.6921203136444092, 'I can’t see much advantage of pruning and getting high speed-up if you are sacrificing so much accuracy.': 0.6931464672088623, '(Issue fixed with updated data)': 0.014293678104877472, 'Why sparsity for table3 and table5 are different?': 0.6931186318397522, 'In text: “average sparsity of 88%” but in table5 is 95%?': 0.6931471824645996, 'Are the models used in table3 different from table5?': 0.6931467652320862, '(Issue fixed)': 0.6931471824645996}"
120,https://openreview.net/forum?id=ByldLrqlx,"{'This is a good paper, well written, that presents a simple but effective approach to predict code properties from input output pairs.': 1.0986121892929077, 'The experiments show superiority to the baseline, with speedup factors between one to two orders of magnitude.': 1.0986123085021973, 'This is a solid gain!': 1.0986123085021973, 'The domain of programs is limited, so there is more work to do in trying such ideas on more difficult tasks.': 1.0986123085021973, 'Using neural nets to augment the search is a good starting point and a right approach, instead of generating full complex code.': 1.0986123085021973, 'I see this paper as being above the threshold for acceptance.': 1.0986123085021973, 'The paper presents a technique to combine deep learning style input-output training with search techniques to match the input of a program to the provided output.': 0.6575011610984802, 'Orders of magnitude speedup over non-augmented baselines are presented.': 1.079138159751892, 'Summary:': 1.0986123085021973, '———': 1.0562573671340942, 'The proposed search for source code implementations based on a rather small domain specific language (DSL) is compelling but also expected to some degree': 1.0986101627349854, 'Quality: The paper is well written.': 1.0935862064361572, 'Clarity: Some of the derivations and intuitions could be explained in more detail but the main story is well described.': 1.0058006048202515, 'Originality: The suggested idea to speed up search based techniques using neural nets is perfectly plausible.': 1.0985580682754517, 'Significance: The experimental setup is restricted to smaller scales but the illustrated improvements are clearly apparent.': 0.4339076578617096, 'Details:': 1.0986123085021973, '————': 1.0985482931137085, '1. The employed test set of 100 programs seems rather small. in addition the authors ensure that the test set programs are semantically disjoint from the training set programs. Could the authors provide additional details about the small size of the test set and how to the disjoint property is enforced?': 1.0986113548278809, '2. The length of the programs is rather small at this point in time. A more detailed ablation regarding the runtime seems useful. The search based procedure is probably still the computationally most expensive part. Hence the neural net provides some additional prior information rather than tackling the real task.': 1.0985954999923706, 'This paper presents an approach to learn to generate programs.': 1.0986123085021973, 'Instead of directly trying to generate the program, the authors propose to train a neural net to estimate a fix set of attributes, which then condition a search procedure.': 1.0986123085021973, 'This is an interesting approach, which make sense, as building a generative model of programs is a very complex task.': 1.0986123085021973, 'Faster computation times are shown in the experimental section with respect to baselines including DFS, Enumeration, etc. in a setup with very small programs of length up to 5 instructions have to be found.': 1.0986123085021973, 'It is not clear to me how the proposed approach scales to larger programs, where perhaps many attributes will be on.': 1.0986123085021973, 'Is there still an advantage?': 1.0986123085021973, 'The authors use as metric the time to find a single program, whose execution will result in the set of 5 input-output pairs given as input.': 1.0986123085021973, 'However, as mentioned in the paper, one is not after a generic program but after the best program, or a rank list of all programs (or top-k programs) that result in a correct execution.': 1.098608374595642, 'Could the authors show experiments in this setting?': 1.0986123085021973, 'would still be useful to have the proposed approach?': 1.0986123085021973, 'what would the challenges be in this more realistic scenario?': 1.0986123085021973, 'In the second experiment the authors show results where the length of the program at training time is different than the length at test time.': 1.0986123085021973, 'However, the results are shown when only 20% of the programs are finished.': 1.0986123085021973, 'Could you show results for finding all programs?': 1.0986123085021973, 'The paper is missing an analysis of the results.': 1.0986123085021973, 'What type of programs are difficult?': 1.0986123085021973, 'how often is the NNet wrong?': 1.0986123085021973, 'how does this affect speed?': 1.0986123085021973, 'what are the failure modes of the proposed method?': 1.0986123085021973, 'The authors proposed to have a fix-length representation of the each input-output pair, and then use average pooling to get the final representation.': 1.0986123085021973, 'However, why would average pooling make sense here?': 1.0986123085021973, 'would it make more sense to combine the predictions at the decoder, not the encoder?': 1.0986123085021973, 'Learning from only 5 executions seems very difficult to me.': 1.0986123085021973, 'For programs so small it might be ok, but going to more difficult and longer programs this setting does not seem reasonable.': 1.0986123085021973, 'In summary an interesting paper.': 1.0986123085021973, 'This paper tackles a problem that is outside my area of expertise so I might have miss something important.': 1.0986123085021973}"
121,https://openreview.net/forum?id=BymIbLKgl,"{""I'm torn on this one."": 1.0969300270080566, 'Seeing the MPEG-7 dataset and references to curvature scale space brought to mind the old saying that ""if it\'s not worth doing, it\'s not worth doing well.""': 1.0884302854537964, ""There is no question that the MPEG-7 dataset/benchmark got saturated long ago, and it's quite surprising to see it in a submission to a modern ML conference."": 1.0986123085021973, 'I brought up the question of ""why use this representation"" with the authors and they said their ""main purpose was to connect the theory of differential geometry of curves with the computational engine of a convolutional neural network.""': 1.0986123085021973, 'Fair enough.': 1.0986123085021973, 'I agree these are seemingly different fields, and the authors deserve some credit for connecting them.': 1.0986123085021973, 'If we give them the benefit of the doubt that this was worth doing, then the approach they pursue using a Siamese configuration makes sense, and their adaptation of deep convnet frameworks to 1D signals is reasonable.': 1.0986123085021973, ""To the extent that the old invariant based methods made use of smoothed/filtered representations coupled with nonlinearities, it's sensible to revisit this problem using convnets."": 1.0986123085021973, ""I wouldn't mind seeing this paper accepted, since it's different from the mainstream, but I worry about there being too narrow an audience at ICLR that still cares about this type of shape representation."": 1.061499834060669, 'Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves.': 1.0986123085021973, 'With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants, as evaluated on few toy examples.': 1.0986123085021973, 'The paper is generally well written and shows an interesting application of the Siamese architecture.': 1.098475694656372, 'However, the experimental evaluation and the results show that these are rather preliminary results as not many of the choices are validated.': 1.0983405113220215, 'My biggest concern is in the choice of the negative samples, as the network basically learns only to distinguish between shapes at different scales, instead of recognizing different shapes.': 1.0986123085021973, 'It is well known fact that in order to achieve a good performance with the contrastive loss, one has to be careful about the hard negative sampling, as using too easy negatives may lead to inferior results.': 1.0986123085021973, 'Thus, this may be the underlying reason for such choice of the negatives?': 0.4236486256122589, 'Unfortunately, this is not discussed in the paper.': 1.0986121892929077, 'Furthermore the paper misses a more thorough quantitative evaluation and concentrates more on showing particular examples, instead of measuring more robust statistics over multiple curves (invariance to noise and sampling artifacts).': 1.0986123085021973, 'In general, the paper shows interesting first steps in this direction, however it is not clear whether the experimental section is strong and thorough enough for the ICLR conference.': 1.0761418342590332, 'Also the novelty of the proposed idea is limited as Siamese networks are used for many years and this work only shows that they can be applied to a different task.': 1.0986121892929077, 'Pros :': 1.0986123085021973, 'New representation with nice properties that are derived and compared with a mathematical baseline and background': 1.0986123085021973, 'A simple algorithm to obtain the representation': 1.0986123085021973, 'Cons :': 1.0986123085021973, 'The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first.': 1.0986121892929077}"
122,https://openreview.net/forum?id=ByqiJIqxg,"{'This paper proposes an online inference algorithm by using online Bayesian moment matching for HMM-GMM.': 1.0986095666885376, 'The method uses transfer learning by utilizing individual sequence estimators to predict a target sequence based on a weighted combination of individual HMM-GMM.': 1.0959365367889404, 'Online Bayesian moment matching has a benefit of updating HMM-GMM parameters frame-by-frame, and fits to this problem.': 1.0985302925109863, 'The authors compare the proposed method with the other sequential modeling methods including RNN and EM, and show the effectiveness of the proposed method.': 1.0983614921569824, 'The paper is well written overall.': 1.0986123085021973, 'Comments:': 1.0986123085021973, '1) Could you provide the average performance in table?': 1.0986123085021973, 'It is difficult to compare the performance only with individual performance.': 1.0986123085021973, 'Also, it seems that the EM performance is sometimes good': 1.0986123085021973, '2) I’m curious how initialization and hyper-parameter settings affect the final performance.': 1.0986123085021973, 'If you provide some information about it, that is great.': 1.0986123085021973, '3) It would be better to provide a figure of describing the transfer-learning-based proposed methods, since this is a unique and a little bit complicated setup.': 1.0986121892929077, ""I'm bumping up my score to a 7 to acknowledge that the authors responded satisfactorily to reviewer feedback, and to indicate that I think that the updated manuscript is a strong paper and that I do not object to its acceptance to ICLR."": 1.0986123085021973, 'However, I also would not fight for its acceptance.': 1.0986123085021973, 'I still think that it is a better fit for a venue with an explicit interest in more traditional Bayesian latent variable models (ICML, UAI, NIPS).': 1.0986123085021973, 'This manuscript describes a novel Bayesian approach to transfer learning focused on online sequence modeling settings where he primary concern is less distribution drift in an ongoing sequence and more variability between individual sequences (since each sequence can be thought of as defining its own conditional distribution over subsequent states).': 1.0986123085021973, ""They provide the example of human gait classification, where each individual's gait may differ from others even while performing the same activity."": 0.9431256651878357, 'In this setting we typically train the gait model on gait sequences from a set of ""source"" individuals but then apply it to previously unseen people.': 0.6725090146064758, 'The core model is an HMM, which they give a full Bayesian treatment; the central problem this introduces is that each new observation that arrives introduces an additional product term to the posterior that is itself a product over M components (clusters or hidden states).': 1.0986043214797974, 'This rapidly becomes intractable.': 1.0986123085021973, 'This paper applies Bayesian moment matching (BMM) to this problem, in which the posterior is approximated via projection onto a more tractable distribution that is adjusted to match some moments of the posterior.': 1.020596981048584, 'The experimental results are quite promising': 1.0986123085021973, 'Strengths:': 1.0986123085021973, 'The problem setting is very appropriately framed as online sequence prediction via Bayesian transfer learning.': 0.8719682097434998, 'There is almost certainly individual variability between the training sequences (for which explanatory variables not be available in the inputs).': 1.0978566408157349, 'The Bayesian approach gives a natural approach to performing transfer and handling low data regimes (common in all experiments).': 1.0985780954360962, 'The Bayesian formulation creates a computational challenge (the posterior becomes intractable).': 1.0986123085021973, 'The proposed BMM approximation is both reasonable and relatively novel (particularly given the popularity of MCMC and variational methods).': 1.0716091394424438, 'With the caveat that I am not well-versed in recent work on Bayesian moment matching, a cursory literature search suggests this is a novel application of BMM.': 1.0366748571395874, 'A lot of the related BMM work is roughly contemporaneous with this work, and none of it seems concerned with online transfer learning.': 1.0696475505828857, 'The overall results look quite strong: in activity recognition and flow prediction, the transfer learning approach is in general superior to the one-size-fits-all HMM, even when trained using BMM (which in turn is generally superior to the EM-based HMM).': 1.0969949960708618, 'The proposed approach appears to beat the LSTM across all tasks (including sleep stage classification), possibly due to the lack of training data.': 1.0774688720703125, 'Weaknesses:': 1.0986123085021973, 'The description of the LSTM training and architecture search is vague (and in one instance, contradictory), strongly implying that it was not fully tuned and may be an artificially weak baseline.': 1.0394564867019653, 'While it is plausible that the proposed approach might excel given the small data sets used in the experiments, there is not sufficient evidence and detail to support this claim.': 1.0985777378082275, 'The authors should provide more detail about architecture search, hyperparameter tuning, and most important, attempts at regularization, given the limited training data.': 1.0986077785491943, 'In particular, the authors should experiment with a sufficiently rich set of settings for # hidden layers, # hidden units, weight decay, and dropout.': 1.0985931158065796, 'Given the emphasis on transfer learning and the use of a Bayesian framework, the decision to train source models independently is a little odd.': 1.0986123085021973, 'Why not perform some kind of joint training?': 1.0986123085021973, ""The authors provide no analysis, analytical or empirical, of the proposed framework's (storage and computational) complexity, especially at prediction time."": 1.098609447479248, 'At the top of page 10, they mention using only one EEG channel in order to ""reduce complexity and processing time.""': 1.0986109972000122, 'This is an ominous hint that the proposed framework may not scale practically.': 1.0986123085021973, ""Additionally, I have a meta-concern about this paper's fit for ICLR."": 1.098601222038269, '""Representation learning""': 1.0986123085021973, 'the general theme of ICLR': 1.098611831665039, 'is not featured prominently in this work.': 1.0986123085021973, 'Given the competitive nature of ICLR, we should consider seriously whether this paper is of interest to the wider ICLR community or whether it might be a better fit for a meeting such as AISTATS, ICML, or UAI.': 1.0353106260299683, 'The plots in Figures 1-3 are difficult to interpret.': 1.0964235067367554, 'Putting patients along the X-axis is unintuitive since their order is arbitrary.': 1.0985867977142334, ""Why not just make scatter plots of one vs. the other model's accuracy."": 0.5206170678138733, 'The shape of the scatter should hopefully make it clear if there is a general trend.': 0.8660591840744019, 'In Experimental Setup, the authors make conflicting claims about the LSTM architecture.': 1.0415908098220825, 'They first state the single hidden layer has number of cells equal to the number of inputs.': 1.0963677167892456, 'They then say that the number of LSTM units is finetuned based on empirical performance.': 1.097212314605713, 'While generally well-written, the paper has several obscenely long paragraphs.': 1.0653538703918457, 'The single paragraph ""Experimental Setup"" section, for example, takes up more than half of page 8.': 0.9636170268058777, 'These should be broken up into shorter paragraphs to make them easier to read.': 0.4599141478538513, 'A good rule of thumb is that no paragraph should take up more than ~1/6 of a page.': 0.9135497212409973, ""In general, I like this work, but the vague details around the LSTM training raise serious red flags about their experimental results, at least the comparison vs. LSTMs, and I have concerns about how well it meets ICLR's CFP."": 0.8143180012702942, 'That said, my policy for interactive review is to carefully consider author responses with an open mind, so I will serious consider changing my score, if warranted.': 0.8974357843399048, 'This paper looks at transfer learning on sequence.': 1.0402436256408691, 'First individual Bayesian moment matched algorithms are used on each individual.': 0.9954888820648193, 'Then for a test setting, the probabilities for the new domain are a similarity weighted set of probabilities from the training setting.': 1.0986123085021973, 'The similarity weighting is given by the (approximate) posterior probability of the observation (either complete or so far, depending on whether an online scheme is used) for each domain.': 1.0986123085021973, 'A few comments on the model.': 1.0986123085021973, 'First there is a discrepancy between train and test: in the test domain there is an assumption in some sort of relationship between individuals, but at training all individuals are treated independently.': 1.0986123085021973, 'This contrasts with models that try to analyze between-subject and within-subject variation inherently in the training data.': 1.0986123085021973, 'A discussion of these points would be valuable.': 1.0986123085021973, 'Does this assume the data about individuals is extensive, so such sharing is not necessary?': 1.0986123085021973, 'The Bayesian posterior on lambda and pi provides a means of model averaging.': 1.0986123085021973, 'But weighted averaging of models is very different from weighted averaging of transition probabilities.': 1.0986123085021973, 'Given this discrepancy, it would have been good to have a bit more discussion about this choice.': 1.0986123085021973, 'Clearly it is pragmatic, but what do you lose and what do you gain?': 1.0986123085021973, 'Does this fit into the moment matching interpretation?': 1.0986123085021973, 'Of course parameterized sharing is common in multitask settings, but more could be said about this.': 1.0986123085021973, 'Experiments: This seems to be the real point of the paper: the authors have an actual problem to solve and developed this as a method to do this.': 1.0986123085021973, 'Yet from the title it feels like the authors felt they had to bill it as a methodological paper for submission to ICLR.': 1.0986123085021973, 'Personally I think it is unfortunate that this was a perceived need.': 1.0986123085021973, 'A demonstration of what can be transferred in a domain like this is as important as how it is done.': 1.0986123085021973, 'The experiments are on a valuable real world problem that people widely care about.': 1.0986123085021973, 'This is the real strength of this paper, and a focus on this demonstrative aspect, and a corresponding conclusion would strengthen the paper.': 1.0986123085021973}"
123,https://openreview.net/forum?id=BysZhEqee,"{'The proposed approach consists in a greedy layer wise initialization strategy for a deep MLP model, which is followed by global gradient-descent with dropout for fine-tuning.': 1.0985831022262573, 'The initialization strategy uses a first randomly initialized sigmoid layer for dimensionality expansion followed by 2 sigmoid layers whose weights are initialized by Marginal Fisher Analysis (MFA) which learns a linear dimensionality reduction based on a neighborhood graph constructed using class label information (i.e. supervised dimensionality reduction).': 1.0986123085021973, 'Output layer is a standard softmax layer.': 1.0986086130142212, 'The approach is thus to be added to a growing list of heuristic layer-wise initialization schemes.': 1.0986123085021973, 'The particular choice of initialization strategy, while reasonable, is not sufficiently well motivated in the paper relative to alternatives, and thus feels rather arbitrary.': 1.0986119508743286, 'The paper lacks clarity in the description of the approach:  MFA is poorly explained with undefined notations (in Eq. 4, what is A?': 1.0986098051071167, 'It has not been properly defined); the precise use of alluded denoising in the model is also unclear (is there really training of an additional denoting objective, or just input corruption?).': 1.0986121892929077, 'The question of the (arguably mild) inconsistency of applying a linear dimensionality reduction algorithm, that is trained without any sigmoid, and then passing its learned representation through a sigmoid is not even raised.': 1.0986123085021973, 'This, in addition to the fact that sigmoid hidden layers are no longer commonly used (why did you not also consider using RELUs?).': 1.098562479019165, 'More importantly I suspect methodological problems with the experimental comparisons: the paper mentions using *default* values for learning-rate and momentum, and having (arbitrarily?) fixed epoch to 400 (no early stopping?) and L2 regularization to 1e-4 for some models.': 1.0986123085021973, '*All* hyper parameters should always be properly hyper-optimized using a validation set (or cross-validation) including early-stopping, and this separately for each model under comparison (ideally also including layer sizes).': 1.050456166267395, 'This is all the more important since you are considering smallish datasets, so that the various initialization strategies act mainly as different indirect regularization schemes: they thus need to be carefully tuned.': 1.0986120700836182, 'This casts serious doubts as to the amount of hyper-parameter tuning (close to none?)': 1.0129529237747192, 'that went into training the alternative models used for comparison.': 1.0664823055267334, 'The Marginal Fisher Analysis dimensionality reduction initialization strategy may well offer advantages, but as it currently stands this paper doesn’t yet make a sufficiently convincing case for it, nor provide useful insights into the nature of the expected advantages.': 1.0986055135726929, 'I would also suggest, for image inputs such as CIFAR10, to use the qualitative tool of showing the filters (back projected to input space) learned by the different initialization schemes under consideration, as this could help visually gain insight as to what sets methods apart.': 1.0963757038116455, 'The authors pointed out some limitations of existing deep architectures, in particular hard to optimize on small or mid size datasets, and proposed to stack marginal fisher analysis (MFA) to build deep models.': 1.0986123085021973, 'The proposed method is tested on several small to mid size datasets and compared with several feature learning methods.': 1.0986123085021973, 'The authors also applied some existing techniques in deep learning, such as backprop, denoising and dropout to improve performance.': 1.0986123085021973, 'The new contribution of the paper is limited.': 1.0986123085021973, 'MFA has long been proposed.': 1.0986123085021973, 'The authors fail to theoretically or empirically justify the stacking of MFAs.': 1.0986123085021973, 'The authors did not include any deep architectures that requires backprop over multiple layers in the comparison, which the authors set out to address, instead all the methods compared were learned layer by layer.': 1.0986123085021973, 'Will a randomly initialized deep model such as DBN or CNN perform poorly on these datasets?': 1.0986123085021973, 'It is also not clear how the authors came up with each particular model architecture and hyper-parameters used in the different datasets.': 1.0986123085021973, 'The writing of the paper needs to be significantly improved.': 1.0986123085021973, 'A lot of details were omitted, for example, how is dropout applied in the MFA.': 1.0986123085021973, 'This paper proposes to initialize the weights of a deep neural network layer-wise with a marginal Fisher analysis model, making use of potentially the similarity metric.': 1.0986123085021973, 'Pros:': 1.0986123085021973, 'There are a lot of experiments, albeit small datasets, that the authors tested their proposed method on.': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'lacking baseline such as discriminatively trained convolutional network on standard dataset such as CIFAR-10.': 1.0986123085021973, 'It is also unclear how costly in computation to compute the association matrix A in equation 4.': 1.0986123085021973, 'This is an OK paper, where a new idea is proposed, and combined with other existing ideas such as greedy-layerwise stacking, dropout, and denoising auto-encoders.': 1.0986123085021973, 'However, there have been many papers with similar ideas perhaps 3-5 years ago, e.g. SPCANet.': 1.0986123085021973, 'Therefore, the main novelty is the use of marginal Fisher Analysis as a new layer.': 1.0986123085021973, 'This would be ok, but the baselines to demonstrate that this approach works better is missing.': 1.0986123085021973, ""In particular, I'd like to see a conv net or fully connected net trained from scratch with good initialization would do at these problems."": 1.0986123085021973, 'To improve the paper, the authors should try to demonstrate without doubt that initializing layers with MFA is better than just random weight matrices.': 1.0986123085021973}"
124,https://openreview.net/forum?id=BysvGP5ee,"{'The AR prior and its equivalent - the inverse AR posterior - is one of the more elegant ways to improve the unfortunately poor generative qualities of VAE-s.': 1.3862930536270142, 'It is only an incremental but important step.': 1.3862943649291992, 'Incremental, because, judging by the lack of, say, CIFAR10 pictures of the VLAE in its ""creative"" regime ( i.e., when sampling from prior), it will not answer many of the questions hanging over.': 1.3862943649291992, 'We hope to see the paper accepted: in relative terms, the paper shines in the landscape of the other papers which are rich on engineering hacks but lacking on theoretical insights.': 1.3862943649291992, 'Some disagreements with the theoretical suppositions in the paper:': 1.3862943649291992, 'i)': 1.3862943649291992, 'The VAE-s posterior converges to the prior faster than we would like because the gradients of the ""generative"" error (the KL divergence of prior and posterior) w.r.t.': 1.3862943649291992, 'mu & sigma are simple, inf differentiable functions and their magnitude far exceeds the magnitude of the resp.': 1.3862943649291992, 'gradients of the reconstruction error.': 1.3862943649291992, 'Especially when more ""hairy"" decoders like pixelCNN are used.': 1.3862943649291992, 'We always considered this obvious and certainly not worthy of one page of CS mumbo-jumbo to explain.': 1.3862943649291992, 'Dumbing-down the decoder via variations of dropout or ""complexifying"" the sampler as in here, or slapping the generative error with a ""DeepMind"" constant (beta_VAE), are the natural band-aids, but seem to fail in the ""creative"" regime, for real-life sets like CIFAR10 or more complex ones.': 1.3860785961151123, 'Other conceptual solutions are needed, some are discussed in [2].': 1.3862943649291992, 'ii)': 1.2369135618209839, 'The claim near the end of section 2.2 that ""the extra coding cost a.k.a. variational error will exist and will not be negligible"" is a speculation, which, in our empirical experience at least, is incorrect.': 1.3862943649291992, 'The variational error is quantifiable for the Gibbs/exponential family of priors/posteriors, as described in [1], section 3.8, and as Tim Salimans knows from his own previous work.': 1.3861608505249023, 'In the case of CIFAR10 for example, the variational error is negligible, even for simple sampling families like Gaussian, Laplacian, etc.': 1.3862875699996948, 'Moreover, in hindsight, using the closed-form for generative error (the KL divergence of prior and posterior) in the pioneering VAE papers, was likely a mistake inherited by the unnecessary Bayseanism which inspired them (beautiful but a mistake nonetheless): The combo of generative and variational error should together be approximated by the same naive Monte Carlo used for the reconstruction error (easily follows from equation (3.13) in [1]) i.e. arithmetic average over observations.': 1.386198878288269, 'On the lighter side, guys, please do not recycle ridiculous terms like ""optimizationally challenged"", as in section 2.2!': 1.3799104690551758, 'The English language has recently acquired ""mentally-challenged"", ""emotionally-challenged"", etc, and now political correctness has sadly found its way to machines?': 1.1554803848266602, '[1] https://arxiv.org/pdf/1508.06585v5.pdf': 1.2910650968551636, '[2] https://arxiv.org/pdf/1511.02841v3.pdf': 1.2596139907836914, 'This paper proposes a Variational Autoencoder model that can discard information found irrelevant, in order to learn interesting global representations of the data.': 1.3862943649291992, 'This can be seen as a lossy compression algorithm, hence the name Variational Lossy Autoencoder.': 1.3862943649291992, 'To achieve such model, the authors combine VAEs with neural autoregressive models resulting in a model that has both a latent variable structure and a powerful recurrence structure.': 1.3862943649291992, 'The authors first present an insightful Bits-Back interpretation of VAE to show when and how the latent code is ignored.': 1.3862943649291992, 'As it was also mentioned in the literature, they say that the autoregressive part of the model ends up explaining all structure in the data, while the latent variables are not used.': 1.3862943649291992, 'Then, they propose two complementary approaches to force the latent variables to be used by the decoder.': 1.3862943649291992, 'The first one is to make sure the autoregressive decoder only uses small local receptive field so the model has to use the latent code to learn long-range dependency.': 1.3862943649291992, 'The second is to parametrize the prior distribution over the latent code with an autoregressive model.': 1.3862943649291992, 'They also report new state-of-the-art results on binarized MNIST (both dynamical and statically binarization), OMNIGLOT and Caltech-101 Silhouettes.': 1.3862943649291992, 'Review:': 1.3862943649291992, 'The bits-Back interpretation of VAE is a nice contribution to the community.': 0.9687111973762512, 'Having novel interpretations for a model helps to better understand it and sometimes, like in this paper, highlights how it can be improved.': 1.366915225982666, 'Having a fine-grained control over the kind of information that gets included in the learned representation can be useful for a lot of applications.': 1.3778916597366333, 'For instance, in image retrieval, such learned representation could be used to retrieve objects that have similar shape no matter what texture they have.': 1.3738199472427368, 'However, the authors say they propose two complementary classes of improvements to VAE, that is the lossy code via explicit information placement (Section 3.1) and learning the prior with autoregressive flow (Section 3.2).': 1.3858522176742554, 'However, they never actually showed how a VAE without AF prior but that has a PixelCNN decoder performs.': 0.6913182735443115, 'What would be the impact on the latent code is no AF prior is used?': 0.8880682587623596, 'Also, it is not clear if WindowAround(i) represents only a subset of x_{<i} or it can contain any data other than x_i.': 1.3577100038528442, 'The authors mentioned the window can be represented as a small rectangle adjacent to a pixel x_i, must it only contains pixels above and to the left of x_i (similar to PixelCNN)': 1.2842512130737305, 'Minor:': 1.3862943649291992, 'In Equation 8, should there be an expectation over the data distribution?': 1.3862935304641724, 'This paper introduces the notion of a ""variational lossy autoencoder"", where a powerful autoregressive conditional distribution on the inputs x given the latent code z is crippled in a way that forces it to use z in a meaningful way.': 1.144619107246399, 'Its three main contributions are:': 1.3862943649291992, ""(1) It gives an interesting information-theoretical insight as to why VAE-type models don't tend to take advantage of their latent representation when the conditional distribution on x given z is powerful enough."": 1.3862943649291992, '(2) It shows that this insight can be used to efficiently train VAEs with powerful autoregressive conditional distributions such that they make use of the latent code.': 1.3862943649291992, '(3) It presents a powerful way to parametrize the prior in the form of an autoregressive flow transformation which is equivalent to using an inverse autoregressive flow transformation on the approximate posterior.': 1.3862355947494507, 'By itself, I think the information-theoretical explanation of why VAEs do not use their latent code when the conditional distribution on x given z is powerful enough constitutes an excellent addition to our understanding of VAE-related approaches.': 1.3862909078598022, 'However, the way this intuition is empirically evaluated is a bit weak.': 1.3862941265106201, 'The ""crippling"" method used feels hand-crafted and very task-dependent, and the qualitative evaluation of the ""lossyness"" of the learned representation is carried out on three datasets (MNIST, OMNIGLOT and Caltech-101 Silhouettes) which feature black-and-white images with little-to-no texture.': 1.3710124492645264, 'Figures 1a and 2a do show that reconstructions discard low-level information, as observed in the slight variations in strokes between the input and the reconstruction, but such an analysis would have been more compelling with more complex image datasets.': 1.3862642049789429, 'Have the authors tried applying VLAE to such datasets?': 1.3862943649291992, 'I think the Caltech101 Silhouettes benchmark should be treated with caution, as no comparison is made against other competitive approaches like IAF VAE, PixelRNN and Conv DRAW.': 1.2701823711395264, 'This means that VLAE significantly outperforms the state-of-the-art in only one of the four settings examined.': 1.3685641288757324, 'A question which is very relevant to this paper is ""Does a latent representation on top of an autoregressive model help improve the density modeling performance?""': 1.3677308559417725, 'The paper touches this question, but very briefly: the only setting in which VLAE is compared against recent autoregressive approaches shows that it wins against PixelRNN by a small margin.': 1.3853918313980103, 'The proposal to transform the latent code with an autoregressive flow which is equivalent to parametrizing the approximate posterior with an inverse autoregressive flow transformation is also interesting.': 1.3851699829101562, 'There is, however, one important distinction to be made between the two approaches: in the former, the prior over the latent code can potentially be very complex whereas in the latter the prior is limited to be a simple, factorized distribution.': 1.3021783828735352, 'It is not clear to me that having a very powerful prior is necessarily a good thing from a representation learning point of view: oftentimes we are interested in learning a representation of the data distribution which is untangled and composed of roughly independent factors of variation.': 1.2174243927001953, 'The degree to which this can be achieved using something as simple as a spherical gaussian prior is up for discussion, but finding a good balance between the ability of the prior to fit the data and its usefulness as a high-level representation certainly warrants some thought.': 0.9561616778373718, ""I would be interested in hearing the authors' opinion on this."": 1.3855444192886353, 'Overall, the paper introduces interesting ideas despite the flaws outlined above, but weaknesses in the empirical evaluation prevent me from recommending its acceptance.': 1.3461881875991821, 'UPDATE:': 1.3862943649291992, ""The rating has been revised to a 7 following the authors' reply."": 1.381861925125122, 'This paper motivates the combination of autoregressive models with Variational Auto-Encoders and how to control the amount the amount of information stored in the latent code.': 1.3862158060073853, 'The authors provide state-of-the-art results on MNIST, OMNIGLOT and Caltech-101.': 1.3862943649291992, 'I find that the insights provided in the paper, e.g. with respect to the effect of having a more powerful decoder on learning the latent code, the bit-back coding, and the lossy decoding are well-written but are not novel.': 1.3862943649291992, 'The difference between an auto-regressive prior and the inverse auto-regressive posterior is new and interesting though.': 1.3862943649291992, 'The model presented combines the recent technique of PixelRNN/PixelCNN and Variational Auto-Encoders with Inverse Auto-Regressive Flows, which enables the authors to obtain state-of-the-art results on MNIST, OMNIGLOT and Caltech-101.': 1.3862943649291992, 'Given the insights provided in the paper, the authors are also able to control the amount of information contained in the latent code to an extent.': 1.3862943649291992, 'This paper gather several insight on Variational Auto-Encoders scattered through several publications in a well-written way.': 1.3862943649291992, 'From these, the authors are able to obtain state-of-the-art models on small complexity datasets.': 1.3862943649291992, 'Larger scale experiments will be necessary.': 1.3862943649291992}"
125,https://openreview.net/forum?id=ByvJuTigl,"{'Summary: This paper presents a differentiable histogram filter for state estimation/tracking. The proposed histogram filter is a particular Bayesian filter that represents the discretized states using beliefs. The prediction step is parameterized by a locally linear and translation-invariant motion model while the measurement model is represented by a multi-layered neural network. The whole system is learned with both supervised and unsupervised objectives and experiments are carried out on two synthetic robot localization tasks (1D and 2D). The major claim of this paper is that the problem-specific model structure (Bayesian filter for state estimation) should improve pure deep learning approach in data-efficiency and generalization ability.': 1.0986123085021973, '+This paper has nice arguments about the importance of prior knowledge to deep learning approach for specific tasks.': 0.4061625003814697, '+An end-to-end histogram filter is derived for state estimation and unsupervised learning is possible in this model.': 1.0986101627349854, 'This paper seems to have a hidden assumption that deep learning (RNN) is a natural choice for recursive state estimation and the rest of paper is built upon this assumption including LSTM baselines.': 1.0986090898513794, 'However, this assumption itself may not be true, because Bayesian filter is a first-established approach for this classic problem, so it it more important to justify if deep learning is even necessary for solving the tasks presented.': 1.09861159324646, 'This requests pure Bayesian filter baselines in the experiments.': 1.0986090898513794, 'The derived histogram filter seems to be particularly designed for discretized state space.': 0.4154977798461914, 'It is not clear how well it can be generalized to continuous state space using the notation ""x"".': 0.7291521430015564, 'More interestingly, the observation is discrete (binary) as well, which eventually makes it possible to derive a closed-form measurement update model.': 0.7844425439834595, 'This setup might be too constrained.': 1.09861159324646, 'Generalizing to continuous observations is not a trivial task, not even to mention using images as observations like Haarnoja et al 2016.': 1.0986123085021973, 'These design choices overall narrow down the scope of applicability.': 1.0974817276000977, 'Summary:': 1.0986123085021973, 'The authors propose a histogram based state representation with differentiable motion models and observation updates for state tracking from observations.': 1.0986117124557495, 'Linear model with Gaussian noise is used as the motion model, while a neural network is used to learn the measurement model.': 1.0986123085021973, 'They track robot states in: (1) 1-D hallway, and (2) a 2D arena.': 1.098605990409851, 'Positives:': 1.0986123085021973, '1. Show how to encode prior knowledge about state-transitions in the architecture.': 1.0986120700836182, '2. No assumptions about the observation model, which is learned purely from data.': 0.3384230136871338, '3. Better accuracy than baselines with limited training data.': 0.8585824966430664, 'Negatives:': 1.0986123085021973, '1. The motion model is too simplistic. The authors in their response to earlier questions say that a generic feed-forward neural network could be used to model more complicated motions. However, then the novelty of their framework is not clear': 1.0294839143753052, 'as then the proposed model would just be a couple of neural networks to learn the motion and observation models.': 1.032901644706726, '2. The observation model again is too simplistic (e.g., one dimensional observations), and is proposed to be a generic feed-forward network. Here again, the technical novelty is not clear.': 0.2083911895751953, '3. The histogram based representation is not scalable as also highlighted by the authors. Hence, the proposed approach as it is, cannot be applied to more complicated settings.': 0.9046049118041992, '4. In Figure 5(a,b), where they compare the state-estimation accuracy with other baselines (i.e., LSTMs), it is clear that the accuracy of the LSTM has not saturated, while that of their model has. They should do larger scale experiments with more training data (e.g., 10k,100k,500k samples).': 1.062519907951355, 'Note that while sample efficiency is a desirable property (also discussed in Section 6.2), we do expect models with prior knowledge to work better for small number of samples than models which do not assume any structure.': 1.0986123085021973, 'Experiments with larger number of samples would be insightful.': 0.6453543305397034, 'The authors propose a time-series model with discrete states for robotics applications.': 1.0986123085021973, 'I think the proposed method is too simplistic to be useful in the presented form, eg. 1) the state space (dimensionality & topology) is exactly matched to the experiments 2) displacements in the transition model are linear in the actions 3) observations are one-dimensional.': 1.098588466644287, 'This seems to be quite behind the current state of the art, eg “Embed to Control” by Watter et al 2015, where a state representation is learned directly from pixels.': 1.0986123085021973, 'Furthermore the authors do not compare to any other method except for an out-of-the-box LSTM model.': 1.0986123085021973, 'Also, I feel like there must be a lot of prior work for combining HMMs + NNs out there, I think it would be necessary for the authors to relate their work to this literature.': 1.0986123085021973}"
126,https://openreview.net/forum?id=Byx5BTilg,"{'The topic is very interesting, but the paper is not convincing.': 1.0522013902664185, 'Specifically, the experiment part is weak.': 1.031693935394287, 'The study should include datasets that are familiar to the community as well as the ones ""that are not often addressed by deep learning"".': 1.0986037254333496, 'The comparison to other approaches is not comprehensive.': 1.0124568939208984, 'This paper aims at attacking the problem of preselecting deep learning model structures for new domains.': 1.0760395526885986, 'It reported a series of experiments on various small tasks and feed-forward DNNs.': 1.0927543640136719, 'It claims that some ranking algorithm can be learned based on these results to guide the selection of model structures for new domains.': 1.0965615510940552, 'Although the goal is interesting I found their conclusion is neither convincing nor useful in practice for several reasons:': 0.6248425841331482, '1. They only explored really simple networks (feed-forward DNNs). While this significantly limited the search space, it also limited the value of the experiments. In fact, the best model architecture is highly task (domain) dependent and the type of model (DNN vs CNN vs LSTM) is often much more important than size of the network itself.': 0.4429420530796051, '2. Their experiments were conduced with some important hyper parameters (e.g., learning rate schedule) fixed. However, it is well known  that learning rate often is the most important hyper parameter during training. Without adjusting these important hyper parameters the conclusion on the best model architecture is not convincing.': 0.5784557461738586, '3. Their experiments seem to indicate that the training data difference is not important. However, this is unlikely to be true as you would definitely want to use larger models (total number of parameters) when your training set is magnitude larger (i.e., log(datasize) can be an important feature). This is likely because they did not run experiments on large datasets.': 0.4055916965007782, 'In addition, I think the title of the paper does not accurately reflect what the paper is about and should be modified.': 1.0985733270645142, 'Also, this paper cited Sainath et al. 2015 as the work that leads to breakthrough in speech recognition.': 1.0986123085021973, 'However, the breakthrough in ASR happened much earlier.': 1.0986123085021973, 'The first paper with all three key components was published in 2010:': 1.0986123085021973, 'Yu, D., Deng, L. and Dahl, G., 2010, December.': 1.0986123085021973, 'Roles of pre-training and fine-tuning in context-dependent DBN-HMMs for real-world speech recognition.': 1.0986123085021973, 'In Proc. NIPS Workshop on Deep Learning and Unsupervised Feature Learning.': 1.0986123085021973, 'and the more detailed paper was published in 2012': 1.0986123085021973, 'Dahl, G.E., Yu, D., Deng, L. and Acero, A., 2012.': 1.0986123085021973, 'Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition.': 1.0986123085021973, 'IEEE Transactions on Audio, Speech, and Language Processing, 20(1), pp.30-42.': 1.0986123085021973, 'As a conclusion, this paper presented some very preliminary result.': 1.0986123085021973, ""Although it's interesting it's not ready for publishing."": 1.0986123085021973, 'This paper presents an intriguing study of how one can pose architecture search as a meta learning problem.': 1.0986123085021973, 'By collecting features from networks trained on various datasets and training a “ranking classifier” (the actual details of the classifier do not seem to be described in detail) one can potentially infer what a good architecture for a new problem could be by simply running the ranker on the extracted features for a new problem setup.': 1.0986123085021973, 'One notable comment from the paper is that the authors fix some important hyper-parameters for all the networks.': 1.0986123085021973, 'I am of the opinion that optimizing the learning rate (and its decay schedule) is actually quite important.': 1.0986123085021973, 'I hypothesize that a lot of the conclusions of this paper may change quite a bit if the authors did an actual search over the rates instead.': 1.0986123085021973, 'I suspect that instead of training 11k nets, one can train 2k nets with 5 learning rates each and get a much better result that is actually compelling.': 1.0986123085021973, 'I am not convinced that the protocol for generating the various architectures is doing a good job at creating a diversity of architecture (simply because of the max depth of 8 layers and 14 components overall).': 1.0986123085021973, 'I suspect that most of these generated architectures are actually almost identical performance-wise and that it’s a waste to train so many of them on so many tasks.': 1.0986123085021973, 'Unless the authors are already doing this, they should define a pruning mechanism that filters out nets that are too similar to already existing ones.': 1.0986123085021973, 'The batch normalization experiments in Table 2 seem odd and under-explained.': 1.0986123085021973, 'It is also well-known that the optimal learning rates when using batch norm vs. not using batch norm can differ by an order of magnitude so given the fixed learning rate throughout all experiments, I take these results with some grain of salt.': 1.0986123085021973, 'I am not sure we got many insights into the kinds of architectures that ended up being at the top.': 1.0986123085021973, 'Either visualizations, or trends (or both), would be great.': 1.0986123085021973, 'This work seems to conflate the study of parallel vs. serial architectures with the study of meta learning, which are somewhat distinct issues.': 1.0986123085021973, 'I take issue with the table that compares parallel vs. serial performance (table 2) simply because the right way would be to filter the architectures by the same number of parameters / capacity.': 1.0986123085021973, 'Ultimately the conclusion seems to be that when applying deep nets in a new domain, it is difficult to come up with a good architecture in advance.': 1.0986123085021973, 'In that sense, it is hard to see the paper as a constructive result, because it’s conclusions are that while the ranker may do a good job often-times, it’s not that reliable.': 1.0986123085021973, 'Thus I am not convinced that this particular result will be of practical use to folks who are intending to use deep nets for a new domain.': 1.0986123085021973}"
127,https://openreview.net/forum?id=ByxpMd9lx,"{'The authors propose transfer learning variants for neural-net-based models, applied to a bunch of NLP tagging tasks.': 1.0986123085021973, 'The field of multi-tasking is huge, and the approaches proposed here do not seem to be very novel in terms of machine learning: parts of a general architecture for NLP are shared, the amount of shared ""layers"" being dependent of the task of interest.': 1.0986123085021973, 'The novelty lies in the type of architecture which is used in the particular setup of NLP tagging tasks.': 1.0986032485961914, 'The experimental results show that the approach seems to work well when there is not much labeled data available (Figure 2).': 1.0986109972000122, 'Table 3 show some limited improvement at full scale.': 1.0986120700836182, 'Figure 2 results are debatable though: it seems the authors fixed the architecture size while varying the amount of labeled data; it is very likely that tuning the architecture for each size would have led to better results.': 1.0986123085021973, 'Overall, while the paper reads well, the novelty seems a bit limited and the experimental section seems a bit disappointing.': 1.0986121892929077, ""Authors' response well answered my questions."": 1.035453200340271, 'Thanks!': 1.0986123085021973, 'Evaluation not changed.': 1.098527193069458, '###': 1.098601222038269, 'This paper proposes a hierarchical framework of transfer learning for sequence tagging, which is expected to help the target task with the source task, by sharing as many levels of representation as possible.': 1.0850248336791992, 'It is a general framework for various neural models.': 1.0985757112503052, 'The paper has extensive and solid experiments, and the performance is competitive with the state of the art on multiple benchmark datasets.': 0.7626571655273438, 'The framework is clear by itself, except that more details about training procedure, i.e. sec-3.3, need to be added.': 1.091320276260376, 'The experimental results show that for some task pairs {s,t}, this framework can help low-resource target task t, and the improvement increases with more levels of representations can be shared.': 1.0986123085021973, 'Firstly, I suggest that the terms *source* and *target* should be more precisely defined in the current framework, because, due to Sec-3.3, the s and t in each pair are sort of interchangeable.': 1.0934723615646362, 'That is, either of them can be the *source* or *target* task, especially when p(X=s)=p(X=t)=0.5 is used in the task sampling.': 1.098509669303894, 'The difference is: one is low-resourced and the other is not.': 1.098608136177063, 'Thus it could be thought of as multi-tasking between tasks with imbalanced resource.': 0.423340380191803, 'So one question is: does this framework simultaneously help both tasks in the pair, by learning more generalizable representations for different domains/applications/languages?': 1.0985019207000732, 'Or is it mostly likely to only help the low-resourced one?': 1.098611831665039, 'Does it come with sacrifice on the high-resourced side?': 0.40537869930267334, 'Secondly, as the paper shows that the low-resourced tasks are improved for the selected task pairs, it would also be interesting and helpful to know how often this could happen.': 1.098363995552063, 'That is, when the tasks are randomly paired (one chosen from a low-resource pool and the other from a high resource pool), how often could this framework help the low-resourced one?': 1.0951837301254272, 'Moreover, the choice of T-A/T-B/T-C lies intuitively in how many levels of representation *could* be shared as possible.': 0.6363197565078735, 'This implicitly assumes share more, help more.': 1.0986123085021973, 'Although I tend to believe so, it would be interesting to have some empirical comparison.': 1.0986123085021973, 'For example, one could perhaps select some cross-domain pair, and see if T-A > T-B > T-C on such pairs, as mentioned in the author’s answer to the pre-review question.': 1.0745255947113037, 'In general, I think this is a solid paper, and more exploration could be done in this direction.': 1.0986123085021973, 'So I tend to accept this paper.': 1.0986123085021973, 'This paper presents a clear hierarchical taxonomy of transfer learning methods as applicable to sequence tagging problems.': 1.0986123085021973, 'This contextualizes and unifies previous work on specific instances of this taxonomy.': 1.0986123085021973, 'Moreover, the paper shows that previously unexplored places in this taxonomy are competitive with or superior to the state of the art in key benchmark problems.': 1.0986123085021973, ""It'd be nice to see this explored further, such as highlighting what is the loss as you move from the more restrictive to the less restrictive transfer learning approaches, but I believe this paper is interesting and acceptable as-is."": 1.0986123085021973}"
128,https://openreview.net/forum?id=H12GRgcxg,"{'This paper looks at how to train if there are significant label noise present.': 1.0986123085021973, 'This is a good paper where two main methods are proposed, the first one is a latent variable model and training would require the EM algorithm, alternating between estimating the true label and maximizing the parameters given a true label.': 1.0985503196716309, 'The second directly integrates out the true label and simply optimizes the p(z|x).': 1.0986123085021973, 'Pros: the paper examines a training scenario which is a real concern for big dataset which are not carefully annotated.': 1.0986120700836182, ""Cons: the results on mnist is all synthetic and it's hard to tell if this would translate to a win on real datasets."": 1.0986123085021973, 'comments:': 1.0984737873077393, 'Equation 11 should be expensive, what happens if you are training on imagenet with 1000 classes?': 1.0986123085021973, 'It would be nice to see how well you can recover the corrupting distribution parameter using either the EM or the integration method.': 1.098597526550293, 'Overall, this is an OK paper.': 1.0986123085021973, 'However, the ideas are not novel as previous cited papers have tried to handle noise in the labels.': 1.0986123085021973, 'I think the authors can make the paper better by either demonstrating state-of-the-art results on a dataset known to have label noise, or demonstrate that a method can reliably estimate the true label corrupting probabilities.': 1.0986123085021973, 'The paper addressed the erroneous label problem for supervised training.': 1.0986078977584839, 'The problem is well formulated and the presented solution is novel.': 1.0986051559448242, 'The experimental justification is limited.': 0.9534069895744324, 'The effectiveness of the proposed method is hard to gauge, especially how to scale the proposed method to large number of classification targets and whether it is still effective.': 1.0986123085021973, 'For example, it would be interesting to see whether the proposed method is better than training with only less but high quality data.': 1.0986114740371704, 'From Figure 2, it seems with more data, the proposed method tends to behave very well when the noise fraction is below a threshold and dramatically degrades once passing that threshold.': 1.0986123085021973, 'Analysis and justification of this behavior whether it is just by chance or an expected one of the method would be very useful.': 1.0986119508743286, 'This work address the problem of supervised learning from strongly labeled data with label noise.': 1.0985995531082153, 'This is a very practical and relevant problem in applied machine learning.': 0.9428151249885559, ""The authors note that using sampling approaches such as EM isn't effective, too slow and cannot be integrated into end-to-end training."": 1.0986123085021973, 'Thus, they propose to simulate the effects of EM by a noisy adaptation layer, effectively a softmax, that is added to the architecture during training, and is omitted at inference time.': 1.0986123085021973, 'The proposed algorithm is evaluated on MNIST and shows improvements over existing approaches that deal with noisy labeled data.': 1.0986109972000122, 'A few comments.': 1.0986123085021973, '1. There is no discussion in the work about the increased complexity of training for the model with two softmaxes.': 0.35937079787254333, '2. What is the rationale for having consecutive (serialized) softmaxes, instead of having a compound objective with two losses, or a network with parallel losses and two sets of gradients?': 0.5588116645812988, ""3. The proposed architecture with only two hidden layers isn't not representative of larger and deeper models that are practically used, and it is not clear that shown results will scale to bigger networks."": 0.8552709817886353, '4. Why is the approach only evaluated on MNIST, a dataset that is unrealistically simple.': 0.13252854347229004}"
129,https://openreview.net/forum?id=H13F3Pqll,"{'In this work, the authors propose to use a (perhaps deterministic) retrieval function to replace uniform sampling over the train data in training the discriminator of a GAN.': 1.0982471704483032, 'Although I like the basic idea, the experiments are very weak.': 1.0986123085021973, 'There are essentially no quantitative results, no real baselines, and only a small amount of not especially convincing qualititative results.': 1.0986123085021973, ""It is honestly hard to review the paper- there isn't any semblance of normal experimental validation."": 1.0986123085021973, 'Note:  what is happening with the curves in fig.': 1.0986123085021973, '6?': 1.0986123085021973, 'This paper proposes a model that generates a latent representation of input image(s) and optimizes a reconstruction loss with an adversarial loss (Eq (1)) over nearest neighbors from a bank of images (“memory”).': 1.0986123085021973, 'The framework is adapted to three tasks: (i) image in-painting, (ii) intrinsic image decomposition, (iii) figure-ground layer extraction.': 1.0986123085021973, 'Qualitative results are shown for all three tasks.': 1.0986123085021973, 'I think the proposed model has potential merits.': 1.0986123085021973, 'I particularly like the fact that it seems to be reasoning over image composites via matching against a bank of images (somewhat similar to “Segmenting Scenes by Matching Image Composites” work in NIPS 2009).': 1.0986123085021973, 'However, I won’t champion the paper as the overall clarity and evaluation could be improved.': 1.0986123085021973, 'More detailed comments:': 1.0986123085021973, 'I believe the fatal flaw of the paper is there is no quantitative evaluation of the approach.': 1.0986123085021973, 'At the very least, there should be a comparison against prior work on intrinsic image decomposition (e.g., SIRFS, maybe benchmark on ""intrinsic images in the wild” dataset).': 1.0986123085021973, 'I found the writing vague and confusing throughout.': 1.0986123085021973, 'For instance, “memory database” could mean a number of things, and in the end it seems that it’s simply a set of images.': 1.040686845779419, '“Imagination” is also vague.': 1.0985479354858398, 'On page 4, R(M,x) has the database and input image as arguments, but Fig 2 doesn’t show the input image as an input to R.  The contributions listed on page 3 should be tightened (e.g., it’s not clear what “Relevant memory retrieval for informative adversarial priors” means).': 1.0946168899536133, 'Fig 3 seems inconsistent with Fig 2 as the module for “memory database” is not present.': 1.0980480909347534, 'The fully-convolutional discriminator could use more details; one possibility is to provide a cost function.': 1.0956238508224487, 'The paper describes a network architecture for inverse problems in computer vision.': 0.45571470260620117, 'Example inverse problems considered are image inpainting, computing intrinsic image decomposition and foreground/background separation.': 0.5193177461624146, 'The architecture is composed of (i) a generator that produces target (latent) output (such as foreground / background regions),': 1.095853328704834, '(ii) renderer that composes that latent output back to the image that can be compared with the input to measure reconstruction error,': 1.0986123085021973, 'and (iii) adversarial prior that ensures the target output (latent) image respects a certain image statistics.': 1.0986123085021973, 'Strong  points.': 1.0986123085021973, 'The proposed architecture with memory database is interesting and appears to be novel.': 1.0986123085021973, 'Weak points:': 1.0986123085021973, 'Experimental results are only proof-of-concept in toy set-ups and do not clearly demonstrate benefits of the proposed architecture.': 1.0986123085021973, 'It is unclear whether the memory retrieval engine that retrieves images based on L2 distance on pixel values is going generalize to other more realistic scenarios.': 1.0986123085021973, 'Clarity.': 1.0986123085021973, 'The clarity of explanation can be also improved (see below).': 1.0986123085021973, 'Detailed evaluation.': 1.0986123085021973, 'Originality:': 1.0986123085021973, 'The novelty of this work lies in the (iii) adversarial prior that places an adversarial loss between the generated latent output and a single image retrieved from a large unlabelled database of target output examples (called memory).': 1.0982489585876465, 'The adversarial prior has a convolutional form matching local image statistics, rather than the entire image.': 1.0986123085021973, 'The particular form of network architecture with the memory-based fully convolutional adversarial loss appears to be novel and potentially interesting.': 1.0986123085021973, 'Motivation for the Architecture.': 1.0986123085021973, 'The weakest point of the proposed architecture is the ""Memory retrieval engine"" R (section 2.4),': 1.0986123085021973, 'where images are retrieved from the memory by measuring L2 distance on pixel intensities.': 1.0986123085021973, 'While this maybe ok for simple problems considered in this work, it is unclear how this can generalize to other more complicated datasets and problems.': 1.0986123085021973, 'This should be better discussed, better justified and ideally results in some more realistic set-up shown (see below).': 1.0986123085021973, 'Quality:': 1.0986123085021973, 'Experiments.': 1.0986123085021973, 'Results are shown for inpainting of MNIST digits, intrinsic image decomposition on the MIT intrinsic image database, and figure/ground layer extraction on the synthesized dataset of 3D chairs rendered onto background from real photographs.': 1.0985608100891113, 'The experimental validation of the model is not very strong and proof-of-concept only.': 1.0986123085021973, 'All the experiments are performed in simplified toy set-ups.': 1.0986123085021973, 'The MNIST digit inpainting is far from current state-of-the-art on image inpainting in real photographs (see e.g. Pathak et al., 2016).': 1.0985666513442993, 'The foreground background separation is done on  only synthetically generated test data.': 1.0986123085021973, 'Even for intrinsic image demposition problem there is now relatively large-scale dataset of (Bell et al., 2014), see the citation below.': 1.0986104011535645, 'While this is probably ok for the ICLR paper, it diminishes the significance of the work.': 1.0986123085021973, 'Is this model going to be useful in a real settings?': 1.0986123085021973, 'One possibility to address this would be to focus on one of the problems and show results on a challenging state-of-the-art data.': 1.0986123085021973, 'It would be great to see the benefits of the memory database.': 1.0986123085021973, 'S. Bell, K. Bala, and N. Snavely.': 1.0986123085021973, 'Intrinsic images in the wild.': 1.0986067056655884, 'ACM Transactions on Graphics, 33(4):159, 2014.': 1.0984395742416382, 'Clarity:': 0.6585850715637207, 'The clarity of the writing can be improved.': 1.0982308387756348, 'I found some of the terminology of the paper, specially the “imagination” and “memory” confusing.': 1.0979952812194824, 'From figure 2, it is not clear how the “memories” for the given input image are obtained, which also took me some time to understand.': 0.8426765203475952, 'To help understand the proposed architecture, it would be useful to draw an illustration of what is happening in the ""feature space”, similar in spirit e.g. to figure 2 in https://arxiv.org/pdf/1612.02136.pdf.': 0.9807292819023132, 'Specially, it would be interesting to understand the role of the memory database in this way.': 1.0986123085021973, 'Significance:': 1.0982038974761963, 'The paper describes potentially interesting architecture.': 1.0986123085021973, 'Given the only proof-of-concept results in toy set-ups, the significance, in the current version, appears only limited.': 1.0941760540008545, 'Rather than addressing many different problems, it would be interesting to see benefits of the proposed architecture on realistic challenging data for one of the problems.': 1.0456688404083252, 'Overall:': 1.0986080169677734, 'The proposed architecture seems novel and potentially interesting, but experiments are only proof-of-concept and clarity can be improved.': 1.095939040184021, 'It is unclear whether the memory matching engine will generalize to other more complicated datasets and problems.': 1.0936534404754639, 'Overall, I am on the edge with this paper, giving the authors the benefit of doubt with a score slightly above the threshold.': 1.0798779726028442}"
130,https://openreview.net/forum?id=H178hw9ex,"{'This paper presents an improved formulation of CNN, aiming to separate geometric transformation from inherent features.': 1.0986123085021973, 'The network can estimate the transformation of filters given the input images.': 1.0986123085021973, 'This work is based on a solid technical foundation and is motivated by a plausible rationale.': 1.0986123085021973, 'Yet, the value of this work in practice is subject to questions:': 1.0986123085021973, '(1) It relies on the assumption that the input image is subject to a transformation on a certain Lie group (locally).': 1.0986123085021973, 'Do such transformations constitute real challenges in practice?': 1.0986123085021973, 'State-of-the-art CNNs, e.g. ResNet, are already quite resilient to such local deformations.': 1.0986123085021973, 'What such components would add to the state of the art?': 1.0986123085021973, 'Limited experiments on Cifar-10 does not seem to provide a very strong argument.': 1.0986123085021973, '(2) The computational cost is not discussed.': 1.0986123085021973, 'I sincerely apologize for the late review!': 1.0986123085021973, 'The first part has a strong emphasis on the technical part.': 1.0986123085021973, 'It could benefit from some high level arguments on what the method aims to achieve, what limitation is there to overcome.': 1.0986123085021973, 'I may have misunderstood the contribution (in which case please correct me) that the main novel part of the paper is the suggestion to learn the group parameterizations instead of pre-fixing them.': 1.0986123085021973, 'So instead of applying it to common spatial filters as in De Brabandere et al., it is applied to Steerable Frames?': 1.0986123085021973, 'The first contribution suggests that ""general frame bases are better suited to represent sensory input data than the commonly used pixel basis."".': 1.0986123085021973, 'The experiments on Cifar10+ indicate that this is not true in general.': 1.098611831665039, 'Considering the basis as a hyper-parameter, expensive search has to be conducted to find that the Gauss-Frame gives better results.': 1.0986123085021973, 'I assume this does not suggest that the Gauss-Frame is always better, at least there is weak evidence on a single network presented.': 1.0986123085021973, 'Maybe the first contribution has to be re-stated.': 1.0986123085021973, 'Further is the ""Pixel"" network representation corrected for the larger number of parameters.': 1.0980939865112305, 'As someone who is interested in using this, what are the runtime considerations?': 1.0919291973114014, 'I would strongly suggest to improve Fig.3.': 1.0986123085021973, 'The Figure uses ""w"" several times in different notations and depictions.': 1.0986123085021973, 'It mixes boxes, single symbols and illustrative figures.': 1.0986123085021973, 'It took some time to decipher the Figure and its flow.': 1.0982531309127808, 'Summary: The paper is sufficiently clear, technical at many places and readability can be improved. E.g., the introduction of frames in the beginning lacks motivation and is rather unclear to someone new to this concept. The work falls in the general category of methods that impose knowledge about filter transformations into the network architecture. For me that has always two sides, the algorithmic and technical part (there are several ways to do this) and the practical side (should I do it)? This is a possible approach to this problem but after the paper I was a bit wondering what I have learned, I am certainly not inspired based on the content of the paper to integrate or build on this work. I am lacking insights into transformational parameters that are relevant for a problem. While the spatial transformer network paper was weaker on the technical elegance side, it provided exactly this: an insight into the feature transformation learned by the algorithm. I am missing this here, e.g., from Table 2  I learn that among four choices one works empirically better. What is destroyed by the x^py^p and Hermite frames that the ResNet is *not* able to recover from? You can construct network architectures that are the superset of both, so that inferior performance could be avoided.': 1.0677944421768188, 'The algorithm is clear but it is similar to the Dynamic Filter Networks paper.': 1.0986121892929077, 'And I am unfortunately not convinced about the usefulness of this particular formulation.': 1.0986123085021973, ""I'd expect a stronger paper with more insights into transformations and comparisons to standard techniques, a clear delineation of when this is advised."": 1.0936555862426758, 'This works applies steerable frames for various tasks where convolutional neural networks with location invariant operators are traditionally applied.': 1.0985361337661743, 'Authors provide a detailed overview of steerable frames followed with an experimental section which applies dynamic steerable network to small machine learning problems where the steerability is conceptually useful.': 1.0986123085021973, 'Even though the evaluation is performed only on few small tasks, the reason why more tasks were not evaluated is that piece-wise pose invariance is needed only for a subset of tasks.': 0.937767744064331, 'The fact, that simply using overcomplete bases as a sort of ""feature pre-processing"" improves the results for already highly optimized ResNet and DenseNet architectures is quite interesting achievement.': 1.0986121892929077, 'For the edge detection, a relatively hard baseline is selected - the Dynamic Filter Networks, which already attempts to achieve position invariant filters.': 1.0986119508743286, 'The fact that DSFN improves the performance on this task verifies that regressing the parametrization of the steerable filters yields better results than regressing the filters directly.': 1.0981518030166626, 'In the last experiment authors apply the network to video classification using LSTMs and they show that the improved performance is not due to increased capacity of the network.': 1.0986120700836182, 'In general, it is quite interesting work.': 1.09853994846344, 'Even though it does not offer ground-breaking results (mainly in a sense of not performing experiments on larger tasks), it is theoretically interesting and shows promising results.': 0.9697988033294678, 'There are few minor issues and suggestions related to the paper:': 1.0986123085021973, '* For the LSTM experiment, in order to be more exact, it would be useful to include information about total number of parameters, as the network which estimates the pose also increases the number of parameters.': 1.0957742929458618, '* Would it be possible to provide more details about how the back-propagation is done through the steerable filters?': 0.6327689290046692, '* For the Edge Detection experiment, it would be useful to provide results for some standard baseline - e.g. CNN with a similar number of parameters.': 1.0928744077682495, 'Simply to see how useful it is to have location-variant filters for this task.': 1.097214698791504, '*': 1.0986123085021973, 'The last sentence in second paragraph on page 1 is missing a verb.': 1.0986123085021973, 'Also it is maybe unnecessary.': 1.0821149349212646, '* The hyphenation for ConvNet is incorrect on multiple places (probably `\\hyphenation{Conv-Net}` would fix it).': 1.0966676473617554}"
131,https://openreview.net/forum?id=H1Fk2Iqex,"{'While I understand the difficulty of collecting audio data from animals, I think this type of feature engineering does not go in the right direction.': 1.0986123085021973, 'I would rather see a model than learns the feature representation from data.': 1.0986123085021973, 'I would think it should be possible to collect a more substantial corpus in zoos / nature etc, and then train a generative model.': 1.0986123085021973, 'The underlying learned feature representation could be then used to feed a classifier.': 1.0986123085021973, ""I'm not familiar with the particularities of this task, it's hard to judge the improvements by using chirplets."": 1.0986123085021973, 'Pros:': 0.8303285241127014, 'Introduction of a nice filter banks and its implementation': 1.0986123085021973, 'Good numerical results': 1.0986123085021973, 'Refinement of the representation via back propagation, and a demonstration that it speeds up learning': 0.5768122673034668, 'Cons:': 1.0982297658920288, 'The algorithms (section 3.1) are not necessary, and they even affect the presentation of the paper.': 1.0986039638519287, 'However, a source code would be great!': 1.0986073017120361, 'The link with a scattering transform is not clear': 0.9064303040504456, 'Sometimes (as mentionned in some of my comments), the writing could be improved.': 1.097974419593811, 'From a personal point of view, I also believe the negative points I mention can be easily removed.': 1.0986123085021973, 'The authors advocate use of chirplets as a basis for modeling audio signals.': 1.0986123085021973, 'They introduce a fast chiplet transform for efficient computation.': 1.0655999183654785, 'Also introduced is the idea of initializing (pre-training) CNN layers to mimic chirplet transform of audio signal (similar to ideas proposed by Mallet et al. on scattering transforms).': 1.0986123085021973, 'The paper is fairly easy to follow but in a few places contains undefined terms (e.g. AM-FM, MAP).': 1.0986114740371704, 'While the idea of using chirplet transform is interesting, my main concern is that the empirical evidence provided is in a rather narrow domain of bird call classification.': 1.0986123085021973, 'Furthermore, the accuracy gains shown in that domain are relatively small (61% MAP for log-Mel features vs 61.5% for chirplet transforms).': 1.0986123085021973, 'I would recommend that authors provide evidence for how this generalizes to other audio (including speech) tasks.': 1.0986123085021973}"
132,https://openreview.net/forum?id=H1GEvHcee,"{'The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM.': 1.3862943649291992, 'A sampling method was presented to train the leaky-ReLU RBM.': 1.3862943649291992, 'In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported.': 1.3862943649291992, ""It's interesting for trying different nonlinear hidden units for RBM."": 1.3862943649291992, 'However, there are some concerns for the current work.': 1.3862943649291992, '1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed.': 1.3703230619430542, '2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly.': 1.2383196353912354, '3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.': 1.0993432998657227, 'Based on previous work such as the stepped sigmoid units and ReLU hidden units for discriminatively trained supervised models, a Leaky-ReLU model is proposed for generative learning.': 1.199643611907959, 'Pro: what is interesting is that unlike the traditional way of first defining an energy function and then deriving the conditional distributions, this paper propose the forms of the conditional first and then derive the energy function.': 1.280975580215454, 'However this general formulation is not novel to this paper, but was generalized to exponential family GLMs earlier.': 1.3862943649291992, 'Con:': 1.3862943649291992, 'Because of the focus on specifying the conditionals, the joint pdf and the marginal p(v) becomes complicated and hard to compute.': 1.3862943649291992, 'On the experiments, it would been nice to see a RBM with binary visbles and leaky ReLu for hiddens.': 1.3862943649291992, 'This would demonstrate the superiority of the leaky ReLU hidden units.': 1.3862943649291992, 'In addition, there are more results on binary MNIST modeling with which the authors can compare the results to.': 1.3862943649291992, 'While the authors is correct that the annealing distribution is no longer Gaussian, perhaps CD-25 or (Faast) PCD experiments can be run to compare agains the baseline RBM trained using (Fast) PCD.': 1.3860793113708496, 'This paper is interesting as it combines new hidden function with the easiness of annealed AIS sampling, However, the baseline comparisons to Stepped Sigmoid Units (Nair &Hinton) or other models like the spike-and-slab RBMs (and others) are missing, without those comparisons, it is hard to tell whether leaky ReLU RBMs are better even in continuous visible domain.': 1.3862943649291992, 'The authors propose a novel energy-function for RBMs, using the leaky relu max(cx, x) activation function for the hidden-units.': 1.3771209716796875, 'Analogous to ReLU units in feed-forward networks, these leaky relu RBMs split the input space into a combinatorial number of regions, where each region defines p(v) as a truncated Gaussian.': 1.3862696886062622, 'A further contribution of the paper is in proposing a novel sampling scheme for the leaky RBM: one can run a much shorter Markov chain by initializing it from a sample of the leaky RBM with c=1 (which yields a standard multi-variate normal over the visibles) and then slowly annealing c.': 1.3862943649291992, 'In low-dimension a similar scheme is shown to outperform AIS for estimating the partition function.': 1.3862943649291992, 'Experiments are performed on both CIFAR-10 and SVHN.': 1.3776668310165405, 'This is an interesting paper which I believe would be of interest to the ICLR community.': 1.3862930536270142, 'The theoretical contributions are strong: the authors not only introduce a proper energy formulation of ReLU RBMs, but also a novel sampling mechanism and an improvement on AIS for estimating their partition function.': 1.3862942457199097, 'Unfortunately, the experimental results are somewhat limited.': 1.386291742324829, 'The PCD baseline is notably absent.': 1.3862943649291992, 'Including (bernoulli visible, leaky-relu hidden) would have allowed the authors to evaluate likelihoods on standard binary RBM datasets.': 1.3862943649291992, 'As it stands, performance on CIFAR-10 and SVHN, while improved with leaky-relu, is a far cry from more recent generative models (VAE-based, or auto-regressive models).': 1.3862943649291992, 'While this comparison may be unfair, it will certainly limit the wider appeal of the paper to the community.': 1.3857074975967407, 'Furthermore, there is the issue of the costly projection method which is required to guarantee that the energy-function remain bounded (covariance matrix over each region be PSD).': 1.3862943649291992, 'Again, while it may be fair to leave that for future work given the other contributions, this will further limit the appeal of the paper.': 1.3862937688827515, 'PROS:': 1.3862943649291992, 'Introduces an energy function having the leaky-relu as an activation function': 1.3862943649291992, 'Introduces a novel sampling procedure based on annealing the leakiness parameter': 1.3862943649291992, 'Similar sampling scheme shown to outperform AIS': 1.3862943649291992, 'CONS:': 1.3862943649291992, 'Results are somewhat out of date': 1.3862943649291992, 'Missing experiments on binary datasets (more comparable to prior RBM work)': 1.3862943649291992, 'Missing PCD baseline': 1.3862943649291992, 'Cost of projection method': 1.3862943649291992, 'This paper proposed a new variant of RBM, which has a nonlinearity of leaky ReLU, in contrast to the sigmoid function nonlinearity in RBM.': 1.3862943649291992, 'By gradually annealing the leakiness coefficient (corresponding to from Gaussian to non-Gaussian model), the authors can sample from their model with a higher mixing rate.': 1.3862943649291992, 'With the same idea annealing leakiness, they show they can estimate the partition function of the new model more accurately.': 1.3862943649291992, 'Main comments:': 1.3862943649291992, 'The proposed model can only account for real-valued data.': 1.3862943649291992, 'However, RBM is primarily used to model binary data, real-valued RBM (Gaussian-RBM) is not a well-recognized model for real-valued data.': 1.3862943649291992, 'So, to demonstrate the superiority of the model, the author should also include the comparison with binary data.': 1.3862943649291992, 'And it is also not enough to only compare two datasets for a newly proposed model.': 1.3862943649291992, 'The claim that the marginal distribution of visible variables is truncated Gaussian is incorrect.': 1.385785460472107, 'For a truncated normal, the values of variables are constrained to be within some region, e.g. requiring variable v from the region a1<v<a2.': 1.3862943649291992, 'But here, there is no constraint on the visible v, i.e. v \\in (-inf to +inf).': 1.3862943649291992, 'The model just yields a marginal distribution which has a region-wise energy function, that is, for different regions, the energy functions are different.': 1.3862943649291992, 'The authors should be aware of the claims of connection to truncated Gaussian.': 1.3860243558883667, 'That the authors claims that the model proposed by Nair & Hinton (2010) has no strict monotonicity and thus cannot use the Ravanbakhsh’s framework is incorrect.': 1.3862943649291992, 'Nair & Hinton’s model use max(0, x+n)': 1.3862943649291992, 'to introduce a ReLU-like nonlinearity.': 1.118332862854004, 'The output expectation is actually a strict monotonic increasing function of x.': 1.3790286779403687, 'Besides Nair & Hinton’s work, there is also another closely-related work ‘Unsupervised learning with truncated Gaussian graphical model’, which introduces ReLU into RBM using truncated normal.': 1.3862943649291992}"
133,https://openreview.net/forum?id=H1Go7Koex,"{'This paper proposes a new model for sentence classification.': 0.4059986472129822, 'Pros:': 1.0986123085021973, 'Some interesting architecture choices in the network.': 0.7910192608833313, 'Cons:': 1.0986123085021973, 'No evaluation of the architecture choices.': 1.0984866619110107, 'An ablation study is critical here to understand what is important and what is not.': 0.5800966024398804, 'No evaluation on standard datasets.': 1.0986123085021973, 'On the only pre-existing dataset evaluated on a simple TFIDF-SVM method is state-of-the-art, so results are unconvincing.': 1.0942078828811646, 'This paper proposes a new neural network model for sentence representation.': 1.0538665056228638, 'This new model is inspired by the success of residual network in Computer Vision and some observation of word morphology in Natural Language Processing.': 1.0984643697738647, 'Although this paper shows that this new model could give the best results on several datasets, it lacks a strong evidence/intuition/motivation to support the network architecture.': 1.0986123085021973, 'To be specific:': 1.0986080169677734, 'I was confused by the contribution of this paper: character-aware word embedding or residual network or both?': 1.0986123085021973, 'The claim of using residual network in section 3.3 seems pretty thin, since it ignores some fundamental difference between image representation and sentence representation.': 1.0986123085021973, 'Even though the results show that adding residual network could help, I was still not be convinced.': 1.0986123085021973, 'Is there any explanation about what is captured in the residual component from the perspective of sentence modeling?': 1.0986121892929077, 'This paper combines several components in the classification framework, including character-aware model for word embedding, residual network and attention weight in Type 1 feature.': 1.0986096858978271, 'I would like to see the contribution from each of them to the final performance, while in Table 3 I only saw one of them.': 1.0961445569992065, 'Is it possible to add more results on the ablation test?': 1.0595237016677856, 'In equation (5), what is the meaning of  in ?': 1.0985289812088013, 'The citation format is impropriate': 1.0986123085021973, 'This paper proposes a character-aware attention residual network for sentence embedding.': 1.0986123085021973, 'Several text classification tasks are used to evaluate the effectiveness of the proposed model.': 1.0305910110473633, ""On two of the three tasks, the residual network outforms a few baselines, but couldn't beat the simple TFIDF-SVM on the last one."": 1.0957794189453125, 'This work is not novel enough.': 1.0924078226089478, 'Character information has been applied in many previously published work, as cited by the authors.': 1.0810935497283936, 'Residual network is also not new.': 1.0227166414260864, 'Why not testing the model on a few more widely used datasets for short text classification, such as TREC?': 1.0975909233093262, 'More competitive baselines can be compared to.': 1.0986123085021973, 'Also, it\'s not clear how the ""Question"" dataset was created and which domain it is.': 1.0723090171813965, 'Last, it is surprising that the format of citations throughout the paper is all wrong.': 0.3330906629562378, 'For example:': 1.0481284856796265, 'like Word2Vec Mikolov et al. (2013)': 1.0986123085021973, '>': 1.0986123085021973, 'like Word2Vec (Mikolov et al., 2013)': 1.098528265953064, ""The citations can't just mix with the normal text."": 1.01581609249115, 'Please refer to other published papers.': 0.6515784859657288}"
134,https://openreview.net/forum?id=H1Gq5Q9el,"{'In this paper, the authors propose to pretrain the encoder/decoder of seq2seq models on a large amount of unlabeled data using a LM objective.': 1.0986121892929077, 'They obtain improvements using this technique on machine translation and abstractive summarization.': 1.0986123085021973, 'While the effectiveness of pretraining seq2seq models has been known among researchers and explored in a few papers (e.g. Zoph et al. 2016,  Dai and Le 2015), I believe this is the first paper to pretrain using a LM for both the encoder/decoder.': 1.0985976457595825, 'The technique is simple, but the gains are large (e.g. +2.7 BLEU on NMT).': 1.0985468626022339, 'In addition, the authors perform extensive ablation studies to analyze where the performance is coming from.': 1.0986108779907227, 'Hence, I think this paper should be accepted.': 0.6303009390830994, 'strengths:': 0.3885679244995117, 'A method is proposed in this paper to initialize the encoder and decoder of the seq2seq model using the trained weights of language models with no parallel data.': 1.0986123085021973, 'After such pretraining, all weights are jointly fine-tuned with parallel labeled data with an additional language modeling loss.': 1.0986123085021973, 'It is shown that pretraining accelerates training and improves generalization of seq2seq models.': 1.098611831665039, 'The main value of the proposed method is to leverage separate source and target corpora, contrasting the common methods of using large amounts of parallel training corpora.': 1.0986120700836182, 'weaknesses:': 1.0986123085021973, 'The objective function shown in the middle of pg 3 is highly empirical, not directly linked to how non-parallel data helps to improve the final prediction results.': 1.0974217653274536, 'The paper should compare with and discuss the objective function based on expectation of cross entropy which is directly linked to improving prediction results as proposed in arXiv:1606.04646, Chen et al.:': 1.0927685499191284, 'Unsupervised Learning of Predictors from Unpaired Input-Output Samples, 2016.': 1.0986123085021973, 'The pre-training procedure proposed in this paper is also closely connected with the DNN pretraining method presented in Dahl et al. 2011, 2012.': 1.0986120700836182, 'Comparisons should be made in the paper, highlighting why the proposed one is conceptually superior if the authors believe so.': 1.0986123085021973, 'Authors propose the use of layer-wise language model-like pretraining for encoder-decoder models.': 1.0986123085021973, 'This allows to leverage separate source and target corpora (in unsupervised manner) without necessity of large amounts of parallel training corpora.': 1.0986123085021973, 'The idea is in principle fairly simple, and rely on initial optimising both encoder and decoder with LSTM tasked to perform language modelling.': 1.0986123085021973, 'The ideas are not new, and the paper is more like a successful compilation of several approaches that have been around for some time.': 1.0986123085021973, 'The experimental validation, though, offers some interesting insights into importance of initialization, and the effectiveness of different initialisations approaches in enc-dec setting.': 1.0986123085021973, 'The regulariser you propose to use on page 3, looks like typical multi-task objective function, especially it is used in an alternating manner would be interesting to see whether similar performance might have been obtained starting with this objective, from random initialisation.': 1.0986123085021973, 'You should probably give credit for encoder-decoder like-RNN models published in 1990s.': 1.0986123085021973, 'Minors:': 1.0986123085021973, 'Pg.': 1.0986123085021973, '2, Sec 2.1 2nd paragraph: can be different sizes -> can be of different sizes': 1.0986123085021973}"
135,https://openreview.net/forum?id=H1Heentlx,"{'This paper considers the case where multiple views of data are learned through a probabilistic deep neural network formulation.': 1.0756862163543701, 'This makes the model non-linear (unlike e.g. CCA) but makes inference difficult.': 1.0972920656204224, 'Therefore, the VAE framework is invoked for inference.': 0.9209173321723938, 'In [Ref 1] the authors show that maximum likelihood estimation based on their linear latent model leads to the canonical correlation directions.': 1.0107808113098145, ""But in the non-linear case with DNNs it's not clear (at least with the present analysis) what the solution is wrt to the canonical directions."": 0.7489194869995117, ""There's no such analysis in the paper, hence I find it a stretch to refer to this model as a CCA type of model."": 0.693487286567688, 'In contrast, e.g. DCCA / DCCAE are taking the canonical correlation between features into account inside the objective and provide interpretations.': 1.0986123085021973, '[Ref 1] F. R. Bach and M. I. Jordan.': 1.0986123085021973, 'A probabilistic interpretation of canonical correlation analysis.': 1.0986123085021973, 'Technical Report 688, 2005.': 1.0986123085021973, 'There is also a significant body of very related work on non-linear multi-view models which is not discussed in this paper.': 1.0986123085021973, ""For example, there's been probabilistic non-linear multi-view models"": 1.0986123085021973, '[Ref 2, 3], also extended to the Bayesian case with common/private spaces [Ref 4] and the variational / deep learning case [Ref 5].': 0.9056527018547058, '[Ref 2]': 1.0786304473876953, 'Ek et al.': 1.096520185470581, 'Gaussian process latent variable models for human pose estimation.': 1.0986123085021973, 'MLMI, 2007.': 1.0986123085021973, '[Ref 3] Shon et al.': 0.9158872961997986, 'Learning shared latent structure for image synthesis and robotic imitation.': 0.8749212622642517, 'NIPS, 2006.': 1.0985950231552124, '[Ref 4] Damianou et al.': 1.0986121892929077, 'Manifold relevance determination.': 1.0986123085021973, 'ICML, 2012.': 1.0986119508743286, '[Ref 5] Damianou and Lawrence.': 1.0858092308044434, 'Deep Gaussian processes.': 1.0986123085021973, 'AISTATS, 2013.': 1.0166406631469727, 'I can see the utility of this model as bringing together two elements: multi-view modeling and VAEs.': 0.8563278317451477, ""This seems like an obvious idea but to the best of my knowledge it hasn't been done before and is actually a potentially very useful model."": 0.8054394125938416, 'However, the question is, what is the proper way of extending VAE to multiple views?': 0.8617162108421326, ""The paper didn't convince me that VAE can work well with multiple views using the shown straightforward construction."": 0.5057586431503296, ""Specifically, VCCA doesn't seem to promote the state of the art in terms of results (it actually is overall below the SOA), while the VCCA-private seems a quite ill-posed model: the dimensionalities d have to be manually tuned with exhaustive search; further, the actual model does not provide a consinstent way of encouraging the private and common variables to avoid learning redundant information."": 0.9074413180351257, 'Relying only on dropout for this seems a quite ad-hoc solution (in fact, from Fig. 4 (ver2) it seems that the dropout rate is quite crucial).': 0.6334824562072754, 'Perhaps good performance might be achieved with a lot of tuning (which might be why the FLICKR results got better in ver2 without changing the model), but it seems quite difficult to optimize for the above reasons.': 1.0986123085021973, ""From a purely experimental point of view, VCCA-private doesn't seem to promote the SOA either."": 1.0986123085021973, ""Of course one wouldn't expect any new published paper to beat all previous baselines, but it seems that extension of VAE to multiple views is a very interesting idea which deserves some more investigation of how to do it efficiently."": 1.0986123085021973, 'Another issue is the approximate posterior being parameterized only from one of the views.': 1.0986123085021973, 'This makes the model less useful as a generic multi-view model, since it will misbehave in tasks other than classification.': 1.0986123085021973, 'But if classification is the main objective, then one should compare to a proper classification model, e.g. a feedforward neural network.': 1.0986123085021973, 'The plots of Fig. 8 are very nice.': 1.0986123085021973, 'Overall, the paper convinced me that there is merit in attaching multiple views to VAE.': 1.0689524412155151, ""However, it didn't convince me a) that the proposed way to achieve this is practical b) that there is a connection to CCA (other than being a method for multiple views)."": 0.9974976778030396, 'The bottom line is that, although the paper is interesting, it needs a little more work.': 1.0984824895858765, 'UPDATE: I have read the replies on this thread.': 1.0986123085021973, 'My opinion has not changed.': 1.0986123085021973, 'The authors propose deep VCCA, a deep version of the probabilistic CCA model by using likelihoods parameterized by nonlinear functions (neural nets).': 1.0986123085021973, 'Variational inference is applied with an inference network and reparameterization gradients.': 1.0986123085021973, 'An additional variant, termed VCCA-private, is also introduced, which includes local latent variables for each data point (view).': 1.0986064672470093, 'A connection to the multi view auto encoder is also shown.': 1.0986090898513794, 'Since the development of black box variational inference and variational auto-encoders, the methodology in model-specific papers like this one are arguably not very interesting.': 1.0986121892929077, 'The model is a straightforward extension of probabilistic CCA with neural net parameterized likelihoods.': 1.0986113548278809, 'Inference is mechanically the same as any black box approach using the reparameterization gradient and inference networks.': 1.0986123085021973, 'The approach also uses a mean-field approximation, which is quite old given the many recent developments in more expressive approximations (see, e.g., Rezende and Mohamed (2015); Tran et al. (2016)).': 1.0986114740371704, 'The connection to multi-view auto encoders is at first insightful, but no more than the difference between MAP and variational inference.': 1.0963250398635864, 'This is a well-known insight: in the abstract, the authors argue that the key distinction is the additional sampling, but ultimately what matters is the KL regularizer.': 1.0986119508743286, 'Even with noisy samples, the variances of a normal variational approximation would collapse to zero and thus become a point mass approximation, equivalent to optimizing a point estimate from the MVAE objective.': 1.0986123085021973, '(I suspect the authors know this to some degree due to their remarks in the paper, but it is unclear.)': 0.8279154896736145, 'That said, I think the paper has strong merits in application.': 1.0986121892929077, 'The experiments are strong, comparing to alternative multi-view approaches under a number of interesting data sets.': 1.0986090898513794, 'While the use of ""private variables"" is simple, they demonstrate how it can successfully disentangle the per-view latent representation from the shared view.': 1.0986123085021973, 'It would have been preferable to compare to methods using probabilistic inference, such as full Bayes for the linear CCA.': 1.0984078645706177, 'There are also a number of approximations taken to almost be standard in the paper which may not be necessary, such as the use of a mean-field family or the use of an inference network.': 1.0985246896743774, 'To separate out how much the approximate inference is influencing the fit of the model, I strongly recommend using MCMC and non-amortized variational inference on at least one experiment.': 1.0986069440841675, '+ Rezende, D. J., & Mohamed, S. (2015).': 1.0128494501113892, 'Variational Inference with Normalizing Flows.': 0.6801301836967468, 'Presented at the International Conference on Machine Learning.': 1.0986123085021973, '+': 1.0984731912612915, 'Tran, D., Ranganath, R., & Blei, D. M. (2016).': 0.7850775718688965, 'The Variational Gaussian Process.': 1.0984097719192505, 'Presented at the International Conference on Learning Representations.': 1.0986123085021973, '7': 1.0986123085021973, 'Summary:': 1.0986123085021973, 'This paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE).': 1.098610520362854, 'Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated.': 1.097113013267517, 'The method’s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset.': 1.0981494188308716, 'Review:': 1.0986087322235107, 'Variational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community.': 1.0982602834701538, 'The paper is well written and clear.': 1.0986123085021973, 'The experiments are thorough.': 1.0986123085021973, 'It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions.': 1.0986123085021973, 'I further find the analyses of the effects of dropout and private variables useful.': 1.0986123085021973, 'As the authors point out, “VCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA”.': 1.0986123085021973, 'It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively.': 1.0986123085021973, 'While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better.': 1.0986123085021973, 'The derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints.': 1.0986123085021973, 'Perhaps move to the Appendix?': 1.0986123085021973, 'In Section 3 the authors claim that “if we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data”.': 1.0986123085021973, 'This is not correct, a model which hasn’t learned a thing can have perfectly realistic samples (see Theis et al., 2016).': 1.0986123085021973, 'Please remove or revise the sentence.': 1.0986123085021973, 'Minor:': 1.0986123085021973, 'In the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer.': 1.0986123085021973}"
136,https://openreview.net/forum?id=H1MjAnqxg,"{'Summary:  The authors present a simple RNN with linear dynamics for language modeling. The linear dynamics greatly enhance the interpretability of the model, as well as provide the potential to improve performance by caching the dynamics for common sub-sequences. Overall, the quantitative comparison on a benchmark task is underwhelming. It’s unclear why the authors didn’t consider a more common dataset, and they only considered a single dataset. On the other hand, they present a number of well-executed techniques for analyzing the behavior of the model, many of which would be impossible to do for a non-linear RNN.': 1.0986123085021973, 'Overall, I recommend that the paper is accepted, despite the results.': 1.098602294921875, 'It provides an interesting read and an important contribution to the research dialogue.': 0.559197187423706, 'Feedback': 1.0986123085021973, 'The paper could be improved by shortening the number of analysis experiments and increasing the discussion of related sequence models.': 1.098610281944275, 'Some of the experiments were very compelling, whereas some of them (eg. 4.6) sort of feels like you’re just showing the reader that the model fits the data well, not that the model has any particularly important property.': 1.0986121892929077, 'We trust that the model fits the data well, since you get reasonable perplexity results.': 1.0986123085021973, 'LSTMS/GRUs are great for for language modeling for data with rigid combinatorial structure, such as nested parenthesis.': 1.0986123085021973, 'It would have been nice if you compared your model to non-linear methods on this sort of data.': 1.0986123085021973, 'Don’t be scared of negative results!': 1.0986123085021973, 'It would be interesting if the non-linear methods were substantially better on these tasks.': 1.0986123085021973, 'You should definitely add a discussion of Belanger and Kakade 2015 to the related work.': 1.0986123085021973, 'They have different motivations (fast, scalable learning algorithms) rather than you (interpretable latent state dynamics and simple credit assignment for future predictions given past).': 1.0986123085021973, 'On the other hand, they also have linear dynamics, and look at the singular vectors of the transition matrix to analyze the model.': 1.0986123085021973, 'More broadly, it would be useful for readers if you discussed LDS more directly.': 1.0986123085021973, 'A lot of this comparison came up in the openreview discussion, and I recommend folding this into the paper.': 1.0986123085021973, 'For example, it would be useful to emphasize that the bias vectors correspond to columns of the Kalman gain matrix.': 1.0986123085021973, 'One last thing regarding LDS: your model corresponds to Kalman filtering but in an LDS you can also do Kalman smoothing, where state vectors are inferred using the future in addition to the past observations.': 1.0986123085021973, 'Could you do something similar in your model?': 1.0986123085021973, 'What if you said that each matrix is a sparse/convex combination of a set of dictionary matrices?': 1.0901820659637451, 'This parameter sharing could provide even more interpretability, since the characters are then represented by the low-dimensional weights used to combine the dictionary elements.': 0.8831415772438049, 'This could also provide more scalability to word-level problems.': 1.0796085596084595, 'Summary: The authors propose an input switched affine network to do character-level language modeling, a kind of RNN without pointwise nonlinearity, but with switching the transition matrix & bias based on the input character. This is motivated by intelligibility, since it allows decomposition of output contribution into these kappa_s^t terms, and use of basic linear algebra to probe the network.': 1.0986123085021973, 'Regarding myself as a reviewer, I am quite sure I understood the main ideas and arguments of this paper, but am not an expert on RNN language models or intelligibility/interpretability in ML.': 1.098611831665039, ""I did not read any papers with a similar premise - closest related work I'm familiar with would be deconvnet for insight into vision-CNNs."": 1.0986119508743286, 'PRO:': 1.0986123085021973, 'I think this is original and novel work.': 1.0986123085021973, 'This work is high quality, well written, and clearly is the result of a lot of work.': 1.0986123085021973, 'I found section 4.5 about projecting into readout subspace vs ""computational"" subspace most interesting and meaningful.': 1.0986123085021973, 'CON:': 1.0986123085021973, '+': 1.0986123085021973, 'The main hesitation I have is that the results on both parts (ISAN model, and analysis of it) are not entirely convincing:': 1.0986123085021973, '(1) ISAN is only trained on small task (text8), not clear whether it can be a strong char-LM on larger scale tasks,': 1.0986123085021973, '(2) nor do the analysis sections provide all that much real insight in the learned network.': 1.0986123085021973, '(1b)': 1.0986123085021973, 'Other caveat towards ISAN architecture: this model in its proposed form is really only fit for small-vocabulary (i.e. character-based) language modeling, not a general RNN with large-vocab discrete input nor continuous input.': 1.0986123085021973, '(2a)': 1.0986123085021973, 'For analysis: many cute plots and fun ideas of quantities to look at, but not much concrete insights.': 1.0986123085021973, '(2b) Not very clear which analysis is specific to the ISAN model, and which ideas will generalize to general nonlinear RNNs.': 1.0986123085021973, ""(2c) Re sec 4.2 - 4.3: It seems that the quantity \\kappa_s^t on which analysis rests, isn't all that meaningful."": 1.0986123085021973, 'Elaborating a bit on what I wrote in the question:': 1.0986123085021973, 'For example: Fig 2, for input letter ""u"" in revenue, there\'s a red spot where \'_\' character massively positively impacts the logit of \'e\'.': 1.0986123085021973, ""This seems quite meaningless, what would be the meaning of influence of '_' character?"": 1.0986123085021973, 'So it looks ot me that the switching matrix W_u (and prior W_n W_e etc) are using previous state in an interesting way to produce that following e.': 1.0986123085021973, ""So that metric \\kappa_s^t just doesn't seem very meaningful."": 1.0986123085021973, 'This remark relates to the last paragraph of Sec4.2.': 1.0986123085021973, ""Even though the list of cons here is longer than pro's, I recommend accept; specifically because the originality of this work will in any case make it more vulnerable to critiques."": 1.0986123085021973, 'This work is well-motivated, very well-executed, and can inspire many more interesting investigations along these lines.': 1.0986123085021973, 'The authors present a character language model that gains some interpretability without large losses in predictivity.': 1.0986123085021973, 'CONTRIBUTION:': 1.0986123085021973, ""I'd characterize the paper as some experimental investigation of a cute insight."": 1.0986109972000122, 'Recall that multi-class logistic regression allows you to apportion credit for a prediction to the input features: some features raised the probability of the correct class, while others lowered it.': 1.0986121892929077, 'This paper points out that a sufficiently simple RNN model architecture is log-linear in the same way, so you can apportion credit for a prediction among elements of the past history.': 1.0986123085021973, 'PROS:': 1.0795069932937622, 'The paper is quite well-written and was fun to read.': 1.0038561820983887, ""It's nice to see that a simple architecture still does respectably."": 1.0985573530197144, ""It's easy to imagine using this model for a classroom assignment."": 1.09859037399292, ""It should be easy to implement, and the students could replicate the authors' investigation of what influences the network's predictions."": 1.0986026525497437, 'The authors present some nice visualizations.': 1.0961871147155762, 'Section 5.2 also describes some computational benefits.': 1.0986123085021973, 'CAVEATS ON PREDICTIVE ACCURACY:': 0.9542550444602966, '* Figure 1 says that the ISAN has ""near identical performance to other architectures.""': 1.0986123085021973, 'But this appears true only when comparing the largest models.': 1.0921361446380615, ""Explanation: It appears that for smaller parameter sizes, a GRU still beats the authors' model by 22% to 39% in the usual metric of perplexity per word (ppw)."": 0.5847440958023071, ""(That's how LM people usually report performance, with a 10% reduction in ppw traditionally being considered a good Ph.D. dissertation."": 0.871454119682312, 'I assumed an average of 7 chars/word when converting cross-entropy/char to perplexity/word.)': 1.0986123085021973, '*': 1.0986123085021973, ""In addition, it's not known whether this model family will remain competitive beyond the toy situations tested here."": 1.0986121892929077, 'Explanation: The authors tried it only on character-based language modeling, and only on a 10M-char dataset, so their ppw is extremely high: 2135 for the best models in this paper.': 0.6067087650299072, 'By contrast, a word-based RNN LM trained on 44M words gets ppw of 133, and trained on 800M words gets ppw of 51.': 1.0986120700836182, '[Numbers copied from the paper I cited before: https://transacl.org/ojs/index.php/tacl/article/view/561 .]': 0.7522602081298828, ""Those are language models that are good enough to use for something; maybe the authors' model would continue to fare well in this regime, but we just don't know."": 1.094132900238037, '(Has the Text8 benchmark in this paper been seriously used for language modeling before?': 1.0806362628936768, 'It was designed for text compression, a rather different setting where smaller datasets are meaningful because compression is done online, without a training/test split as done for language modeling.': 1.0986123085021973, 'The baseline results in this paper are drawn from a contemporaneous submission with many of the same authors.)': 1.0986123085021973, 'CAVEATS ON INTERPRETABILITY:': 1.0986123085021973, 'I liked the visualizations as an educational tool.': 1.0986123085021973, ""Maybe they'll inspire other visualization ideas for other models."": 1.0986123085021973, ""On the other hand, I'm not sure whether one gets much actionable information from these visualizations:"": 1.0986123085021973, '* Sometimes, visualization is used as a way to understand what a model is doing wrong so that you can fix the model.': 1.0986123085021973, ""But that might not work here: this model doesn't seem to have a lot of room for adjustment before it would stop being interpretable."": 1.0986123085021973, '(Although you could leave the model alone and preprocess the input data, I guess ...)': 1.0986123085021973, ""* Sometimes, visualization is used to explain a single machine prediction to a human who will make the final decision about whether to trust that prediction (e.g., Singh et al.'s LIME paper)."": 1.0986123085021973, ""It's hard to imagine how that would work in this kind of SEQUENTIAL prediction setting, though."": 1.0986123085021973, 'OTHER COMMENTS:': 1.0986123085021973, 'Most of my technical reactions are already given in my pre-review questions.': 1.0986123085021973, 'Thanks to the authors for their answers, and I appreciate that they are running followup experiments for the next version of the paper!': 1.0986123085021973}"
137,https://openreview.net/forum?id=H1VyHY9gg,"{'Strengths': 1.0986108779907227, 'A simple “noising” method for improving LM': 1.0986096858978271, '“noise” added to word history by using a probabilistic distribution using N-gram smoothing': 0.9501608610153198, 'Experimental evidence that such simple techniques improve LM and MT': 1.098611831665039, 'Weaknesses': 1.0935299396514893, 'Purely empirical, with no theoretical justification': 1.0986086130142212, 'Rather primitive step so far; would be nice to see future work in modeling different types of LM “noise” as hidden variables and then “denoise” them.': 1.010259747505188, 'This paper proposes a data noising technique for language modeling.': 1.0805374383926392, 'The main idea is to noise a word history by using a probabilistic distribution based on N-gram smoothing techniques.': 1.0986123085021973, 'The paper is clearly written and shows that such simple techniques improve the performance in various tasks including language modeling and machine translation.': 1.0986123085021973, 'May main concern is that the method is too simple and sounds ad hoc, e.g., there is no theoretical justification of why n-gram smoothing based data noising would be effective for recurrent neural network based language modeling.': 1.0986123085021973, 'Comments:': 0.3284105062484741, 'p. 3 “can be seen has a way” -> “can be seen as a way” (?)': 1.0450947284698486, 'p. 3.': 1.0986002683639526, 'In general, the explanation about blank noising should be improved.': 1.0986123085021973, 'Why does it avoid overfitting on specific contexts?': 1.0982645750045776, 'p. 4.': 1.0971481800079346, 'It would be better to provide more detailed derivations for a general form of unigram and blank noising equations.': 1.098610758781433, 'p. 5, Section 3.6: Is there any discussions about noising either/both input and output sequences with some numbers?': 0.8354293704032898, 'This would be helpful information.': 1.0986123085021973, 'This paper discusses data noising as a regularization technique for language modelling as an alternative to dropout regularization.': 1.0986123085021973, 'The key idea is to adapt smoothing methods from ngram language modelling in such a fashion that they can be applied to continuous language models through noise.': 1.0986123085021973, 'Through this motivation, the authors present noising analogies to standard discounting, as well as Kneser-Ney smoothing.': 1.0986123085021973, 'The experiments are convincing in that the smoothed (noised) models outperform their unregularized baselines.': 1.0986123085021973, 'My main issue with the evaluation in this paper is that there is no comparison between the noising/smoothing idea and more conventional regularizers (such as L2 and dropout) which were discussed in the paper.': 1.0986123085021973, 'Likewise, it would have been interesting if the techniques proposed here had been applied to stronger base models (such as the models compared with in Table 2).': 1.0986123085021973, ""Seeing that the noising technique is effectively just data augmentation it should have been reasonably trivial to blackbox the model and plug in Zaremba's or Gal's!"": 1.0984573364257812, 'Those weaknesses aside (and I recommend the authors investigate improving their paper by adding these experiments), this paper presents a novel method for improving neural network learning for a number of sequence based problems, and does so convincingly.': 1.0986123085021973, 'I strongly recommend the paper for acceptance.': 1.0986123085021973}"
138,https://openreview.net/forum?id=H1W1UN9gg,"{""I'm not familiar enough with mean-field techniques to judge the soundness of Eq 2, but I'm willing to roll with it."": 1.0985876321792603, 'Minor point on presentation: Speaking of the ""evolution"" of x_{i;a} as it travels through the network could give some readers helpful intuition, but for me it was confusing because x_{*;a} is the immutable input vector, and it\'s the just-introduced z and y variables that represent its so-called evolution, no?': 1.0986067056655884, 'In interpreting this analysis - A network may be trainable if information does not pass through it, if the training steps, by whatever reason, perturb the weights so that information starts to pass through it (without subsequently perturbing the weights to stop information from passing through it.)': 1.087112545967102, 'Perhaps this could be clarified by a definition of “training algorithm”?': 1.0986123085021973, 'Comments on central claims:': 1.0986117124557495, 'Previous work on initializing neural networks to promote information flow (e.g. Glorot & Bengio, http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf) concluded (1) that the number of units in the next layer and the previous layer should both figure into the variance of the elements of the weight matrices, and (2) that they should be drawn from a Uniform distribution rather than a Gaussian.': 1.0939586162567139, 'Could the authors comment on the merit of that initialization strategy in light of this analysis?': 1.0981485843658447, 'Comments on evaluation:': 1.0986123085021973, 'Why does the dashed line in the result figures correspond to twice the depth scale instead of just the depth scale?': 1.0730684995651245, 'What is the significance of 14000 steps of SGD on MNIST?': 0.525999903678894, 'Does it represent convergence of SGD?': 1.0986114740371704, 'Why are all the best SGD models well above the depth scale?': 1.09856116771698, 'Why is there a little dark area precisely under the peak in Figure 5(a) and (c)?': 0.3925682306289673, 'That’s interesting - initializations that propagate error best seem untrainable at the depths traditionally used - but only with SGD not RMSProp?': 1.0983749628067017, 'The accuracy of the trained models on CIFAR-10 and MNIST are not reported - it seems important to the overall argument of the paper that the sorts of networks underlying Figures 5 and 6 are the same as the ones that people would consider state-of-art within the model class (fully connected, sigmoidal nonlinearities, etc.).': 0.7846993207931519, 'The paper expands a recent mean-field approximation of deep random neural networks to study depth-dependent information propagation, its phase-dependence and the influence of drop-out.': 1.0986123085021973, 'The paper is extremely well written, the mathematical analysis is thorough and numerical experiments are included that underscore the theoretical results.': 1.0981764793395996, 'Overall the paper stands out as one of the few papers that thoroughly analyses training and performance of deep nets.': 1.0986123085021973, 'This paper presents a mathematical analysis of how information is propagated through deep feed-forward neural networks, with novel analysis addressing the problem of vanishing and exploding gradients in the backward pass of backpropagation and the use of the dropout algorithm.': 1.0986123085021973, 'The paper is clear and well-written, the analysis is thorough, and the experimental results showing agreement with the model are very nice.': 1.0984747409820557}"
139,https://openreview.net/forum?id=H1_EDpogx,"{'For more than a decade, near data processing has been a key requirement for large scale linear learning platforms, as the time to load the data exceeds the learning time, and this has justified the introduction of approaches such as Spark': 1.0973256826400757, 'Deep learning usually deals with the data that can be contained in a single machine and the bottleneck is often the CPU-GPU bus or the GPU-GPU-bus, so a method that overcomes this bottleneck could be relevant.': 0.9738692045211792, 'Unfortunately, this work is still very preliminary and limited to linear training algorithms, so of little interest yet to ICLR readership.': 1.0985157489776611, 'I would recommend publication to a conference where it can reach the large-scale linear ML audience first, such as ICML.': 1.0986121892929077, 'This paper is clear and well written in the present form and would probably mostly need a proper benchmark on a large scale linear task.': 1.0986123085021973, 'Obviously, when the authors have convincing DNN learning simulations, they are welcome to target ICLR, but can the flash memory FPGA handle it?': 1.0983695983886719, 'For experiments, the choice of MNIST is somewhat bizarre: this task is small and performance is notoriously terrible when using linear approaches (the authors do not even report it)': 1.0986123085021973, 'Combining storage and processing capabilities is an interesting research topic because data transfer is a major issue for many machine learning tasks.': 1.0986123085021973, 'The paper itself is well-written, but unfortunately addresses a lot of things only to medium depth (probably due length constraints).': 1.0986121892929077, 'My opinion is that a journal with an in-depth discussion of the technical details would be a better target for this paper.': 1.0985658168792725, ""Even though the researchers took an interesting approach to evaluate the performance of the system, it's difficult for me to grasp the expected practical improvements of this approach."": 1.0986123085021973, 'With such a big focus on GPU (and more specialized hardware such as TPUs), the one question that comes to mind: By how much does this - or do you expect it to - beat the latest and greatest GPU on a real task?': 1.0986123085021973, ""I don't consider myself an expert on this topic even though I have some experience with SystemC."": 1.098602294921875, ""While the idea of moving the processing for machine learning into silicon contained within the (SSD) data storage devices is intriguing and offers the potential for low-power efficient computation, it is a rather specialized topic, so I don't feel it will be of especially wide interest to the ICLR audience."": 1.0986111164093018, 'The paper describes simulation results, rather than actual hardware implementation, and describes implementations of existing algorithms.': 1.098231315612793, ""The comparisons of algorithms' train/test performance does not seem relevant (since there is no novelty in the algorithms) and the use of a single layer perceptron on MNIST calls into question the practicality of the system, since this is a tiny neural network by today's standards."": 1.0984675884246826, 'I did not understand from the paper how it was thought that this could scale to contemporary scaled networks, in terms of numbers of parameters for both storage and bandwidth.': 1.0986123085021973, 'I am not an expert in this area, so have not evaluated in depth.': 1.0986123085021973}"
140,https://openreview.net/forum?id=H1_QSDqxl,"{'This paper aims to mine explicit rules from KB embedding space, and casts it into a sparse reconstruction problem.': 1.0985716581344604, 'Experiments demonstrate its ability of extracting reasonable rules on a few link prediction datasets.': 1.098611831665039, 'The solution part sounds plausible.': 1.0986113548278809, 'However, it confuses me that why we need to mine rules from learned KB embeddings.': 1.0967518091201782, 'It is still unclear what information these KB embeddings encode and it looks strange that we aim to learn rules including negation / disjunction from them.': 1.0986095666885376, 'If the goal is to extract useful rules (for other applications), it is necessary to compare it to “graph random walk” (http://rtw.ml.cmu.edu/papers/lao-emnlp11.pdf) which could learn rules from KB graph directly.': 1.0908780097961426, 'As there is only one KNN baseline, the experimental results seem pretty weak.': 1.0936506986618042, 'At the least, it is necessary to show the original precision / recall of RESCAL, together with the proposed rule mining approach (with different max length), so we know how much the current information the current rule miner could recover.': 1.0986123085021973, 'In addition, the four datasets are all very small.': 1.0986123085021973, 'Would it be able to scale it to WordNet or Freebase?': 1.0946626663208008, '[Minor comments]': 0.6405842304229736, '“Relational embedding” and “relation embedding” are used mixedly throughout the paper.': 1.0986120700836182, 'I am not sure if they are well-defined terms (it is better to cite relevant paper).': 1.0986123085021973, 'This paper proposes a process to mine rules from vector space representations learned from KBs (using nonnegative RESCAL).': 1.0986123085021973, 'The paper is nicely written.': 1.0985676050186157, 'But its motivations are unclear: what is the underlying motivation to mine rules from embedding spaces?': 1.0986064672470093, 'If it is for better performance on link prediction then the paper does not show this.': 1.0986123085021973, 'The experiments do not compare FRM against the performance of the original vector space model.': 1.0986123085021973, 'If it is for a better interpretability and debugging of the representations learned by vector space models, then there should have more elements on this in the paper.': 1.0986123085021973, 'Other remarks:': 1.0986121892929077, 'The fact that the performance of the methods in Figure 1 and 2 are not compared to any baseline is problematic.': 1.0986123085021973, 'The scalability of the rule miner is a big drawback that should be addressed.': 1.0986123085021973, 'Figure 3 does not do a good job at convincing that rule based systems should be used for prediction or interpretation.': 1.0986123085021973, 'The learned rules are bad for both cases.': 1.0986123085021973, 'The paper presents a nice idea of directly finding rules such as brother(father)': 1.0986123085021973, '=> uncle in knowledge bases, by directly searching in embedding space.': 1.0986123085021973, 'The idea is to interpret the successive application of relationships as the multiplication of the relation-dependent matrices in non-negative RESCAL.': 1.0986123085021973, 'The experimental section provides an evaluation of the rules that are found by the algorithm.': 1.0986123085021973, 'Nonetheless, the work seems only at its first stages for now, and many questions are left open:': 1.0986123085021973, '1) while the approach to find rules seems very general, the reason why it should work is unclear.': 1.0986123085021973, 'What properties of the embedding space or of the initial algorithm are required for this approach to find meaningful rules?': 1.0986123085021973, 'Can we apply the same principles to other algorithms than non-negative RESCAL?': 1.0986123085021973, '2) there is no real evaluation in terms of link prediction.': 1.0986123085021973, 'How can we use these rules in conjunction with the original algorithm to improve link prediction?': 1.0986123085021973, 'What performance gains can be expected?': 1.0986123085021973, 'Can these rules find links that would not be found be the original algorithm in the first place?': 1.0986123085021973, '3) scaling: for now the number of parameters of the rule miner is (#relationships)^(max. path length + 1).': 1.0986123085021973, 'How does this method scale on standard benchmarks such as FB15k where there is more than a 1000 of relationships?': 1.0986123085021973}"
141,https://openreview.net/forum?id=H1acq85gx,"{'This paper applies the idea of normalizing flows (NFs), which allows us to build complex densities with tractable likelihoods, to maximum entropy constrained optimization.': 1.0986123085021973, 'The paper is clearly written and is easy to follow.': 1.0986123085021973, 'Novelty is a weak factor in this paper.': 1.0986123085021973, 'The main contributions come from (1) applying previous work on NFs to the problem of MaxEnt estimation and (2) addressing some of the optimization issues resulting from stochastic approximations to E[||T||] in combination with the annealing of Lagrange multipliers.': 1.0986123085021973, 'Applying the NFs to MaxEnt is in itself not very novel as a framework.': 1.0986123085021973, 'For instance, one could obtain a loss equivalent to the main loss in eq.': 1.0986123085021973, '(6) by minimizing the KLD between KL[p_{\\phi};f], where f is the unormalized likelihood f \\propto exp \\sum_k( - \\lambda_k T - c_k ||T_k||^2  ).': 1.098602056503296, 'This type of derivation is typical in all previous works using NFs for variational inference.': 0.6936682462692261, ""A few experiments on more complex data would strengthen the paper's results."": 1.0986123085021973, 'The two experiments provided show good results but both of them are toy problems.': 1.0986123085021973, 'Minor point:': 1.0842604637145996, 'Although intuitive, it would be good to have a short discussion of step 8 of algorithm 1 as well.': 1.0929508209228516, 'Much existing deep learning literature focuses on likelihood based models.': 0.5733463168144226, 'However maximum entropy approaches are an equally valid modelling scenario, where information is given in terms of constraints rather than data.': 1.0977413654327393, 'That there is limited work in flexible maximum entropy neural models is surprising, but  is due to the fact that optimizing a maximum entropy model requires (a) establishing the effect of the constraints on some distribution, and formulating the entropy of that complex distribution.': 1.0986123085021973, 'There is no unbiased estimator of entropy from samples alone, and so an explicit model for the density is needed.': 1.0986104011535645, 'This challenge limits approaches.': 1.0986120700836182, 'The authors have identified that invertible neural models provide a powerful class of models for solving the maximum entropy network problem, and this paper goes on to establish this approach.': 1.0986123085021973, 'The contributions of this paper are (a) recognising that, because normalising flows provide an explicit model for the density, they can be used to provide unbiased estimators for the entropy (b) that the resulting Lagrangian can be implemented as a relaxation of a augmented Lagrangian (c) establishing the practical issues in doing the augmented Lagrangian optimization.': 1.0986123085021973, 'As far as the reviewer is aware this work is novel – this approach is natural and sensible, and is demonstrated on an number of models where clear evaluation can be done.': 1.0986123085021973, 'Enough experiments have been done to establish this is an appropriate method, though not that it is entirely necessary – it would be good to have an example where the benefits of the flexible flow transformation were much clearer.': 1.0986123085021973, 'Further discussion of the computational and scaling aspects would be valuable.': 1.0986123085021973, 'I am guessing this approach is probably appropriate for model learning, but less appropriate for inferential settings where a known model is then conditioned on particular instance based constraints?': 1.0986123085021973, 'Some discussion of appropriate use cases would be good.': 1.0986123085021973, 'The issue of match to the theory via the regularity conditions has been brought up, but it is clear that this can be described well, and exceeds most of the theoretical discussions that occur regarding the numerical methods in other papers in this field.': 1.0986123085021973, 'Quality: Good sound paper providing a novel basis for flexible maximum entropy models.': 1.0986123085021973, 'Clarity: Good.': 1.0986123085021973, 'Originality: Refreshing.': 1.0986123085021973, 'Significance: Significant in model development terms.': 1.0986123085021973, 'Whether it will be an oft-used method is not clear at this stage.': 1.0986123085021973, 'Minor issues': 1.0986123085021973, 'Please label all equations.': 1.0986123085021973, 'Others might wish to refer to them even if you don’t.': 1.09828782081604, 'Top of page 4: algorithm 1→ Algorithm 1.': 1.0986123085021973, 'The update for c to overcome stability appears slightly opaque and is mildly worrying.': 1.0978708267211914, 'I assume there are still residual stability issues?': 1.098477840423584, 'Can you comment on why this solves all the problems?': 1.0986123085021973, 'The issue of the support of p is glossed over a little.': 1.0986123085021973, 'Is the support in 5 an additional condition on the support of p?': 0.909132719039917, 'If so, that seems hard to encode, and indeed does not turn up in (6).': 0.8772324919700623, 'I guess for a Gaussian p0 and invertible unbounded transformations, if the support happens to be R^d, then this is trivial, but for more general settings this seems to be an issue that you have not dealt?': 0.96346116065979, 'Indeed in your Dirichlet example, you explicitly map to the required support, but for more complex constraints this may be non trivial to do with invertible models with known Jacobian?': 0.6927770972251892, 'It would be nice to include this in the more general treatment rather than just relegating it to the specific example.': 0.9032244086265564, 'Overall I am very pleased to see someone tackling this question with a very natural approach.': 0.40341508388519287, 'The authors propose a new approach for estimating maximum entropy distributions': 0.5934680700302124, 'subject to expectation constraints.': 1.0986101627349854, 'Their approach is based on using': 1.0986055135726929, 'normalizing flow networks to non-linearly transform samples from a tractable': 1.0760908126831055, 'density function using invertible transformations.': 1.0897576808929443, 'This allows access to the': 1.0896278619766235, 'density of the resulting distribution.': 1.0978466272354126, 'The parameters of the normalizing flow': 1.0986123085021973, 'network are learned by maximizing a stochastic estimate of the entropy': 1.0986123085021973, 'obtained by sampling and evaluating the log-density on the obtained samples.': 1.0986123085021973, 'This stochastic optimization problem includes constraints on expectations with': 1.0986123085021973, 'respect to samples from the normalizing flow network.': 1.0986123085021973, 'These constraints are': 1.0986123085021973, 'approximated in practice by sampling and are therefore stochastic.': 1.0986123085021973, 'The': 1.0986123085021973, 'optimization problem is solved by using the augmented Lagrangian method.': 1.0986123085021973, 'proposed method is validated on a toy problem with a Dirichlet distribution and': 1.0986123085021973, 'on a financial problem involving the estimation of price changes from option': 1.0986123085021973, 'price data.': 1.0986123085021973, 'Quality:': 1.0986123085021973, 'The paper seems to be technically sound.': 1.0985316038131714, 'My only concern would the the approach': 0.8994563817977905, 'followed to apply the augmented Lagrangian method when the objective and the': 0.6842373609542847, 'constraints are stochastic.': 0.9448553323745728, 'The authors propose their own solution to this': 1.0659323930740356, 'problem, based on a hypothesis test, but I think it is likely that this has': 0.6718616485595703, 'already been addressed before in the literature.': 0.7210197448730469, 'It would be good if the': 0.9529695510864258, 'authors could comment on this.': 0.6155369281768799, 'The experiments performed show that the proposed approach can outperform Gibbs': 1.0986123085021973, 'sampling from the exact optimal distribution or at least be equivalent, with': 1.0182071924209595, 'the advantage of having a closed form solution for the density.': 0.4662353992462158, 'I am concern about the difficulty of he problems considered.': 1.098111867904663, 'The Dirichlet distributions are relatively smooth and the distribution in the': 1.0985625982284546, 'financial problem is one-dimensional (in this case you can use numerical': 0.8871057629585266, 'methods to compute the normalization constant and plot the exact density).': 0.9053627252578735, 'They seem to be very easy and do not show how the method would perform in more': 0.4085099697113037, 'challenging settings: high-dimensions, more complicated non-linear constraints,': 0.6780877113342285, 'etc...': 1.0981744527816772, 'Clarity:': 1.0986121892929077, 'The paper is clearly written and easy to follow.': 1.097827672958374, 'Originality:': 0.42060142755508423, 'The proposed method is not very original since it is based on applying an': 0.46037086844444275, 'existing technique (normalizing flow networks) to a specific problem: that of': 0.40589451789855957, 'finding a maximum entropy distribution.': 0.8727619647979736, 'The methodological contributions are': 1.0959126949310303, 'almost non-existing.': 0.9038113951683044, 'One could only mention the combination of the normalizing': 1.0850515365600586, 'flow networks with the augmented Lagrangian method.': 1.0817204713821411, 'Significance:': 1.0969041585922241, 'The results seem to be significant in the sense that the authors are able to': 0.9221901297569275, 'find densities of maximum entropy distributions, something which did not seem': 0.828283429145813, 'to be possible before.': 1.0986123085021973, 'However, it is not clearly how useful this can be in': 1.0986123085021973, 'practice.': 1.0986123085021973, 'The problem that they address with real-world data (financial data)': 1.0986123085021973, 'could have been solved as well by using 1-dimensional quadrature.': 1.0986123085021973, 'The authors': 1.0986123085021973, 'should consider more challenging problems which have a clear practical': 1.0986123085021973, 'interest.': 1.0986123085021973, 'Minor comments:': 1.0986123085021973, 'More details should be given about how the plot in the bottom right of Figure 2 has been obtained.': 1.0986123085021973, '""a Dirichlet whose KL to the true p∗ is small"": what do you mean by this?': 1.0986123085021973, 'Can you give more details on how you choose that Dirichlet?': 1.0986123085021973, 'I changed updated my review score after having a look at the last version of the paper submitted by the authors, which includes new experiments.': 1.0986123085021973}"
142,https://openreview.net/forum?id=H1eLE8qlx,"{'In this paper, the authors study the problem of discovering options for reinforcement learning.': 1.3851670026779175, 'They introduce the Bi-POMDP model, which is a POMDP where the observations are structured as a pair of elements, with the first element only available to the option-choosing component (the ""option level"") and the second element only to the action-choosing component (the ""action level"") and the termination component (the ""acquisition model"").': 1.3859922885894775, 'They also detail the BONN learning model, which consists of three artificial neural network that implement these three components.': 1.3861488103866577, 'Finally, they suggest optimizing a tradeoff between the value achieved by the model and the cost of switching between options (the ""cognitive effort""), and demonstrate this approach in three simple domains: Cart-Pole, Lunar Lander and two variants of a grid-world maze.': 1.374057650566101, 'The paper is interesting, and adds considerably to the increasing body of research in hierarchical reinforcement learning (HRL).': 1.3861831426620483, 'I found no critical flaws in the paper, but also no high-impact insights or impressive improvements.': 1.3862943649291992, 'This paper offers some good ideas that are moderately novel and may advance the field, but has some issues.': 1.3862299919128418, 'The first issue is that it is unclear how much easier it is to compose Bi-POMDPs than hand-crafted options or subgoals.': 1.3862665891647339, 'If Bi-POMDPs are to alleviate the design costs of using HRL with human-defined structure, one needs to show that the splitting of observations into two elements (x, y) is easier to do well enough.': 1.3842546939849854, 'For example, in Section 5 the authors are correct in pointing out that sequences of actions (""macro-actions"") are open-loop, and therefore not as expressive as closed-loop options.': 1.3654676675796509, 'However, by setting x empty in all but one experiment, the authors also restrict themselves to open-loop sub-policies, albeit stochastic ones.': 1.385805606842041, 'Such sub-policies may be sufficiently expressive for the simple domains in this paper, but this is unlikely so in more realistic domains.': 0.9393094778060913, ""In the MAZE_2 domain, x is the agent's position relative to the current room."": 1.290315866470337, 'Designing this domain-specific observation model requires domain knowledge, arguably no less than designing relevant subgoals.': 1.3832693099975586, 'It is hard to judge the effectiveness of this approach without design principles for these domain-specific observation models, and more realistic experiments to evaluate their quality.': 1.3862918615341187, 'Finally, it is revealing that the drive for hierarchy is only achieved by limiting x.': 1.3447864055633545, 'In Section 3.3 the authors mention that acquisition of y is ""crucial for discovering a good policy: an agent only using the observations x_t would be unable to solve the task"", which suggests that the design choice of x directly impacts one side of the trade-off between the value and the cognitive effort.': 1.364413857460022, 'Such an important factor should be addressed explicitly.': 1.3622177839279175, 'The authors may be interested in the paper ""Learning and Transfer of Modulated Locomotor Controllers"" (Heess et al., 2016), which has a similar split observation model, and suffers from the same issue.': 0.9227023720741272, 'The second issue is that the option space seems to be expressive enough to represent y with high fidelity.': 0.9416898488998413, 'If this is the case, and if the learned option model indeed maintains a good image of y, then the algorithm is really solving a different problem: reinforcement learning with costly observability of y.': 0.4863692820072174, 'This means that it learns how to act given a stale value of y and when to refresh it.': 1.3590564727783203, 'It should then be framed accordingly and compared with the relevant literature.': 0.6439628005027771, 'It may or may not be as interesting or novel.': 1.3825030326843262, 'In contrast, the standard options framework calls for compression of y into the choice of option o (usually in a small finite space).': 0.9229112863540649, 'The agent should learn to extract subtask-relevant information from y, in a way that generalizes to unseen states or subtasks.': 1.382129430770874, 'That said, the embedding of y in the option space can be interesting in itself, even if it is lossless (1-to-1).': 1.363122820854187, 'Unfortunately, no such analysis was offered by the authors.': 1.384762167930603, 'Minor issues:': 1.3862943649291992, 'In 3.3: error in citation, (?) appears instead.': 1.3862943649291992, 'In 4.1: it is confusing to say that ""the environments are more stochastic"" when epsilon is increased.': 1.3862943649291992, ""It is the agent's policy, not the environment, that becomes more stochastic, which is useful for exploration."": 1.3862943649291992, 'If this hurts performance, some discussion is needed of why too much exploration is detrimental to learning.': 1.359747052192688, 'In particular, the paper does not make explicit the number of iterations in the experiments, and it is not clear whether learning with larger epsilon is worse after some fixed number of iterations or asymptotically.': 1.2927331924438477, 'The paper presents an approach to constructing hierarchical RL representations which relies on assuming agents that need to spend cognitive effort in order to choose their actions.': 1.3862943649291992, 'The paper p[roposes a specific way of formulating option construction via what they call a ""Bi-POMDP"".': 1.3862943649291992, 'This idea is potentially very interesting, plausible form a cognitive science point of view, and definitely deserves attention.': 1.3862942457199097, 'However, there are some problems which do not make the paper acceptable in its current form.': 1.3862943649291992, 'I am listing them here in order of importance.': 1.3406163454055786, '1. It is not clear from the description why a Bi-POMDP is not a POMDP. POMDPs allow for vector-based observations.  Suppose the observation vector is (x_t, \\sigma_t * y_t). This seems like it would result in a POMDP which is identical to the proposed model. The paper should include an example of a Bi-POMDP which is *not* a POMDP, or be revised to use specific POMDP terminology (see eg the use of augmented MDPs in hierarchical RL, which *are MDPs* but do not work in the original state space)': 1.198717474937439, '2. The paper make some specific assumptions about the abstractions (eg determinism in certain places). It is not clear why these are needed at all. Similarly, there are some very specific assumptions regarding the form of the approximations used (Relu, GRUs etc).  Are these necessary? In principle one could implement the ideas in the paper with other, simpler architectures. Was this the first set of choices, or was it arrived at after some experimentation? It is important to understand how much of the performance achieved is due to the specific (fairly powerful) architectures and what one could get through simpler means (eg, feedforward nets)': 0.6698794960975647, '3. The paper seems quite similar in spirit to Bacon & Precup, 2015b; in fact, it seems that the use of a value function or model that they discuss is a way to provide a y_t. However, there is no direct comparison to that approach. Since it is very related, it would be useful to perform some of those same experiments. Also, their paper works entirely in the MDP, not POMDP framework, so some clarification is needed here regarding the use of POMDPs instead.': 0.6286043524742126, '4. The choice of domains is somewhat limited to simple tasks, while some of the recent approaches in hierarchical RL use more complex domains (Atari, Minecraft etc). Ideally, the experiments should be extended to some of these more complex tasks.': 0.6685593128204346, '5. What are the theoretical properties of the proposed approach? Eg, is the proposed algorithm convergent? If Bi-POMDP is a POMDP, then one should be able to leverage POMDP results to build some theory here. If it is not a POMDP, then we need some understanding of how easy/hard a Bi-POMDP is to solve': 0.7591422200202942, '6. The paper contains many grammar problems and some broken references, and should be proof-read thoroughly': 1.383276343345642, 'In summary, while the proposed approach is quite interesting and definitely worth exploring, the paper is not ready for publication in its current form.': 1.3855435848236084, 'This paper proposes an approach to learning hierarchical actions or options in reinforcement learning using the so-called BONN model (for budgeted options with neural networks).': 1.3862943649291992, 'The approach is an interesting mixture of the old and the new.': 1.0768896341323853, 'Some ideas seem very related to previous work in the literature, such as variants of hidden Markov models proposed by Hung Bui and others (abstract HMM, hierarchical POMDP by Theocharous et al., IROS 2005; Murphy et al., NIPS, ICRA).': 1.386256217956543, 'The major difference is that unlike the prior work using a graphical model, this paper uses a gated recurrent network neural model to implement the learning of options from data.': 1.3862943649291992, 'The approach is based on minimizing some quantity called the ""cognitive effort"", but this is confusingly explained, and not very precise.': 1.3862943649291992, 'The basic idea here is to define a budget that modified the immediate reward, and so its minimization is viewed as minimizing cognitive effort.': 1.3862942457199097, 'The approach seems a bit ad hoc.': 1.3862943649291992, 'Experiments are reported on a variety of simple discrete and continuous control benchmark domains.': 1.3862943649291992, 'The paper tackles the very important problem of learning options from data.': 1.3862943649291992, 'The introduction of the budget constraint is an interesting twist on this problem, which I had not seen before (though other methods apply other constraints.)': 1.3862943649291992, 'I must say I’m not very convinced by the need to introduce the Bi-POMDP framework, where the conventional POMDP framework would do.': 1.3862943649291992, 'In discussions, the authors suggest this makes for simpler comparison with RL models, but I find that it rather obscures the link to POMDP models.': 1.3862943649291992, 'The proposed method makes an interesting contribution, distinct from the existing literature as far as I know.': 1.3862943649291992, 'The extension to discover a discrete set of options is a nice feature for practical applications.': 1.3862943649291992, 'In terms of the algorithm itself, I am actually unclear about lines 4 & 6.': 1.3862943649291992, 'At line 4, I don’t know how \\sigma_t is computed.': 1.3862943649291992, 'Can you give the precise equation?': 1.3862943649291992, 'At line 6, I don’t know how the new option o_t is generated.': 1.3862943649291992, 'Again, can you give the precise procedure?': 1.3862943649291992, 'The paper contains several empirical results, on contrasting simulated domains.': 1.3862943649291992, 'For some of these domains, such as CartPole, it’s really not clear that options are necessary.': 1.3862943649291992, 'In my mind, the lack of comparison to other options learning methods is a limitation of the current draft.': 1.3862943649291992}"
143,https://openreview.net/forum?id=H1fl8S9ee,"{'This paper introduces an approach for model-based control of stochastic dynamical systems with policy search, based on (1) learning the stochastic dynamics of the underlying system with a Bayesian deep neural network (BNN) that allows some of its inputs to be stochastic, and (2) a policy optimization method based on simulated rollouts from the learned dynamics.': 1.0986123085021973, 'BNN training is carried out using \\alpha-divergence minimization, the specific form of which was introduced in previous work by the authors.': 1.0986123085021973, 'Validation and comparison of the approach is undertaken on a simulated domain, as well as real-world scenarios.': 1.0986123085021973, 'The paper is tightly written, and easy to follow.': 1.0494378805160522, 'Its approach to fitting Bayesian neural networks with \\alpha divergence is interesting and appears novel in this context.': 1.0986123085021973, 'The resulting application to model-based control appears to have significant practical impact, particularly in light of the explainability that a system model can bring to specific decisions made by the policy.': 1.0986123085021973, 'As such, I think that the paper brings a valuable contribution to the literature.': 1.0986121892929077, 'That said, I have a few questions and suggestions:': 1.0986123085021973, '1) In section 2.2, it should be explained how the random z_n input is used by the neural network: is it just concatenated to the other inputs and used as-is, or is there a special treatment?': 1.0986123085021973, '2)': 1.0986123085021973, 'Moreover, much case is made for the need to have stochastic inputs, but only a scalar input seems to be provided throughout.': 1.0986123085021973, 'Is this enough?': 1.0986123085021973, 'How computationally difficult would providing stochastic inputs of higher dimensionality be?': 1.0986123085021973, '3) How important is the normality assumption in z_n?': 1.0986123085021973, 'How is the variance \\gamma established?': 1.0986123085021973, '4) It is mentioned that the hidden layers of the neural network are made of rectifiers, but no further utilization of this fact is made in the paper.': 1.0986123085021973, 'Is this assumption somehow important in the optimization of the alpha-divergence (beyond what we know about rectifiers to mitigate the vanishing gradient problem) ?': 1.0986123085021973, '5) Equation (3), denominator \\mathbf{y} should be \\mathbf{Y} ?': 1.0986123085021973, '6) Section 2.3: it would be helpful to have an overview or discussion of the computational complexity of training BNNs, to understand whether and when they can practicably be used.': 1.0986123085021973, '7) Between eq (12) and (13), a citation to the statement of the time embedding theorem would be helpful, as well as an indication of how the embedding dimension should be chosen.': 1.0986123085021973, '8) Figure 1: the subplots should have the letters by which they are referenced in the text on p. 7.': 1.0986108779907227, '9)': 1.0986123085021973, 'In section 4.2.1, it is not clear if the gas turbine data is publicly available, and if so where.': 1.0986123085021973, 'In addition more details should be provided, such as the dimensionality of the variables E_t, N_t and A_t.': 1.0971466302871704, '10)': 1.0986123085021973, 'Perhaps the comparisons with Gaussian processes should include variants that support stochastic inputs, such as Girard et al. (2003), to provide some of the same modelling capabilities as what’s made use of here.': 1.0982747077941895, 'At least, this strand of work should be mentioned in Section 5.': 1.0986121892929077, 'References:': 1.0986123085021973, 'Girard, A., Rasmussen, C. E., Quiñonero Candela, J., & Murray Smith, R. (2003).': 1.0968071222305298, 'Gaussian process priors with uncertain inputs-application to multiple-step ahead time series forecasting.': 1.0986123085021973, 'Advances in Neural Information Processing Systems, 545-552.': 1.0982533693313599, 'The authors propose a novel way of using Bayesian NNs for policy search in stochastic dynamical systems.': 1.0986123085021973, 'Specifically, the authors minimize alpha-divergence with alpha=0.5 as opposed to standard VB.': 1.0939111709594727, ""The authors claim that their method is the first model-based system to solve a 20 year old benchmark problem; I'm not very familiar with this literature, so it's difficult for me to assess this claim."": 1.0986030101776123, 'The paper seems technically sound.': 1.0986123085021973, 'I feel the writing could be improved.': 1.0986123085021973, 'The notation in sections 2-3 feels a bit dense and there are a lot of terminology / approximations introduced, which makes it hard to follow.': 1.0986123085021973, 'The writing could be better structured to distinguish between novel contributions vs review of prior work.': 1.0986121892929077, ""If I understand section 2.3 correctly, it's mostly a review of black box alpha divergence minimization."": 0.964189887046814, 'If so, it would probably make sense to move this to the appendix.': 1.0986109972000122, 'There was a paper at NIPS 2016 showing promising results using SGHMC for Bayesian optimization: ""Bayesian optimization with robust Bayesian neural networks"" by Springenberg et al.': 0.4322693347930908, 'Could you comment on applicability of stochastic gradient MCMC (SGLD / SGHMC) for your setup?': 1.0986123085021973, 'Can you comment on the computational complexity of the different approaches?': 1.0986123085021973, ""Section 4.2.1: why can't you use the original data?"": 1.0986123085021973, 'in what sense is it fair to simulate data using another neural network?': 1.0986123085021973, 'can you evaluate PSO-P on this problem?': 1.0986123085021973, 'This paper considers the problem of model-based policy search.': 1.0986123085021973, 'The authors': 1.0986123085021973, 'consider the use of Bayesian Neural Networks to learn a model of the environment': 1.0986123085021973, 'and advocate for the -divergence minimization rather than the more usual': 1.0986123085021973, 'variational Bayes.': 1.0986123085021973, 'The ability of alpha-divergence to capture bi-modality however': 1.0986123085021973, 'comes at a price and most of the paper is devoted to finding tractable approximations.': 1.0986123085021973, 'The authors therefore use the approach of Hernandez-Lobato': 1.0986123085021973, 'et al. (2016) as proxy to the alpha-divergence .': 1.0986123085021973, 'The environment/system dynamics is clearly defined as a well as the policy parametrization': 1.0986123085021973, '(section 3) and would constitute a useful reference point for other researchers.': 1.0986123085021973, 'Simulated roll-outs, using the learned model, then provide samples of the expected': 1.0986123085021973, 'return.': 1.0986123085021973, 'Since a model of the environment is available, stochastic gradient descent': 1.0986123085021973, 'can be performed in the usual way, without policy gradient estimators, via automatic': 1.0986123085021973, 'differentiation tools.': 1.0986123085021973, 'The experiments demonstrate that alpha-divergence is capable of capturing multi-model': 1.0986123085021973, 'structure which competing methods (variational Bayes and GP)': 1.0986123085021973, 'would otherwise': 1.0986123085021973, 'struggle with.': 1.0986123085021973, 'The proposed approach also compares favorably in a real-world': 1.0986123085021973, 'batch setting.': 1.0986123085021973, 'The paper is well-written, technically rich and combines many recent tools': 1.0986123085021973, 'into a coherent algorithm.': 1.0986123085021973, 'However, the repeated use of approximations to original': 1.0986123085021973, 'quantities seems to somehow defeat the benefits of the original problem formulation.': 1.0986123085021973, 'The scalability and computational effectiveness of this approach is also questionable': 1.0986123085021973, 'and I am uncertain if many problem would warrant such complexity in their solution.': 1.0986123085021973, 'As with other Bayesian methods, the proposed approach would probably shine in low-samples': 1.0986123085021973, 'regime and in this case might be preferable to other methods in the same class (VB, GP).': 1.0986123085021973}"
144,https://openreview.net/forum?id=H1hoFU9xe,"{'This paper proposes an interesting application of the GAN framework in steganography domain.': 1.0986123085021973, 'In addition to the normal GAN discriminator, there is a steganalyser discriminator that receives the negative examples from the generator and positive examples from the generator images that contain a hidden payload.': 1.0986123085021973, 'As a result, the generator, not only learn to generate realistic images by fooling the discriminator of the GAN, but also learn to be a secure container by fooling steganalyser discriminator.': 1.0986123085021973, 'The method is tested by training an independent steganalyser S* on real images and generated images.': 1.0986123085021973, 'Given that in the ICLR community, not many people are familiar with the literature of steganography, I think this paper should have provided more context about how exactly this method can be used in practice, what are the related works on setganalysis-secure message embedding and probably a more thorough sets of experiments on more than one dataset.': 1.0986123085021973, 'The proposed SGAN framework (Figure 2) does make sense to me, and I think it is very general and can have more applications other than the steganography domain.': 1.0986123085021973, 'But it is not clear to me why fooling the steganalyser discriminator S, necessarily mean that we can fool an independent discriminator S*?': 1.0986123085021973, 'Also I find it surprising that a different seed value, can make such a huge difference in the accuracy.': 1.0986090898513794, 'In short, the ideas of this paper are interesting and potentially useful, but I think the presentation of this paper should be improved so that it becomes more suitable for the ICLR and machine learning community.': 1.09861159324646, 'I found this paper very original and thought-provoking, but also a bit difficult to understand.': 1.0986106395721436, 'It is very exciting to see a practical use case for image-generating GANs, with potentially meaningful benchmarks aside from subjective realism.': 1.098595380783081, 'I found eq.': 1.0321766138076782, '4 interesting because it introduces a potentially non-differentiable black-box function Stego(...) into the training of (S, G).': 0.9808858633041382, 'Do you in fact backprop through the Stego function?': 1.0986123085021973, 'For the train/test split, why is the SGAN trained on all 200k images?': 1.0986123085021973, 'Would it not be cleaner to use the same splits for training SGAN as for ""steganalysis purposes""?': 1.0986123085021973, 'Could this account for the sensitivity to random seed shown in table 2?': 1.0986123085021973, 'Sec. 5.3: ""Steganographic Generative Adversarial Networks can potentially be used as a universal tool for generating Steganography containers tuned to deceive any specific steganalysis algorithm.': 1.0986123085021973, '"".': 1.0986123085021973, 'This experiment showed that SGAN can fool HUGO, but I do not see how it was ""tuned"" to deceive HUGO, or how it could be tuned in general for a particular steganalyzer.': 1.0986123085021973, 'Although S* seems to be fooled by the proposed method, in general for image generation the discriminator D is almost never fooled.': 1.0986123085021973, 'I.e. contemporary GANs never converge to actually fooling the discriminator, even if they produce samples that sometimes fool humans.': 1.0986123085021973, 'What if I created an additional steganalyzer S**(x) = S*(x) * D(x)?': 1.0986123085021973, 'This I think would be extremely difficult to fool reliably because it requires realistic image generation.': 1.0986123085021973, 'After reading the paper several times, it is still a bit unclear to me how or why precisely one would use a trained SGAN.': 1.0986123085021973, 'I think the paper could be greatly improved by detailing, step by step, the workflow of how a hypothetical user would use a trained SGAN.': 1.0986123085021973, 'This description should be aimed at a reader who knows nothing or very little about steganography (e.g. most of ICLR attendees).': 1.0986120700836182, 'I reviewed the manuscript as of December 6th.': 1.0986123085021973, 'Summary:': 1.0986123085021973, 'The authors build upon generative adversarial networks for the purpose of steganalysis': 1.0986123085021973, 'i.e. detecting hidden messages in a payload.': 1.0986123085021973, ""The authors describe a new model architecture in which a new element, a 'steganalyser' is added a training objective to the GAN model."": 1.0986123085021973, 'Major Comments:': 1.0986123085021973, 'The authors introduce an interesting new direction for applying generative networks.': 1.0986123085021973, 'That said, I think the premise of the paper could stand some additional exposition.': 1.0986123085021973, 'How exactly would a SGAN method be employed?': 1.0986123085021973, 'This is not clear from the paper.': 1.0986123085021973, 'Why does the model require a generative model?': 1.0986123085021973, 'Steganalysis by itself seems like a classification problem (i.e. a binary decision if there a hidden message?)': 1.0986123085021973, 'Would you envision that a user has a message to send and does not care about the image (container) that it is being sent with?': 1.0986123085021973, 'Or does the user have an image and the network generates a synthetic version of the image as a container and then hide the message in the container?': 1.0986123085021973, 'Or is the SGAN somehow trained as a method for detecting hidden codes performed by any algorithm in an image?': 1.0986123085021973, 'Explicitly describing the use-case would help with interpreting the results in the paper.': 1.0986123085021973, 'Additionally, the experiments and analysis in this paper is quite light as the authors only report a few steganalysis performance numbers in the tables (Table 1,2,3).': 1.0986123085021973, 'A more extensive analysis seems warranted to explore the parameter space and provide a quantitative comparison with other methods discussed (e.g. HUGO, WOW, LSB, etc.)': 1.0986123085021973, 'When is it appropriate to use this method over the others?': 1.0986123085021973, 'Why does the seed effect the quality of results?': 1.0986123085021973, 'Does a fixed seed correspond realistic scenario for employing this method?': 1.0986123085021973, 'Minor comments:': 1.0986123085021973, 'Is Figure 1 necessary?': 1.0986123085021973, 'Why does the seed value effect the quality of the predictive performance of the model?': 1.0986123085021973}"
145,https://openreview.net/forum?id=H1kjdOYlx,"{'The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL, based on actor-critic methods.': 1.0986123085021973, 'The approach is illustrated in two tasks: gridworld with objects and a simplified Minecraft problem).': 1.0986117124557495, 'The idea of providing symbolic descriptions of tasks and learning corresponding ""implementations"" is potentially interesting and the empirical results are promising.': 1.0986123085021973, 'However, there are two main drawbacks of the current incarnation of this work.': 1.0983539819717407, 'First, the ideas presented in the paper have all been explored in other work (symbolic specifications, actor-critic, shared representations).': 1.0986123085021973, 'While related work is discussed, it is not really clear what is new here, and what is the main contribution of this work besides providing a new implementation of existing ideas in the context of deep learning.': 1.0986113548278809, 'The main contribution if the work needs to be clearly spelled out.': 1.0986101627349854, 'Secondly, the approach presented relies crucially on curriculum learning (this is quite clear from the experiments).': 1.0986123085021973, 'While the authors argue that specifying tasks in simplified language is easy, designing a curriculum may in fact be pretty complicated, depending on the task at hand.': 1.0986123085021973, 'The examples provided are fairly small, and there is no hint of how curriculum can be designed for larger problems.': 1.0986114740371704, 'Because the approach is sensitive to the curriculum, this limits the potential utility of the work.': 1.0986071825027466, 'It is also unclear if there is a way to provide supervision automatically, instead of doing it based on prior domain knowledge.': 1.0986123085021973, 'More minor comments:': 1.0054244995117188, 'The experiments are not described in enough detail in the paper.': 0.7537665367126465, ""It's great to provide github code, but one needs to explain in the paper why certain choices were made in the task setup (were these optimized?"": 1.0986121892929077, ""What's this the first thing that worked?)"": 1.0092673301696777, 'Even with the code, the experiments as described are not reproducible': 1.0986123085021973, 'The description of the approach is pretty tangled with the specific algorithmic choices.': 1.098010540008545, 'Can the authors step back and think more generally of how this approach can be formalized?': 1.0986123085021973, 'I think this would help relate it to the prior work more clearly as well.': 1.0986123085021973, 'This paper studies the problem of abstract hierarchical multiagent RL with policy sketches, high level descriptions of abstract actions.': 1.0986123085021973, 'The work is related to much previous work in hierarchical RL, and adds some new elements by using neural implementations of prior work on hierarchical learning and skill representations.': 1.098522663116455, 'Sketches are sequences of high level symbolic labels drawn from some fixed vocabulary, which initially are devoid of any meaning.': 1.0981817245483398, 'Eventually the sketches get mapped into real policies and enable policy transfer and temporal abstraction.': 1.0986123085021973, 'Learning occurs through a variant of the standard actor critic architecture.': 1.0986123085021973, 'Experiments are provided through a standard game like domain (maze, minecraft etc.).': 1.0986123085021973, 'The paper as written suffers from two problems.': 1.0754635334014893, 'One, the idea of policy sketches is nice, but not sufficiently fleshed out to have any real impact.': 1.0986120700836182, 'It would have been useful to see this spelled out in the context of abstract SMDP models to see what they bring to the table.': 1.0985063314437866, 'What one gets here is some specialized invocation of this idea in the context of the specific approach proposed here.': 1.0986121892929077, 'Second, the experiments are not thorough enough in terms of comparing with all the related work.': 1.0982304811477661, 'For example, Ghavamzadeh et al. explored the use of MAXQ like abstractions in the context of mulitagent RL.': 1.0986123085021973, 'It would be great to get a more detailed comparison to MAXQ based multiagent RL approaches, where the value function is explicitly decomposed.': 1.0986123085021973, 'The paper proposes a new RL architecture that aims at learning policies from sketches': 1.0986123085021973, 'i.e sequence of high-level operations to execute for solving a particular task.': 1.0943679809570312, 'The model relies on a hierarchical structure where the sub-policy is chosen depending on the current operation to execute in the sketch .': 1.0986120700836182, 'The learning algorithm is based on an extension of the actor-critic model for that particular case, and also involves curriculum learning techniques when the task to solve is hard.': 1.0986123085021973, 'Experimental results are provided on different learning problems and compared to baseline methods.': 1.0986123085021973, 'The paper is well-written and very easy to follow.': 1.0986123085021973, 'I am not really convinced by the impact of such a paper since the problem solved here can be seen as an option-learning problem with a richer supervision (i.e the sequence of option is given).': 1.0986123085021973, 'It thus corresponds to an easier problem with a limited impact.': 1.0985215902328491, 'Moreover, I do not really understand to which concrete application this setting corresponds.': 1.0986123085021973, 'For example, learning from natural langage instructions is clearly more relevant.': 1.0986123085021973, 'So since the model proposed in this article is not a major contribution and shares many common ideas with existing hierarchical reinforcement learning methods,  the paper lacks a strong motivation and/or concrete application.': 1.0986123085021973, 'So, the paper only has a marginal interest for the RL community': 1.0986101627349854, '@pros:': 1.0986123085021973, '* Original problem with well design experiments': 1.0986123085021973, '* Simple adaptation of the actor-critic method to the problem of learning sub policies': 1.0984277725219727, '@cons:': 1.0319602489471436, '* Very simple task that can be seen as a simplification of more complex problems like options discovery, hierarchical RL or learning from instructions': 1.0973631143569946, ""* No strong underlying applications that could help to 'reinforce' the interest of the approach"": 1.0983539819717407}"
146,https://openreview.net/forum?id=H1oRQDqlg,"{'This paper proposes an amortized version of the Stein variational gradient descent (SVGD) method in which ""a neural network is trained to mimic the SVGD dynamics"".': 1.0986123085021973, 'It applies the method to generative adversarial training to yield a training procedure where the discriminator is interpreted as an energy-based probabilistic model.': 1.0986121892929077, 'One criticism I have of the presentation is that a lot of time and energy is spent setting the table for a method which is claimed to be widely applicable, and the scope of the empirical evaluation is narrowed down to a single specific setting.': 1.0986123085021973, 'In my view, either the paper falls short of its goal of showing how widely applicable the proposed method is, or it spends too much time setting the table for SteinGAN and not enough time evaluating it.': 1.0986123085021973, 'The consequence of this is that the empirical results are insufficient in justifying the approach proposed by the paper.': 1.0986071825027466, 'As another reviewer pointed out, DCGAN is becoming outdated as a benchmark for comparison.': 1.0986123085021973, ""Qualitatively, SteinGAN samples don't look significantly better than DCGAN samples, except for the CelebA dataset."": 1.0985537767410278, ""In that particular case, the DCGAN samples don't appear to be the ones presented in the original paper; where do they come from?"": 1.0986063480377197, 'Quantitatively, DCGAN beats SteinGAN by a small margin for the ImageNet Inception Score and SteinGAN beats DCGAN by an even smaller margin for the CIFAR10 Inception Score.': 1.020548939704895, 'Also, in my opinion, the ""testing accuracy"" score is not a convincing evaluation metric: while it is true that it measures the amount of information captured in the simulated image sets, it is only sensitive to information useful for the discrimination task, not for the more general modeling task.': 1.0986123085021973, 'For instance, this score is likely completely blind to information present in the background of the image.': 1.0194623470306396, ""Because of the reasons outlined above, I don't think the paper is ready for publication at ICLR."": 1.0981707572937012, 'The authors propose amortized SVGD, an amortized form of prior work on SVGD, which is a particle variational method that maximally decreases the KL divergence at each update.': 1.0985392332077026, '""amortized SVGD"" is done by training a neural network to learn this dynamic.': 1.0985645055770874, 'They then apply this idea to train energy-based models, which admit a tractable unnormalized density.': 1.0986076593399048, 'In SVGD, the main difference from just MAP is the addition of a ""repulsive force"" that prevents degeneracy by encouraging probability mass to be spread to locations outside the mode.': 1.0986123085021973, 'How this is able to still act as a strong enough entropy-like term in high dimensions is curious.': 1.0986123085021973, 'From my understanding of their previous work, this was not a problem as the only experiments were on toy and UCI data sets.': 1.0986123085021973, 'In the experimental results here, they apply the kernel on the hidden representation of an autoencoder, which seems key, similar to Li et al. (2015) where their kernel approach for MMD would not work as well otherwise.': 1.0986123085021973, 'However, unlike Li et al. (2015)': 1.0986123085021973, 'the autoencoder is part of the model itself and not fixed.': 1.0986123085021973, ""This breaks much of the authors' proposed motivation and criticisms of prior work, if they must autoencode onto some low-dimensional space (putting most effort then on the autoencoder, which changes per iteration) before then applying their method."": 1.0986123085021973, 'Unlike previous literature which uses inference networks, their amortized SVGD approach seems in fact slower than the non-amortized approach.': 1.0986119508743286, 'This is because they must make the actual update on xi before then regressing to perform the update on eta (in previous approaches, this would be like having to perform local inferences before then updating inference network parameters, or at least partially performing the local inference).': 1.0986123085021973, 'This seems quite costly during training.': 1.0986123085021973, 'I recommend the paper be rejected, and that the authors provide more comprehensive experimental results, expecially around the influence of the autoencoder, the incremental updates versus full updates, and the training time of amortized vs non-amortized approaches.': 1.0986123085021973, 'The current results are promising but unclear why given the many knobs that the authors are playing with.': 1.0986123085021973, 'References': 1.0986123085021973, 'Li, Y., Swersky, K., & Zemel, R. (2015).': 1.0986123085021973, 'Generative Moment Matching Networks.': 1.0986123085021973, 'Presented at the International Conference on Machine Learning.': 1.0986123085021973, 'This paper considers the energy-based model interpretation of GAN, where the discriminator is an unnormalized model for the likelihood of a generative model p(x|theta) and the generator is a directed model that approximates this distribution.': 1.0986123085021973, 'The generator is used to draw approximate negative phase samples that are used in stochastic maximum likelihood / contrastive divergence learning of the EBM / discriminator.': 1.0986123085021973, 'The main idea in the paper is to fit the generator by following the Stein variational gradient.': 1.0986123085021973, 'In practice this gradient consists of the usual gradient provided by the discriminator with an added term that provides a repulsive force between the sampled data points to increase sample diversity.': 1.0986123085021973, 'The idea of using a kernel to push apart the sampled points is interesting, and will work in low dimensions, but it is hard to see how it can work in full scale images.': 1.0986123085021973, 'For high dimensional samples x, the proposed kernel is unlikely to provide a useful distance measure between points.': 1.0986123085021973, 'There are no convincing experiments in the paper that show otherwise.': 1.076944351196289, 'Specifically:': 1.0986123085021973, 'There is no experiment that compares between standard GAN and GAN + repulsion, using the same architecture.': 1.0986123085021973, '(please address this in the rebuttal)': 1.0986055135726929, 'If the Stein variational idea is taken literally, the right thing to do would be to fully optimize the generator at every step, and then taking a single optimization step on the discriminator.': 1.0986123085021973, 'Instead, each is updated in turn, and the learning rates of both steps are adjusted to keep the two ""in line"".': 1.0986123085021973, 'The kernel used to fit the generator is defined in the auto-encoder space of the discriminator, and thus depends on the discriminator parameters.': 1.0986123085021973, 'The objective that is used to fit the generator thus changes at every step, and the procedure can no longer be interpreted as stochastic gradient descent with respect to any single well defined objective.': 1.0986120700836182, 'The authors obtain good results: The generated images clearly look better than those generated by DCGAN.': 1.0986121892929077, 'However, their approach has a number of changes compared to DCGAN, so it is not clear where the improvement comes from.': 1.0986114740371704, 'In addition, by now the DCGAN is no longer a very strong baseline, as various other techniques have been proposed.': 1.0986123085021973, 'Note: The use of phi for both the ""particle gradient direction"" and energy function is confusing': 1.0986123085021973}"
147,https://openreview.net/forum?id=H1oyRlYgg,"{'Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well': 1.0986123085021973, 'The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima, 2. flatter minima have better generalization ability.': 1.0986123085021973, 'Pros and Cons:': 1.0986123085021973, 'Although there is little novelty in the paper, I think the work is of great value in shedding light into some interesting questions around generalization of deep networks.': 1.098558783531189, 'Significance:': 1.0986123085021973, 'I think such results may have impact on both theory and practice, respectively by suggesting what assumptions are legitimate for real scenarios for building new theories, or be used heuristically to develop new algorithms with generalization by smart manipulation of mini-batch sizes.': 1.0986123085021973, 'Comments:': 1.0986123085021973, 'Earlier I had some concern about the correctness of a claim made by the authors, which is resolved now.': 1.098605990409851, 'They had claimed their proposed sharpness criterion is scale invariance.': 1.0986123085021973, 'They took care of it by removing this claim in the revised version.': 1.0986123085021973, 'I think that the paper is quite interesting and useful.': 1.0986123085021973, 'It might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime one can get advantages of the SB regime.': 1.0986123085021973}"
148,https://openreview.net/forum?id=H1wgawqxl,"{'This paper provides a principled framework for nonparametrically learning activation': 1.0986123085021973, 'functions in deep neural networks.': 1.0986123085021973, ""A theoretical justification for authors' choice of"": 1.0986123085021973, 'nonparametric activation functions is given.': 1.0986123085021973, 'Theoretical results are satisfactory but I particularly like the experimental setup': 1.0986123085021973, 'where their methods are tested on image': 1.0986123085021973, 'recognition datasets and achieve up to a 15% relative increase in test performance': 1.0986123085021973, 'compared to the baseline.': 1.0986123085021973, 'Well-written paper and novel theoretical techniques.': 1.0986123085021973, 'The intuition behind the proof of Theorem 4.7 can be given in a little bit more clear way in the main body of the paper, but the': 1.0492033958435059, 'Appendix clarifies everything.': 1.0986123085021973, 'Summary:': 1.0986123085021973, 'The paper introduces a parametric class for non linearities used in neural networks.': 1.0986123085021973, 'The paper suggests two stage optimization to learn the weights of the network, and the non linearity weights.': 1.0986123085021973, 'significance:': 1.0986123085021973, 'The paper introduces a nice idea, and present nice experimental results.': 1.0986123085021973, 'however  I find the theoretical analysis not very informative, and  distractive from the main central idea of the paper.': 1.0986123085021973, 'A more thorough experimentation with the idea using different basis and comparing it to wider networks (equivalent to the number of cosine basis used in the leaned one ) would help more supporting results in the paper.': 1.0986123085021973, 'Comments:': 1.0986123085021973, 'Are the weights of the non -linearity learned shared across all units in all layers ?': 1.0986123085021973, 'or each unit has it is own non linearity?': 1.0986123085021973, 'If all weights are tied across units and layers.': 1.0986123085021973, 'One question that would be interesting to study , if there is an optimal non linearity.': 1.097235083580017, 'How different is the non linearity learned if the hidden units are normalized or un-normalized.': 1.0986123085021973, ""In other words how does the non linearity change if you use or don't use batch normalization?"": 1.0986123085021973, 'Does normalization affect  the conclusion that polynomial basis fail?': 1.0986123085021973, 'This paper describes an approach to learning the non-linear activation function in deep neural nets.': 1.098138689994812, 'This is achieved by representing the activation function in a basis of non-linear functions and learning the coefficients.': 1.0985654592514038, 'Authors use Fourier basis in the paper.': 0.8734527826309204, 'A theoretical analysis of the proposed approach is also presented, using algorithmic stability arguments, to demonstrate good generalization behavior (vanishing generalization error with large data sets) of networks with learnt non-linearities.': 1.0986123085021973, 'The main question I have about this paper is that writing a non-linear activation function as a linear or affine combination of other non-linear basis functions is equivalent to making a larger network whose nodes have the basis functions as non-linearities and whose weights have certain constraints on them.': 1.0986123085021973, 'Thus, the value of the proposed approach of learning non-linearities over optimizing network capacity for a given task (with fixed non-linearities) is not clear to me.': 1.0986123085021973, 'Or could it be argued that the constrained implied by learnt non-linearity approach are somehow good thing to do?': 0.9506222605705261, 'Another question - In the two stage training process for CNNs, when ReLU activation is replaced by NPFC(L,T), is the NPFC(L,T) activation initialized to approximate ReLU, or is it initialized using random coefficients?': 0.5300045609474182, 'Few minor corrections/questions:': 1.0986123085021973, 'Pg 2. “': 1.0986123085021973, '… the interval [-L+T, L+T] …” should be “ … the interval': 1.0986123085021973, '[-L+T, L-T] … “ ?': 1.0986123085021973, 'Pg 2., Equation for f(x), should it be “ (-L+T)': 1.0986123085021973, 'i \\pi x / L “ in both sin and cos terms, or without “ x “ ?': 1.0986123085021973, 'Theorem 4.2 “ … some algorithm \\eps-uniformly stable …” remove the word “algorithm”': 1.0986123085021973, 'Theorem 4.5.': 1.0986123085021973, 'SGM undefined': 1.0986123085021973}"
149,https://openreview.net/forum?id=H1zJ-v5xl,"{'This paper introduces the Quasi-Recurrent Neural Network (QRNN) that dramatically limits the computational burden of the temporal transitions in': 1.3862919807434082, 'sequence data.': 1.3862943649291992, 'Briefly (and slightly inaccurately) model starts with the LSTM structure but removes all but the diagonal elements to the transition': 0.8875766396522522, 'matrices.': 1.38545823097229, 'It also generalizes the connections from lower layers to upper layers to general convolutions in time (the standard LSTM can be though of as a convolution with a receptive field of 1 time-step).': 1.3862767219543457, 'As discussed by the authors, the model is related to a number of other recent modifications of RNNs, in particular ByteNet and strongly-typed RNNs (T-RNN).': 1.3862581253051758, 'In light of these existing models, the novelty of the QRNN is somewhat diminished, however in my opinion their is still sufficient novelty to justify publication.': 1.386259913444519, 'The authors present a reasonably solid set of empirical results that support the claims of the paper.': 1.3862929344177246, 'It does indeed seem that this particular modification of the LSTM warrants attention from others.': 1.3862375020980835, 'While I feel that the contribution is somewhat incremental, I recommend acceptance.': 1.3862943649291992, 'This paper introduces a novel RNN architecture named QRNN.': 1.3862930536270142, 'QNNs are similar to gated RNN , however their gate and state update  functions depend only on the recent input values, it does not depend on the previous hidden state.': 1.385847806930542, 'The gate and state update functions are computed through a temporal convolution applied on the input.': 1.3603862524032593, 'Consequently, QRNN allows for more parallel computation since they have less  operations in their hidden-to-hidden transition depending on the previous hidden state compared to a GRU or LSTM.': 1.386284351348877, 'However, they possibly loose in expressiveness relatively to those models.': 1.0143835544586182, 'For instance, it is not clear how such a model deals with long-term dependencies without having to stack up several QRNN layers.': 1.3862943649291992, 'Various extensions of QRNN, leveraging Zoneout, Densely-connected or seq2seq with attention, are also proposed.': 1.3862942457199097, 'Authors evaluate their approach on various tasks and datasets (sentiment classification, world-level language modelling and character level machine translation).': 1.3862587213516235, 'Overall the paper is an enjoyable read and the proposed approach is interesting,': 1.3631095886230469, 'Pros:': 1.3862943649291992, 'Address an important problem': 1.3862943649291992, 'Nice empirical evaluation showing the benefit of their approach': 1.379240870475769, 'Demonstrate up to 16x speed-up relatively to a LSTM': 1.3758163452148438, 'Cons:': 1.3862937688827515, 'Somewhat incremental novelty compared to (Balduzizi et al., 2016)': 1.3853271007537842, 'Few specific questions:': 1.3862935304641724, 'Is densely layer necessary to obtain good result on the IMDB task.': 1.3537901639938354, 'How does a simple 2-layer QRNN compare with 2-layer LSTM?': 1.3819671869277954, 'How does the i-fo-ifo pooling perform comparatively?': 1.343165397644043, 'How does QRNN deal with long-term time depency?': 0.8907226920127869, 'Did you try on it on simple toy task such as the copy or the adding task?': 1.3647303581237793, 'The authors describe the use of convolutional layers with intermediate pooling layers to more efficiently model long-range dependencies in sequential data compared with recurrent architectures.': 0.8377375602722168, 'Whereas the use of convolutional layers is related to the PixelCNN architecture (Oord et al.), the main novelty is to combine them with gated pooling layers to integrate information from previous time steps.': 1.3862943649291992, 'Additionally, the authors describe extensions based on zone-out regularization, densely connected layers, and an efficient attention mechanism for encoder-decoder models.': 1.3862943649291992, 'The authors report a striking speed-up over RNNs by up to a factor of 16, while achieving similar or even higher performances.': 1.3862943649291992, 'Major comment': 1.3862943649291992, '=============': 1.3862943649291992, 'QRNNs are closely related to PixelCNNs, which leverage masked dilated convolutional layers to speed-up computations.': 1.3862943649291992, 'However, the authors cite ByteNet, which builds upon PixelCNN, only at the end of their manuscript and do not include it in the evaluation.': 1.3862943649291992, 'The authors should cite PixelCNN already when introducing QRNN in the methods sections, and include it in the evaluation.': 1.3862943649291992, 'At the very least, QRNN should be compared with ByteNet for language translation.': 1.3862943649291992, 'How well does a fully convolutional model without intermediate pooling layers perform, i.e. what is the effect to the introduced pooling layers?': 1.3862943649291992, 'Are their performance difference between f, fo, and ifo pooling?': 1.3862943649291992, 'Did the authors investigate dilated convolutional layers?': 1.3862943649291992, 'Minor comments': 1.3848116397857666, '==============': 1.3858604431152344, '1. How does a model without dense connections perform, i.e. what is the effect of dense connections? To illustrate dense connections, the authors might draw them in figure 1 and refer to it in section 2.1.': 0.7069316506385803, '2. The run-time results shown in figure 4 are very helpful, but as far as I understood, the breakdown shown on the left side was measured for language modeling (referred in 3.2), whereas the dependency on batch- and sequence size shown on the right side for sentiment classification (referred in 3.1). I suggest to consistently show the results for either sentiment classification or language modeling, or both. At the very least, the figure caption should describe the task explicitly. Labeling the left and right figure by a) and b) would further improve readability.': 0.43112537264823914, '3. Section 3.1 describes a high speed-up for long sequences and small batch sizes. I suggest motivating why this is the case. While computations can be parallelized along the sequence length, it is less obvious why smaller batch sizes speed-up computations.': 0.39346250891685486, '4. The proposed encoder-decoder attention is different from traditional attention in that attention vectors are not computed and used as input to the decoder sequentially, but on top of decoder output states in parallel. This should be described and motivated in the text.': 0.5159592032432556, 'Sentiment classification': 1.3862943649291992, '5. What was the size of the hold-out development set and how was it created? The text describes that data were split equally into training and test set, without describing the hold-out set.': 1.3862943649291992, '6. What was the convolutional filter size?': 1.3862943649291992, '7. What is the speed-up for the best hyper-parameters (batch size 24, sequence length 231)?': 1.3862943649291992, '8. Figure 3 would be easier to interpret by actually showing the text on the y-axis. For the sake of space, one might use a smaller text passage, plot it along the x-axis, and show the activations of fewer neurons along the y-axis. Showing more examples in the appendix would make the authors’ claim that neurons are interpretable even more convincing.': 1.3862943649291992, 'Language modeling': 1.3862943649291992, '9. What was the size of the training, test, and validation set?': 1.3862943649291992, '10. What was the convolutional filter size, denoted as ‘k’?': 1.358393907546997, '11. Is it correct that a very high learning rate of 1 was used for six epochs at the beginning?': 1.3862909078598022, '12. The authors should show learning curves for a models with and without zone-out.': 1.3862941265106201, 'Translation': 1.3862943649291992, '13. What was the size of the training, test, and validation set?': 1.3862943649291992, '14. How does translation performance depend on k?': 1.3862943649291992, 'This paper points out that you can take an LSTM and make the gates only a function of the last few inputs  - h_t = f(x_t, x_{t-1}, ...x_{t-T}) - instead of the standard - h_t = f(x_t, h_{t-1})': 1.361744999885559, '-, and that if you do so the networks can run faster and work better.': 1.3862943649291992, ""You're moving compute from a serial stream to a parallel stream and also making the serial stream more parallel."": 1.3862943649291992, 'Unfortunately, this simple, effective and interesting concept is somewhat obscured by confusing language.': 1.3862943649291992, 'I would encourage the authors to improve the explanation of the model.': 1.3862943649291992, 'Another improvement might be to explicitly go over some of the big Oh calculations, or give an example of exactly where the speed improvements are coming from.': 1.3862943649291992, 'Otherwise the experiments seem adequate and I enjoyed this paper.': 1.3862943649291992, 'This could be a high value contribution and become a standard neural network component if it can be replicated and if it turns out to work reliably in multiple settings.': 1.3862943649291992}"
150,https://openreview.net/forum?id=HJ0NvFzxl,"{'The main contribution of this paper seems to be an introduction of a set of differential graph transformations which will allow you to learn graph->graph classification tasks using gradient descent.': 1.0986123085021973, 'This maps naturally to a task of learning a cellular automaton represented as sequence of graphs.': 1.0986123085021973, 'In that task, the graph of nodes grows at each iteration, with nodes pointing to neighbors and special nodes 0/1 representing the values.': 1.0986123085021973, 'Proposed architecture allows one to learn this sequence of graphs, although in the experiment, this task (Rule 30) was far from solved.': 1.0986123085021973, 'This idea is combined with ideas from previous papers (GGS-NN) to allow the model to produce textual output rather than graph output, and use graphs as intermediate representation, which allows it to beat state of the art on BaBi tasks.': 1.0986123085021973, 'This paper proposes learning on the fly to represent a dialog as a graph (which acts as the memory), and is first demonstrated on the bAbI tasks.': 1.0986121892929077, 'Graph learning is part of the inference process, though there is long term representation learning to learn graph transformation parameters and the encoding of sentences as input to the graph.': 1.0985926389694214, 'This seems to be the first implementation of a differentiable memory as graph: it is much more complex than previous approaches like memory networks without significant gain in performance in bAbI tasks, but it is still very preliminary work, and the representation of memory as a graph seems much more powerful than a stack.': 1.0986123085021973, 'Clarity is a major issue, but from an initial version that was constructive and better read by a computer than a human, the author proposed a hugely improved later version.': 1.0986123085021973, 'This original, technically accurate (within what I understood) and thought provoking paper is worth publishing.': 1.0984324216842651, 'The preliminary results do not tell us yet if the highly complex graph-based differentiable memory has more learning or generalization capacity than other approaches.': 1.0985610485076904, 'The performance on the bAbI task is comparable to the best memory networks, but still worse than more traditional rule induction (see http://www.public.asu.edu/~cbaral/papers/aaai2016-sub.pdf).': 1.0986123085021973, 'This is still clearly promising.': 0.7498472929000854, 'The sequence of transformation in algorithm 1 looks sensible, though the authors do not discuss any other operation ordering.': 1.0630029439926147, 'In particular, it is not clear to me that you need the node state update step': 1.0985506772994995, 'T_h if you have the direct reference update step T_h,direct.': 1.0981882810592651, 'It is striking that the only trick that is essential for proper performance is the ‘direct reference’ , which actually has nothing to do with the graph building process, but is rather an attention mechanism for the graph input: attention is focused on words that are relevant to the node type rather than the whole sentence.': 1.0985902547836304, 'So the question “how useful are all these graph operations” remain.': 1.070845365524292, 'A much simpler version of a similar trick may have been proposed in the context of memory networks, also for ICLR\'17 (see match type in ""LEARNING END-TO-END GOAL-ORIENTED DIALOG"" by Bordes et al)': 0.9254578351974487, 'The authors also mention the time and size needed to train the model: is the issue arising for learning, inference or both?': 0.2227718085050583, 'A description of the actual implementation would help  (no pointer to open source code is provide).': 0.7232269048690796, 'The author mentions Theano in one of my questions: how are the transformations compiled in advance as units?': 1.098601222038269, 'How is the gradient back-propagated through the graph is this one is only described at runtime?': 0.37140876054763794, 'Typo: in the appendices B.2 and B.2.1, the right side of the equation that applies the update gate has h’_nu while it should be h_nu.': 0.9357211589813232, 'In the references, the author could mention the pioneering work  of Lee Giles on representing graphs with  RNNs.': 0.5004489421844482, 'Revision: I have improved my rating for the following reasons:': 0.6615277528762817, 'Pointers to an highly readable and well structured Theano source is provided.': 1.0966628789901733, 'The delta improvement of the paper has been impressive over the review process, and I am confident this will be an impactful paper.': 1.092097282409668, 'Much simpler alternatives approaches such as Memory Networks seem to be plateauing for problems such as dialog modeling, we need alternatives.': 1.0986104011535645, 'The architecture is this work is still too complex, but this is often as we start with DNNs, and then find simplifications that actually improve performance': 1.0919702053070068, 'The paper proposes an extension of the Gated Graph Sequence Neural Network by including in this model the ability to produce complex graph transformations.': 1.0958571434020996, 'The underlying idea is to propose a method that will be able build/modify a graph-structure as an internal representation for solving a problem, and particularly for solving question-answering problems in this paper.': 1.0986123085021973, 'The author proposes 5 different possible differentiable transformations that will be learned on a training set, typically in a supervised fashion where the state of the graph is given at each timestep.': 1.0986123085021973, 'A particular occurence of the model is presented that takes a sequence as an input a iteratively update an internal graph state to a final prediction, and which can be applied for solving QA tasks (e.g BaBi) with interesting results.': 1.0986123085021973, 'The approach  in this paper is really interesting since the proposed model is able to maintain a representation of its current state as a complex graph, but still keeping the property of being differentiable and thus easily learnable through gradient-descent techniques.': 1.0986123085021973, 'It can be seen as a succesfull attempt to mix continuous and symbolic representations.': 1.0974854230880737, ""It moreover seems more general that the recent attempts made to add some 'symbolic' stuffs in differentiable models (Memory networks, NTM, etc...)"": 1.0986123085021973, 'since the shape of the state is not fixed here and can evolve.': 1.0409417152404785, 'My main concerns is about the way the model is trained i.e by providing the state of the graph at each timestep which can be done for particular tasks (e.g Babi) only, and cannot be the solution for more complex problems.': 1.0986117124557495, 'My other concern is about the whole content of the paper that would perhaps best fit a journal format and not a conference format, making the article still difficult to read due to its density.': 1.0986123085021973}"
151,https://openreview.net/forum?id=HJ0UKP9ge,"{'This is a solid paper with good results.': 1.0986123085021973, ""However, there aren't many very interesting takeaways (most of the architecture seems a concatenation of standard elements to do well in the leaderboard) and some issues in the writing."": 1.0986123085021973, 'The second paragraph of the introduction is very confusing.': 1.0986123085021973, ""It's clear the authors got really deep into their world and made no attempts to actually clearly explain their model, not even to an expert in the field, let a lone somebody who isn't familiar with similar approaches."": 1.0986123085021973, 'The authors keep referring to ""previously popular attention paradigms"" without any citation and then, I believe, incorrectly describe whatever those are supposed to be by writing that these unknown but popular approaches ""summarize each modality into a single vector.""': 1.0986123085021973, ""That's one of the most incorrect descriptions I've yet seen for attention mechanisms."": 1.0986123085021973, ""First, I don't know what model works over several modalities in a single attention pass."": 1.0986123085021973, 'Maybe the authors don\'t know what a ""modality"" is?': 1.0986123085021973, 'More importantly, the whole point of most attention mechanisms is that one does not simply summarize the whole input but instead can access all elements of it.': 1.0986123085021973, ""So, this paper's supposedly new way of using attention is pretty much exactly the standard way."": 1.0986123085021973, 'Both modeling and modelling spellings are in the text.': 1.0986123085021973, 'I understand the need to sometimes invent new terminology to describe a model but in this paragraph, the authors 3 times talk about a ""modeling layer (RNN)""...': 1.0986123085021973, 'It\'s just an RNN, you don\'t need to give an RNN another name, especially one that\'s as nondescript as ""modeling layer"" all layers are part of a model?': 1.0986114740371704, 'Typo: ""let\'s the modeling (RNN) layer to learn""': 1.0985713005065918, 'This paragraph is supposed to give an overview of the model but just confuses readers.': 1.0986123085021973, 'I would delete it.': 1.0986123085021973, '""Phrase embedding layer""': 1.0986123085021973, 'terrible word choice as you are not embedding phrases here.': 1.0986123085021973, ""It's a standard bidirectional LSTM over words, not phrases."": 1.0986123085021973, 'In all subsequent parts of the paper you just give examples of words embedded in context.': 1.0986123085021973, 'No phrases.': 1.0986123085021973, 'Please change this to ""contextual word embedding layer"" or something less incorrect.': 1.0986123085021973, 'Your phrase layer embeddings only show single words, as expected in Table 2.': 1.0986123085021973, 'Section 2: point 4.': 1.0986123085021973, 'Second sentence needs citations for ""popular""': 1.0986123085021973, 'Typo: ""from both *of* the context and query word""': 1.0986123085021973, 'Typo: ""aveaged""': 1.0986123085021973, ""It seems like your output layer changes quite substantially so your claim in the abstract/intro of using the same model isn't quite accurate."": 1.0986077785491943, ""I'd say you're changing one module or part of your model."": 1.0986123085021973, 'Section 4: attention isn\'t countable (no ""a"" in front of ""huge attention"").': 0.9660701155662537, ""Also, academic writing usually doesn't include such adjectives in the first place."": 1.0986123085021973, 'Paper Summary:': 1.0986123085021973, 'The paper presents a novel approach for machine comprehension.': 1.0774133205413818, 'The proposed model, Bi-Directional Attention Flow (BIDAF) network is a multi-stage hierarchical approach that represents the context and query at different levels of granularity and uses bi-directional attention mechanisms to predict an answer.': 1.0986121892929077, 'The proposed approach achieves state-of-the-art results on SQuAD dataset and CNN/DailyMail cloze test.': 1.0786267518997192, 'Paper Strengths:': 1.0986114740371704, 'The proposed model uses attention in both directions instead of uni-directional attention mechanisms used in previous approaches, prevents early summarization by passing all information to RNNs and uses memory-less attention mechanism, which is a novel combination for machine comprehension task.': 1.0986123085021973, 'The proposed model is modular, and can be easily changed for other related tasks, as shown through two types of experiments in the paper.': 1.0985987186431885, 'The paper presents thorough ablation study of the proposed model, clearly presenting the importance of all major components in the model.': 1.0984545946121216, 'The authors also added further detailed studies as requested by reviewer.': 0.6981063485145569, 'Relation of their model to Visual Question Answering approaches, and computing features from context and query at different levels of granularity to multiple layers in Convolutional Neural Networks, are interesting and help in better understanding of the model.': 1.0595736503601074, 'It would be very interesting to see how this approach would work for the task of Visual Question Answering.': 1.0903360843658447, 'Further visualizations and error analysis can help in identifying failure modes of the model and help in designing next generation of models.': 1.0986050367355347, 'The proposed model achieves state-of-the-art result on SQuAD dataset, and CNN/DailyMail cloze test.': 1.098346471786499, 'The paper is well written and the architecture is described in sufficient detail.': 1.0986123085021973, 'Paper Weaknesses / Future Thoughts:': 1.095900535583496, 'As with many deep learning approaches, the overall architecture seems quite complex, and the design choices seem to be driven by performance numbers.': 1.0784778594970703, 'As future work, authors might try to analyze qualitative advantages of different modules in the proposed model.': 1.0252622365951538, 'What type of questions are correctly answered after adding Context-to-Query attention?': 0.757897675037384, 'What about Query-to-Context attention?': 1.0985654592514038, 'Some qualitative examples of success and failure cases should be added to the paper.': 0.8925987482070923, 'Preliminary Evaluation:': 1.0986123085021973, 'Novel, state-of-the-art, and well-studied machine comprehension approach.': 1.0986123085021973, 'Paper is well written.': 1.0986123085021973, 'In my thoughts, a clear accept.': 1.0986123085021973, 'The paper presents an architecture for answering questions about text.': 1.0986123085021973, 'The paper proposes a novel architecture which jointly attends over the context and the query.': 1.0986123085021973, '1. The paper is clearly written and illustrated.': 1.0986123085021973, '2. The architecture is new and incorporates novel and interesting aspects:': 1.088519811630249, '2.1. The attention is not summarized immediately but the features are only weighted with the attention to not loose information.': 1.0969501733779907, '2.2. The approach estimates two directions of attention, by maximizing in two directions of the similarity matrix S – towards the context and towards the query.': 0.6945012807846069, '3. The paper extensively evaluates the approach on three datasets SQuAD, CNN and Daily Mail. In all cases showing state-of-the-art performance. It is worth noting that the SQuAD and the CNN/Daily Mail are slightly different tasks and it is positive that the model works well in both scenarios.': 1.0840270519256592, '3.1. It is worth noting that the paper even compares mainly favorably to concurrent work (including other ICLR 2017 submissions), recently published/listed on the evaluation server for SQuAD': 1.0927637815475464, '4. The paper also includes an ablation study and qualitative results.': 1.0441287755966187, '5. I think the paper provides a good discussion of related work and I like that it points out the relations to Visual question answering (VQA). It would be interesting to see how the architecture can be adapted and works on the VQA task.': 1.0986123085021973, '6. The authors revised the paper based on the comments from reviewers and others.': 1.0986123085021973, '7. It would be interesting to see more qualitative results, e.g. in an appendix.': 1.0986123085021973, '7.1. Fig. 3 seems to miss the predicted answer.': 1.0986123085021973, '7.2. It would also be interesting to compare the results of different approaches, maybe in a more compact format.': 1.0986123085021973, 'Given the new architecture with novel aspects and the strong experimental evaluation I recommend to accept the paper.': 1.0986123085021973}"
152,https://openreview.net/forum?id=HJ1JBJ5gl,"{'This paper investigates whether the variational inference interpretation of dropout, as introduced in [Gal & Ghahramani (2016), and Kingma et al (2015)], can lead to good estimates of mode uncertainty outside of the training distribution.': 1.0986123085021973, 'This is an area of research that indeed warrants more experimental investigation.': 1.0942683219909668, 'One very interesting finding is that MC integration leads to much calibration, thus probably much better out-of-sample prediction, than the more usual.': 1.0985679626464844, 'Critique:': 1.0986123085021973, 'As explained in Kingma et al (2015), when using continuous posterior distributions over the weights, the dropout rate can be optimized, leading to better regularization.': 1.0985163450241089, 'While the paper is cited in the introduction, this adaptive form of dropout is missing from experiments, without clarification.': 1.0166257619857788, 'Only the dropout rate p=0.5 was used across experiments, while the optimal rate is problem dependent, as found by earlier published work.': 1.0986123085021973, 'No new ideas are presented, and the analysis in the paper is quite limited.': 0.6200413703918457, 'As it stands, this would be more appropriate for a workshop.': 1.030713438987732, 'This paper presents an empirical evaluation of the effect of training with noise by dropout or dropconnect, on the predictive uncertainty of neural networks and convnets.': 0.46495428681373596, 'The conclusion seems to be that applying both training and inference with noise can help the network produce better uncertainty estimates in terms of better calibrated predictions.': 1.0777335166931152, 'Although the experiments were thorough, the issue with an empirical paper like this is that it is very difficult to ensure that the lessons learned will generalize across problems and domains.': 0.8213238716125488, 'This paper only investigated two simple image datasets and two neural network architectures on the task of classification.': 1.0975724458694458, 'There are several ways in which I think this paper could be made stronger.': 1.0922337770462036, 'First, the problem is not well motivated: why do we care about uncertainty (although I believe we do)?': 1.0953737497329712, 'A good application, or a motivation section would be beneficial here.': 1.0978424549102783, 'Next, what about investigating a different domain such as text or speech recognition?': 1.0979137420654297, 'Or other problems such as regression?': 1.0965955257415771, 'Do the results hold across different domains?': 1.0986117124557495, 'Finally, if we really care about uncertainty, there are a number of other techniques for inference in Bayesian neural networks such as stochastic variational inference using the reparameterization trick, or MCMC methods like stochastic gradient Langevin dynamics.': 1.0986108779907227, 'The advantage of dropout is that it’s simple and fast, but do we lose anything by doing this in terms of calibration?': 0.35962748527526855, 'Other than the empirical comparison, there is little novelty to this paper, and therefore I think the conclusions drawn need to be more general or the motivation more compelling.': 1.0680780410766602, 'Even addressing a subset of these suggestions would make the paper stronger in my opinion.': 0.967038631439209, 'In Figure 4 the calibration MSE does not look so robust when MC sampling is not used.': 0.5874083638191223, 'Do you have any hypothesis on why MC sampling is so important here?': 1.098555564880371, 'In Figure 5, the legend says “CNN” when it should just be “NN”.': 1.0986123085021973, 'Also the caption says convolutional, when it should say fully connected.': 1.0984623432159424, 'In Table 3, why are the results so poor for non-MC-based dropout?': 1.09861159324646, 'Although this agrees with Gal and Ghahramani, this seems to be in direct contradiction to Table 4 of the original JMLR dropout paper by Srivastava et al from 2014.': 1.0949218273162842, 'How is the noise in the test set applied for Figures 5-10?': 1.0986002683639526, 'Is it Gaussian noise applied to the pixels?': 1.0986123085021973, 'The authors study the calibration of neural networks using different variants of dropout and weight noise.': 1.0986120700836182, 'They find that sampling during training and testing improves calibration.': 0.9217557311058044, 'Most of the results are not novel and have been discussed previously by Yarin Gal and other authors.': 1.0958373546600342, 'What this paper adds is a more systematic evaluation of multiple different variants of dropout.': 1.093651533126831, 'A comparison against bootstrapped uncertainty estimates would be useful': 1.0582958459854126, 'Ian Osband had a paper at NIPS investigating exactly this kind of uncertainty representation in neural nets.': 1.036505103111267, 'He found that dropout represents the risk of the model, but not really the uncertainty.': 0.7332713603973389, 'This difference becomes apparent when e.g. the target of the model is bi-modal.': 1.097611427307129}"
153,https://openreview.net/forum?id=HJ1kmv9xx,"{'The paper presents an interesting framework for image generation, which stitches the foreground and background to form an image.': 1.0985996723175049, 'This is obviously a reasonable approach there is clearly a foreground object.': 1.0985064506530762, 'However, real world images are often quite complicated, which may contain multiple layers of composition, instead of a simple foreground-background layer.': 1.0986087322235107, 'How would the proposed method deal with such situations?': 1.0986123085021973, 'Overall, this is a reasonable work that approaches an important problem from a new angle.': 1.089929223060608, 'Yet, I think sizable efforts remain needed to make it a generic methodology.': 1.0986123085021973, 'The paper proposes a model for image generation where the back-ground is generated first and then the foreground is pasted in by generating first a foregound mask and corresponding appearance, curving the appearance image using the mask and transforming the mask using predicted affine transform to paste it on top of the image.': 1.0986121892929077, 'Using AMTurkers the authors verify their generated images are selected 68% of the time as being more naturally looking than corresponding images from a DC-GAN model that does not use a figure-ground aware image generator.': 1.098605990409851, 'The segmentations masks learn to depict objects in very constrained datasets (birds) only, thus the method appears limited for general shape datasets, as the authors also argue in the paper.': 1.0986123085021973, 'Yet, the architectural contributions have potential merit.': 1.0986120700836182, 'It would be nice to see if multiple layers of foreground (occluding foregrounds) are ever generated with this layered model or it is just figure-ground aware.': 1.0986123085021973, 'The authors propose a method that generates naturally looking images by first generating the background and then conditioned on the previous layer one or multiple foreground objects.': 1.0986106395721436, 'Additionally they add a image transformer layer that allows the model to more easily model different appearances.': 1.0943725109100342, 'I would like to see some discussion about the choice of foreground+mask rather than just predicting foreground directly.': 1.0983608961105347, 'For MNIST, for example the foreground seems completely irrelevant.': 0.600064218044281, 'For CUB and CIFAR of course the fg adds the texture and color while the masks ensures a crisp boundary.': 0.9197307825088501, 'Is the mask a binary mask or a alpha blending mask?': 1.0986123085021973, 'I find the fact that the model learns to decompose images this nicely and learns to produce crisp foreground masks w/o too much spurious elements (though there are some in CIFAR) pretty fascinating.': 1.098044991493225, 'The proposed evaluation metric makes sense and seems reasonable.': 1.0422048568725586, 'However, AFAICT, theoretically it would be possible to get a high score even though the GAN produces images not recognizable to humans, but only to the classifier network that produces P_g.': 1.0985862016677856, ""E.g. if the Generator encodes the class in some subtle way (though this shouldn't happen given the training with an adversarial network)."": 1.0986123085021973, 'Fig 3 shows indeed nicely that the decomposition is much nicer when spatial transformers are used.': 0.8597209453582764, 'However, it also seems to indicate that the foreground prediction and the foreground mask are largely redundant.': 0.9349286556243896, 'For the final results the ""niceness"" of the decomposition appears to be largely irrelevant.': 1.098339557647705, 'Furthermore, the transformation layer seems to have a small effect, judging from the transformed masked foreground objects.': 0.10009639710187912, 'They are mainly scaled down.': 1.0440782308578491, 'What is the 3rd & 6th column in Fig 9?': 1.0986120700836182, 'It is not clear if the final composed images are really as bad as ""advertised"".': 0.82353675365448, 'Regarding the eval experiment using AMT it is not clear why it is better to provide the users with L2 minimized NN matches rather than random pairs.': 1.0892819166183472, 'I assume that Tab 1 Adversarial Divergence for Real images was not actually evaluated?': 1.096582055091858, 'It would be interesting to see how close to 0 multiple differently initialized networks actually are.': 1.0760619640350342, 'Also please mention how the confidences/std where generated, i.e. different training sets, initialisations, eval sets, and how many runs.': 1.0983341932296753}"
154,https://openreview.net/forum?id=HJ5PIaseg,"{'This paper addresses the issue of how to evaluate automatic dialogue responses.': 1.0985524654388428, 'This is an important issue because current practice to automatically evaluate (e.g. BLEU, based on N-gram overlap, etc.) is NOT correlated well with the desired quality (i.e. human annotation).': 1.0986123085021973, 'The proposed approach is based on the use of an LSTM encoding of dialogue context, reference response and model response with appropriate scoring, with the essence of training one dialogue model to evaluate another model.': 1.0986056327819824, 'However, the proposed solution depends on a reasonably good dialogue model to begin with, which is not guaranteed, rendering the new metric possibly meaningless.': 1.0986123085021973, 'This paper propose a new evaluation metric for dialogue systems, and show it has a higher correlation with human annotation.': 1.09858238697052, 'I agree the MT based metrics like BLEU are too simple to capture enough semantic information, but the metric proposed in this paper seems to be too compliciated to explain.': 1.098611831665039, 'On the other hand, we could also use equation 1 as a retrieval based dialogue system.': 1.0986121892929077, 'So what is suggested in this paper is basically to train one dialogue model to evaluate another model.': 1.0204707384109497, 'Then, the high-level question is why we should trust this model?': 1.0986123085021973, 'This question is also relevant to the last item of my detail comments.': 1.0986065864562988, 'Detail comments:': 1.0986123085021973, 'How to justify what is captured/evaluated by this metric?': 1.0986074209213257, 'In terms of BLEU, we know it actually capture n-gram overlap.': 1.0986123085021973, 'But for this model, I guess it is hard to say what is captured.': 1.0986123085021973, 'If this is true, then it is also difficult to answer the question like: will the data dependence be a problem?': 1.0984538793563843, 'why not build model incrementally?': 0.5623224377632141, 'As shown in equation (1), this metric uses both context and reference to compute a score.': 1.0985902547836304, 'Is it possible to show the score function using only reference?': 1.0915920734405518, 'It will guarantee this metric use the same information source as BLEU or ROUGE.': 0.6889326572418213, 'Another question about equation (1), is it possible to design the metric to be a nonlinear function.': 1.0985935926437378, 'Since from what I can tell, the comparison between BLEU (or ROUGE) and the new metric in Figure 3 is much like a comparison between the exponential scale and the linear scale.': 0.9255948662757874, 'I found the two reasons in section 5.2 are not convincing if we put them together.': 1.0985838174819946, 'Based on these two reasons, I would like to see the correlation with average score.': 1.039903998374939, 'A more reasonable way is to show the results both with and without averaging.': 1.0959255695343018, 'In table 6, it looks like the metric favors the short responses.': 0.9997245669364929, 'If that is true, this metric basically does the opposite of BLEU, since BLEU will panelize short sentences.': 0.42247188091278076, 'On the other hand, human annotators also tends to give short respones high scores, since long sentences will have a higher chance to contain some irrelevant words.': 1.0975593328475952, 'Can we eliminate the length factor during the annotation?': 1.098610758781433, 'Otherwise, it is not surprise that the correlation.': 1.0986123085021973, 'Overall the paper address an important problem: how to evaluate more appropriately automatic dialogue responses given the fact that current practice to automatically evaluate (BLEU, METEOR, ...) is often insufficient and sometimes misleading.': 1.0986005067825317, 'The proposed approach using an LSTM-based encoding of dialogue context, reference response and model response(s) that are then scored in a linearly transformed space.': 1.0986123085021973, 'While the overall approach is simple it is also quite intuitiv and allows end-to-end training.': 1.0986123085021973, 'As the authors rightly argue simplicity is a feature both for interpretation as well as for speed.': 1.0986123085021973, 'The experimental section reports on quite a range of experiments that seem fine to me and aim to convince the reader about the applicability of the approach.': 1.0986123085021973, 'As mentioned also by others more insights from the experiments would have been great.': 1.0986123085021973, 'I mentioned an in-depth failure case analysis and I would also suggest to go beyond the current dataset to really show generalizability of the proposed approach.': 1.0986123085021973, 'In my opinion the paper is somewhat weaker on that front that it should have been.': 1.0986123085021973, 'Overall I like the ideas put forward and the approach seems sensible though and the paper can thus be accepted.': 1.0986123085021973}"
155,https://openreview.net/forum?id=HJ6idTdgg,"{'Paper summary: the authors proposed to use EdgeBoxes + Fast-RCNN with': 1.3862943649291992, 'batch normalization for pedestrian detection': 1.3862943649291992, 'Review summary: results do not cover enough datasets, the reported': 1.3862943649291992, 'results do not improve over state of the art, writing is poor, and': 1.3862943649291992, 'overall the work lacks novelty.': 1.3862943649291992, 'This is a clear reject.': 1.3862943649291992, 'Pros:': 1.3862943649291992, '* Shows that using batch normalization does improve results': 1.3862943649291992, 'Cons:': 1.3862943649291992, '* Only results on ETH and INRIA.': 1.3862943649291992, 'Should include Caltech or KITTI.': 1.3862943649291992, '* Reported results are fair, but not improving over state of the art': 1.3862943649291992, '* Overall idea of limited interest when considering works like S.': 1.3862943649291992, 'Zhang CVPR 2016 (Fast R-CNN for pedestrian detection) and L. Zhang': 1.3862943649291992, 'ECCV 2017 (Faster R-CNN for pedestrian detection)': 1.3862943649291992, '* Issues with the text quality': 1.3862943649291992, '* Limited takeaways': 1.3862943649291992, 'Quality: low': 1.3862943649291992, 'Clarity: fair, but poor English': 1.3862943649291992, 'Originality: low': 1.3862943649291992, 'Significance: low': 1.3862943649291992, 'For acceptance at future conferences, this work would need more': 1.3862943649291992, 'polish, improving over best known results on INRA, ETH, and Caltech or': 1.3862943649291992, 'KITTI.': 1.3862943649291992, 'And ideally, present additional new insights.': 1.3862943649291992, 'Minor comments:': 1.3862943649291992, '* The text lacks polish.': 1.3862943649291992, 'E.g. influent -> influence, has maken ->': 1.3862943649291992, 'made, is usually very important -> is important, achieve more': 1.3862943649291992, 'excellent results -> achieve better results; etc.': 1.3862943649291992, 'Please consider': 1.3862943649291992, 'asking help from a native speaker for future submissions.': 1.3862943649291992, 'There are': 1.3862943649291992, 'also non-sense sentences such as “it is computational”.': 1.3862943649291992, '* Citations should be in parentheses': 1.3862943649291992, '* Some of the citations are incorrect because the family name is in': 1.3862943649291992, 'the wrong position, e.g. Joseph Lim, Lawrence Zitnick, and Rodrigo Benenson.': 1.3862943649291992, 'This paper proposes a pedestrian detection method using Fast RCNN framework with batch normalization, where EdgeBoxes is used to collect pedestrian proposals instead of selective search as used in the original Fast RCNN method.': 1.3862943649291992, 'The proposed method is evaluated in INRIA and ETH dataset.': 1.3862943649291992, 'The proposed method shows good performance(but not state-of-the-art).': 0.5272364020347595, 'Lack of novelty.': 1.3862943649291992, 'Fast RCNN and its variants (e.g. FasterRCNN, https://arxiv.org/abs/1506.01497; Faster RCNN with ResNet, https://arxiv.org/abs/1512.03385) are widely used in object detection tasks in many literatures.': 1.3856006860733032, 'Meanwhile, batch normalization is a common practice to train / tune deep networks.': 1.2547640800476074, 'It is not new to use batch normalization in CNNs for object / pedestrian detection.': 1.3862943649291992, 'The authors claim using EdgeBoxes for pedestrian proposal is another main contribution, and it is interesting that EdgeBoxes (93% recall) performs significantly better than Selective Search (23% recall) on INRIA dataset.': 1.3776415586471558, 'It will be better to compare proposal generation method in more details (running time, recall, overlap statistics and number of proposals used), and preferably with more recent methods (RPN in https://arxiv.org/abs/1506.01497, YOLO in https://arxiv.org/abs/1506.02640, and SSD in https://arxiv.org/abs/1512.02325).': 1.3862789869308472, 'In summary, the contribution of this paper is minor.': 1.3862943649291992, 'It cannot be accepted.': 1.3862943649291992, 'The authors apply the commonly used Fast RCNN detection system to pedestrian detection.': 1.3862943649291992, 'They use “EdgeBoxes” object proposals and incorporate batch norm into their network.': 1.3862943649291992, 'Results are shown on the INRIA and ETH pedestrian datasets.': 1.3862943649291992, 'They are reasonable but not state-of-the-art.': 1.3861926794052124, 'Results are not shown on Caltech Pedestrians, the standard modern dataset used to evaluate pedestrian detection.': 1.3862943649291992, 'Perhaps more importantly, the paper has no novelty.': 1.3374651670455933, 'The detection system described in this paper is a standard application of Fast RCNN to pedestrian detection.': 1.3861231803894043, 'The implementation is not state-of-the-art, and there is no novelty in this work.': 1.3165923357009888, 'EdgeBoxes has been used with Fast RCNN before.': 1.1732897758483887, 'The authors don’t seem to be aware of more recent developments in object detection, including Faster RCNN (https://arxiv.org/abs/1506.01497), which uses a deep net to perform object proposals and performs much better than EdgeBoxes, or ResNets (https://arxiv.org/abs/1512.03385) which give a huge boost to Fast RCNN.': 1.3862799406051636, 'The authors also propose that using batch norm inside of Fast RCNN is an innovation.': 1.3862943649291992, 'It is not, the ResNet Faster RCNN system definitely uses batch norm (as really do most modern implementation of most variants of RCNN).': 1.3862617015838623, 'While the system described in this paper is reasonable, it is quite dated at this point (judging by the fast progress in object detection).': 1.3845760822296143, 'The authors should also see the paper “Is Faster R-CNN Doing Well for Pedestrian Detection?”': 1.2981468439102173, '(https://arxiv.org/abs/1607.07032) which shows a substantially improved result building on Faster RCNN for pedestrian detection.': 1.3793524503707886, 'Finally, most recent research on pedestrian detection has been performed on Caltech Peds, that dataset is a must for publishing in this space.': 1.3855044841766357, 'The paper cannot be accepted.': 1.3862943649291992, 'This paper presents experimental results from an EdgeBoxes + Fast R-CNN detector on the task of localizing pedestrians.': 1.3862943649291992, 'It uses an AlexNet (CaffeNet) backbone architecture modified to include batch normalization.': 1.3862943649291992, 'Experimental results are presented on the INRIA and ETH datasets.': 1.3862943649291992, 'Pros': 1.3862943649291992, 'The paper is clearly written and easy to follow': 1.3862943649291992, 'Cons': 1.3862943649291992, ""The paper's two contributions are too minor to merit publication"": 1.3862924575805664, 'Experimental results should include at least the Caltech pedestrian dataset but likely also the KITTI pedestrian dataset': 0.8902856111526489, 'Recent work from ECCV 2016': 1.010074496269226, '[a], with superior results and much more experimental evaluation, is not cited or discussed': 1.379651427268982, 'My rating is due primarily to the lack luster contributions.': 1.3372522592544556, 'The first claimed contribution is the use of EdgeBoxes as proposals for pedestrian detection.': 1.3846720457077026, 'Unless the result of this choice produced a truly surprising experimental result, this is simply too minor to be considered a contribution.': 1.2778478860855103, 'Moreover, if this choice is important, then the paper should justify it by showing that other proposal methods (of which there are a great many in addition to Selective Search and Edge Boxes) are worse performing in some regard (speed, accuracy, memory, etc.).': 0.8571203947067261, 'The second claimed contribution is the use of batch normalization (BN) in their network architecture.': 1.3258641958236694, ""There is a case to be made that BN hasn't been explored in Fast R-CNN."": 0.7577196955680847, 'However, if the goal of the paper was to thoroughly explore BN + Fast R-CNN, then why focus narrowly on pedestrian detection?': 0.8880743384361267, 'Instead, it should focus more broadly on generic object category detection for which there are well established Fast R-CNN baselines on PASCAL VOC and COCO.': 0.995053231716156, 'The use of BN + Fast R-CNN only for pedestrian detection does not provide much signal about this choice.': 0.7219974398612976, 'There are also potential technical issues that are not discussed.': 1.3832213878631592, 'BN is typically avoided in Fast R-CNN because the batch size seen by most of the network is usually only one or two images.': 1.3862943649291992, 'This is likely too few images for the naive application of BN.': 1.3862943649291992, '[a] ""Is Faster R-CNN Doing Well for Pedestrian Detection?""': 1.3862943649291992, 'Zhang et al.': 1.3862943649291992}"
156,https://openreview.net/forum?id=HJ7O61Yxe,"{'Because the authors did not respond to reviewer feedback, I am maintaining my original review score.': 1.0986123085021973, 'This paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach: they design a flexible parametric (but not generative) model with Gaussian latent factors and fit it using a rich training objective including terms for reconstruction (of observed time series) error, smoothness in the latent state space (via a KL divergence term encouraging neighbor states to be similarly distributed), and a final regularizer that encourages related time series to have similar latent state trajectories.': 1.0986123085021973, 'Relations between trajectories are hard coded based on pre-existing knowledge, i.e., latent state trajectories for neighboring (wind speed) base stations should be similar.': 1.0986123085021973, 'The model appears to be fit using gradient simple descent.': 1.0986123085021973, 'The authors propose several elaborations, including a nonlinear transition function (based on an MLP) and a reconstruction error term that takes variance into account.': 1.0986123085021973, 'However, the model is restricted to using a linear decoder.': 1.0986123085021973, 'Experimental results are positive but not convincing.': 1.0986123085021973, 'Strengths:': 1.0986123085021973, 'The authors target a worthwhile and challenging problem: incorporating the modeling of uncertainty over hidden states with the power of flexible neural net-like models.': 1.072095513343811, 'The idea of representing relationships between hidden states using KL divergence between their (distributions over) corresponding hidden states is clever.': 1.0650274753570557, 'Combined with the Gaussian distribution over hidden states, the resulting regularization term is simple and differentiable.': 1.07930588722229, 'This general approach': 1.0986123085021973, 'focusing on writing down the problem as a neural network-like loss function': 1.0986123085021973, 'seems robust and flexible and could be combined with other approaches, including variants of variational autoencoders.': 0.5137064456939697, 'Weaknesses:': 1.0986123085021973, 'The presentation is a muddled, especially the model definition in Sec. 3.3.': 0.9144794344902039, 'The authors introduce four variants of their model with different combinations of decoder (with and without variance term) and linear vs. MLP transition function.': 0.8836874961853027, 'It appears that the 2,2 variant is generally better but not on all metrics and often by small margins.': 1.083436369895935, 'This makes drawing a solid conclusions difficult: what each component of the loss contributes, whether and how the nonlinear transition function helps and how much, how in practice the model should be applied, etc.': 0.832449197769165, 'I would suggest two improvements to the manuscript: (1) focus on the main 2,2 variant in Sec. 3.3 (with the hypothesis that it should perform best) and make the simpler variants additional ""baselines"" described in a paragraph in Sec. 4.1; (2) perform more thorough experiments with larger data sets to make a stronger case for the superiority of this approach.': 0.5050976276397705, 'The authors only allude to learning (with references to gradient descent and ADAM during model description) in this framework.': 0.42523032426834106, 'Inference gets its one subsection but only one sentence that ends in an ellipsis (?).': 1.087449312210083, ""It's unclear what is the purpose of introducing the inequality in Eq. 9."": 1.0733519792556763, ""Experimental results are not convincing: given the size of the data, the differences vs. the RNN and KF baselines is probably not significant, and these aren't particularly strong baselines (especially if it is in fact an RNN and not an LSTM or GRU)."": 1.0986123085021973, 'The position of this paper is unclear with respect to variational autoencoders and related models.': 1.0962018966674805, 'Recurrent variants of VAEs (e.g., Krishnan, et al., 2015) seem to achieve most of the same goals as far as uncertainty modeling is concerned.': 0.6876665949821472, 'It seems like those could easily be extended to model relationships between time series using the simple regularization strategy used here.': 0.561861515045166, 'Same goes for Johnson, et al., 2016 (mentioned in separate question).': 1.0362175703048706, 'This is a valuable research direction with some intriguing ideas and interesting preliminary results.': 1.0960856676101685, 'I would suggest that the authors restructure this manuscript a bit, striving for clarity of model description similar to the papers cited above and providing greater detail about learning and inference.': 1.0492311716079712, 'They also need to perform more thorough experiments and present results that tell a clear story about the strengths and weaknesses of this approach.': 0.6579734086990356, 'This manuscript proposes an approach for modeling correlated timeseries through a combination of loss functions which depend on neural networks.': 1.0986123085021973, 'The loss functions correspond to: data fit term, autoregressive latent state term, and a term which captures relations between pairs of timeseries (relations have to be given as prior information).': 1.0986123085021973, 'Modeling relational timeseries is a well-researched problem, however little attention has been given to it in the neural network community.': 1.0986123085021973, 'Perhaps the reason for this is the importance of having uncertainty in the representation.': 1.0986123085021973, 'The authors correctly identify this need and consider an approach which considers distributions in the state space.': 1.0986123085021973, 'The formulation is quite straightforward by combining loss functions.': 1.0986123085021973, 'The model adds to Ziat et al. 2016 in certain aspects which are well motivated, but unfortunately implemented in an unconvincing way.': 1.0986123085021973, ""To start with, uncertainty is not treated in a very principled way, since the inference in the model is rather naive; I'd expect employing a VAE framework"": 1.0985878705978394, '[1] for better uncertainty handling.': 1.0986113548278809, 'Furthermore, the Gaussian co-variance collapses into a variance, which is the opposite of what one would want for modelling correlated time-series.': 1.0986123085021973, 'There are approaches which take these correlations into account in the states, e.g. [2].': 1.0986121892929077, 'Moreover, the treatment of uncertainty only allows for linear decoding function f.': 1.0986123085021973, 'This significantly reduces the power of the model.': 1.0953221321105957, 'State of the art methods in timeseries modeling have moved beyond this constraint, especially in the Gaussian process community e.g.': 1.0986123085021973, '[2,3,4,5].': 1.0986123085021973, 'Comparing to a few of these methods, or at least discussing them would be useful.': 1.0983368158340454, 'References:': 1.0986123085021973, '[1] Kingma and Welling.': 1.0986080169677734, 'Auto-encoding Variational Bayes.': 1.0985186100006104, 'arXiv:1312.6114': 1.0986123085021973, '[2] Damianou et al.': 1.0986123085021973, 'Variational Gaussian process dynamical systems.': 0.751625657081604, 'NIPS 2011.': 1.0883123874664307, '[3] Mattos et al.': 1.0986123085021973, 'Recurrent Gaussian processes.': 1.0986123085021973, 'ICLR 2016.': 1.0986123085021973, '[4] Frigola.': 1.0986123085021973, 'Bayesian Time Series Learning with Gaussian Processes, University of Cambridge, PhD Thesis, 2015.': 1.096535325050354, '[5] Frigola et al.': 1.078398585319519, 'Variational Gaussian Process State-Space Models.': 1.0986123085021973, 'NIPS 2014': 0.41179847717285156, 'One innovation is that the prior structure of the correlation needs to be given.': 1.0986121892929077, 'This is a potentially useful and also original structural component.': 0.9220396280288696, 'However, it also constitutes a limitation in some sense, since it is unrealistic in many scenarios to have this prior information.': 1.095459222793579, 'Moreover, the particular regularizer that makes ""similar"" timeseries to have closeness in the state space seems problematic.': 1.0986121892929077, 'Some timeseries groups might be more ""similar"" than others, and also the similarity might be of different nature across groups.': 1.098610758781433, 'These variations cannot be well captured/distilled by a simple indicator variable e_ij.': 1.0986123085021973, 'Furthermore, these variables are in practice taken to be binary (by looking at the experiments), which would make it even harder to model rich correlations.': 1.0986123085021973, 'The experiments show that the proposed method works, but they are not entirely convincing.': 1.0986123085021973, 'Importantly, they do not shed enough light into the different properties of the model w.r.t its different parts.': 1.0986123085021973, 'For example, the effect and sensitivity of the different regularizers.': 1.0986123085021973, ""The authors state in a pre-review answer that they amended with some more results, but I can't see a revision in openreview (please let me know if I've missed it)."": 1.0986123085021973, ""From the performance point of view, the results are not particularly exciting, especially given the fact that it's not clear which loss is better (making it difficult to use the method in practice)."": 1.0986123085021973, 'It would also be very interesting to report the optimized values of the parameters \\lambda, to get an idea of how the different losses behave.': 1.0986123085021973, 'Timeseries analysis is a very well-researched area.': 1.0986123085021973, ""Given the above, it's not clear to me why one would prefer to use this model over other approaches."": 1.0986123085021973, 'Methodology wise, there are no novel components that offer a proven advantage with respect to past methods.': 1.0986123085021973, 'The uncertainty in the states and the correlation of the time-series are the aspects which could add an advantage, but are not adequately researched in this paper.': 1.0986123085021973, ""In absence of authors' response, the rating is maintained."": 1.0986123085021973, 'This paper introduces a nonlinear dynamical model for multiple related multivariate time series.': 1.0986123085021973, 'It models a linear observation model conditioned on the latent variables, a linear or nonlinear dynamical model between consecutive latent variables and a similarity constraint between any two time series (provided as prior data and non-learnable).': 1.0986123085021973, 'The predictions/constraints given by the three components of the model are Gaussian, because the model predicts both the mean and the variance or covariance matrix.': 1.0986123085021973, 'Inference is forward only.': 1.0986123085021973, 'The model is evaluated on four datasets, and compared to several baselines: plain auto-regressive models, feed-forward networks, RNN and dynamic factor graphs DFGs, which are RNNs with forward and backward inference of the latent variables.': 1.0986123085021973, 'The model, which introduces lateral constraints between different time series, and which predicts both the mean and covariance seems interesting, but presents two limitations.': 1.0986123085021973, 'First of all, the paper should refer to variational auto-encoders / deep gaussian models, which also predict the mean and the variance during inference.': 1.0986123085021973, 'Secondly, the datasets are extremely small.': 1.0986123085021973, 'For example, the WHO contains only 91 times series of 52*10 = 520 time points.': 1.0986123085021973, 'Although the experiments seem to suggest that the proposed model tends to outperform RNNs, the datasets are very small and the high variance in the results indicates that further experiments, with longer time series, are required.': 1.0986123085021973, 'The paper could also easily be extended with more information about the model (what is the architecture of the MLP) as well as time complexity comparison between the models (especially between DFGs and this model).': 1.0986123085021973, 'Minor remark:': 1.0986123085021973, 'The footnote 2 on page 5 seems to refer to the structural regularization term, not to the dynamical term.': 1.0986123085021973}"
157,https://openreview.net/forum?id=HJ9rLLcxg,"{'TDLR:': 1.0986123085021973, 'The authors present a regularization method wherein they add noise to some representation space.': 1.0985629558563232, 'The paper mainly applies the technique w/ sequence autoencoders (Dai et al., 2015) without the usage of attention (i.e., only using the context vector).': 1.0910885334014893, ""Experimental results show improvement from author's baseline on some toy tasks."": 1.0985889434814453, '=== Augmentation ===': 1.0986123085021973, 'The augmentation process is simple enough, take the seq2seq context vector and add noise/interpolate/extrapolate to it (Section 3.2).': 1.0411959886550903, 'This reviewer is very curious whether this process will also work in non seq2seq applications.': 0.9129011631011963, 'This reviewer would have liked to see comparison with dropout on the context vector.': 1.097536563873291, '=== Experiments ===': 1.0977293252944946, ""Since the authors are experimenting w/ seq2seq architectures, its a little bit disappointing they didn't compare it w/ Machine Translation (MT), where there are many published papers to compare to."": 1.0985575914382935, 'The authors did compare their method on several toy datasets (that are less commonly used in DL literature) and MNIST/CIFAR.': 1.0986123085021973, 'The authors show improvement over their own baselines on several toy datasets.': 1.0754196643829346, ""The improvement on MNIST/CIFAR over the author's baseline seems marginal at best."": 1.0986049175262451, ""The author also didn't cite/compare to the baseline published by Dai et al., 2015 for CIFAR"": 0.9896220564842224, ""here they have a much better LSTM baseline of 25% for CIFAR which beats the author's baseline of 32.35% and the author's method of 31.93%."": 1.0618702173233032, 'The experiments would be much more convincing if they did it on seq2seq+MT on say EN-FR or EN-DE.': 1.0986042022705078, ""There is almost no excuse why the experiments wasn't run on the MT task, given this is the first application of seq2seq was born from."": 1.0985816717147827, 'Even if not MT, then at least the sentiment analysis tasks (IMDB/Rotten Tomatoes) of the Dai et al., 2015 paper which this paper is so heavily based on for the sequence autoencoder.': 1.0986123085021973, '=== References ===': 1.0690248012542725, 'Something is wrong w/ your references latex setting?': 1.0986031293869019, 'Seems like a lot of the conference/journal names are omitted.': 1.0985019207000732, 'Additionally, you should update many cites to use the conference/journal name rather than just ""arxiv"".': 1.0092213153839111, 'Listen, attend and spell (should be Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition) -> ICASSP': 0.2477332353591919, 'if citing ICASSP paper above, should also cite Bahandau paper ""End-to-End Attention-based Large Vocabulary Speech Recognition"" which was published in parallel (also in ICASSP).': 0.9894309043884277, 'Adam: A method for stochastic optimization -> ICLR': 0.4476831555366516, 'Auto-encoding variational bayes -> ICLR': 1.0922929048538208, 'Addressing the rare word problem in neural machine translation -> ACL': 0.8098605871200562, 'Pixel recurrent neural networks -> ICML': 1.0985931158065796, 'A neural conversational model -> ICML Workshop': 1.096629023551941, 'The concept of data augmentation in the embedding space is very interesting.': 1.0986123085021973, 'The method is well presented and also justified on different tasks such as spoken digits and image recognition etc.': 1.0986123085021973, 'One comments of the comparison is the use of a simple 2-layer MLP as the baseline model throughout all the tasks.': 1.0986123085021973, ""It's not clear whether the gains maintain when a more complex baseline model is used."": 1.0440741777420044, 'Another comment is that the augmented context vectors are used for classification, just wondering how does it compare to using the reconstructed inputs.': 1.0981569290161133, 'And furthermore, as in Table 4, both input and feature space extrapolation improves the performance, whether these two are complementary or not?': 1.095401644706726, 'In this paper authors propose a novel data augmentation scheme where instead of augmenting the input data, they augment intermediate feature representations.': 1.0681523084640503, 'Sequence auto-encoder based features are considered, and random perturbation, feature interpolation, and extrapolation based augmentation are evaluated.': 1.0981762409210205, 'On three sequence classification tasks and on MNIST and CIFAR-10, it is shown that augmentation in feature space, specifically extrapolation based augmentation, results in good accuracy gains w.r.t.': 1.0794267654418945, 'authors baseline.': 1.0986123085021973, 'My main questions and suggestions for further strengthening the paper are:': 1.0986123085021973, 'a)': 1.0986123085021973, 'The proposed data augmentation approach is applied to a learnt auto-encoder based feature space termed ‘context vector’ in the paper.': 1.0552350282669067, 'The context vectors are then augmented and used as input to train classification models.': 1.0986123085021973, 'Have the authors considered applying their feature space augmentation idea directly to the classification model during training, and applying it to potentially many layers of the model?': 1.0758472681045532, 'Also, have the authors considered convolutional neural network (CNN) architectures as well for feature space augmentation?': 0.9784348011016846, 'CNNs are now the state-of-the-art in many image and sequence classification task, it would be very valuable to see the impact of the proposed approach in that model.': 1.0876307487487793, 'b) When interpolation or extrapolation based augmentation was being applied, did the authors also consider utilizing nearby samples from competing classes as well?': 1.0986123085021973, 'Especially in case of extrapolation based augmentation it will be interesting to check if the extrapolated features are closer to competing classes than original ones.': 1.0986123085021973, 'c)': 1.0986123085021973, 'With random interpolation or nearest neighbor interpolation based augmentation the accuracy seems to degrade pretty consistently.': 1.0986123085021973, 'This is counter-intuitive.': 1.0986123085021973, 'Do the authors have explanation for why the accuracy degraded with interpolation based augmentation?': 1.0986123085021973, 'd)': 1.0986123085021973, 'The results on MNIST and CIFAR-10 are inconclusive.': 1.0986123085021973, 'For instance the error rate on CIFAR-10 is well below 10% these days, so I think it is hard to draw conclusions based on error rates above 30%.': 1.0986123085021973, 'For MNIST it is surprising to see that data augmentation in the input space substantially degrades the accuracy (1.093% -> 1.477%).': 1.0986123085021973, 'As mentioned above, I think this will require extending the feature space augmentation idea to CNN based models.': 1.0986123085021973}"
158,https://openreview.net/forum?id=HJDBUF5le,"{'This paper proposes a hierarchical generative model where the lower level consists of points within datasets and the higher level models unordered sets of datasets.': 1.0985747575759888, 'The basic idea is to use a ""double"" variational bound where a higher level latent variable describes datasets and a lower level latent variable describes individual examples.': 1.0986123085021973, ""Hierarchical modeling is an important and high impact problem, and I think that it's under-explored in the Deep Learning literature."": 0.41519713401794434, 'Pros:': 1.0985045433044434, ""-The few-shot learning results look good, but I'm not an expert in this area."": 1.075429916381836, '-The idea of using a ""double"" variational bound in a hierarchical generative model is well presented and seems widely applicable.': 0.41372373700141907, 'Questions:': 1.0986123085021973, '-When training the statistic network, are minibatches (i.e. subsets of the examples) used?': 0.42181384563446045, '-If not, does using minibatches actually give you an unbiased estimator of the full gradient (if you had used all examples)?': 0.908112645149231, 'For example, what if the statistic network wants to pull out if *any* example from the dataset has a certain feature and treat that as the characterization.': 1.0985978841781616, 'This seems to fit the graphical model on the right side of figure 1.': 0.9509575366973877, ""If your statistic network is trained on minibatches, it won't be able to learn this characterization, because a given minibatch will be missing some of the examples from the dataset."": 1.0986121892929077, 'Using minibatches (as opposed to using all examples in the dataset) to train the statistic network seems like it would limit the expressive power of the model.': 1.0986123085021973, 'Suggestions:': 1.0986123085021973, '-Hierarchical forecasting (electricity / sales) could be an interesting and practical use case for this type of model.': 0.4253636598587036, 'Sorry for the late review': 0.8713550567626953, ""I've been having technical problems with OpenReview which prevented me from posting."": 1.0338369607925415, 'This paper presents a method for learning to predict things from sets of data points.': 1.0010226964950562, 'The method is a hierarchical version of the VAE, where the top layer consists of an abstract context unit that summarizes a dataset.': 0.7495332360267639, 'Experiments show that the method is able to ""learn to learn"" by acquiring the ability to learn distributions from small numbers of examples.': 1.0274509191513062, 'Overall, this paper is a nice addition to the literature on one- or few-shot learning.': 0.5946180820465088, 'The method is conceptually simple and elegant, and seems to perform well.': 0.9872128963470459, 'Compared to other recent papers on one-shot learning, the proposed method is simpler, and is based on unsupervised representation learning.': 0.497420996427536, 'The paper is clearly written and a pleasure to read.': 0.4198364019393921, 'The name of the paper is overly grandiose relative to what was done; the proposed method doesn’t seem to have much in common with a statistician, unless one means by that ""someone who thinks up statistics"".': 1.0978140830993652, 'The experiments are well chosen, and the few-shot learning results seem pretty solid given the simplicity of the method.': 0.9660096764564514, 'The spatial MNIST dataset is interesting and might make a good toy benchmark.': 1.0759702920913696, 'The inputs in Figure 4 seem pretty dense, though; shouldn’t the method be able to recognize the distribution with fewer samples?': 0.07985147088766098, '(Nitpick: the red points in Figure 4 don’t seem to correspond to meaningful points as was claimed in the text.)': 0.7628360390663147, 'Will the authors release the code?': 0.85454922914505, 'The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables.': 1.0986121892929077, 'The idea is clearly motivated and well described.': 1.0986123085021973, 'In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community.': 1.0986120700836182, 'Comments:': 1.0978422164916992, 'It\'s not clear to me why this should be called a ""statistician"".': 1.0978354215621948, 'Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network.': 1.0965718030929565, 'One could consider a maximum likelihood approach, etc.': 0.7795320749282837, 'In general it felt like the paper could be more clear, if it avoided coining new terms like ""statistic network"" and stuck to the more accurate ""approximate posterior"".': 1.0986120700836182, 'The experiments are nice, and I appreciate the response to my question regarding ""one shot generation"".': 1.0986123085021973, 'I still think that language needs to be clarified, specifically at the end of page 6.': 0.6134992837905884, 'My understanding of Figure 5 is the following: Take an input set, compute the approximate posterior over the context vector, then generate from the forward model given samples from the approximate posterior.': 1.09861159324646, 'I would like clarification on the following:': 1.0986120700836182, '(a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior?': 1.0985981225967407, '(b) I agree that the samples are of high-quality, but that is not a quantified statement.': 1.0986087322235107, 'The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities.': 1.098461389541626, 'To that end, one ""proper"" way of computing the ""one shot generation"" performance is to report log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets.': 1.0986008644104004, 'I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive.': 0.9737058877944946, ""I still don't see a reason not to include that."": 1.0983819961547852}"
159,https://openreview.net/forum?id=HJDdiT9gl,"{'The paper proposes modification to seq2seq model to help it handle the problems when long responses are needed.': 1.0986123085021973, 'Though the technical contributions may be of value, the work in my personal opinion is not in the right direction towards helping dialog systems.': 1.0986123085021973, 'Essentially we try to generate long responses that sound ``nice"" yet are not grounded to any reality, they just need to be related to the question and not suffers from obvious mistakes.': 1.0986123085021973, 'Yet, the architectural innovations proposed may be of merit.': 1.0986123085021973, 'The paper is clearly interesting in that it does address important problems (length and diversity of responses) in sequence-to-sequence models.': 1.0986123085021973, 'The two ideas put forward (glimpse model and segment-based stochastic decoding) both seem ideas in the right direction.': 1.0986119508743286, 'I was however not so sold on the argument that these are particularly suitable for conversations.': 1.098284125328064, 'The results indicate that the ideas do indeed generate longer and also somewhat more sensible target sequences and as such the paper makes progress w.r.t these important problems.': 1.0986121892929077, 'So overall I would suggest accepting the paper even though the flavor of the proposed ideas are somewhat ""small steps"".': 1.0986121892929077, 'This paper considers the problem of generating long and diverse responses in dialog systems.': 1.0985935926437378, 'Two techniques are proposed to the seq-to-seq framework: (1) glimpse model that trains on fixed-length segments of the target side at a time, and (2) a segment-based stochastic decoding technique which injects diversity earlier in the generated responses.': 1.0986123085021973, 'The large scale experiments on 2.3B conversation messages are quite impressive.': 1.0986062288284302, 'Experiments on human evaluation should also be encouraged.': 1.0979574918746948, 'With all these said, I am still not 100% convinced that machine generated long sequence is the right direction for dialog systems.': 1.0983279943466187, 'As shown in Figure 3 (a), human evaluation shows that the proposed system is not significantly better than the baselines.': 1.09861159324646, 'I think more analysis and user preference mining should be done in the future to help us understand the nature of this problem.': 1.0986123085021973}"
160,https://openreview.net/forum?id=HJF3iD9xe,"{'Pros:': 1.3862943649291992, '* Part of the paper addresses an industrially important topic, namely how to make deep networks work properly on point clouds, i.e. in many (most?)': 1.386161208152771, 'potential applications they should be invariant to permutations of the points within the cloud, as well as rigid transformations of the cloud (depends on the application).': 1.3862943649291992, '*': 1.3862943649291992, 'The authors propose a formalism for dealing with compositions of different kinds of invariance.': 1.3862943649291992, 'Cons:': 1.3695772886276245, '* For me the explanation of the generalization is really hard to follow.': 1.3862943649291992, 'For me, the paper would be stronger if were less broad, but went into more depth for the permutation-invariance case.': 1.3862943649291992, '* It is very easy to sit down and come up with network structures that are permutation invariant.': 1.3862943649291992, ""It seems the author tried a few networks in the family (a few different point cloud sizes, a couple options for the number of parameters, averaging vs. max in the set, dropout vs. no dropout), but unless the space is more completely and systematically explored, there's not much reason for a practitioner to use the proposed structure vs. some other random structure they cook up that is also permutation invariant."": 1.3862943649291992, 'i.e. what about just using a FC layer that is shared between the points instead of your three ""set invariant"" layers?': 1.3862943649291992, 'Seems simpler, more general, and also permutation invariant...': 1.3862943649291992, '* It is not clear to me how valuable the author\'s definition of ""minimally invariant"" is.': 1.3862919807434082, 'Is a sufficiently large composition of ""set invariant"" layers a universal approximator for permutation invariant functions?': 1.3862897157669067, '* I\'m concerned that proposed ""set invariant layer"" might be strongly variant to spatial transformations, as well as vulnerable to large outliers.': 1.3858569860458374, 'In particular there is a term that subtracts a corner of the clouds bounding box (i.e. the max over set operator inside the first layer), before the cloud goes through a learned affine transform and pixelwise nonlinearity.': 1.3862943649291992, 'Seems like that could saturate the whole network...': 1.3862943649291992, ""I'm reviewing with low confidence, because there's a chance the formalism in the first part of the paper is more valuable than I realize; I haven't fully understood it."": 1.3862943649291992, 'Pros :': 1.3862943649291992, 'New and clear formalism for invariance on signals with known structure': 1.3862943649291992, 'Good numerical results': 1.3862943649291992, 'Cons :': 1.3862935304641724, 'The structure must be specified.': 1.2590540647506714, 'The set structure dataset is too simple': 0.8691113591194153, 'There is a gap between the large (and sometimes complex) theory introduced and the numerical experiments ; consequently a new reader could be lost since examples might be missing': 1.3862943649291992, 'Besides, from a personal point of view, I think the topic of the paper and its content could be suitable for a big conference as the author improves its content.': 1.386279582977295, 'Thus, if rejected, I think you should not consider the workshop option for your paper if you wish to publish it later in a conference, because big conferences might consider the workshop papers of ICLR as publications.': 1.3862930536270142, ""(that's an issue I had to deal with at some points)"": 1.3704355955123901, 'This paper discusses ways to enforce invariance in neural networks using weight sharing.': 0.8488437533378601, 'The authors formalize a way for feature functions to be invariant to a collection of relations and the main invariance studied is a “set-invariant” function, which is used in an anomaly detection setting and a point cloud classification problem.': 1.31050705909729, '“Invariance” is, at a high level, an important issue of course, since we don’t want to spend parameters to model spurious ordering relationships, which may potentially be quite wasteful and I like the formalization of invariance presented in this paper.': 1.358981966972351, 'However, there are a few weaknesses that I feel prevent this from being a strong submission.': 1.3808549642562866, 'First, the exposition is too abstract and this paper could really use a running and *concrete* example starting from the very beginning.': 1.1932072639465332, 'Second, “set invariance”, which is the main type of invariance studied in the paper is defined via the author’s formalization of invariance, but is never explicitly related to what I might think of as “set invariance” — e.g. to permutations of input or output dimensions.': 1.0362913608551025, 'Explicitly defining set invariance in some other way, then relating it to the  “structural invariance” formulation may be a better way to explain things.': 1.3862943649291992, 'It is never made clear, for example, why Figure 1(b) is *the* set data-structure.': 1.3862943649291992, 'I like the discussion of compositionality of structures (one question I have here is: are the resulting compositional structures are still valid as structures?).': 1.3862943649291992, 'But the authors have ignored the other kind of compositionality that is important to neural networks — specifically that relating the proposed notion of invariance to function composition seems important — i.e. under what conditions do compositions of invariant functions remain invariant?': 1.3862943649291992, 'And  It is clear to me that just by having one layer of invariance in a network doesn’t make the entire network invariant, for example.': 1.3862943649291992, 'So if we look at the anomaly detection network at the end for example, is it clear that the final predictor is “set invariant” in some sense?': 1.3862943649291992, 'Regarding experiments, there are no baselines presented for anomaly detection.': 1.3862943649291992, 'Baselines *are* presented in the point cloud classification problem, but the results of the proposed model are not the best, and this should be addressed.': 1.3862943649291992, '(I should say that I don’t know enough about the dataset to say whether these are exactly fair comparisons or not).': 1.3862943649291992, 'It is also never really made clear why set invariance is a desirable property for a point cloud classification setting.': 1.3862943649291992, 'As a suggestion: try a network that uses a fully connected layer at the end, but uses data augmentation to enforce set invariance.': 1.3862943649291992, 'Also, what about classical set kernels?': 1.3862943649291992, 'Other random things:': 1.3862943649291992, '* Example 2.2: Shouldn’t |S|=5 in the case of left-right and up-down symmetry?': 1.3862943649291992, '* “Parameters shared within a relation” is vague and undefined.': 1.3862943649291992, '* Why is “set convolution” called “set convolution” in the appendix?': 1.3862943649291992, 'What is convolutional about it?': 1.3862943649291992, '* Is there a relationship to symmetric function theory?': 1.3862943649291992, 'This review is only an informed guess - unfortunately I cannot assess the paper due to my lack of understanding of the paper.': 1.3862943649291992, 'I have spent several hours trying to read this paper - but it has not been possible for me to follow - partially due to my own limitations, but also I think due to an overly abstract level of presentation.': 1.3862943649291992, 'The paper is clearly written, but in the same way that a N. Bourbaki book is clearly written.': 1.3862943649291992, 'I would prefer to leave the accept/reject decision to the other reviewers who may have a better understanding - even if the authors had made a serious mistake, I would not be able to tell.': 1.3862943649291992, 'My proposal is positive because the paper is apparently clearly written and the empirical evaluation is quite promising.': 1.3862943649291992, 'But some effort will be needed in order to address the broader audience that could potentially be interested in the topic.': 1.3862943649291992, 'I therefore would like to provide feedback only at the level of presentation.': 1.3862943649291992, 'My main source of problems is that the authors do not try to ground their abstract formalism with concrete examples; when the examples show up it is by ""revelation"" rather than by explaining how they connect to the previous concepts.': 1.3862943649291992, ""The one example that could unlock most people's understanding is how convolution, or inner product operations connect with the setting described here."": 1.3862943649291992, 'For what I know convolution is tied with space (or time) and is understood as an equivariant operation - shifting the signal shifts the output.': 1.3862943649291992, ""It is not explained how the '(x, x')' pairs used by the authors in order to build relations, structures and then to define invariance relate to this setting."": 1.3862943649291992, 'Going from sets, to relations, to functions, to operators, and then to shift-invariant operators (convolutions) involves many steps, and some hand-holding is needed.': 1.3862943649291992, 'Why is the 3x3 convolution associated to 9 relations?': 1.3862943649291992, 'Are these relations referring to the input at a given coordinate and its contribution to the output?': 1.3862943649291992, '(w_{offset} x_{i-offset})?': 1.163843035697937, 'In that case, why is there a backward arrow from the center node to the other nodes?': 1.386293649673462, 'And why are there arrows across nodes?': 1.3862414360046387, 'What is a Cardinal and what is a Cartesian convolution in signal processing terms?': 0.7392370104789734, '(clearly these are not standard terms).': 1.3777756690979004, 'Are we talking about separable filters?': 1.3862601518630981, 'What are the X and Square symbols in Figure 2?': 0.6962294578552246, 'And what are the horizontal and vertical sub-graphs standing for?': 1.3862943649291992, 'What is x_1 and what is x_{11},x_{1,2},x_{1,3} and what is the relationship between them?': 1.3722983598709106, 'I realize that to the authors these questions may seem to be trivial and left as  homework for the reader.': 1.3675249814987183, 'But I think part of publishing a paper is doing a big part of the homework for the readers so that it becomes easy to get the idea.': 1.2063214778900146, 'Clearly the authors target the more general case - but spending some time to explain how the particular case is an instance of the the general case would be a good use of space.': 1.3393189907073975, 'I would propose that the authors explain what are  x, x_{I}, and x_{S} for the simplest possible example, e.g. convolving a 1x5 signal with a 1x3 filter, how the convolution filter parameters show up in the function f, as well as how the spatial invariance (or, equivariance) of convolution is reflected here.': 1.287243366241455}"
161,https://openreview.net/forum?id=HJGODLqgx,"{'This paper proposes a novel and interesting way to tackle the difficulties of performing inference atop HSMM.': 1.0986123085021973, 'The idea of using an embedded bi-RNN to approximate the posterior is a reasonable and clever idea.': 1.0986120700836182, 'That being said, I think two aspects may need further improvement:': 1.038221836090088, '(1) An explanation as to why a bi-RNN can provide more accurate approximations than other modeling choices (e.g. structured mean field that uses a sequential model to formulate the variational distribution) is needed.': 1.0986090898513794, 'I think it would make the paper stronger if the authors can explain in an intuitive way why this modeling choice is better than some other natural choices (in addition to empirical verification).': 1.0986121892929077, '(2) The real world datasets seem to be quite small (e.g. less than 100 sequences).': 1.098608374595642, 'Experimental results reported on larger datasets may also strengthen the paper.': 1.0986123085021973, 'Putting the score for now, will post the full review tomorrow.': 1.0986121892929077, 'This paper presents a novel model for unsupervised segmentation and classification of time series data.': 1.0985820293426514, 'A recurrent hidden semi-markov model is proposed.': 1.0986123085021973, 'This extends regular hidden semi-markov models to include a recurrent neural network (RNN) for observations.': 1.0986123085021973, 'Each latent class has its own RNN for modeling observations for that category.': 0.5127758979797363, 'Further, an efficient training procedure based on a variational approximation.': 0.7479426860809326, 'Experiments demonstrate the effectiveness of the approach for modeling synthetic and real time series data.': 0.8231399655342102, 'This is an interesting and novel paper.': 0.257709801197052, 'The proposed method is a well-motivated combination of duration modeling HMMs with state of the art observation models based on RNNs.': 1.0979797840118408, 'The combination alleviates shortcomings of standard HSMM variants in terms of the simplicity of the emission probability.': 1.0692683458328247, 'The method is technically sound and demonstrated to be effective.': 1.0986042022705078, 'It would be interesting to see how this method compares quantitatively against CRF-based methods (e.g. Ammar, Dyer, and Smith NIPS 2014).': 1.0180037021636963, 'CRFs can model more complex data likelihoods, though as noted in the response phase there are still limitations.': 1.0986123085021973, 'Regardless, I think the merits of using RNNs for the class-specific generative models are clear.': 1.0946494340896606}"
162,https://openreview.net/forum?id=HJGwcKclx,"{'This paper proposes to use an empirical Bayesian approach to learn the parameters of a neural network, and their priors.': 0.9163345694541931, 'A mixture model prior over the weights leads to a clustering effect in the weight posterior distributions (which are approximated with delta peaks).': 1.0935320854187012, 'This clustering effect can exploited for parameter quantisation and compression of the network parameters.': 1.0943551063537598, 'The authors show that this leads to compression rates and predictive accuracy comparable to related approaches.': 1.09254789352417, 'Earlier work [Han et al. 2015] is based on a three-stage process of pruning small magnitude weights, clustering the remaining ones, and updating the cluster centres to optimise performance.': 0.5094067454338074, 'The current work provides a more principled approach that does not have such an ad-hoc multi-stage structure, but a single iterative optimisation process.': 1.0859863758087158, 'A first experiment, described in section 6.1 shows that an empirical Bayes’ approach, without the use of hyper priors, already leads to a pronounced clustering effect and to setting many weights to zero.': 1.0894724130630493, 'In particular a compression rate of 64.2 is obtained on the LeNet300-100 model.': 1.0986123085021973, 'In section 6.1 the text refers to figure C, I suppose this should be figure 1.': 1.0986123085021973, 'Section 6.2 describes an experiment where hyper-priors are used, and the parameters of these distributions, as well as other hyper-parameters such as the learning rates, are being optimised using Spearmint (Snoek et al., 2012).': 1.0986123085021973, 'Figure 2 shows the performance of the  different points in the hyper-parameter space that have been evaluated (each trained network gives an accuracy-compressionrate point in the graph).': 1.0986123085021973, 'The text claims that best results lie on a line, this seems a little opportunistic interpretation given the limited data.': 1.0986123085021973, 'Moreover, it would be useful to add a small discussion on whether such a linear relationship would be expected or not.': 1.0986123085021973, 'Currently the results of this experiment lack interpretation.': 1.0986123085021973, 'Section 6.3 describes results obtained for both CNN models and compares results to the recent results of (Han et al., 2015) and (Guo et al., 2016).': 0.8929747343063354, 'Comparable results are obtained in terms of compression rate and accuracy.': 0.30790501832962036, 'The authors state that their current algorithm is too slow to be useful for larger models such as VGG-19, but they do briefly report some results obtained for this model (but do not compare to related work).': 1.0973386764526367, 'It would be useful here to explain what slows the training down with respect to standard training without the weight clustering approach, and how the proposed algorithm scales in terms of the relevant quantities of the data and the model.': 0.3777157962322235, 'The contribution of this paper is mostly experimental, leveraging fairly standard ideas from empirical Bayesian learning to introduce weight clustering effects in CNN training.': 1.0920928716659546, 'This being said, it is an interesting result that such a relatively straightforward approach leads to results that are on par with state-of-the-art, but more ad-hoc, network compression techniques.': 0.4722537100315094, 'The paper could be improved by clearly describing the algorithm used for training, and how it scales to large networks and datasets.': 0.9402683973312378, 'Another point that would deserve further discussion is how the hyper-parameter search is performed ( not using test data I assume), and how the compared methods dealt with the search over hyper-parameters to determine the accuracy-compression tradeoff.': 1.0637998580932617, 'Ideally, I think, methods should be evaluated across different points on this trade-off.': 1.0930835008621216, 'The authors propose a method to compress neural networks by retraining them while putting a mixture of Gaussians prior on the weights with learned means and variances which then can be used to compress the neural network by first setting all weights to the mean of their infered mixture component (resulting in a possible loss of precision) and storing the network in a format which saves only the fixture index and exploits the sparseness of the weights that was enforced in training.': 1.0985805988311768, 'Quality:': 1.0976579189300537, ""Of course it is a serious drawback that the method doesn't seem to work on VGG which would render the method unusable for production (as it is right now, maybe this can be improved)."": 1.0986111164093018, 'I guess AlexNet takes too long to process, too, otherwise this might be a very valuable addition.': 0.8773777484893799, 'In Figure 2 I am noticing two things: On the left, there is a large number of points with improved accuracy which is not the case for LeNet5-Caffe.': 1.0986120700836182, ""Is there any intuition for why that's the case?"": 1.0973681211471558, 'Additionally regarding the spearmint optimization: Do they authors have found any clues about which hyperparameter settings worked well?': 0.47210559248924255, 'This might be helpful for other people trying to apply this method.': 0.6381951570510864, ""I really like Figure 7 in it's latest version."": 1.0986123085021973, 'Clarity:': 1.0986123085021973, 'Especially section 2 on MDL is written very well and gives a nice theoretic introduction.': 1.0986123085021973, 'Sections 4, 5 and 6 are very short but seem to contain most relevant information.': 1.0986123085021973, 'It might be helpful to have at least some more details about the used models in the paper (maybe the number of layers and the number of parameters).': 1.0986123085021973, 'In 6.1 the authors claim ""Even though most variances seem to be reasonable small there are some that are large"".': 1.0986123085021973, 'From figure 1 this is very hard to assess, especially as the vertical histogram essentially shows only the zero component.': 1.0986123085021973, 'It might be helpful to have either a log histogram or separate histograms for each componenent.': 1.0986123085021973, 'What are the large points in Figure 2 as opposed to the smaller ones?': 0.6436306238174438, 'They seem to have a very good compression/accuracy loss ratio, is that it?': 0.9284393787384033, 'Some other points are listed below': 1.0986123085021973, 'originality: While there has been some work on compressing neural networks by using a reduced number of bits to store the parameters and exploiting sparsity structure, I like the idea to directly learn the quantization by means of a gaussian mixture prior in retraining which seems to be more principled than other approaches': 0.5191217660903931, 'significance: The method achievs state-of-the-art performance on the two shown examples on MNIST, however these networks are far from the deep networks used in state-of-the-art models.': 0.9280503988265991, ""This obviously is a drawback for the practical usability of the methods and therefor it's significance."": 0.40697675943374634, 'If the method could be made to work on more state-of-the-art networks like VGG or ResNet, I would consider this a contribution of high significance.': 1.098608136177063, 'Minor issues:': 1.0986123085021973, ""page 1: There seems to be a space in front of the first author's name"": 1.0986123085021973, 'page 3: ""in this scenario, pi_0 may be fixed..."".': 1.0986123085021973, 'Missing backslash in TeX?': 1.0986123085021973, 'page 6: 6.2: two wrong blanks in ""the number of components_, \\tau_.""': 1.0986123085021973, 'page 6, 6.3: ""in experiences with VGG"": In experiments?': 1.0986123085021973, 'page 12: ""Figure C"": Figure 7?': 1.0986123085021973, 'This paper revives a classic idea involving regularization for purposes of compression for modern CNN models on resource constrained devices.': 1.0986123085021973, ""Model compression is hot and we're in the midst of lots of people rediscovering old ideas in this area so it is nice to have a paper that explicitly draws upon classic approaches from the early 90s to obtain competitive results on standard benchmarks."": 1.0986123085021973, ""There's not too much to say here: this study is an instance of a simple idea applied effectively to an important problem, written up in an illuminating manner with appropriate references to classic approaches."": 1.0986123085021973, 'The addition of the filter visualizations enhances the contribution.': 1.0986123085021973}"
163,https://openreview.net/forum?id=HJIY0E9ge,"{'Summary:': 1.0986123085021973, 'In this paper, the authors introduce NoiseOut, a way to reduce parameters by pruning neurons from a network.': 1.0986123085021973, 'They do this by identifying pairs of neurons produce the most correlated outputs, and replacing the pair by one neuron, and then appropriately adjusting weights.': 1.0986123085021973, 'This technique relies on neurons having high correlations however, so they introduce an additional output neuron': 1.0986123085021973, 'a noise output, which results in the network trying to predict the mean of the noise distribution.': 1.0980443954467773, 'As this is a constant, it increases correlation between neurons.': 1.0986123085021973, 'Experiments test this out on MNIST and SVHN': 1.0986123085021973, 'Comments:': 1.0986123085021973, 'This is an interesting suggestion on how to prune neurons, but more experiments (on larger datasets) are probably need to be convincing that this is an approach that is guaranteed to work well.': 1.0986123085021973, 'Equation (5) seems to be very straightforwards?': 1.0986123085021973, 'It seems like that for larger datasets, more noise outputs might have to be added to ensure higher correlations?': 1.0986123085021973, 'Is there a downside to this in terms of the overall accuracy?': 1.0986123085021973, 'The paper is presented clearly, and was definitely interesting to read, so I encourage the authors to continue this line of work.': 1.0986123085021973, 'This paper proposes and tests two ideas.': 1.0986123085021973, '(1) a method of pruning networks by identifying highly correlated neuron pairs, pruning one of the pair, and then modifying downstream weights to compensate for the removal (which works well if the removed neurons were highly correlated).': 1.098589539527893, '(2) a method, dubbed NoiseOut, for increasing neuron correlation by adding auxiliary noise target outputs to the network during training.': 1.0986123085021973, 'The first idea (1) is fairly straightforward, and it is not clear if it has been tried before.': 1.0986123085021973, 'It does seem to work.': 1.0986123085021973, 'The second idea (2) is of unclear value and seems to this reviewer that it may merely add a regularizing effect.': 1.0986119508743286, 'Comments in this direction:': 1.0986123085021973, '- In Fig 4 (right), the constant and Gaussian treatments seem to produce the same effect in both networks, right? And the Binomial effect seems the same as No_Noise. If this is true, can we conclude that the NoiseOut targets are simply serving to regularize the network, that is, to reduce its capacity slightly?': 0.9133936166763306, '- To show whether this effect is true, one would need to compare to other methods of reducing the network capacity, for example: by reducing the number of neurons, by applying L2 regularization of various values, or by applying Dropout of various strengths. Fig 7 makes an attempt at this direction, but critically misses several comparison treatments: “Pruned without any regularization”, “Pruned with only L2”, and “Pruned with only DropOut”. Have these experiments been run? Can their results be included and used to produce plots like Fig 5 and Fig 7?': 0.35710805654525757, 'Without these comparisons, it seems impossible to conclude that NoiseOut does anything but provide similar regularization to DropOut or L2.': 1.0986123085021973, 'The combined ideas (1) + (2) DO produce a considerable reduction in parameters, but sadly the experiments and exposition are somewhat too lacking to really understand what is going on.': 1.0986123085021973, 'With a little more work the paper could be quite interesting, but as is it should probably not be accepted.': 1.0986123085021973, 'Additional comments:': 1.0986123085021973, '- Section 4 states: “In all of these experiments, the only stop criteria is the accuracy decay of the model. We set the threshold for this criteria to match the original accuracy; therefore all the compressed network have the same accuracy as the original network.” Is this accuracy the train accuracy or test accuracy? If train, then test accuracy needs to be shown (how much test performance is lost when pruning?). If test, then this would typically be referred to as “cheating” and so the choice needs to be very clearly stated and then defended.': 0.672156572341919, '- Lowercase rho is used to indicate correlation but this is never actually specified, which is confusing for. Just state once that it indicates correlation.': 1.0983483791351318, '- How do these results compare to other pruning methods? No numerical comparison is attempted.': 1.0986123085021973, 'The paper proposes to prune a neural network by removing neurons whose operation is highly correlated with other neurons.': 1.0986123085021973, ""The idea is nice and somewhat novel - most pruning methods concentrate on removal of individual weights, however I haven't done a through research on this topic."": 1.0986123085021973, 'However, the experimental and theoretical justification of this method need to be improved before publication:': 1.0986123085021973, '1. Experiments. The authors do not report accuracy degradation while pruning in the tables, laconically stating that the networks did not degrade. This is not convincing. The only details are given in Figure 5, however this Figure disagrees with Table 2: in the Table, the number of parameters ranges from 40k-600k, while the Figure pictures the range 12k-24k. Unless more details are provided, simply claiming that a network can remove 50% neurons with no number on the degradation of accuracy is not convincing.': 1.0986119508743286, ""2. Theory. The proofs do not match the experimental conditions and make unreasonable assumptions. The proofs show that in the absence of biases a network with a constant output will have two correlated neurons that generate the output offset. However, this is exactly why networks have biases and doesn't explain why noise injection helps (the proof suggests that all should be fine with deterministic auxiliary neuron). My interpretation is that the noisy output injects gradient noise (see e.g. the concurrent ICLR submission https://openreview.net/forum?id=rkjZ2Pcxe). As such the proof muddies the picture more than it helps in understanding what is happening."": 0.6855167150497437, 'Verdict:': 0.4905366897583008, 'Reject and resubmit.': 1.0864180326461792, 'The pruning idea has potential, however its efficiency must be more soundly demonstrated (please provide network accuracies at various pruning levels, the method removes one neuron at a time, this allows making of nice plots) rather than laconically stating that a degradation on mnist from 97% accuracy to 92% is not significant (Figure 5.).': 1.0986089706420898, 'Please provide Figures and Tables that agree with the text in terms of numbers provided.': 0.912550687789917}"
164,https://openreview.net/forum?id=HJKkY35le,"{'The paper proposes two regularization approaches for training GAN, aiming to provide stronger gradient signal to move the generated distribution to data distribution and to avoid the generated distribution from getting trapped in only one or a few modes of the data distribution.': 1.3590184450149536, 'The presented approaches are entirely based on some intuitive arguments.': 1.3862943649291992, 'As such intuitions are interesting, likely useful, and deserve further exploration in a broader context, they stay as heuristics as this point.': 1.3862943649291992, 'The paper will benefit from more rigorous theoretical justification of the presented approaches.': 1.3862943649291992, 'Summary:': 1.3862943649291992, 'This paper proposes several regularization objective such as ""geometric regularizer"" and ""mode regularizer"" to stabilize the training of GAN models.': 1.3862943649291992, 'Specifically, these regularizes are proposed to alleviate the mode-missing behaviors of GANs.': 1.3862943649291992, 'Review:': 1.3862943649291992, 'I think this is an interesting paper that discusses the mode-missing behavior of GANs and proposes new evaluation metric to evaluate this behavior.': 1.3862943649291992, 'However, the core ideas of this paper are not very innovative to me.': 1.3862943649291992, 'Specifically, there has been a lot of papers that combine GAN with an autoencoder and the settings of this paper is very similar to the other papers such as Larsen et al.': 1.3862943649291992, 'As I pointed out in my pre-review comments, in the Larsen et al.': 1.386274814605713, 'both the geometric regularizer and model regularizer has been proposed in the context of VAEs and the way they are used is essentially the same as this paper.': 1.3862441778182983, 'I understand the argument of the authors that the VAEGAN is a VAE that is regularized by GAN and in this paper the main generative model is a GAN that is regularized by an autoencoder, but at the end of the day, both the models are combining the autoencoder and GAN in a pretty much same way, and to me the resulting model is not very different.': 1.1086108684539795, 'I also understand the other argument of the authors that Larsen et al is using VAE while this paper is using an autoencoder, but I am still not convinced how this paper outperforms the VAEGAN by just removing the KL term of the VAE.': 1.1621497869491577, 'I do like that this paper looks at the autoencoder objective as a way to alleviate the missing mode problem of GANs, but I think that alone does not have enough originality to carry the paper.': 1.3334547281265259, 'As pointed out in the public comments by other people, I also suggest that the authors do an extensive comparison of this work and Larsen et al.': 0.7219218611717224, 'in terms of missing mode, sample quality and quantitative performances such as inception score.': 1.3862941265106201, 'The authors identify two very valid problem of mode-missing in Generative Adversarial Networks, explain their intuitions as to why these problems occur and propose ways to remedy it.': 1.3862943649291992, 'The first problem is about the discriminator becoming too good (close to 0 on fake, and 1 on real data) and providing 0 gradients to the generator.': 1.3862943649291992, 'The second problem is that GANs are prone to missing modes of the data generating distribution entirely.': 1.3862943649291992, 'The authors propose two regularization techniques to address these problems: Geometric Metrics Regularizer and Mode Regularizer': 1.3862943649291992, 'Overall, I felt that this is a good paper, providing a good analysis of the problems and proposing sensible solutions - if lacking solid from-first-principles motivation for the particular choices made.': 1.3862943649291992, 'My other critique is the focus on manifolds, almost completely disregarding the probability density on the manifold - see my detailed comment below.': 1.3862943649291992, 'Detailed comments on the Geometric Metrics Regularizer: The motivation for this is to provide a way to measure and penalize distance between two degenerate probability distributions concentrated on non-overlapping manifolds, those of the generator and of the real data.': 1.3862943649291992, 'There are different ways one could go about measuring difference between two manifolds or probability distributions concentrated on manifolds, for example:': 1.3862943649291992, 'projection heuristic: measure the average distance between each point x on manifold A and the corresponding nearest point on manifold B (let’s call it the projection of x onto B).': 1.3862943649291992, 'earth mover’s distance: establish a smooth mapping between the two manifolds that maps denser areas on manifold A to nearby denser areas of manifold B, and measure the average distance between corresponding pairs.': 1.3862943649291992, 'The two heuristics are similar but while the earth mover distance is a divergence measure for distributions, the projection heuristic only measures the divergence of the manifolds, disregarding the distributions in question.': 1.3862943649291992, 'The authors propose measuring the average distance between a point on the real data manifold and a point it gets mapped to by the composition of the encoder and the generator.': 1.3862943649291992, 'While E○G will map to the generative manifold, it is unclear to me if they would map to a high-probability region on that manifold, so this probably doesn’t implement anything like Earth Mover’s Distance.': 1.3862943649291992, 'On this note, I have just remembered seeing this before: https://github.com/danielvarga/earth-moving-generative-net As the encoder is trained so that E○G(x) is close to x on average, it feels like a variant of the projection heuristic above.': 1.3862943649291992, 'Would the authors agree with this assessment?': 1.3862943649291992, 'This paper does a good job of clearly articulating a problem in contemporary training of GANs, coming up with an intuitive solution via regularizers in addition to optimizing only the discriminator score, and conducting clever experiments to show that the regularizers have the intended effect.': 1.3862943649291992, 'There are recent related and improved GAN variants (ALI, VAEGAN, potentially others), which are included in qualitative comparisons, but not quantitative.': 1.3862943649291992, 'It would be interesting to see whether these other types of modified GANs already make some progress in addressing the missing modes problem.': 1.3862943649291992, 'If code is available for those methods, the paper could be strengthened a lot by running the mode-missing benchmarks on them (even if it turns out that a ""competing"" method can get a better result in some cases).': 1.3862870931625366, 'The experiments on digits and faces are good for validating the proposed regularizers.': 1.3862943649291992, 'However, if the authors can show better results on CIFAR-10, ImageNet, MS-COCO or some other more diverse and challenging dataset, I would be more convinced of the value of the proposed method.': 1.3862943649291992}"
165,https://openreview.net/forum?id=HJOZBvcel,"{'I sincerely apologize for the late-arriving review.': 1.0986123085021973, 'This paper proposes to frame the problem of structure estimation as a supervised classification problem.': 1.0986123085021973, 'The input is an empirical covariance matrix of the observed data, the output the binary decision whether or not two variables share a link.': 1.095534086227417, 'The paper is sufficiently clear, the goals are clear and everything is well described.': 1.0986123085021973, 'The main interesting point is the empirical results of the experimental section.': 1.0986123085021973, 'The approach is simple and performs better than previous non-learning based methods.': 1.0986123085021973, 'This observation is interesting and will be of interest in structure discovery problems.': 1.0986123085021973, 'I rate the specific construction of the supervised learning method as a reasonable attempt attempt to approach this problem.': 0.8387285470962524, 'There is not very much technical novelty in this part.': 1.0986123085021973, 'E.g., an algorithmic contribution would have been a method that is invariant to data permutation could have been a possible target for a technical contribution.': 1.0986123085021973, 'The paper makes no claims on this technical part, as said, the method is well constructed and well executed.': 1.098482370376587, 'It is good to precisely state the theoretical parts of a paper, the authors do this well.': 1.0985926389694214, 'All results are rather straight-forward, I like that the claims are written down, but there is little surprise in the statements.': 1.0986123085021973, 'In summary, the paper makes a very interesting observation.': 0.7382553815841675, 'Graph estimation can be posed as a supervised learning problem and training data from a separate source is sufficient to learn structure in novel and unseen test data from a new source.': 1.0986123085021973, 'Practically this may be relevant, on one hand the empirical results are stronger with this method, on the other hand a practitioner who is interested in structural discovery may have side constraints about interpretability of the deriving method.': 0.18565429747104645, 'From the Discussion and Conclusion I understand that the authors consider this as future work.': 0.34772539138793945, 'It is a good first step, it could be stronger but also stands on its own already.': 1.098591923713684, 'The paper proposes a novel algorithm to estimate graph structures by using a convolutional neural network to approximate the function that maps from empirical covariance matrix to the sparsity pattern of the graph.': 1.094675898551941, 'Compared with existing approaches, the new algorithm can adapt to different network structures, e.g. small-world networks, better under the same empirical risk minimization framework.': 1.0929985046386719, 'Experiments on synthetic and real-world datasets show promising results compared with baselines.': 1.0794737339019775, 'In general, I think it is an interesting and novel paper.': 1.0986121892929077, 'The idea of framing structure estimation as a learning problem is especially interesting and may inspire further research on related topics.': 1.071457028388977, 'The advantage of such an approach is that it allows easier adaptation to different network structure properties without designing specific regularization terms as in graph lasso.': 1.0986119508743286, 'The experiment results are also promising.': 1.0986123085021973, 'In both synthetic and real-world datasets, the proposed algorithm outperforms other baselines in the small sample region.': 1.0986015796661377, 'However, the paper can be made clearer in describing the network architectures.': 1.0867531299591064, 'For example, in page 5, each o^k_{i,j} is said be a d-dimensional vector.': 1.0986123085021973, 'But from the context, it seems o^k_{i,j} is a scalar (from o^0_{i,j} = p_{i,j}).': 1.0985591411590576, 'It is not clear what o^k_{i,j} is exactly and what d is.': 1.0645383596420288, 'Is it the number of channels for the convolutional filters?': 1.0985969305038452, 'Figure 1 is also quite confusing.': 1.0986123085021973, 'Why in (b) the table is 16 x 16 whereas in (a) there are only six nodes?': 1.0986121892929077, 'And from the figure, it seems there is only one channel in each layer?': 1.0986123085021973, 'What do the black squares represent and why are there three blocks of them.': 1.0986123085021973, 'There are some descriptions in the text, but it is still not clear what they mean exactly.': 1.0986111164093018, 'For real-world data, how are the training data (Y, Sigma) generated?': 1.0986123085021973, 'Are they generated in the same way as in the synthetic experiments where the entries are uniformly sparse?': 1.0986121892929077, 'This is also related to the more general question of how to sample from the distribution P, in the case of real-world data.': 1.0986120700836182, 'This paper proposes a new method for learning graphical models.': 1.0986123085021973, 'Combined with a neural network architecture, some sparse edge structure is estimated via sampling methods.': 1.0982064008712769, 'In introduction, the authors say that a problem in graphical lasso is model selection.': 0.877579927444458, 'However, the proposed method still implicitly includes model selection.': 1.0985920429229736, 'In the proposed method,  is a sparse prior, and should include some hyper-parameters.': 0.996726393699646, 'How do you tune the hyper-parameters?': 1.0799107551574707, 'Is this tuning an equivalent problem to model section?': 1.0986123085021973, 'Therefore, I do not understand real advantage of this method over previous methods.': 1.0981584787368774, 'What is the advantage of the proposed method?': 1.0986123085021973, 'Another concern is that this paper is unorganized.': 1.0986123085021973, 'In Algorithm 1, first, G_i and \\Sigma_i are sampled, and then x_j is sampled from N(0, \\Sigma).': 1.0986096858978271, 'Here, what is \\Sigma?': 1.0986123085021973, 'Is it different from \\Sigma_i?': 1.0986123085021973, 'Furthermore, how do you construct (Y_i, \\hat{\\Sigma}_i) from (G_i, X_i )?': 1.0986123085021973, 'Finally, I have a simple question: Where is input data X (not sampled data) is used in Algorithm 1?': 1.0986123085021973, 'What is the definition of the receptive field in Proposition 2 and Proposition 3?': 1.0986123085021973}"
166,https://openreview.net/forum?id=HJPmdP9le,"{'Summary: This paper proposes a read-again attention-based representation of the document with the copy mechanism for the summarization task. The model reads each sentence in the input document twice and creates a hierarchical representation of it instead of a bidirectional RNN. During the decoding, it uses the representation of the document obtained via the read-again mechanism and points the words that are OOV in the source document. The model does abstractive summarization. The authors show improvements on DUC 2004 dataset and provide an analysis of their model with different configurations.': 1.0986123085021973, 'Contributions:': 1.0986123085021973, 'The main contribution of this paper is the read-again attention mechanism where the model reads the same sentence twice and obtains a better representation of the document.': 1.0986121892929077, 'Writing:': 1.0985804796218872, 'The text of this paper needs more work.': 1.0962568521499634, 'There are several typos and the explanations of the model/architecture are not really clear, some parts of the paper feel somewhat bloated.': 1.0983922481536865, 'Pros:': 1.0986120700836182, 'The proposed model is a simple extension to the model to the model proposed in [2] for summarization.': 1.097862958908081, 'The results are better than the baselines.': 1.0986100435256958, 'Cons:': 1.0986123085021973, 'The improvements are not that large.': 1.0830638408660889, 'Justifications are not strong enough.': 1.0908019542694092, 'The paper needs a better writeup.': 1.0540663003921509, 'Several parts of the text are not using a clear/precise language and the paper needs a better reorganization.': 1.0986120700836182, 'Some parts of the text is somewhat informal.': 1.0977834463119507, 'The paper is very application oriented.': 1.0986078977584839, 'Question:': 1.0986123085021973, 'How does the training speed when compared to the regular LSTM?': 1.0986123085021973, 'Some Criticisms:': 1.0986123085021973, 'A similar approach to the read again mechanism which is proposed in this paper has already been explored in [1] in the context of algorithmic learning and I wouldn’t consider the application of that on the summarization task a significant contribution.': 1.0986123085021973, 'The justification behind the read-again mechanism proposed in this paper is very weak.': 1.0986123085021973, 'It is not really clear why additional gating alpha_i is needed for the read again stage.': 1.0986123085021973, 'As authors also suggest, pointer mechanism for the unknown/rare words [2] and it is adopted for the read-again attention mechanism.': 1.0986123085021973, 'However, in the paper, it is not clear where the real is the gain coming from, whether from “read-again” mechanism or the use of “pointing”.': 1.0986123085021973, 'The paper is very application focused, the contributions of the paper in terms of ML point of view is very weak.': 1.0986123085021973, 'It is possible to try this read-again mechanism on more tasks other than summarization, such as NMT, in order to see whether if those improvements are': 1.0986123085021973, 'The writing of this paper needs more work.': 1.0986123085021973, 'In general, it is not very well-written.': 1.0986123085021973, 'Minor comments:': 1.0986123085021973, 'Some of the corrections that I would recommend fixing,': 1.0986123085021973, 'On page 4: “… better than a single value … ” —> “… scalar gating …”': 1.0986123085021973, 'On page 4: “… single value lacks the ability to model the variances among these dimensions.”': 1.0986123085021973, '—> “… scalar gating couldn’t capture the ….”': 1.0750483274459839, 'On page 6: “ … where h_0^2 and h_0^\'2 are initial zero vectors … “ —> “… h_0^2 and h_0^\'2 are initialized to a zero vector in the beginning of each sequence …""': 1.0796310901641846, 'There are some inconsistencies for example parts of the paper refer to Tab.': 0.9038292169570923, '1 and some parts of the paper refer to Table 2.': 0.4470563530921936, 'Better naming of the models in Table 1 is needed.': 1.0986123085021973, 'The location of Table 1 is a bit off.': 0.40954965353012085, '[1] Zaremba, Wojciech, and Ilya Sutskever.': 0.9476450681686401, '""Reinforcement learning neural Turing machines."" arXiv preprint arXiv:1505.00521 362 (2015).': 1.0060149431228638, '[2] Gulcehre, Caglar, et al.': 1.0961670875549316, '""Pointing the Unknown Words."" arXiv preprint arXiv:1603.08148 (2016).': 1.0986121892929077, 'This paper proposed two incremental ideas to extend the current state-of-the-art summarization work based on seq2seq models with attention and copy/pointer mechanisms.': 1.0985981225967407, '1. This paper introduces 2-pass reading where the representations from the 1st-pass is used to  re-wight the contribution of each word to the sequential representation of the 2nd-pass. The authors described how such a so-called read-again process applies to both GRU and LSTM.': 0.3947254419326782, '2. On the decoder side, the authors also use the softmax to choose between generating from decoder vocabulary and copying a source position, with a new twist of representing the previous decoded word Y_{t-1} differently. This allows the author to explore a smaller decoder vocabulary hence led to faster inference time without losing summarization performance.': 0.9817759394645691, 'This paper claims the new state-of-the-art on DUC2004': 1.0980234146118164, 'but the comparison on Gigaword seems to be incomplete (missing more recent results after Rush 2015 etc).': 0.6339966058731079, 'While the overall work is solid, there are also other things missing scientifically.': 1.0885506868362427, 'For example,': 1.0986123085021973, 'how much computational costs does the 2nd pass reading add to the end-to-end system?': 1.0986123085021973, 'How does the decoder small vocabulary trick work without 2nd-pass reading on the encoder side for both summarization performance and runtime speed?': 1.0986123085021973, 'There are other ways to improve the embedding of a sentence.': 1.0986123085021973, 'How does the 2nd-pass reading compare to recent work from multiple authors on self-attention and/or LSTMN?': 1.0986123085021973, 'For example, Cheng et al. 2016, Long Short-Term Memory-Networks for Machine Reading; Parikh et al. 2016, A Decomposable Attention Model for Natural Language Inference?': 1.0986123085021973, 'This work explores the neural models for sentence summarisation by using a read-again attention model and a copy mechanism which grants the ability of direct copying word representations from the source sentences.': 1.0986045598983765, 'The experiments demonstrate the model achieved better results on DUC dataset.': 1.0986123085021973, 'Overall, this paper is not well-written.': 1.0986123085021973, 'There are confusing points, some of the claims are lack of evidence and the experimental results are incomplete.': 1.0986123085021973, 'Detailed comments:': 1.0986123085021973, 'Read-again attention.': 1.0986123085021973, 'How does it work better than a vanilla attention?': 1.0986123085021973, 'What would happen if you read the same sentences multiple times?': 1.0986123085021973, 'Have you compared it with staked LSTM (with same number of parameters)?': 1.0986123085021973, 'There is no model ablation in the experiment section.': 1.0986123085021973, 'Why do you need reading two sentences?': 1.046452522277832, 'The Gigaword dataset is a source-to-compression dataset which does not need multiple input sentences.': 1.0986123085021973, 'How do you compare your model with single sent input and two sent input?': 1.0986123085021973, 'Copy mechanism.': 1.0986123085021973, 'What if there are multiple same words appeared in the source sentences to be copied?': 1.0986123085021973, 'According to equation (5), you only copy one vector to the decoder.': 1.0986123085021973, 'However, there is no this kind of issue for a hard copy mechanism.': 1.0986123085021973, 'Besides, there is no comparison between the hard copy mechanism and this vector copy mechanism in the experiment section': 1.0986123085021973, 'Vocabulary size.': 1.0986123085021973, 'This part is a bit off the main track of this paper.': 1.0986123085021973, 'If there is no evidence showing this is the special property of vector copy mechanism, it would be trivial in this paper.': 1.0986123085021973, 'Experiments.': 1.0986123085021973, 'On the DUC dataset, it compares the model with other up-to-date models, while on the Gigaword dataset paper only compares the model with the ABS Rush et al. (2015) and the GRU (?), which are quite weak baseline models.': 1.0986111164093018, 'It is irresponsible to claim this model achieved the state-of-the-art performance in the context of summarization.': 0.7436993718147278, 'Typos: (1) Tab.': 1.0986123085021973, '1. -> Table 1. (2) Fig. 3.1.2.?': 1.0986123085021973}"
167,https://openreview.net/forum?id=HJSCGD9ex,"{'This paper discusses multi-sense embedddings and proposes learning those by using aligned text across languages.': 1.0986123085021973, 'Further, the paper suggests that adding more languages helps improve word sense disambiguation (as some ambiguities can be carried across language pairs).': 1.0986123085021973, ""While this idea in itself isn't new, the authors propose a particular setup for learning multi-sense embeddings by exploiting multilingual data."": 1.0986123085021973, 'Broadly this is fine, but unfortunately the paper then falls short in a number of ways.': 0.9521154165267944, 'For one, the model section is unnecessarily convoluted for what is a nice idea that could be described in a far more concise fashion.': 1.0986123085021973, 'Next (and more importantly), comparison with other work is lacking to such an extent that it is impossible to evaluate the merits of the proposed model in an objective fashion.': 1.0986120700836182, 'This paper could be a lot stronger if the learned embeddings were evaluated in downstream tasks and evaluated against other published methods.': 1.0986123085021973, ""In the current version there is too little of this, leaving us with mostly relative results between model variants and t-SNE plots that don't really add anything to the story."": 1.0986121892929077, 'this work aims to address representation of multi-sense words by exploiting multilingual context.': 1.0838886499404907, 'Experiments on word sense induction and word similarity in context show that the proposed solution improves over the baseline.': 1.0986123085021973, 'From a computational linguistics perspective, the fact that languages less similar to English help more is intriguing.': 1.0986058712005615, 'I see following problem with this work:': 0.5232563018798828, ""the paper is hard to follow and hard to see what's new compared to the baseline model"": 1.0885831117630005, '[1].': 1.0986123085021973, 'A paragraph of discussion should clearly compare and contrast with that work.': 1.0986123085021973, 'the proposed model is a slight variation of the previous work': 1.097777009010315, '[1] thus the experimental setup should be designed in a way so that we compare which part helps improvement and how much.': 0.6496593952178955, ""thus MONO has not been exposed the same training data and we can't be sure that the proposed model is better because MONO does not observe the data or lacks the computational power."": 1.0914201736450195, 'I suggest following baseline: turning multilingual data to monolingual one using the alignment, then train the baseline model[1] on this pseudo monolingual data.': 1.0986123085021973, 'the paper provides good benchmarks for intrinsic evaluation but the message could be conveyed more strongly if we see improvement in a downstream task.': 1.0458186864852905, '[1] https://arxiv.org/pdf/1502.07257v2.pdf': 1.0830180644989014, 'In this paper, the authors propose a Bayesian variant of the skipgram model to learn word embeddings.': 1.0986123085021973, 'There are two important variant compared to the original model.': 1.0986119508743286, 'First, aligned sentences from multiple languages are used to train the model.': 1.0981589555740356, 'Therefore, the context words of a given target word can be either from the same sentence, or from an aligned sentence in a different language.': 1.0976794958114624, 'This allows to learn multilingual embedding.': 1.0986123085021973, 'The second difference is that each word is represented by multiple vectors, one for each of its different senses.': 1.0986123085021973, 'A latent variable z models which sense should be used, given the context.': 1.0986117124557495, 'Overall, I believe that the idea of using a probabilistic model to capture polysemy is an interesting idea.': 1.0986104011535645, 'The model introduced in this paper is a nice generalization of the skipgram model in that direction.': 0.9948588013648987, 'However, I found the paper a bit hard to follow.': 0.49007996916770935, 'The formulation might probably be simplified (e.g. why not consider a target word w and a context c, where c is either in the source or target language.': 1.098488688468933, 'Since all factors are independent, this should not change the model much, and would make the presentation easier).': 1.0986123085021973, 'The performance of all models reported in Table 2 & 3 seem pretty low.': 1.0986123085021973, 'Overall, I like the main idea of the paper, which is to represent word senses by latent variables in a probabilistic model.': 1.0985954999923706, 'I feel that the method could be presented more clearly, which would make the paper much stronger.': 1.0982364416122437, 'I also have some concerns regarding the experimental results.': 1.0986123085021973, 'Pros:': 1.0910711288452148, 'Interesting extension of skipgram to capture polysemy.': 1.0986123085021973, 'Cons:': 1.067315697669983, 'The paper is not clearly written.': 1.0985386371612549, 'Results reported in the paper seems pretty low.': 1.0986123085021973}"
168,https://openreview.net/forum?id=HJStZKqel,"{'I think the paper is a bit more solid now': 1.0986123085021973, 'and I still stand by my positive review.': 1.0986123085021973, 'I do however agree with other reviewers that the tasks are very simple.': 1.0986123085021973, 'While NPI is trained with stronger supervision, it is able to learn quicksort perfectly as shown by Dawn Song and colleagues in this conference.': 1.0986123085021973, 'Reed et al had already demonstrated it for bubblesort.': 1.0986123085021973, 'If the programs are much shorter, it becomes easy to marginalise over latent variables (pointers) and solve the task end to end.': 1.0986123085021973, 'The failure to attack much longer combinatorial problems is my main complaint about this paper, because it makes one feel that it is over-claiming.': 1.0986123085021973, 'In relation to the comments concerning NPI,  Reed et al freeze the weights of the core LSTM to then show that an LSTM with fixed weights can continue learning new programs that re-use the existing programs (ie the trained model can create new programs).': 1.0986123085021973, 'However, despite this criticism, I still think this is an excellent paper, illustrating the power of combining traditional programming with neural networks.': 1.0986123085021973, 'It is very promising and I would love to see it appear at ICLR.': 1.0986123085021973, '===========': 1.0986123085021973, 'This paper makes a valuable contribution to the emerging research area of learning programs from data.': 1.0986123085021973, 'The authors mix their TerpreT framework, which enables them to compile programs with finite integer variables to a (differentiable) TensorFlow graph,  and neural networks for perceiving simple images.': 1.0986123085021973, 'This is made possible through the use of simple tapes and arithmetic tasks.': 1.0986123085021973, 'In these arithmetic tasks, two networks are re-used, one for digits and one for arithmetic operations.': 1.0986123085021973, 'This clean setup enables the authors to demonstrate not only the avoidance of catastrophic interference, but in fact some reverse transfer.': 1.0986123085021973, 'Overall, this is a very elegant and potentially very useful way to combine symbolic programming with neural networks.': 1.0986123085021973, 'As a full-fledged tool, it could become very useful.': 1.0986123085021973, 'Thus far it has only been demonstrated on very simple examples.': 1.0986123085021973, 'It would be nice for instance to see it demonstrated in all the tasks introduced in other approaches to neural programming and induction: sorting, image manipulation, semantic parsing, question answering.': 1.0986123085021973, 'Hopefully, the authors will release neural TerpreT to further advance research in this domain.': 1.0960499048233032, 'This paper proposes an extension of TerpreT by adding a set of functions that can deal with inputs in the form of tensor with continuous values.': 1.0985928773880005, 'This potentially allows TerpreT to learn programs over images or other “natural” sources.': 1.0876147747039795, 'TerpreT generates a source code from a set of input/output examples.': 0.5014773011207581, 'The code is generate in the form of a TensorFlow computation graph based on a set of simple and elegant program representations.': 0.8531080484390259, 'One of the limitation of TerpreT is the type of inputs it can work with, this work aim at enriching it by adding “learnable functions” that can deal with more complex input variables.': 1.0986120700836182, 'While I really like this direction of research and the development of TerpreT, I find the contribution of this work to be a bit limited.': 1.09859299659729, 'This would have been fine if it was supported by a strong and convincing experimental section, but unfortunately, the experimental section is a bit weak: the tasks studied are relatively simple and the baselines are not very strong.': 1.0986078977584839, 'For example let us consider the SUM2x2 problem:  all the images of digits are from MNIST, which can be classify with an error of 8% with a linear model (and even better with neural networks), There is also a linear model that given 4 numbers will compute the 2x2sum of them that is: y=Ax where x is the vector containing the 4 numbers and A =': 1.098597764968872, '[1 0 1 0;1 1 0 0;1 0 0 1; 0 1 0 1].': 1.0986121892929077, 'This means a succession of two linear models can solve the sum2x2 problems with little trouble.': 1.0986123085021973, ""While I'm aware that this work aims at automatically finding the combination of simple models to achieve this task end-to-end, the fact that the solution is a set of 2 consecutive linear models makes it a bit too simple in my humble opinion."": 1.0986117124557495, 'Overall, I think that this paper proposes a promising extension of TerpreT that is unfortunately not backed by experiments that are convincing enough.': 1.0986123085021973, 'The authors explore the idea of life-long learning in the context of program generation.': 1.0986123085021973, 'The main weakness of this paper is that it mixes a few issues without showing strong results on any of them.': 1.0986123085021973, 'The test tasks are about program generation, but these are toy tasks even by the low standards of deep-learning for program generation (except for the MATH task, they are limited to 2x2 grid).': 1.0986123085021973, 'Even on MATH, the authors train and discuss generalization from 2-digit expressions': 1.0986123085021973, 'these are very short, so the conclusiveness of the experiment is unclear.': 1.0986123085021973, 'The main point of the paper is supposed to be transfer learning though.': 1.0986123085021973, 'Unluckily, the authors do not compare to other transfer learning models (e.g., ""Progressive Neural Networks"") nor do they test on tasks that were previously used by others.': 1.0986123085021973, 'We find that only testing on a newly-created task with a weak baseline is not sufficient for ICLR acceptance.': 1.0986123085021973, ""After clarifying comments from the authors and more experiments (see the discussion above), I'm now convinced that the authors mostly measure overfitting, which in their model is prevented because the model is hand-fitted to the task."": 1.0986123085021973, 'While the idea might still be valid and interesting, many harder and much more diverse experiments are needed to verify it.': 1.0986123085021973, 'I consider this paper a clear rejection at present.': 1.0986123085021973}"
169,https://openreview.net/forum?id=HJTXaw9gx,"{'This paper presents an algorithm for approximating the solution of certain time-evolution PDEs.': 1.0986123085021973, 'The paper presents an interesting learning-based approach to solve such PDEs.': 1.0986123085021973, 'The idea is to alternate between:': 1.0986123085021973, '1. sampling points in space-time': 1.0986123085021973, '2. generating solution to PDE at ""those"" sampled points': 1.0986123085021973, '3. regressing a space-time function to satisfy the latter solutions at the sampled points (and hopefully generalize beyond those points).': 1.0986123085021973, 'I actually find the proposed algorithm interesting, and potentially useful in practice.': 1.0986123085021973, 'The classic grid-based simulation of PDEs is often too expensive to be practical, due to the curse of dimensionality.': 1.0986123085021973, 'Hence, learning the solution of PDEs makes a lot of sense for practical settings.': 1.0986123085021973, 'On the other hand, as the authors point out, simply running gradient descent on the regression loss function does not work, because of the non-differentiablity of the ""min"" that shows up in the studied PDEs.': 1.0986123085021973, 'Therefore, I think the proposed idea is actually very interesting approach to learning the PDE solution in presence of non-differentability, which is indeed a ""challenging"" setup for numerically solving PDEs.': 1.0986123085021973, 'The paper motivates the problem (time-evolution PDE with ""min"" operator applied to the spatial derivatives) by applications in control thery, but I think there is more direct interest in such problems for the machine learning community, and even deep learning community.': 1.0986123085021973, 'For example http://link.springer.com/chapter/10.1007/978-3-319-14612-6_4 studies approximate solution to PDEs with very similar properties (evolution+""min"") to develop new optimization algorithms.': 1.0986123085021973, 'The latter is indeed used to training deep networks: https://arxiv.org/abs/1601.04114': 1.0986123085021973, 'I think this work would catch even more attention if the authors could show some experiments with higher-dimensional problems (where grid-based methods are absolutely inapplicable).': 1.0986123085021973, 'Approximating solutions to PDEs with NN approximators is very hard.': 1.0986123085021973, ""In particular the HJB and HJI eqs have in general discontinuous and non-differentiable solutions making them particularly tricky (unless the underlying process is a diffusion in which case the Ito term makes everything smooth, but this paper doesn't do that)."": 1.0986123085021973, ""What's worse, there is no direct correlation between a small PDE residual and a well performing-policy [tsitsiklis?"": 1.0986123085021973, 'beard?': 1.0986123085021973, 'todorov?, I forget].': 1.0986123085021973, ""There's been lots of work on this which is not properly cited."": 1.0986123085021973, 'The 2D toy examples are inadequate.': 1.0986123085021973, 'What reason is there to think this will scale to do anything useful?': 1.0986123085021973, 'There are a bunch of typos (""Range-Kutta""?) .': 1.0986123085021973, 'More than anything, this paper is submitted to the wrong venue.': 1.0986123085021973, 'There are no learned representations here.': 1.0986123085021973, ""You're just using a NN."": 1.0986123085021973, ""That's not what ICLR is about."": 1.0986123085021973, 'Resubmit to ACC, ADPRL or CDC.': 1.0986123085021973, 'Sorry for terseness.': 1.0986123085021973, 'Despite rough review, I absolutely love this direction of research.': 1.0986123085021973, 'More than anything, you have to solve harder control problems for people to take notice...': 1.0986123085021973, ""I have no familiarity with the HJI PDE (I've only dealt with parabolic PDE's such as diffusion in the past)."": 0.8019906282424927, 'So the details of transforming this problem into a supervised loss escape me.': 0.4134012460708618, 'Therefore, as indicated below, my review should be taken as an ""educated guess"".': 1.0986047983169556, 'I imagine that many readers of ICLR will face a similar problem as me, and so, if this paper is accepted, at the least the authors should prepare an appendix that provides an introduction to the HJI PDE.': 1.029564380645752, 'At a high level, my comments are:': 1.0986123085021973, '1. It seems that another disadvantage of this approach is that a new network must be trained for each new domain (including domain size), system function f(x) or boundary condition. If that is correct, I wonder if it\'s worth the trouble when existing tools already solve these PDE\'s. Can the authors shed light on a more ""unifying approach"" that would require minimal changes to generalize across PDE\'s?': 1.0986045598983765, ""2. How sensitive is the network's result to domains of different sizes? It seems only a single size 51 x 51 was tested. Do errors increase with domain size?"": 0.9410644173622131, ""3. How general is this approach to PDE's of other types e.g. diffusion?"": 1.0658279657363892}"
170,https://openreview.net/forum?id=HJTzHtqee,"{'This paper proposed a compare-aggregate model for the NLP tasks that require semantically comparing the text sequences, such as question answering and textual entailment.': 1.0986123085021973, 'The basic framework of this model is to apply a convolutional neural network (aggregation) after a element-wise operation (comparison) over the attentive outputs of the LSTMs.': 1.0986123085021973, 'The highlighted part is the comparison, where this paper compares several different methods for matching text sequences, and the element-wise subtraction/multiplication operations are demonstrated to achieve generally better performance on four different datasets.': 1.0986123085021973, 'While the weak point is that this is an incremental work and a bit lack of innovation.': 1.0986123085021973, 'A qualitative evaluation about how subtraction, multiplication and other comparison functions perform on varied kinds of sentences would be more interesting.': 1.0986123085021973, 'This paper proposes a compare-aggregate framework that performs word-level matching followed by aggregation with convolutional neural networks.': 1.098591685295105, 'It compares six different comparison functions and evaluates them on four datasets.': 1.0986067056655884, 'Extensive experimental results have been reported and compared against various published baselines.': 1.0982223749160767, 'The paper is well written overall.': 0.9731838703155518, 'A few detailed comments:': 1.0986123085021973, '* page 4, line5: including a some -> including some': 1.0979686975479126, '*': 1.0931532382965088, ""What's the benefit of the preprocessing and attention step?"": 1.0934076309204102, 'Can you provide the results without it?': 1.0986123085021973, '* Figure 2 is hard to read, esp.': 1.0670461654663086, 'when on printed hard copy.': 1.0496270656585693, 'Please enhance the quality.': 1.0986114740371704, 'The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way.': 1.0986123085021973, 'In the approach, soft attention is first used to derive alignments between the tokens of the two texts, then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed.': 1.0986123085021973, 'The paper both presents this as an overall modeling strategy that can be made to work quite well, and offers a detailed empirical analysis of the comparison component of the model.': 1.098611831665039, 'This work is timely.': 1.0985832214355469, 'Language understanding problems of this kind are a major open issue in NLP, and are just at the threshold of being addressable with representation learning methods.': 1.0985755920410156, 'The work presents a general approach which is straightforward and reasonable, and shows that it can yield good results.': 1.0986121892929077, ""The work borders on incremental (relative to their earlier work or that of Parikh et al.), but it contributes in enough substantial ways that I'd strongly recommend acceptance."": 1.0986123085021973, 'Detail:': 1.0986123085021973, 'The model, at least as implemented for the problems with longer sequences (everything but SNLI), is not sensitive to word order.': 1.0797423124313354, 'It is empirically competitive, but this insensitivity places a strong upper bound on its performance.': 1.0986096858978271, 'The paper does make this clear, but it seems salient enough to warrant a brief mention in the introduction or discussion sections.': 1.0967885255813599, ""If I understand correctly, your attention strategy is based more closely on the general/bilinear strategy of Luong et al. '15 than it is on the earlier Bahdanau work."": 1.0986123085021973, 'You should probably cite the former (or some other more directly relevant reference for that strategy).': 1.0986123085021973, 'Since the NTN risks overfitting because of its large number of parameters, did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)?': 1.0986123085021973, 'You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite.': 1.0986123085021973, 'Is there a reason you use the same parameters for preprocessing the question and answer in (1)?': 1.0986123085021973, 'These could require different things to be weighted highly.': 1.0986123085021973}"
171,https://openreview.net/forum?id=HJV1zP5xg,"{'The paper addresses an important problem - namely on how to improve diversity in responses.': 1.0986121892929077, 'It is applaudable that the authors show results on several tasks showing the applicability across different problems.': 1.0986123085021973, 'In my view there are two weaknesses at this point': 1.0986123085021973, '1) the improvements (for essentially all tasks) seem rather minor and do not really fit the overall claim of the paper': 1.09739351272583, '2) the approach seems quite ad hoc and it unclear to me if this is something that will and should be widely adopted.': 0.6284419298171997, 'Having said this the gist of the proposed solution seems interesting but somewhat premature.': 1.0986123085021973, '[ Summary ]': 1.0986123085021973, 'This paper presents a new modified beam search algorithm that promotes diverse beam candidates.': 1.0663422346115112, 'It is a well known problem —with both RNNs and also non-neural language models— that beam search tends to generate beam candidates that are very similar with each other, which can cause two separate but related problems: (1) search error: beam search may not be able to discover a globally optimal solution as they can easily fall out of the beam early on, (2) simple, common, non-diverse output: the resulting output text tends to be generic and common.': 1.0986082553863525, 'This paper aims to address the second problem (2) by modifying the search objective function itself so that there is a distinct term that scores diversity among the beam candidates.': 1.0986121892929077, 'In other words, the goal of the presented algorithm is not to reduce the search error of the original objective function.': 0.46391430497169495, 'In contrast, stack decoding and future cost estimation, common practices in phrase-based SMT, aim to address the search error problem.': 1.0986053943634033, '[ Merits ]': 1.0648469924926758, 'I think the Diverse Beam Search (DBS) algorithm proposed by the authors has some merits.': 1.0985792875289917, 'It may be useful when we cannot rely on traditional beam search on the original objective function either because the trained model is not strong enough, or because of the search error, or because the objective itself does not align with the goal of the application.': 1.0985080003738403, '[ Weaknesses ]': 1.097627878189087, 'It is however not entirely clear how the proposed method compares against more traditional approaches like stack decoding and future cost estimation, on tasks like machine translation, as the authors compare their algorithm mainly against L&J’s diverse LM models and simple beam search.': 1.0986123085021973, 'In fact, modification to the objective function has been applied even in the neural MT context.': 1.0986123085021973, 'For example, see equation (14) in page 12 of the following paper:': 1.0986123085021973, '""Google\'s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"" (https://arxiv.org/pdf/1609.08144v2.pdf)': 1.0986123085021973, 'where the attention coverage term serves a role similar to stack decoding (though unlike stack decoding, the objective term is entirely re-defined, more similarly to DBS proposed in this work), and the length penalty may have an effect that indirectly promotes more informative (thus more likely diverse) responses.': 1.0986123085021973, 'Comparison against these existing algorithms would make the proposed work more complete.': 1.0986123085021973, 'Also, I have a mixed feeling about computing and reporting only *oracle* BLUE, CIDEr, METEOR, etc.': 1.0986123085021973, 'Especially given how these oracle scores are very close to each other, and that developing a high performing ranking has not been addressed in this work (and that doing so must be not all that trivial), I’m somewhat skeptical how much of DBS results make a practical difference.': 1.0986123085021973, '****': 1.0986123085021973, '[Update after the author responses] ****': 1.0986123085021973, 'The authors addressed some of my concerns by adding a new baseline comparison against Wu et al. 2016.': 1.0986123085021973, 'Thus I will raise my score to 6.': 1.0986123085021973, 'This paper considers the problem of decoding diverge solutions from neural sequence models.': 1.0986123085021973, 'It basically adds an additional term to the log-likelihood of standard neural sequence models, and this additional term will encourage the solutions to be diverse.': 1.0986123085021973, 'In addition to solve the inference, this paper uses a modified beam search.': 1.0986123085021973, 'On the plus side, there is not much work on producing diverse solutions in RNN/LSTM models.': 1.0986123085021973, 'This paper represents one of the few works on this topic.': 1.0986123085021973, 'And this paper is well-written and easy to follow.': 1.0986123085021973, 'The novel of this paper is relatively small.': 1.0986123085021973, 'There has been a lot of prior work on producing diverse models in the area of probailistic graphical models.': 1.0986123085021973, 'Most of them introduce an additional term in the objective function to encourage diversity.': 1.0986123085021973, 'From that perspective, the solution proposed in this paper is not that different from previous work.': 1.0986123085021973, 'Of course, one can argue that most previous work focues on probabilistic graphical models, while this paper focuses on RNN/LSTM models.': 1.0986123085021973, 'But since RNN/LSTM can be simply interpreted as a probabilistic model, I would consider it a small novelty.': 1.0986123085021973, 'The diverse beam search seems to straightforward, i.e. it partitions the beam search space into groups, and does not consider the diversity within group (in order to reduce the search space).': 1.0986123085021973, 'To me, this seems to be a simple trick.': 1.0986123085021973, 'Note most previous work on diverse solutions in probabilistic graphical models usually involve developing some nontrivial algorithmic solutions, e.g. in order to achieve efficiency.': 1.0986123085021973, 'In comparison, the proposed solution in this paper seems to be simplistic for a paper.': 1.0986123085021973, 'The experimental results how improvement over previous methods (Li & Jurafsky, 2015, 2016).': 1.0986123085021973, ""But it is hard to say how rigorous the comparisons are, since they are based on the authors' own implementation of (Li & Jurasky, 2015, 2016)."": 1.0986123085021973, 'update: given that the authors made the code available (I do hope the code will remain publicly available), this has alleviated some of my concerns about the rigor of the experiments.': 1.0986123085021973, 'I will raise my rate to 6.': 1.0986123085021973}"
172,https://openreview.net/forum?id=HJWHIKqgl,"{'This paper provides an interesting idea to use the optimized MMD for generative model evaluation and learning.': 1.0986028909683228, 'Starting from the test power, the authors justified the criterion.': 1.0985171794891357, 'Moreover, they also provided an efficient implementation of perturbation tests for empirical MMD.': 1.0986100435256958, 'Pros:': 1.0986123085021973, '1)': 1.0986123085021973, 'The criterion is principled which is derived from the test power.': 0.9198046326637268, '2) The criterion can be used to detect the difference template by incorporating ARD technique.': 0.40478941798210144, '3) By exploiting kernel in the objective, the generated algorithm, t-GMMN, training can be improved from the GMMN.': 0.590770959854126, 'Cons:': 1.098533034324646, '1) How to train the provided t objective is not clear.': 1.098446249961853, '2)': 1.0986123085021973, 'The algorithm is only tested on MNIST dataset as model criticism and learning objective.': 1.0986104011535645, 'Comprehensive empirical comparison to the state-of-the-art criteria, e.g., log-likelihood, and other learning objectives is missing.': 1.0986123085021973, 'A well written paper that proposes to use MMD to distinguish generated and reference data.': 0.9522018432617188, 'The primary contribution of this paper is to derive a way to optimize the MMD kernels to maximize the test power of the two sample test.': 1.0984498262405396, 'Pros': 1.0986123085021973, 'Principled approach; derivations start from first principles and the theoretical results will probably be applicable to other applications of two sample tests.': 1.0304222106933594, 'Well written; puts the contributions and related approaches into context and explains connections to previous work; especially to GANs.': 1.070928931236267, 'Cons: I don’t expect that this work will have a big impact in the field:': 1.0201842784881592, 'The two sample test are still quadratic in the number of samples.': 1.0986120700836182, 'Experiments only on toy data sets and on binarized MNIST': 1.0983837842941284, 'It would be interesting to know in what way this approach fails on e.g. image data (or other complex, high dimensional data where neural network generalize well).': 0.38682758808135986, 'I could imagine that the neural network based discriminators in GANs generalize better than kernel based MMD methods.': 1.0922082662582397, 'I would like to see follow up work that investigates this in more detail (and potentially profes my intuition wrong).': 0.9691939949989319, 'This is an interesting paper containing three contributions:': 1.0986123085021973, '1) An expression for the variance of the quadratic-time MMD estimate, which can be efficiently minimized for kernel selection.': 0.40566161274909973, '2) Advanced computational optimizations for permutation tests for the quadratic MMD statistic.': 1.0986114740371704, '3) Crystal-clear examples on the importance of reducing the variance in two-sample testing (Figure 2)': 0.47300463914871216, 'Regarding my criticisms,': 1.0971771478652954, 'The conceptual advances in this submission are modest: the use of MMD to train generative models, as well as the importance of variance-reduction in MMD were already known.': 1.0986123085021973, '2) The quadratic-time MMD test may lead to a discriminator that is ""too good"" in practical applications.': 1.0853500366210938, 'Since MMD quickly picks up on pixel-level artifacts, I wonder if its use would be possible to train generators properly on realistic (non-binarized) data.': 1.0986123085021973, 'This of course could be addressed by regularizing (smoothing) the kernel bandwidths, and for sure raises an interesting question/trade-off in generative modeling.': 1.0986119508743286, 'Overall, the submission is technically sound and well-written: I recommend it for publication in ICLR 2017.': 1.0985180139541626}"
173,https://openreview.net/forum?id=HJWzXsKxx,"{'The findings of applying sparsity in the backward gradients for training LSTMs is interesting.': 1.0986123085021973, 'But the paper seems incomplete without the proper experimental justification.': 1.082101583480835, 'Only the validation loss is reported which is definitely insufficient.': 0.4950381815433502, 'Proper testing results and commonly reported evaluation criterion needs to be included to support the claim of no degradation when applying the proposed sparsity technique.': 1.0986123085021973, 'Also actual justification of the gains in terms of speed and efficiency would make the paper much stronger.': 1.0986123085021973, 'This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy.': 1.0986123085021973, 'This observation is novel (although not too surprising) to my knowledge, but I must state that I am not very familiar with research on fast RNN implmentations.': 1.0986123085021973, 'Minor note:': 1.0986123085021973, ""The LSTM language model does not use a 'word2vec' layer."": 1.0986121892929077, 'It is simply a linear embedding layer.': 1.0986123085021973, 'Word2vec is the name of a specific model which is not directly to character level language models.': 1.0986123085021973, 'The paper presents the central observation clearly.': 0.9157366156578064, 'However, it will be much more convincing if a well known dataset and experiment set up are used, such as Graves (2013) or Sutskever et al (2014), and actual training, validation and test performances are reported.': 1.0986117124557495, 'While the main observation is certainly interesting, I think it is not sufficient to be the subject of a full conference paper without implementation (or simulation) and benchmarking of the promised speedups on multiple tasks.': 1.0986123085021973, 'For example, how would the gains be affected by various architecture choices?': 1.098609447479248, 'At present this is an interesting technical report and I would like to see more detailed results in the future.': 1.0986014604568481, 'CONTRIBUTIONS': 0.9767148494720459, 'When training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin.': 1.098591685295105, 'This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model.': 1.098324179649353, 'The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training.': 0.11677498370409012, 'NOVELTY': 1.0986123085021973, 'Thresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge.': 1.0986123085021973, 'MISSING CITATIONS': 1.0986056327819824, 'Prior work has explored low-precision arithmetic for recurrent neural network language models:': 1.0986114740371704, 'Hubara et al, “Quantized Neural Networks: Training Neural Networks with': 0.13720107078552246, 'Low Precision Weights and Activations”, https://arxiv.org/abs/1609.07061': 0.7906909584999084, 'Ott et al, “Recurrent Neural Networks With Limited Numerical Precision”, https://arxiv.org/abs/1608.06902': 0.4099412262439728, 'Low-precision arithmetic for recurrent networks promises to improve both training and inference efficiency.': 1.098588228225708, 'How does the proposed sparsification method compare with low-precision arithmetic?': 1.0986123085021973, 'Are the ideas complementary?': 1.0986123085021973, 'EXPERIMENTS': 0.8305659294128418, 'The main experimental result of the paper (Section 4.1) is that training LSTM language models with sparse gradients does not affect convergence or final performance (Figure 5).': 1.084276556968689, 'This result is promising, but I do not think that this single experiment is enough to prove the utility of the proposed method.': 0.7477008104324341, 'I also have some problems with this experiment.': 1.097844123840332, 'Plotting validation loss for character-level language modeling is not a standard way to report results; it is much more typical to report bits-per-character or perplexity on a held-out test set.': 0.535381019115448, 'These experiments also lack sufficient details for replication.': 0.09547874331474304, 'What optimization algorithm, learning rate, and regularization were used?': 1.098227858543396, 'How were these hyperparameters chosen?': 1.0621421337127686, 'Is the “truncated Wikipedia dataset” used for training the standard text8 dataset?': 0.7615324854850769, 'In addition, the experiments do not compare with existing published results on this dataset.': 1.0981110334396362, 'In the OpenReview discussion, the authors remarked that the “The final validation loss for the sparsified model is [...] almost the same as the baseline.”': 0.9955325722694397, 'Comparing validation loss at the end of training is not the proper way to compare models.': 0.4829355478286743, 'From Figure 5, it is clear that all models achieve minimal validation loss after around 10k iterations, after which the validation losses increase, suggesting that the models have slightly overfit the training data by the end of training.': 1.0624570846557617, 'In Section 4.2 the authors claim to obtain similar experimental results with other network architectures, on other datasets (tiny-shakespeare and War and Peace), and for other tasks (image captioning and machine translation).': 1.0335932970046997, 'However, the details and results of these experiments are not included in the paper, making it difficult to assess the utility of the proposed method and the significance of the results.': 0.38472265005111694, 'ENERGY EFFICIENCY AND TRAINING SPEED': 0.41087496280670166, 'One of the main claims of the paper is that sparse gradients can be exploited in hardware to reduce the training speed and improve the energy efficiency of recurrent network training, but these benefits are neither quantified nor demonstrated experimentally.': 0.9679934978485107, 'Even without actually implementing custom hardware, would it be possible to estimate the expected improvements in efficiency through simulation or other means?': 1.0986123085021973, 'Such results would significantly strengthen the paper.': 1.0986123085021973, 'GRADIENT TRUNCATION AS REGULARIZATION': 0.6615002751350403, 'In Figure 5 all models appear to reach a minimum validation loss at around 10k iterations and then overfit; at this point the Low model achieves even lower loss than the baseline.': 1.077525019645691, 'This is an interesting experimental result, but it is not discussed in the paper.': 0.9423564076423645, 'Perhaps a low truncation threshold acts as a weak regularizer to prevent overfitting?': 0.21234992146492004, 'Is this a general phenomenon of training recurrent networks with sparse gradients, or is it just a quirk of this particular experiment?': 0.8048879504203796, 'This idea deserves more investigation, and could strengthen the paper.': 0.4054843485355377, 'SUMMARY': 1.0986123085021973, 'The core idea of the paper (thresholding gradients to induce sparsity and improve efficiency of RNN training) is interesting and practically useful, if a bit incremental.': 0.6319993734359741, 'Nevertheless with thorough and deliberate experiments quantifying the tradeoffs between task performance, training speed, and energy efficiency across a variety of tasks and datasets, this simple idea could be the core of a strong paper.': 1.054144263267517, 'Unfortunately, as written the paper provides neither theoretical arguments nor convincing experimental results to justify the proposed method, and as such I do not believe the paper is ready for publication in its current form.': 0.3817235231399536, 'PROS': 1.0986123085021973, 'The proposed method is simple, and seems to be a promising direction for improving the speed of training recurrent networks.': 1.0986123085021973, 'CONS': 1.0986123085021973, 'No discussion of prior work on low-precision recurrent networks': 1.0986123085021973, 'Experimental results are not sufficient to validate the method': 1.0986123085021973, 'Many experimental details are missing': 1.0986123085021973, 'Results of key experiments (image captioning and machine translation) are missing': 1.0986123085021973}"
174,https://openreview.net/forum?id=HJcLcw9xg,"{'Summary:': 1.0986123085021973, 'This paper looks at the structure of the preimage of a particular activity at a hidden layer of a network.': 1.0986123085021973, 'It proves that any particular activity has a preimage of a piecewise linear set of subspaces.': 1.0986123085021973, 'Pros:': 1.0986123085021973, 'Formalizing the geometry of the preimages of a particular activity vector would increase our understanding of networks': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'Analysis seems quite preliminary, and no novel theoretical results or clear practical conclusions.': 1.0986123085021973, 'The main theoretical conclusion seems to be the preimage being this stitch of lower dimensional subspaces?': 1.0986123085021973, 'Would a direct inductive approach have worked?': 1.0986123085021973, '(e.g. working backwards from the penultimate layer say?)': 1.0986123085021973, 'This is definitely an interesting direction, and it would be great to see more results on it (e.g. how does the depth/width, etc affect the division of space, or what happens during training)': 1.0986123085021973, ""but it doesn't seem ready yet."": 1.0986123085021973, 'I have not read the revised version in detail yet.': 1.0986123085021973, 'SUMMARY': 1.0986123085021973, 'This paper studies the preimages of outputs of a feedforward neural network with ReLUs.': 1.0953876972198486, 'PROS': 1.0986123085021973, 'The paper presents a neat idea for changes of coordinates at the individual layers.': 1.0986123085021973, 'CONS': 1.0986123085021973, 'Quite unpolished / not enough contributions for a finished paper.': 1.0986123085021973, 'COMMENTS': 1.0986123085021973, 'In the first version the paper contains many typos and appears to be still quite unpolished.': 1.0986123085021973, 'The paper contains nice ideas but in my opinion it does not contribute sufficiently many results for a Conference paper.': 1.0986123085021973, 'I would be happy to recommend for the Workshop track.': 1.0986123085021973, 'Irreversibly mixed and several other notions from the present paper are closely related to the concepts discussed in [Montufar, Pascanu, Cho, Bengio, NIPS 2014].': 1.0985547304153442, 'I feel that that paper should be cited here and the connections should be discussed.': 1.0986123085021973, 'In particular, that paper also contains a discussion on the local linear maps of ReLU networks.': 1.0986123085021973, 'I am curious about the practical considerations when computing the pre-images.': 1.0986123085021973, 'The definition should be rather straight forward really, but the implementation / computation could be troublesome.': 1.0986120700836182, 'DETAILED COMMENTS': 1.0986120700836182, ""On page 1 ``can easily be shown to be many to one'' in general."": 1.0986123085021973, ""On page 2 ``For each point x^{l+1}''"": 1.0986123085021973, 'The parentheses in the superscript are missing.': 1.0986123085021973, ""After eq. 6 ``the mapping is unique''  is missing `when w1 and w2 are linearly independent'"": 1.0986123085021973, 'Eq. 1 should be a vector.': 0.977699339389801, ""Above eq. 3. ``collected the weights a_i into the vector w'' and"": 1.0986078977584839, 'bias b. Period is missing.': 1.0985275506973267, ""On page 2 ``... illustrate the preimage for the case of points on the lines ... respectively''"": 1.0986099243164062, 'Please indicate which is which.': 1.009650468826294, 'In Figure 1.': 1.0984306335449219, 'Is this a sketch, or the actual illustration of a network.': 1.0891129970550537, 'In the latter case, please state the specific value of x and the weights that are depicted.': 1.0985246896743774, 'Also define and explain the arrows precisely.': 1.0968472957611084, 'What are the arrows in the gray part?': 1.0986055135726929, ""On page 3 `` This means that the preimage is just the point x^{(l)}''  the points that W maps to x^{(l+1)}."": 1.0985976457595825, 'On page 3 the first display equation.': 1.0986123085021973, 'There is an index i on the left but not on the right hand side.': 1.097395420074463, 'The quantifier in the right hand side is not clear.': 1.0984925031661987, ""``generated by the mapping ... w^i '' subscript"": 1.0986123085021973, ""``get mapped to this hyperplane'' to zero"": 1.0986123085021973, ""``remaining'' remaining from what?"": 1.0986123085021973, ""``using e.g. Grassmann-Cayley algebra''"": 0.4316874146461487, 'How about using elementary linear algebra?!': 1.0111757516860962, ""``gives rise to a linear manifold with dimension one lower at each intersection''"": 0.5885903835296631, 'This holds if the hyperplanes are in general position.': 1.0986120700836182, ""``is complete in the input space'' forms a basis"": 1.0986123085021973, ""``remaining kernel'' remaining from what?"": 1.0986123085021973, ""``kernel'' Here kernel is referring to nullspace or to a matrix of orthonormal basis vectors of the nullspace, or to what specifically?"": 1.0986123085021973, 'Figure 3.': 1.0986123085021973, 'Nullspaces of linear maps should pass through the origin.': 1.0986123085021973, ""`` from pairwise intersections'' \\cap"": 1.0986123085021973, ""``indicated as arrows or the shaded area''"": 1.0986123085021973, 'this description is far from clear.': 1.0986123085021973, 'typos: peieces, diminsions, netork, me,': 1.0986123085021973, 'I really appreciate the directions the authors are taken and I think something quite interesting can come out of it.': 1.0986123085021973, 'I hope the authors continue on this path and are able to come up with something quite interesting soon.': 1.0986123085021973, 'However I feel this paper right now is not quite ready.': 1.074418544769287, 'Is not clear to me what the results of this work are yet.': 1.0986114740371704, 'The preimage construction is not obviously (at least not to me) helpful.': 1.0754024982452393, ""It feels like the right direction, but it didn't got to a point where we can use it to identify the underlying mechanism behind our models."": 1.0986123085021973, 'We know relu models need to split apart and unite different region of the space, and I think we can agree that we can construct such mechanism (it comes from the fact that relu models are universal approximators) ..': 1.0986123085021973, ""though this doesn't speak to what happens in practice."": 1.0985853672027588, 'All in all I think this work needs a bit more work yet.': 1.02782142162323}"
175,https://openreview.net/forum?id=HJeqWztlg,"{'The paper discusses a method to learn interpretable hierarchical template representations from given data.': 1.0972176790237427, 'The authors illustrate their approach on binary images.': 0.8082610964775085, 'The paper presents a novel technique for extracting interpretable hierarchical template representations based on a small set of standard operations.': 1.0986073017120361, 'It is then shown how a combination of those standard operations translates into a task equivalent to a boolean matrix factorization.': 1.0986113548278809, 'This insight is then used to formulate a message passing technique which was shown to produce accurate results for these types of problems.': 1.09861159324646, 'Summary:': 0.9066365957260132, '———': 1.0986123085021973, 'The paper presents an novel formulation for extracting hierarchical template representations that has not been discussed in that form.': 1.0985840559005737, 'Unfortunately the experimental results are on smaller scale data and extension of the proposed algorithm to more natural images seems non-trivial to me.': 1.0986123085021973, 'Quality: I think some of the techniques could be described more carefully to better convey the intuition.': 1.0986123085021973, 'Clarity: Some of the derivations and intuitions could be explained in more detail.': 1.0953845977783203, 'Originality: The suggested idea is reasonable but limited to binary data at this point in time.': 1.0323234796524048, 'Significance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge.': 1.0984575748443604, 'Details:': 1.0986123085021973, '1. My main concern is related to the experimental evaluation. While the discussed approach is valuable, its application seems limited to binary images at this point in time. Can the authors comment?': 0.5306061506271362, '2. There are existing techniques to extract representations of images which the authors may want to mention, e.g., work based on grammars.': 0.1719834804534912, 'This paper presents an approach to learn object representations by composing a set of templates which are leaned from binary images.': 1.098497748374939, 'In particular, a hierarchical model is learned by combining AND, OR and POOL operations.': 1.08979070186615, 'Learning is performed by using approximated inference with MAX-product BP follow by a heuristic to threshold activations to be binary.': 1.0986123085021973, 'Learning hierarchical representations that are interpretable is a very interesting topic, and this paper brings some good intuitions in light of modern convolutional neural nets.': 1.0986123085021973, 'I have however, some concerns about the paper:': 0.9168271422386169, '1) the paper fails to cite and discuss relevant literature and claims to be the first one that is able to learn interpretable parts.': 0.4679461419582367, 'I would like to see a discussion of the proposed approach compared to a variety of papers e.g.,:': 1.0986123085021973, 'Compositional hierarchies of Sanja Fidler': 0.42142611742019653, 'AND-OR graphs used by Leo Zhu and Alan Yuille to model objects': 0.9790950417518616, ""AND-OR templates of Song-Chun Zhu's group at UCLA"": 1.0985239744186401, 'The claim that this paper is the first to discover such parts should be removed.': 1.0986121892929077, '2) The experimental evaluation is limited to very toy datasets.': 1.0941306352615356, 'The papers I mentioned have been applied to real images (e.g., by using contours to binarize the images).': 1.0986123085021973, ""I'll also like to see how good/bad the proposed approach is for classification in more well known benchmarks."": 1.098604679107666, 'A comparison to other generative models such as VAE, GANS, etc will also be useful.': 0.3226872682571411, ""3) I'll also like to see a discussion of the relation/differences/advantages of the proposed approach wrt to sum product networks and grammars."": 0.08638458698987961, 'Other comments:': 1.0986123085021973, 'the paper claims that after learning inference is feed-forward, but since message passing is used, it should be a recurrent network.': 1.0402084589004517, 'the algorithm and tech discussion should be moved from the appendix to the main paper': 0.5785831809043884, 'the introduction claims that compression is a prove for understanding.': 1.0984432697296143, 'I disagree with this statement, and should be removed.': 0.8934518098831177, ""I'll also like to see a discussion relating the proposed approach to the Deep Rendering model."": 1.0982292890548706, 'It is not obvious how some of the constraints are satisfied during message passing.': 0.7980024218559265, 'Also constraints are well known to be difficult to optimize with max product.': 0.8231497406959534, 'How do you handle this?': 1.098608136177063, 'The learning and inference algorithms seems to be very heuristic (e.g., clipping to 1, heuristics on which messages are run).': 1.0984524488449097, 'Could you analyze the choices you make?': 0.9819862246513367, 'doing multiple steps of 5) 2) is not a single backward pass': 1.0451514720916748, ""I'll reconsider my score in light of the answers"": 0.8271204233169556, 'This paper presents a generative model for binary images.': 1.098599910736084, 'Images are composed by placing a set of binary features at locations in the image.': 1.0986123085021973, ""These features are OR'd together to produce an image."": 1.079521894454956, 'In a hierarchical variant, features/classes can have a set of possible templates, one of which can be active.': 1.0986123085021973, 'Variables are defined to control which template is present in each layer.': 1.0986123085021973, 'A joint probability distribution over both the feature appearance and instance/location variables is defined.': 1.0986123085021973, 'Overall, the goal of this work is interesting': 1.0978977680206299, 'it would be satisfying if semantically meaningful features could be extracted, allowing compositionality in a generative model of images.': 1.0852233171463013, ""However, it isn't clear this would necessarily result from the proposed process."": 1.0985257625579834, 'Why would the learned features (building blocks) necessarily semantically meaningful?': 1.0986051559448242, 'In the motivating example of text, rather than discovering letters, features could correspond to many other sub-units (parts of letters), or other features lacking direct semantic meaning.': 1.0986123085021973, 'The current instantiation of the model is limited.': 1.097781777381897, 'It models binary image patterns.': 1.0986119508743286, 'The experiments are done on synthetic data and MNIST digits.': 0.6639472842216492, 'The method recovers the structure and is effective at classification on synthetic data that are directly compositional.': 1.0986123085021973, 'On the MNIST data, the test errors are quite large, and worse than a CNN except when synthetic data corruption is added.': 1.0980961322784424, 'Further work to enhance the ability of the method to handle natural images or naturally occuring data variation would enhance the paper.': 1.098611831665039}"
176,https://openreview.net/forum?id=HJgXCV9xx,"{'As discussed, the there are multiple concurrent contributions in different packages/submission by the authors that are in parts difficult to disentangle.': 1.0986123085021973, 'Despite this fact, it is impressive to see a system learning from natural feedback in an online fashion.': 1.0986123085021973, 'To the best of my knowledge, this is a new quality of result that was achieved - in particular as close to full supervision results are reached in some cases in this less constraint setting.': 1.0986123085021973, 'several points were raised that were in turn addressed by the authors:': 1.0986123085021973, '1. formalisation of the task (learning dialogue) is not precise. when can we declare success?': 1.0986123085021973, 'The answer of the authors is partially satisfying.': 1.0986123085021973, 'For this particular work, it might make sense to more precisely set goals e.g. to be as good as full supervision.': 1.0986123085021973, '2. (along the line of the previous question:) dialogue can be seen as a form of noisy supervision. can you please report the classic supervision baselines for the particular model used? this would give a sense what fraction of the best case performance is achieved via dialogue learning.': 0.6716399788856506, 'The authors provided additional information along those lines - and I think this helps to understand how much of the overall goal was achieved and open challenges.': 1.0986123085021973, '3. is there an understanding of how much more difficult the MT setting is? feedback could be hand labeled as positive or negative for an analysis (?). or a handcrafted baseline could be tested, that either extracts the reward via template matching … or maybe even uses the length of the feedback as a proxy/baseline. (it looks to me that short feedback is highly correlated with high reward / correct answer (?))': 0.45455411076545715, 'The authors replied - but it would have been clearer if they could have quantified such suggested baseline, in order to confirm that there is no simple handcrafted baseline that would do well on the data - but these concerns are marginal.': 1.0986071825027466, '4. relation to prior work Weston’16 is not fully clear. I understand that this submission should be understood as an independent submission of the prior work Weston’16 - and not replacing it. In this case Weston’16 makes this submission appear more incremental. my understanding is that the punch line of this submission is the online part that leads in turn to more exploration. Is there any analysis on how much this aspect matters? I couldn’t find this in the experiments.': 0.3935382664203644, 'The authors clarified the raised issues.': 1.0986123085021973, 'The application of reinforcement learning and in particular FP is convincing.': 1.0978277921676636, 'There is a incremental nature to the paper - and the impression is emphasised by multiple concurrent contributions of the authors on this research thread.': 1.0986123085021973, ""Comparison to prior work (in particular Weston'16), should be made more explicit."": 1.0986123085021973, 'Not only in text but also in the experiments - as the authors partially do in their reply to the reviewers question.': 1.0986123085021973, 'Nevertheless, this particular contribution is assessed as significant and worth sharing and seems likely to have impact on how we can learn in these less constraint setting.': 1.0986123085021973, 'This paper builds on the work of Weston (2016), using End-to-end memory network models for a limited form of dialogue with teacher feedback.': 1.0986123085021973, 'As the authors state in the comments, it is closely related to the question answering problem with the exception that a teacher provides a response after the model’s answer, which does not always come with a positive reward.': 1.0986123085021973, 'Thus, the model must learn to use the teacher’s feedback to significantly improve performance.': 1.0986123085021973, 'Overall, the paper is written clearly, and several interesting models are tested.': 1.0986123085021973, 'It is certainly only a limited form of dialogue that is considered (closer to question answering, since the questions do not require the agent to look further back into the context), but investigating in this direction could prove fruitful once the tasks are scaled up to be more difficult.': 1.0986123085021973, 'My main concern is with the paper`s novelty.': 1.0986123085021973, 'In the words of the authors, this paper has two primary differences with the work of Weston:': 1.0986123085021973, '“(i) That earlier work did not use the natural reinforcement learning/online setting, but “cheated” with a fixed policy given in advance.': 1.0986123085021973, 'It is important to address the realistic online setting and assess whether the methods, particularly FP, still work, or else what changes (e.g. exploration, balancing, see Fig 4 and Table 1) are needed.': 1.0986123085021973, '(ii) That earlier work had only simulated data, and no real-language data, so was only toy.': 1.0986123085021973, 'This work uses Mechanical Turk to do real experiments, which again is important to assess if these methods, particularly FP, work on real language.”': 1.0986123085021973, 'Point (ii) is very much appreciated, but adding additional human testing data is not sufficient for a conference paper.': 1.0986123085021973, 'Thus, the main point of the paper is that “the model also works if we collect the data online (i.e. the agent’s policy is used to collect data rather than a fixed policy beforehand)”.': 1.0986123085021973, 'While this is a step in the right direction, I’m not sure if it’s significant enough for an ICLR paper.': 1.0986123085021973, 'Little model novelty is required to solve this additional requirement on these tasks beyond using epsilon greedy exploration.': 1.0986123085021973, 'Thus, the paper is borderline accept/reject.': 1.0986123085021973, ""EDIT: I have updated my score slightly in light of the author's response, where they make a good point that real-world implementation should be more strongly considered as part of the contribution."": 1.0986123085021973, 'SUMMARY: This paper describes a set of experiments evaluating techniques for': 1.0986123085021973, 'training a dialogue agent via reinforcement learning.': 1.0986123085021973, 'A': 1.0986123085021973, 'standard memory network architecture is trained on both bAbI and a version of': 1.0986123085021973, 'the WikiMovies dataset (as in Weston 2016, which this work extends).': 1.0986123085021973, 'Numerous': 1.0986123085021973, 'experiments are performed comparing the behavior of different training': 1.0986123085021973, 'algorithms under various experimental conditions.': 1.0986123085021973, 'STRENGTHS: The experimentation is comprehensive. I agree with the authors that': 1.0986123085021973, 'these results provide additional useful insight into the performance of the': 1.0986123085021973, 'model in the 2016 paper (henceforth W16).': 1.0986123085021973, 'WEAKNESSES: This is essentially an appendix to the earlier paper. There is no': 0.9540713429450989, 'new machine learning content.': 1.0985476970672607, 'Secondarily, the paper seems to confuse the': 0.8625921010971069, 'distinction between ""training with an adaptive sampling procedure"" and ""training': 0.534521222114563, 'in interactive environments"" more generally.': 1.0986119508743286, 'In particular, no comparisons are': 0.9845231771469116, 'presented to the to the experiments with a static exploration policy presented': 0.7489218711853027, 'in W16, when the two training can & should be evaluated side-by-side.': 1.0805532932281494, 'The only meaningful changes between this work and W16 involve simple': 1.0938376188278198, '(and already well-studied) changes to the form of this exploration policy.': 1.065051555633545, 'My primary concern remains about novelty: the extra data introduced here is': 1.098608374595642, 'welcome enough, but probably belongs in a *ACL short paper or a technical': 0.40635183453559875, 'report.': 0.5107365846633911, 'This work does not stand on its own, and an ICLR submission is not an': 1.0442718267440796, 'appropriate vehicle for presenting it.': 0.40685907006263733, '""REINFORCEMENT LEARNING""': 0.791958749294281, '[Update: concerns in this section have been addressed by the authors.]': 0.9378405809402466, 'This paper attempts to make a hard distinction between the reinforcement': 1.0982286930084229, 'learning condition considered here and the (""non-RL"") condition considered in': 1.0978105068206787, 'W16.': 0.5603169202804565, ""I don't think this distinction is nearly as sharp as it's"": 0.5131309032440186, 'made out to be.': 1.0986123085021973, 'As already noted in Weston 2016, the RBI objective is a special case of vanilla': 1.0949617624282837, 'policy gradient with a zero baseline and off-policy samples.': 1.0986047983169556, 'In this sense the': 1.0900373458862305, 'version of RBI considered in this paper is the same as in W16, but with a': 1.0371626615524292, 'different exploration policy; REINFORCE is the same objective with a nontrivial': 0.6869175434112549, 'baseline.': 1.0986123085021973, 'Similarly, the change in FP is only a change to the sampling policy.': 1.0645339488983154, 'The fixed dataset / online learning distinction is not especially meaningful': 0.9643857479095459, 'when the fixed dataset consists of endless synthetic data.': 1.0792152881622314, 'It should be noted that some variants of the exploration policy in W16 provide a': 0.43891170620918274, 'stronger training signal than is available in the RL ""from scratch"" setting': 0.8177054524421692, 'here: in particular, when  the training samples will feature much': 0.7608157992362976, 'denser reward.': 1.0986121892929077, 'However, if I correctly understand Figures 3 and 4 in this paper,': 1.0832070112228394, 'the completely random initial policy achieves an average reward of ~0.3 on bAbI': 1.0658422708511353, 'and ~0.1 on movies': 1.0930854082107544, 'as good or better than the other exploration policies in': 0.5767849087715149, 'W16!': 1.0986123085021973, 'I think this paper would be a lot clearer if the delta from W16 were expressed': 1.0986121892929077, 'directly in terms of their different exploration policies, rather than trying to': 0.9473015069961548, 'cast all of the previous work as ""not RL"" when it can be straightforwardly': 0.7606712579727173, 'accommodated in the RL framework.': 1.0956313610076904, 'I was quite confused by the fact that no direct comparisons are made to the': 1.098603367805481, 'training conditions in the earlier work.': 0.9950234293937683, 'I think this is a symptom of the': 0.958390474319458, 'problem discussed above: once this paper adopts the position that this work is': 0.8020625114440918, 'about RL and the previous work is not, it becomes possible to declare that the': 0.9076414704322815, 'two training scenarios are incomparable.': 1.0986114740371704, 'I really think this is a mistake': 1.096561312675476, 'to': 1.0986123085021973, 'the extent that the off-policy sample generators used in the previous paper are': 1.098114013671875, 'worse than chance, it is always possible to compare to them fairly here.': 1.0230677127838135, 'Evaluating everything in the ""online"" setting and presenting side-by-side': 1.094750165939331, 'experiments would provide a much more informative picture of the comparative': 1.0981471538543701, 'behavior of the various training objectives.': 1.0985220670700073, 'ON-POLICY VS OFF-POLICY': 1.0986123085021973, ""Vanilla policy gradient methods like the ones here typically can't use"": 1.0986123085021973, 'off-policy samples without a little extra hand-holding (importance sampling,': 1.0986123085021973, 'trust region methods, etc.).': 1.0986123085021973, 'They seem to work out of the box for a few of the': 1.0986123085021973, 'experiments in this paper, which is an interesting result on its own.': 1.0986123085021973, 'It would': 1.0986123085021973, 'be nice to have some discussion of why that might be the case.': 1.0986123085021973, 'OTHER NOTES': 1.0986123085021973, 'The claim that ""batch size is related to off-policy learning"" is a little': 1.0986123085021973, 'odd.': 1.0986123085021973, 'There are lots of on-policy algorithms that require the agent to collect a': 1.0986123085021973, 'large batch of transitions from the current policy before performing an': 1.0986123085021973, '(on-policy) update.': 1.0986123085021973, 'I think the experiments on fine-tuning to human workers are the most exciting': 1.0986123085021973, 'part of this work, and I would have preferred to see these discussed (and': 1.0986123085021973, 'explored with) in much more detail rather than being relegated to the': 1.0986123085021973, 'penultimate paragraphs.': 1.0986123085021973}"
177,https://openreview.net/forum?id=HJhcg6Fxg,"{""The method in this paper introduces a binary encoding level in the PV-DBOW and PV-DM document embedding methods (from Le & Mikolov'14)."": 1.0986100435256958, 'The binary encoding consists in a sigmoid with trained parameters that is inserted after the standard training stage of the embedding.': 1.098496437072754, 'For a document to encode, the binary vector is obtained by forcing the sigmoid to output a binary output for each of the embedding vector components.': 1.098118543624878, 'The binary vector can then be used for compact storage and fast comparison of documents.': 1.0986123085021973, 'Pros:': 1.0869101285934448, ""the binary representation outperforms the Semantic hashing method from Salakhutdinov & Hinton '09"": 1.010990023612976, ""the experimental approach sound: they compare on the same experimental setup as Salakhutdinov & Hinton '09, but since in the meantime document representations improved (Le & Mikolov'14), they also combine this new representation with an RBM to show the benefit of their binary PV-DBOW/PV-DM"": 0.2762596607208252, 'Cons:': 1.097426176071167, ""the insertion of the sigmoid to produce binary codes (from Lin & al. '15) in the training process is incremental"": 0.42179277539253235, 'the explanation is too abstract and difficult to follow for a non-expert (see details below)': 1.0953809022903442, 'a comparison with efficient indexing methods used in image retrieval is missing.': 0.44186967611312866, 'For large-scale indexing of embedding vectors, derivations of the Inverted multi-index are probably more interesting than binary codes.': 1.098609209060669, 'See eg.': 1.0986123085021973, ""Babenko & Lempitsky, Efficient Indexing of Billion-Scale Datasets of Deep Descriptors, CVPR'16"": 0.4545573890209198, 'Detailed comments:': 1.0986123085021973, 'Section 1: the motivation for producing binary codes is not given.': 1.0985124111175537, 'Also, the experimental section could give some timings and mem usage numbers to show the benefit of binary embeddings': 1.094451904296875, 'figure 1, 2, 3: there is enough space to include more information on the representation of the model: model parameters + training objective + characteristic sizes + dropout.': 0.44189539551734924, 'In particular, in fig 2, it is not clear why ""embedding lookup"" and ""linear projection"" cannot be merged in a single smaller lookup table (presumably because there is an intermediate training objective that prevents this).': 1.0492291450500488, 'p2: ""This way, the length of binary codes is not tied to the dimensionality of word embeddings.""': 0.519112765789032, '-> why not?': 1.0986121892929077, 'section 3: This is the experimental setup of  Salakhutdinov & Hinton 2009.': 1.0941979885101318, 'Specify this and whether there is any difference between the setups.': 1.0970346927642822, '""similarity of the inferred codes"": say here that codes are compared using Hamming distances.': 0.7373527884483337, '""binary codes perform very well, despite their far lower capacity"" -> do you mean smaller size than real vectors?': 1.0986123085021973, 'fig 5: these plots could be dropped if space is needed.': 1.0986123085021973, 'section 3.1: one could argue that ""transferring"" from Wikipedia to anything else cannot be called transferring, since Wikipedia\'s purpose is to include all topics and lexical domains': 1.0986123085021973, 'section 3.2: specify how the 300D real vectors are compared.': 1.0986123085021973, 'L2 distance?': 1.0986123085021973, 'inner product?': 1.0986123085021973, 'fig4: specify what the raw performance of the large embedding vectors is (without pre-filtering with binary codes), or equivalently, the perf of (code-size, Hamming dis)': 1.098394513130188, '= (28, 28), (24, 24), etc.': 1.0986123085021973, 'This paper presents a method to represent text documents and paragraphs as short binary codes to allow fast similarity search and retrieval by using hashing techniques.': 1.0986119508743286, 'The real-valued paragraph vectors by Le & Mikolov is extended by adding a stochastic binary layer on top of the neural network architecture.': 1.0985844135284424, 'Two methods for binarizing the final activations are compared: (1) simply adding noise to sigmoid activations to encourage discritization.': 1.0986123085021973, '(2) binarizing the activations in the forward pass and keeping them real-valued in the backward pass (straight-through estimation).': 1.0986123085021973, 'The paper presents encouraging results by using straight-through estimation on 20 newsgroup and RCV1 text datasets by using 128 and 32 bit binary codes.': 1.0986120700836182, 'On the plus side, the application presented in the paper is interesting and important.': 1.098610758781433, 'The exposition of the paper is clean and clear.': 0.9295029640197754, 'However, the novelty of the approach is limited from a machine learning standpoint.': 0.9976129531860352, ""The literature on binary hashing beyond semantic hashing and Krizhevsky's binary autoencoders in 2011 is not explained."": 1.0985808372497559, 'An important baseline is missing where real-valued paragraph vectors are learned first, and then converted to binary codes using off-the-shelf hashing methods (e.g. random projection LSH by Charikar, BRE by Kulis & Darrell, ITQ by Gong & Lazebnik, MLH by Norouzi & Fleet, etc.)': 1.0986121892929077, ""Given the lack of novelty and the missing baseline, I do not recommend this paper in its current for publication in the ICLR conference's proceeding."": 1.0986111164093018, 'Moving forward, this paper may be more suitable for NLP conferences as it is more on the applied side.': 1.0982699394226074, 'More comments:': 1.0986121892929077, 'I believe from an practical perspective it may be easier to first learn real-valued paragraph vectors and then quantize them for indexing.': 1.0986123085021973, 'That said, an end-to-end approach as proposed in this paper may perform better.': 1.0986123085021973, 'I would like to see an empirical comparison between the proposed end-to-end approach and a simpler two stage quantization method suggested here.': 1.0986123085021973, 'See ""Estimating or Propagating Gradients Through Stochastic Neurons"" By Bengio et al - discussing straight through estimation and some other alternatives.': 1.0986123085021973, 'The paper argues that the length of binary codes cannot be longer than 32 bits because longer codes are not suitable for document hashing.': 1.0986123085021973, 'This is not quite right given multi-probe hashing mechanisms, for example see ""Mult-index Hashing"" by Norouzi et al.': 1.0986123085021973, 'See ""Hashing for Similarity Search: A Survey"" by Wang et al. for a survey of related work on binary hashing and quantization.': 1.0986123085021973, 'You seem to ignore the extensive work done on binary hashing.': 1.0986123085021973, 'This work proposes a model that can learn short binary codes via paragraph vectors to allow fast retrieval of documents.': 1.0986123085021973, 'The experiments show that this is superior to semantic hashing.': 1.0986123085021973, 'The approach is simple and not very technically interesting.': 1.0986123085021973, 'For a code size of 128, the loss compared to a continuous paragraph vector seems moderate.': 1.0986123085021973, 'The paper asks the reader to refer to the Salakhutdinov and Hinton paper for the baseline numbers but I think they should be placed in the paper for easy reference.': 1.0986123085021973, 'For simplicity, the paper could show the precision at 12.5%, 25% and 50% recall for the proposed model and semantic hashing.': 1.0986123085021973, 'It also seems that the semantic hashing paper shows results on RCV2 and not RCV1.': 1.0986123085021973, 'RCV1 is twice the size of RCV2 and is English only so it seems that these results are not comparable.': 1.0986123085021973, 'It would be interesting to see how many binary bits are required to match the performance of the continuous representation.': 1.0986123085021973, 'A comparison to the continuous PV-DBOW trained with bigrams would also make it a more fair comparison.': 1.0986123085021973, 'Figure 7 in the paper shows a loss from using the real-binary PV-DBOW.': 1.0986123085021973, 'It seems that if a user needed high quality ranking after the retrieval stage and they could afford the extra space and computation, then it would be better for them to use a standard PV-DBOW to obtain the continuous representation at that stage.': 1.0986123085021973, 'Minor comments:': 1.0986123085021973, 'First line after the introduction: is sheer -> is the sheer': 1.0986123085021973, '4th line from the bottom of P1: words embeddings -> word embeddings': 1.0986123085021973, 'In table 1: What does code size refer to for PV-DBOW?': 1.0986123085021973, 'Is this the number of elements in the continuous vector?': 1.0986123085021973, '5th line from the bottom of P5: W -> We': 1.0986123085021973, '5th line after section 3.1: covers wide -> covers a wide': 1.0986123085021973}"
178,https://openreview.net/forum?id=HJjiFK5gx,"{'First I would like to apologize for the late review.': 0.82670658826828, 'This paper proposes an extension of the NPI model (Reed & de Freitas) by using an extension of the probabilistic stacks introduced in Mikolov et al..': 0.6933495402336121, 'This allows them to train their model with less supervision than Reed & de Freitas.': 1.0975844860076904, 'Overall the model is a nice extension of NPI.': 1.0986123085021973, 'While it requires less supervision than NPI, it still requires ""sequences of elementary operations paired with environment observations, and [...] a couple of examples which include the full abstraction hierarchy"".': 1.009138584136963, 'This may limit the scope of this work.': 1.0986120700836182, 'The paper claims that their ""method is leverages stronger supervision in the form of elementary action sequences rather than just input-output examples (sic).': 1.0986123085021973, 'Such sequences are relatively easy to gather in many natural settings"".': 1.0986123085021973, 'It would be great if the authors clarify what they mean by ""relatively easy to gather in many natural settings"".': 1.0986123085021973, 'They also claim that ""the additional supervision improves the data efficiency and allow our technique to scale to more complicated problems"".': 1.0986123085021973, 'However, this paper only addresses two toy problems which are neither ""natural settings"" nor of a large scale (or at least not larger than those addressed in the related literature, see Zaremba et al. for addition).': 1.0986123085021973, 'In the introduction, the author states that ""Existing techniques, however, cannot be applied on data like this because it does not contain the abstraction hierarchy.""': 1.0986123085021973, 'What are the ""existing techniques"", they are referring to?': 1.098603367805481, 'This work only addresses the problem of long addition and puzzle solving in a block world.': 1.098514199256897, 'Afaik, Zaremba et al. has shown that with no supervision, it can solve the long addition problem and': 1.097038745880127, 'Sukhbaatar et al. (""Mazebase: A sandbox for learning from games"") shows that a memory network can solve puzzles in a blockworld with little supervision.': 1.0826789140701294, 'In the conclusion,  the author states that ""remarkably, NPL achieves state-of-the-art performances with much less supervision compared to existing models, making itself more applicable to real-world applications where full program traces are hard to get.""': 0.09486180543899536, 'However for all the experiments, they ""include a small number of FULL samples"" (FULL == ""samples with full program traces"").': 1.044500708580017, 'Unfortunately even if this means that they need less FULL examples, they still need ""full program traces"", contradicting their final claim.': 1.0937436819076538, 'Moreover, as shown figure 7, their model does not use a ""small number of FULL samples"" but rather a significantly smaller amount of FULL examples than NPI, i.e., 16 vs 128.': 1.0920330286026, '""All experiments were run with 10 different random seeds"": does the environment change as well between the runs, i.e. are the FULL examples different between the runs?': 1.0101509094238281, 'If it is the case and since you select the best run (on a validation set), the NPL model does not consume 16 FULL examples but 160 FULL examples for nanoCraft.': 0.9330030679702759, 'Concerning the NanoCraft example, it would be good to have more details about how the examples are generated: how do you make sure that the train/val/test sets are different?': 0.32076412439346313, 'How the rectangular shape are generated?': 1.0960636138916016, 'If I consider all possible rectangles in a 6x6 grid, there are (6x6)x(6x6)/2 = 648 possibilities, thus taking 256 examples sum up to ~40% of the total number of rectangles.': 0.7502137422561646, 'This does not even account for the fact that from an initial state, many rectangles can be made, making my estimate probably lower than the real coverage of examples.': 0.3875090479850769, 'Concerning the addition, it would interesting to show what an LSTM would do: Take a 2 layer LSTM that takes the 2 current digits as an input and produce the current output ( ""123+45"" would be input[0] = [3,5], input[1]=[2,4], input[2]=[1, 0] and output[0] = 8...).': 1.0452568531036377, 'I would be curious to see how such baseline would work.': 1.0740265846252441, 'It can be trained on input/output and it is barely different from a standard sequence model.': 1.0930503606796265, 'Also, would it be possible to compare with Zaremba et al.?': 1.0143717527389526, 'Finally, as discussed previously with the authors, it would be good if they discuss more in length the relation between their probabilistic stacks and Mikolov et al..': 0.8040140867233276, 'They have a lot of similarities and it is not addressed in the current version.': 0.9595459699630737, 'It should be addressed in the section describing the approach.': 1.0986123085021973, 'I believe the authors agreed on this and I will wait for the updated version.': 1.0986123085021973, ""Overall, it is a nice extension of Reed & de Freitas, but I'm a bit surprised by the lack of discussion about the rest of the literature (beside Reed & de Freitas, most previous work are only lightly discussed in the related work)."": 1.0986123085021973, 'This would have been fine if this paper would not suffer from a relatively weak experiment section that does not support the claims made in this work or show results that were not obtained by others before.': 1.0986123085021973, 'Missing references:': 1.0986123085021973, '""Learning simple arithmetic procedures"", Cottrell et al.': 1.0986123085021973, '""Neural gpus learn algorithms"", Kaiser & Sutskever': 1.0986123085021973, '""Mazebase: A sandbox for learning from games"", Sukhbaatar et al.': 1.0986123085021973, '""Learning simple algorithms from examples"", Zaremba et al.': 1.0986123085021973, 'The paper presents the Neural Program Lattice (NPL), extending the previous Neural Programmer-Interpreters (NPI).': 1.098474383354187, 'The main idea is to generalize stack manipulation of NPI by making it probabilistic.': 1.0986123085021973, ""This allows the content of the stack to be stochastic than deterministic, and the paper describes the feed-forward steps of NPL's program inference similar to the NPI formulation."": 1.0986120700836182, 'A new objective function is provided to train the model that maximizes the probability of NPL model correctly predicting operation sequences, from execution traces.': 1.0979766845703125, 'We believe this is an important extension.': 1.0986123085021973, 'The experimental results illustrate that the NPL is able to learn task executions in a clean setting with perfect observations.': 1.0985990762710571, 'The paper is clearly presented and its background literature (i.e., NPI) is well covered.': 1.098351001739502, 'We also believe the paper is presenting a conceptually/technically meaningful extension of NPI, which will be of interest to a broad audience.': 1.098611831665039, 'We are still a bit concerned whether the NPL would be directly applicable for noisy observations (e.g., human skeletons) in a continuous space with less explicit structure, so more discussions will be interesting.': 1.0985733270645142, 'Neural Programmer-Interpreters (NPI) achieves greatly reduced sample complexity and better generalization than flat seq2seq models for program induction, but requires program traces at multiple levels of abstraction for training, which is a very strong form of supervision.': 1.0986123085021973, 'One obvious way to improve this situation, addressed in this work, is to only train on the lowest-level traces, with a latent compositional program structure.': 1.0986123085021973, 'This makes sense because the ""raw"" low-level traces can be cheaply gathered in many cases just by watching expert demonstrations, without being explicitly told the more temporally abstract structures.': 1.0986123085021973, 'This paper shows that a variant of NPI, named NPL, can achieve even better generalization performance with weaker supervision (mostly flat traces), and also extends the model to a new grid world task.': 1.0986123085021973, 'Unfortunately, it still requires being told the overall program structure by being given a few *full* execution traces.': 1.0986123085021973, 'Still, I see this as important progress.': 1.0983731746673584, 'It extends NPI in a quite nontrivial way by introducing a stack mechanism modeling the latent program call structure, which makes the training process much more closely match what the model does at test time.': 1.0986123085021973, 'The results tell us that flat execution traces can take us almost all the way toward learning compositional programs from demonstrations - the hard part is of course learning to actually discover the subprogram structure.': 1.0986123085021973}"
179,https://openreview.net/forum?id=HJlgm-B9lx,"{'The authors did not bother responding or fixing any of the pre-review comments.': 1.0985641479492188, 'Hence I repeat here:': 1.0986123085021973, 'Please do not make incredibly unscientific statements like this one:': 1.0986095666885376, '""The working procedure of this model is just like how we human beings read a text and then answer a related question. ""': 0.8665392398834229, 'Really, ""humans beings"" have an LSTM like model to read a text?': 1.0887935161590576, 'Can you cite an actual neuroscience paper for such a claim?': 0.6384804248809814, 'The answer is no, so please delete such statements from future drafts.': 0.4607653319835663, ""Generally, your experiments are about simple classification and the methods you're competing against are simple models like NB-SVM."": 1.094923734664917, 'So I would change the title, abstract ad introduction accordingly and not attempt hyperbole like ""Learning to Understand"" in the title.': 0.9127958416938782, 'Lastly, your attention level approach seems similar to dynamic memory networks by Kumar et al.': 0.9908472299575806, 'they also have experiments for sentiment and it would be interesting to understand the differences to your model and compare to them.': 1.0985965728759766, 'Other reviewers included further missing related work and fitting this paper into the context of current literature.': 0.896242618560791, 'Given that no efforts were made to fix the pre-review questions and feedback, I doubt this will become ready in time for publication.': 1.0917249917984009, 'The paper proposes to enhance the attention mechanism for sentiment classification by using global context computed by a Bi-LSTM.': 1.0986123085021973, 'The proposed models outperform many existing models in the literature on 3 sentiment analysis datasets.': 1.0986123085021973, 'The key idea of using Bi-LSTM to compute global context for attention is actually not novel, as proposed several times in the literature, e.g., Luong et al (2015) and Shen & Lee (2016).': 1.0986120700836182, 'Especially, Luong et al (2015) already proposed to combine global context with local context for attention.': 1.0986123085021973, 'Regarding to the experiments, of course it would be nice if the model can work well without the need of tricks like dropout or pre-trained word embeddings.': 1.0986123085021973, 'However, it would be even better if the model can work well using those tricks.': 1.0986123085021973, 'The authors should show results of the models using those tricks and compare them to the results in the literature.': 1.0986123085021973, 'Ref:': 1.0986123085021973, 'Luong et al.': 1.0986123085021973, 'Effective Approaches to Attention-based Neural Machine Translation.': 1.0986123085021973, 'EMNLP 2015': 1.0986123085021973, 'This paper presents a hierarchical attention-based method for document classification.': 1.0984580516815186, 'The main idea is to first run a bidirectional LSTM to get global context vector, and then run another attention-based bidirectional LSTM that uses the final hidden state from the first pass to weight local context vectors (TS-ATT).': 1.0986123085021973, 'A simpler architecture that removes the first LSTM and uses the output of the second LSTM as the global context vector is also proposed (SS-ATT).': 1.098583459854126, 'Experiments on three datasets are presented, however the results are mostly not state-of-the-art.': 1.0986123085021973, 'I think the idea is nice, but the experiment results are not convincing enough to justify this new model architecture.': 1.0985833406448364, 'Why is your Yelp 2013 dataset smaller than the original Tang et al, 2015 paper that has ~300k documents?': 1.0986002683639526, 'I noticed your other datasets are also quite small.': 1.0986123085021973, 'Is it because your model is difficult to scale to large datasets?': 1.098611831665039, 'You should also include results from Tang et al., 2015 in Table 2 that achieves 65.1% accuracy on Yelp 2013 (why is your number so much lower?)': 0.6744568347930908, 'I also suggest removing phrases such as ""Learning to Understand"" when presenting their model.': 1.0981476306915283, 'Overall, I think that this submission is a better fit for the workshop.': 1.073343276977539, 'Minor comments:': 1.0986123085021973, 'gloal -> global': 1.0986123085021973, 'Not needing a pretrained embeddings, while of course nice, is not that big of a deal.': 0.8641006946563721, 'Various models will work just fine without pretrained embeddings.': 1.0986121892929077}"
180,https://openreview.net/forum?id=HJpfMIFll,"{'On the plus side, the paper proposes a mathematically interesting model for a context of a word (i.e., a Grassmanian manifold).': 1.0986123085021973, 'On the minus side, the paper mostly ignores the long history of Word Sense Induction (WSI) and Word Sense Disambiguation (WSD), citing and comparing only some relatively recent papers.': 1.0986123085021973, 'The experiments in this paper done on SemEval-2010 are not very persuasive.': 0.4448356032371521, ""(It's difficult to evaluate the experiments done on the 2016 data, since they are not directly comparable to published results)."": 0.9366897344589233, 'For example, going back to the SemEval-2010 WSI task in [1], the best system seems to be UoY': 1.0810216665267944, '[2].': 0.4295159876346588, 'The F-measure seems to be a poor metric: always assigning one sense to every word (""MFS"") yields the highest F-measure of 63.5%.': 1.0986123085021973, 'The paper\'s result with ""2 clusters"" (with an average of about 1.9) seems to be close to MFS.': 1.0946193933486938, ""So I don't think we can use F-measure to compare."": 1.0986123085021973, 'The V-measure seems to be tilted towards systems that have high number of senses per word.': 1.0986123085021973, 'UoY has V=15.7%, while the paper (with ""5 clusters"") has 14.4%.': 1.0647472143173218, ""That isn't very convincing that the proposed method has captured the geometry of polysemy."": 1.0986123085021973, 'In general, I have often wondered why people work on pure unsupervised WSI and WSD.': 1.0679476261138916, 'The assessment is very difficult (as described above).': 1.0986120700836182, 'More importantly, some very weakly supervised systems (with minimal labels) can work pretty well to bootstrap.': 1.0311219692230225, 'See, e.g., the classic paper': 1.0984288454055786, '[3].': 1.0986120700836182, 'If the authors used the Grassmannian idea to solve higher-level NLP problems directly (such as analogies), that would be very persuasive.': 1.0973820686340332, ""However, that's a very different paper than what was submitted."": 1.0986121892929077, 'For an example of application of Grassmannian manifolds to analogies, see [4].': 1.0984221696853638, 'References:': 1.0986123085021973, '1. Manandhar, Suresh, et al. ""SemEval-2010 task 14: Word sense induction & disambiguation."" Proceedings of the 5th international workshop on semantic evaluation. Association for Computational Linguistics, 2010.': 0.4055033326148987, '2. Korkontzelos, Ioannis, and Suresh Manandhar. ""Uoy: Graphs of unambiguous vertices for word sense induction and disambiguation."" Proceedings of the 5th international workshop on semantic evaluation. Association for Computational Linguistics, 2010.': 0.6274468302726746, '3. Yarowsky, David. ""Unsupervised word sense disambiguation rivaling supervised methods."" Proceedings of the 33rd annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1995.': 0.6719644665718079, '4. Mahadevan, Sridhar,  and Sarath Chandar Reasoning about Linguistic Regularities in Word Embeddings using Matrix Manifolds https://arxiv.org/abs/1507.07636': 0.40561121702194214, 'This paper describe a new method to capture word polysemy with word embeddings.': 0.42717960476875305, 'In order to disambiguate a word in a given sentence, the word is represented by the subspace spanned by the word vectors of the context in which it appears.': 1.0707042217254639, 'This departs from a traditional approach were the context is represented as a (weighted) sum of the word vectors.': 1.0938835144042969, 'A clustering algorithm (very similar to k-means), is then used to cluster the different usages of a given word, and discover the different senses (each sense corresponding to a cluster).': 0.3187963366508484, 'The proposed method is evaluated on various word sense induction datasets.': 0.8286817669868469, 'It is compared to other word embedding techniques which model word polysemy.': 0.843994677066803, 'The method proposed in the paper to represent words in context is really interesting, simple to apply and seems very effective, based on the strong experimental results reported in the paper.': 0.413418710231781, 'My main concern about this paper is the writing, which is sometimes a bit verbose, making it hard to follow the description of the method.': 1.0986123085021973, 'Some of the justification (""intersection hypothesis"", ""polysemy intersection hypothesis"") might feel a bit like hand waving.': 1.0986123085021973, 'Overall, the work presented in the paper looks solid.': 1.0986123085021973, 'Pros:': 1.0700089931488037, '- I really liked the idea of representing a word in context by a subspace (as opposed to a weighted sum). Indeed, such representations captures much more information than a single vector.': 1.0869956016540527, '- The proposed method also obtain very good results, compared to existing polysemous word embeddings.': 1.0972548723220825, '- It can be used with any word vectors, making its application very easy.': 1.0974106788635254, 'Cons:': 1.0986098051071167, '- I felt that the paper is sometimes a bit verbose and some justifications might be a bit hand waving.': 1.096439242362976, '- I am also wondering how much of the improvement over existing approaches is due to the quality of the word2vec embeddings, or due to the proposed approach. It would therefore be nice to have a comparison with a regular k-means approach, where context are represented as sum of word vectors using the same embeddings.': 1.0963712930679321, 'This paper presents a study of the spaces around existing word embeddings.': 1.0986088514328003, 'I proposes something unorthodox: instead of representing a word token by a vector, represent it by the subspace spanned by embeddings of the context word types around that token.': 1.0980851650238037, 'These subspaces are fairly low-dimensional and are shown to capture some notions of polysemy (subspaces for tokens of the same sense should all roughly intersect in the same direction).': 1.0986053943634033, 'While thinking about the subspace spanned by the context is fairly similar to thinking about a linear combination of the context embeddings, the subspace picture allows for a little more information to be preserved which can improve downstream semantic tasks.': 1.0986082553863525, 'The paper is a little dense reading at times, and some things are hard to understand, but the perspective is original enough and the results are good enough that I think it belongs in ICLR.': 1.0982956886291504}"
181,https://openreview.net/forum?id=HJrDIpiee,"{'This paper combines DRQN with eligibility traces, and also experiment with the Adam optimizer for optimizing the q-network.': 1.0986121892929077, 'This direction is worth exploring, and the experiments demonstrate the benefit from using eligibility traces and Adam on two Atari games.': 1.0986123085021973, 'The methods themselves are not novel.': 1.097924828529358, 'Thus, the primary contributions are (1) applying eligibility traces and Adam to DRQN and (2) the experimental evaluation.': 1.089844822883606, 'The paper is well-written and easy to understand.': 1.0977791547775269, 'The experiments provide quantitative results and detailed qualitative intuition for how and why the methods perform as they do.': 1.0979872941970825, 'However, with only two Atari games in the results, it is difficult to tell how well it the method would perform more generally.': 1.0985971689224243, 'Showing results on several more games and/or other domains would significantly improve the paper.': 1.0985994338989258, 'Showing error bars from multiple random seeds would also improve the paper.': 1.0986123085021973, 'The paper presents a deep RL with eligibility traces.': 1.0299785137176514, 'The authors combine DRQN with eligibility traces for improved training.': 1.0986123085021973, 'The new algorithm is evaluated on a two problems, with a single set of hyper-parameters, and compared with DQN.': 1.0986120700836182, 'The topic is very interesting.': 1.098564624786377, 'Adding eligibility traces to RL updates is not novel, but this family of the algorithms have not been explored for deep RL.': 1.0747904777526855, 'The paper is written clearly, and the related literature is well-covered.': 1.0986123085021973, 'More experiments would make this promising paper much stronger.': 1.0986123085021973, ""As this is an investigative, experimental paper, it is crucial for it to contain a wider range of problems, different hyper-parameter settings, and comparison with vanilla DRQN, Deepmind's DQN implementation, as well as other state of the art methods."": 1.0986123085021973, 'This paper investigates the use of eligibility traces with recurrent DQN agents.': 1.093814492225647, 'As in other recent work on deep RL, the forward view of Sutton and Barto is used to make eligibility traces practical to use with neural networks.': 1.0985357761383057, 'Experiments on the Atari games Pong and Tennis show that traces work better than standard Q-learning.': 1.0974876880645752, 'The paper is well written and the use of traces in deep RL is indeed underexplored, but the experiments in the paper are too limited and do not answer the most interesting questions.': 1.0986119508743286, 'As pointed out in the questions, n-step returns have been shown to work better than 1-step returns both in the classical RL literature and more recently with deep networks.': 1.0986123085021973, '[1] shows that using n-step returns in the forward view with neural networks leads to big improvements on both Atari and TORCS.': 0.3998749256134033, 'Their n-step Q-learning method also combines returns of different length in expectation, while traces do this explicitly.': 1.0986123085021973, 'This paper does not compare traces with n-step returns and simply shows that traces used in the forward view help on two Atari games.': 1.0986123085021973, 'This is not a very significant result.': 1.0986121892929077, 'It would be much more interesting to see whether traces improve on what is already known to work well with neural networks.': 1.0986096858978271, 'The other claimed contribution of the paper is showing the strong effect of optimization.': 1.0986123085021973, 'As with traces, I find it hard to make any conclusions from experiments on two games with fixed hyperparameter settings.': 1.0986123085021973, 'This has already been demonstrated with much more thorough experiments in other papers.': 1.0985604524612427, 'One could argue that these experiments show that importance of hyperparameter values and not of the optimization algorithm itself.': 1.0986123085021973, ""Without tuning the optimization hyperparameters it's hard to claim anything about the relative merits of the methods."": 1.0986123085021973, '[1] ""Asynchronous Methods for Deep Reinforcement Learning"", ICML 2016.': 1.0986123085021973}"
182,https://openreview.net/forum?id=HJtN5K9gx,"{'This paper investigates deep generative models with multiple stochastic nodes and gives them meaning by semi-supervision.': 1.0986123085021973, 'From a methodological point of view, there is nothing fundamentally novel (it is very similar to the semi-supervised work of Kingma et al; although this work has sometimes more than two latent nodes, it is not a complex extension).': 1.0986120700836182, 'There is a fairly classical auxiliary variable trick used to make sure the inference network for y is trained over all data points (by supposing y is in fact is a latent variable with an observation \\tilde y; the observation is y if y is observed, or uninformative for unobserved y).': 1.0986123085021973, ""Alternatively, one can separate the inference used to learn the generative model (which throws out inference over y if it is observed), from an inference used to 'exercise' the model (approximate the complex p(y|x) in the model by a simpler q(y|x) - effectively inferring the target p(y|x) for the data where only x is collected)."": 1.097799301147461, 'Results are strong, although on simple datasets.': 1.098589539527893, 'Overall this is a well written, interesting paper, but lacking in terms of methodological advances.': 1.0959906578063965, 'Minor:': 1.0986123085021973, 'I feel the title is a bit too general for the content of the paper.': 1.0986123085021973, ""I personally don't agree with the strong contrast made between deep generative models and graphical models (deep generative models are graphical models, but they are more typically learned and un-interpretable than classical graphical models; and having multiple stochastic variables is not exclusive to graphical models, see DRAW, Deep Kalman Filter, Recurrent VAE, etc.)."": 1.0984435081481934, ""The word 'structure' is a bit problematic; here, the paper seems more concerned with disentangling and semanticizing the latent representation of a generative model by supervision."": 0.7905000448226929, 'It is debatable whether the models themselves have structure.': 1.0986123085021973, 'This paper proposed a variant of the semi-supervised VAE model which leads to a unified objective for supervised and unsupervised VAE.': 1.0984992980957031, 'This variant gives software implementation of these VAE models more flexibility in specifying which variables are supervised and which are not.': 1.0986123085021973, 'This development introduces a few extra terms compared to the original semi-supervised VAE formulation proposed by Kingma et al., 2014.': 0.776739239692688, 'From the experiment results it seems that these terms do not do much as the new formulation and the performance difference between the proposed method and Kingma et al. 2014 are not very significant (Figure 5).': 1.0986121892929077, 'Therefore the benefit of the new formulation is likely to be just software engineering flexibility and convenience.': 0.8853014707565308, 'This flexibility and convenience is nice to have, but it is better to demonstrate a few situations where the proposed method can be applied while for other previous methods it is non-trivial to do.': 1.0980868339538574, ""The paper's title and the way it is written make me expect a lot more than what is currently in the paper."": 1.078494906425476, 'I was expecting to see, for example, structured hidden variable model for the posterior (page 4, top), or really ""structured interpretation"" of the generative model (title), but I didn\'t see any of these.': 1.0985772609710693, 'The main contribution of this paper (a variant of the semi-supervised VAE model) is quite far from these.': 1.0985583066940308, 'Aside from these, the plug-in estimation for discrete variables only works when the function h(x,y) is a continuous function of y.': 1.0973986387252808, 'If however, h(x, y) is not continuous in y, for example h takes one form when y=1 and another form when y=2, then the approach of using Expectation[y] to replace y will not work.': 1.0097906589508057, 'Therefore the ""plug-in"" estimation has its limitations.': 1.0986123085021973, 'This paper introduces a variant of the semi-supervised variational auto-encoder (VAE) framework.': 1.0986123085021973, 'The authors present a way of introducing structure (observed variables) inside the recognition network.': 1.0986123085021973, 'I find that the presentation of the inference with auxiliary variables could be avoided, as it actually makes the presentation unnecessarily complicated.': 1.0986123085021973, 'Specifically, the expressions with auxiliary variables are helpful for devising a unified implementation, but modeling-wise one can get the same model without these auxiliary variables and recover a minimal extension of VAE where part of the generating space is actually observed.': 1.0986123085021973, 'The observed variables mean that the posterior needs to also condition on those, so as to incorporate the information they convey.': 1.0986123085021973, ""The way this is done in this paper is actually not very different from Kingma et al. 2014, and I am surprised that the experiments show a large deviation in these two methods' results."": 1.0986114740371704, ""Given the similarity of the models, it'd be useful if the authors could give a possible explanation on the superiority of their method compared to Kingma et al. 2014."": 1.0986123085021973, 'By the way, I was wondering if the experimental setup is the same as in Kingma et al. 2014 for the results of Fig.': 1.0986123085021973, ""5 (bottom) - the authors mention that they use CNNs for feature extraction but from the paper it's not clear if Kingma et al."": 1.0979975461959839, 'do the same.': 1.0986123085021973, 'On a related note, I was wondering the same for the comparison with Jampani et al. 2015.': 1.0986123085021973, 'In particular, is that model also using the same rate of supervision for a fair comparison?': 1.0986123085021973, 'The experiment in section 4.3 is interesting and demonstrates a useful property of the approach.': 1.0986123085021973, 'The discussion of the supervision rate (and the pre-review answer) is helpful in giving some insight about what is a successful training protocol to use in semi-supervised learning.': 1.0986123085021973, 'Overall, the paper is interesting but the title and introduction made me expect something more from it.': 1.0986123085021973, 'From the title I expected a method for interpreting general deep generative models, instead the described approach was about a semi-supervised variant of VAE - naturally including labelled examples disentangles the latent space, but this is a general property of any semi-supervised probabilistic model and not unique to the approach described here.': 1.0986123085021973, 'Moreover, from the intro I expected to see a more general approximation scheme for the variational posterior (similar to Ranganath et al. 2015  which trully allows very flexible distributions), however this is not the case here.': 1.0986123085021973, 'Given the above, the contributions of this paper are in defining a slight variant of the semi-supervised VAE, and (perhaps more importantly) formulating it in a way that is amendable to easier automation in terms of software.': 1.0986123085021973, 'But methodologically there is not much contribution to the current literature.': 1.0986123085021973, 'The authors mention that they plan to extend the framework in the probabilistic programming setting.': 1.0986123085021973, 'It seems indeed that this would be a very promising and useful extension.': 1.0986123085021973, ""Minor note: three of Kingma's papers are all cited in the main text as Kingma et al. 2014, causing confusion. I suggest using Kingma et al. 2014a etc."": 1.0986123085021973}"
183,https://openreview.net/forum?id=HJy_5Mcll,"{'Paper summary: this work presents ENet, a new convnet architecture for semantic labeling which obtains comparable performance to the previously existing SegNet while being ~10x faster and using ~10x less memory.': 1.3862940073013306, 'Review summary: Albeit the results seem interesting, the paper lacks detailed experimental results, and is of limited interest for the ICLR audience.': 1.3862941265106201, 'Pros:': 1.3862943649291992, '* 10x faster': 1.3722726106643677, '* 10x smaller': 1.3862942457199097, '* Design rationale described in detail': 1.3862943649291992, 'Cons:': 1.3862943649291992, '*': 1.3862943649291992, 'The quality of the reference baseline is low.': 0.7657291889190674, 'For instance, cityscapes results are 58.3 IoU while state of the art is ~80 IoU.': 1.3854305744171143, 'Thus the results are of limited interest.': 0.7064830660820007, 'The results that support the design rationale are not provided.': 0.8724113702774048, 'It is important to provide the experimental evidence to support each claim.': 1.3438136577606201, 'Quality: the work is interesting but feels incomplete.': 1.3862899541854858, 'If your model is 10x faster and smaller, why not try build a model 10x longer to obtain improved results ?': 1.070099115371704, 'The paper focuses only on  nimbleness at the cost of quality (using a weak baseline).': 1.3844472169876099, 'This limits the interest for the ICLR audience.': 0.9083709120750427, 'Clarity: the overall text is somewhat clear, but the model description (section 3) could be more clear.': 0.4632946848869324, 'Originality: the work is a compendium of “practitioners wisdom” applied to a specific task.': 0.9259169697761536, 'It has thus limited originality.': 1.3199695348739624, 'Significance: I find the work that establishes a new “best practices all in one” quite interesting, but however these must shine in all aspects.': 1.2951555252075195, 'Being fast at the cost of quality, will limit the impact of this work.': 0.6904796361923218, 'Minor comments:': 1.386292815208435, 'Overall the text is proper english but the sentences constructions is often unsound, specific examples below.': 0.4913531541824341, 'To improve the chances of acceptance, I invite the authors to also explore bigger models and show that the same “collected wisdom” can be used both to reach high speed and high quality (with the proper trade-off curve being shown).': 0.9600549340248108, 'Aiming for only one end of the quality versus speed curve limits too much the paper.': 0.7374469041824341, '* Section 1: “mobile or battery powered … require rates > 10 fps“.': 1.3862943649291992, '10 fps with which energy budget ?': 0.47568511962890625, 'Should not this be  > 10 fps && < X Watt.': 1.386277675628662, '* “Rules and ideas” -> rules seem too strong of a word, “guidelines” ?': 1.3862940073013306, '* “Is of utmost importance” -> “is of importance” (important is already important)': 1.3862943649291992, '* “Presents a trainable network … therefore we compare to … the large majority of inference the same way”; the sentence makes no sense to me, I do not see the logical link between before and after “therefore”': 1.3862768411636353, '* Scen-parsing -> scene-parsing': 1.3862943649291992, '* It is arguable if encoder and decoder can be called “separate”': 1.3862943649291992, '* “Unlike in Noh” why is that relevant ? Make explicit or remove': 1.3862943649291992, '* “Real-time” is vague, you mean X fps @ Y W ?': 1.3862943649291992, '* Other existing architectures -> Other architectures': 1.0751906633377075, '* Section 3, does not the BN layer include a bias term ?': 1.1280161142349243, 'Can you get good results without any bias term ?': 1.3237963914871216, '* Table 1: why is the initial layer a downsampling one, since the results has half the size of the input ?': 0.715195894241333, '* Section 4, non linear operations.': 1.327236294746399, 'What do you mean by “settle to recurring pattern” ?': 1.3857029676437378, '* Section 4, dimensionality changes.': 1.3813060522079468, '“Computationally expensive”, relative to what ?': 1.350975513458252, '“This technique ... speeds-up ten times”, but does not provide the same results.': 1.2568765878677368, 'Without an experimental validation changing an apple for an orange does not make the orange better than the apple.': 0.9987857341766357, '“Found one problem”, problem would imply something conceptually wrong.': 1.199781894683838, 'This is more an “issue” or an “miss-match” when using ResNet for semantic labelling.': 0.8150390386581421, '* Section 4, factorizing filters.': 0.771702229976654, 'I am unsure of why you call nx1 filter asymmetric.': 1.1165004968643188, 'A filter could be 1xn yet be symmetric (e.g. -2 -1 0 1 2).': 1.078199028968811, 'Why not simply call them rectangular filters ?': 1.3539371490478516, 'Why would this change increase the variety ?': 1.2525432109832764, 'I would have expected the opposite.': 1.3300306797027588, '* Section 4, regularization.': 1.2004271745681763, 'Define “much better”.': 0.9841179251670837, '* Section 5.1; “640x360 is adequate for practical applications”; for _some_ applications.': 1.3618497848510742, '* Section 5.2, “very quickly” is vague and depends on the reader expectations, please be quantitative.': 0.751798152923584, '* Section 5.2, Haver -> have': 1.336864948272705, '* Section 5.2, in this work ->': 1.3762186765670776, 'In this work': 1.3862937688827515, '* Section 5.2, unclear what you use the class weighting for.': 1.3403431177139282, 'Is this for class balancing ?': 1.2304201126098633, '* Section 5.2, Cityscapes was -> Cityscapes is': 1.3647527694702148, '* Section 5.2, weighted by the average -> is each instance weighted relative the average object size.': 1.3862943649291992, '* Section 5.2, fastest model in the Cityscapes -> fastest model in the public Cityscapes': 1.3862943649291992, 'This paper aims at designing a real-time semantic segmentation network.': 1.3862943649291992, 'The proposed approach has an encoder-decoder architecture with many pre-existing techniques to improvement the performance and speed.': 1.3862943649291992, 'My concern is that the most of design choices are pretty ad-hoc and there is a lack of ablation study to validate each choice.': 1.3862943649291992, 'Moreover, most of the components are not new to the community (indexed pooling, dilated convolution, PReLu, steerable convolution, spatial dropout).': 1.3862943649291992, ""The so-called 'early down-sampling' or 'decoder size' are also just very straightforward trade-off between speed and performance through reducing the size/depth of the layers."": 1.3862943649291992, 'The performance and inference comparison is only conducted against a rather weak baseline, SegNet, which also makes the paper less convincing.': 1.3862943649291992, 'On the public benchmark the proposed model does not achieve comparable results against state-of-the-art.': 1.3862943649291992, 'As some other reviewer raised, there are some stronger model that has similar efficiency compared with SegNet.': 1.3862943649291992, 'The speed-up improvement is good yet reasonable given all the components used.': 1.3862943649291992, 'However, we also did see a big sacrifice in performance on some benchmarks, which makes all these tricks less promising.': 1.3862943649291992, 'The only fact I found impressive is that the model size is 0.7MB, which is of good practical use and helpful to dump on mobile devices.': 1.3862943649291992, 'However, there is NO analysis over how is the trade-off between the model size and the performance, and what design would result how much reduction in model size.': 1.3862943649291992, 'I did not find the memory consumption report for the inference stage, which are perhaps even more crucial for embedded systems.': 1.3862943649291992, 'Perhaps this paper does have a practical value for practical segmentation network design on embedding systems.': 1.3862943649291992, 'But I do not believe the paper brings insightful ideas that are worthy to be discussed in ICLR, either from the perspective of model compression or semantic segmentation.': 1.3862943649291992, 'This paper describes a fast image semantic segmentation network.': 1.3862943649291992, 'Many different techniques are combined to create a system much faster than the baseline SegNet approach, with accuracy comparable or somewhat worse in most of three datasets evaluated.': 1.3862943649291992, 'The choices and techniques used to achieve these speed optimizations are enumerated and described along with intuitions behind them.': 1.3862943649291992, 'However, this section lacks measurements and experimental results showing the effects of these choices.': 1.3862943649291992, 'To me, that would have been a key component to the paper.': 1.3862943649291992, 'As it stands now, we only get to see final evaluation numbers, which appear to describe a speed/accuracy tradeoff with little insight into the pieces sum to get there.': 1.3862943649291992, 'In addition, I feel there could be a more thorough comparison with different existing systems.': 1.3862943649291992, 'Only SegNet is shown in comparison tables, even though many current systems are outlined in the related work.': 1.3862721920013428, 'Additional datasets such as Pascal or COCO may be interesting here as well, perhaps with a larger version of the ENet model.': 1.3862253427505493, 'The system looks to be fast, with decent accuracy on the majority of benchmarks described.': 1.3862943649291992, 'However, as a practical implementation paper, I feel it needs to more thoroughly demonstrate the effects of each component, as well as possibly some of the sizing/tuning, in order to provide a more robust picture.': 1.3862160444259644, 'The paper introduces a lightweight network for semantic segmentation that combines several acceleration ideas.': 0.931511402130127, 'As indicated in my preliminary question, the authors do not make the case about why any of the techniques they propose is beyond what we know already: factorizing filters into alternating 1-D convolutions, using low-rank kernels, or any of the newer inception network architectures.': 1.3862943649291992, 'I have had a hard time figuring out what is the take-home message of this paper.': 1.3862297534942627, 'All of these ideas are known, and have proven their worth for detection.': 1.3862905502319336, 'If a paper is going to be accepted for applying them to semantic segmentation, then in the next conference another paper should be accepted for applying them to normal estimation, another to saliency estimation and so on.': 1.3862943649291992, 'As the authors mention in their preliminary review:': 1.3862688541412354, '""I agree that most improvements from classification architectures are straightforward to apply to object segmentation, and that\'s exactly what we\'ve done - our network is based on current state of the art models.': 1.3861393928527832, 'Instead of repeating most of the discussion on factorizing filters, etc., that has been discussed in a lot of papers already, we have decided that it\'s much more valuable to describe in depth the choices that are related to segmentation only - these are the most important contributions of our paper.""': 1.3862943649291992, 'I do not see however any in-depth discussion of certain choices - e.g. an analysis of how certain choices influence performance or speed.': 1.3862943649291992, 'Instead all one gets are some statements ""these gave a significant accuracy boost"" ""this helped a lot"", ""that did not help"", ""this turned out to work much better than that"" .': 1.3862943649291992, 'This is not informative - and is more like an informal chat rather than an in-depth discussion.': 1.3862943649291992, 'If novelty is not that important, and it is only performance or speed that matter, I am still not convinced.': 1.3862943649291992, 'The authors only compare to [1,2] (SegNet) in terms of both accuracy and speed.': 1.3862943649291992, 'I cannot see the reason why they do so, and they do not really justify it.': 1.3862943649291992, ""According to the authors' evaluation, [1] requires ~1 sec."": 1.3862943649291992, 'per frame,  while Deeplab v2, without the DenseCRF, runs at 5-8fps.': 1.3862943649291992, '(https://arxiv.org/abs/1606.00915)': 1.1771221160888672, 'On Cityscapes, SegNet/ENet are at 57/56 accuracy, while  without the DenseCRF, Deeplab V1 (@8fps) yields a mAP of ~65%; Deeplab V2 is at 70%.': 0.9891217947006226, 'So accuracy-wise this paper is 14% below a strong baseline in terms of mAP - and speed-wise only twice as fast as DeepLab, which was never optimized for speed.': 1.1173722743988037, 'For reference the current state-of-the-art on CityScales is at ~81% (https://www.cityscapes-dataset.com/benchmarks/).': 1.1980421543121338, 'So accuracy-wise the present work is practically irrelevant, and speed-wise the authors choose a very weak baseline (~1fps).': 0.7462252974510193, 'There is no systematic comparison with other acceleration techniques, and no ablation study of the impact of different choices (factorization, pooling, low/high resolution features, etc) on speed and/or efficiency.': 0.8388845324516296, 'As such, I do not see what is the take-home message of this paper and how we could use it elsewhere.': 1.3862673044204712, 'This could be useful for practitioners, e.g. as a technical report (but still the main thing one would care about are the relative improvements); but I cannot see what a researcher would use from this paper as a take-home message.': 1.3692653179168701}"
184,https://openreview.net/forum?id=Hk-mgcsgx,"{'This paper proposes a multiview learning approach to finding dependent subspaces optimized for maximizing cross-view similarity between neighborhoods of data samples.': 1.0986123085021973, 'The motivation comes from information retrieval tasks.': 0.42612168192863464, 'Authors position their work as an alternative to CCA-based multiview learning; note, however, that CCA based techniques have very different purpose and are rather broadly applicable than the setting considered here.': 1.0986123085021973, 'Main points:': 1.0986123085021973, 'I am not sure what authors mean by time complexity.': 1.009749174118042, 'It would appear that they simply report the computational cost of evaluating the objective in equation (7).': 1.0986123085021973, 'Is there a sense of how many iterations of the L-BFGS method?': 1.0906723737716675, 'Since that is going to be difficult given the nature of the optimization problem, one would appreciate some sense of how hard or easy it is in practice to optimize the objective in (7) and how that varies with various problem dimensions.': 1.0986123085021973, 'Authors argue that scalability is not their first concern, which is understandable, but if they are going to make some remarks about the computational cost, it better be clarified that the reported cost is for some small part of their overall approach rather than “time complexity”.': 1.0986123085021973, 'Since authors position their approach as an alternative to CCA, they should remark about how CCA, even though a nonconvex optimization problem, can be solved exactly with computational cost that is linear in the data size and only quadratic with dimensionality even with a naive implementation.': 1.0986120700836182, 'The method proposed in the paper does not seem to be tractable, at least not immediately.': 1.0871083736419678, 'The empirical results with synthetic data are a it confusing.': 1.098030686378479, 'First of all the data generation procedure is quite convoluted, I am not sure why we need to process each coordinate separately in different groups, and then permute and combine etc.': 1.0986086130142212, 'A simple benchmark where we take different linear transformations of a shared representation and add independent noise would suffice to confirm that the proposed method does something reasonable.': 1.0986123085021973, 'I am also baffled why CCA does not recover the true subspace - arguably it is the level of additive noise that would impact the recoverability - however the proposed method is nearly exact so the noise level is perhaps not so severe.': 1.0981091260910034, 'It is also not clear if authors are using regularization with CCA - without regularization CCA can be have in a funny manner.': 1.098608136177063, 'This needs to be clarified.': 1.0607224702835083, 'This paper presents an multi-view learning algorithm which projects the inputs of different views (linearly) such that the neighborhood relationship (transition probabilities) agree across views.': 1.0986123085021973, 'This paper has good motivation': 1.0986121892929077, 'to study multi-view learning from a more information retrieval perspective.': 1.098610758781433, 'Some concerns:': 1.0986123085021973, 'The time complexity of the algorithm in its current form is high (see last paragraph of page 4).': 1.0876502990722656, 'This might be the reason why the authors have conducted experiments on small datasets, and using linear projections.': 1.0786149501800537, 'The proposed method does have some nice properties, e.g., it does not require the projections to have the same dimension across views (I like this).': 1.0985361337661743, 'While it more directly models neighborhood relationship than CCA based approaches, it is still not directly optimizing typical retrieval (e.g., ranking-based) criteria.': 1.098590612411499, 'On the other hand, the contrastive loss in': 1.0986123085021973, 'Hermann and Blunsom.': 1.0986123085021973, 'Multilingual Distributed Representations without Word Alignment.': 1.0986123085021973, 'ICLR 2014.': 1.0985263586044312, 'is certainly a relevant ""information retrieval"" approach, and shall be discussed and compared with.': 1.0099718570709229, 'My major concern about this paper is the experiments.': 1.0986123085021973, 'As I mentioned in my previous comments, there are limited cases where linear mapping is more desirable than nonlinear mappings for dimension reduction.': 1.0986123085021973, 'While the authors have argued that linear projection may provide better interpretability, I have not found empirical justification in this paper.': 1.0986123085021973, 'Moreover, one could achieve interpretability by visualizing the projections and see what variations of the input is reflected along certain dimensions; this is commonly done for nonlinear dimension reduction methods.': 1.0986123085021973, 'I agree that the general approach here generalizes to nonlinear projections easily, but the fact that the authors have not conducted experiments with nonlinear projections and comparisons with nonlinear variants of CCA and other multi-view learning algorithms limits the significance of the current paper.': 1.0985944271087646, 'The authors develop a way learn subspaces of multiple views such that data point neighborhoods are similar in all of the views.': 1.0986108779907227, 'This similarity is measured between distributions of neighbors in pairs of views.': 1.0985597372055054, 'The motivation is that this is a natural criterion for information retrieval.': 1.0968977212905884, 'I like the idea of preserving neighborhood relationships across views for retrieval tasks.': 1.0985383987426758, 'And it is nice that the learned spaces can have different dimensionalities for different views.': 1.098293662071228, 'However, the empirical validation seems preliminary.': 0.5287010669708252, ""The paper has been revised from the authors' ICLR 2016 submission, and the revisions are welcome, but I think the paper still needs more work in order to be publishable."": 1.0984781980514526, 'In its current form it could be a good match for the workshop track.': 1.0125536918640137, 'The experiments are all on very small data sets (e.g. 2000 examples in each of train/test on the MNIST task) and not on real tasks.': 1.0986101627349854, 'The authors point out that they are not focusing on efficiency, and presumably computation requirements keep them from considering larger data sets.': 1.0986119508743286, 'However, it is not clear that there is any conclusion that can be drawn that would apply to more realistic data sets.': 0.9496068954467773, ""Considering the wealth of work that's been done on multi-view subspace learning, with application to real tasks, it is very hard to see this as a contribution without showing that it is applicable in such realistic settings."": 1.0986123085021973, 'On a more minor point, the authors claim that no other information retrieval based approaches exist, and I think this is a bit overstated.': 0.6358568668365479, 'For example, the contrastive loss of Hermann & Blunsom ""Multilingual models for compositional distributed semantics"" ACL 2014 is related to information retrieval and would be a natural one to compare against.': 0.4705905020236969, 'The presentation is a bit sloppy, with a number of vague points and confusing wordings.': 0.9913144111633301, 'Examples:': 1.0986123085021973, 'the term ""dependency"" gets used in the paper a lot in a rather colloquial way.': 1.0986123085021973, 'This gets confusing at times since it is used in a technical context but not using its technical definition.': 1.0986123085021973, '""an information retrieval task of the analyst"": vague and not quite grammatical': 1.0986123085021973, '""the probability that an analyst who inspected item i will next pick j for inspection"" is not well-defined': 1.0986123085021973, 'In the discussion of KL divergence, I do not quite follow the reasoning about its relationship to the ""cost of misses"" etc.': 1.0986123085021973, 'It would help to make this more precise (or perhaps drop it?': 1.0986123085021973, 'KL divergence is pretty well motivated here anyway).': 1.0986123085021973, 'Does C_{Penalty} (7) get added to C (6), or is it used instead?': 1.0986123085021973, 'I was a bit confused here.': 1.0986123085021973, 'It is stated that CCA ""iteratively finds component pairs"".': 1.0986123085021973, 'Note that while CCA can be defined as an iterative operation, it need not (and typically is not) solved that way, but rather all projections are found at once.': 1.0986123085021973, 'How is PCA done ""between X_i^1 and X_i^2""?': 1.0986123085021973, '""We apply nonlinear dimensionality algorithm"": what is this algorithm?': 1.0986123085021973, 'I do not quite follow what the task is in the case of the image patches and stock prices.': 1.0986123085021973, 'Other minor comments, typos, etc.:': 1.0986123085021973, 'The figure fonts are too small.': 1.0986123085021973, '""difference measures""': 1.0986123085021973, '> ""different measures""': 1.0986123085021973, '""...since, hence any two..."": not grammatical': 1.0986123085021973, '""between feature-based views and views external neighborhoods"": ?': 1.0986123085021973}"
185,https://openreview.net/forum?id=Hk1iOLcle,"{'Paper Summary:': 1.0986123085021973, 'This paper presents a new large scale machine reading comprehension dataset called MS MARCO.': 1.0986123085021973, 'It is different from existing datasets in that the questions are real user queries, the context passages are real web documents, and free form answers are generated by humans instead of spans in the context.': 1.0986123085021973, 'The paper also includes some analysis of the dataset and performance of QA models on the dataset.': 1.0986123085021973, 'Paper Strengths:': 1.0986123085021973, 'The questions in the dataset are real queries from users instead of humans writing questions given some context.': 1.0986123085021973, 'Context passages are extracted from real web documents which are used by search engines to find answers to the given query.': 1.0986123085021973, 'Answers are generated by humans instead of being spans in context.': 1.0986123085021973, 'It is large scale dataset, with an aim of 1 million queries.': 1.0986123085021973, 'Current release includes 100,000 queries.': 1.0986123085021973, 'Paper Weaknesses:': 1.0986123085021973, 'The authors say, ""We have found that the distribution of actual questions users ask intelligent agents can be very different from those conceived from crowdsourcing them from the text."", but the statement is not backed up with any study.': 1.0986123085021973, ""The paper doesn't clearly present what additional information can today's QA models learn from MS MARCO which they can't from existing datasets."": 1.0986123085021973, 'The paper should talk about what challenges are involved in obtaining a good performance on this dataset.': 1.0986123085021973, 'What are the human performances as compared to the models presented in the paper?': 1.0986123085021973, 'In section 4.1, what are the train/test splits?': 1.0986123085021973, 'The results are for the subset of MS MARCO where every query has multiple answers.': 1.0986123085021973, 'How big is that subset?': 1.0986123085021973, 'What is DSSM mentioned in row 2, Table 5?': 1.0986123085021973, 'The authors should include in the paper how experiments in section 4.2 prove that MS MARCO is a better dataset.': 1.0986123085021973, 'In Table 6, the performance of Memory Networks is already close to Best Passage.': 1.0986123085021973, 'Does that mean there is not enough room for improvement there?': 1.0986123085021973, 'The paper seems to be written in hurry, with partial analysis, evaluation and various mistakes in the text.': 1.0986123085021973, 'Preliminary Evaluation:': 1.0986123085021973, 'The proposed dataset MS MARCO is unique from existing datasets as it is a good representative of the QA task encountered by search engines.': 1.0986123085021973, 'I think it can be a very useful dataset for the community to benefit from.': 1.0986123085021973, ""Given the huge potential in the dataset, this paper lacks the analysis and evaluation needed to present the dataset's worth."": 1.0986123085021973, 'I think it can benefit a lot with a more comprehensive analysis of the dataset.': 1.0986123085021973, 'This is a dataset paper that brings unique values over existing reading comprehension challenges.': 1.0986123085021973, 'Unlike others, MS MARCO is derived from query logs, thus represents real questions that people ask, rather than solicited questions that might be rather artificial in practical settings.': 1.0986123085021973, 'There are potential downsides of using query logs however.': 1.0986123085021973, 'It may be that people adapt their language and questions for search engines such that users ask questions that they know current search engines can reasonably answer.': 1.0986123085021973, 'Thus, it may be that people limit the complexity of questions or language or both.': 1.0986121892929077, 'I think authors could have addressed this concern by being more selective about the query logs, by down-sampling on simple questions that can be easily answered by keyword matching without any sophisticated reading comprehension, and up-sampling more complex questions that require at least paraphrasing and ideally synthesis of information taken from more than one sentences.': 1.0986123085021973, 'It’s great that there are several new efforts to construct large-scale reading comprehension challenges, but my main concern is whether the majority of the questions can be answered through relatively easy text matching without intelligent reading or reasoning.': 1.0986087322235107, 'Also, the paper reads like the authors were running out of time before the deadline.': 1.0986123085021973, 'I would appreciate more analytic and quantitative comparisons against other existing datasets, and more insights on the degree of challenges required to handle QAs in MS MARCO.': 1.098610520362854, 'For example, the authors could collect statistics on QAs: (1) exact match exists in the text snippet, (2) paraphrasing is required but otherwise the relevant answer is directly available in the text snippet, (3) requires synthesizing information taken from more than one sentences, (4) requires external knowledge.': 1.0965718030929565, 'The author response mentions that (4) is unlikely, but a more formal and complete analysis would be helpful.': 1.0980368852615356, 'Summary: The paper proposes a large-scale dataset for reading comprehension, with the final goal of releasing 1 million questions and answers. The authors have currently released 100,000 queries and their answers. The dataset differs from existing reading comprehension datasets mainly w.r.t queries being sampled from user queries rather than being generated by crowd-workers and answers being generated by crowd-workers rather than being spans of text from the provided passage. The paper presents some analysis of the dataset such as distribution of answer types. The paper also presents the results of some generative and some cloze-style models on the MS MARCO dataset.': 1.0986123085021973, 'Strengths:': 1.0986123085021973, '1. The paper provides useful insights about the limitations of the existing reading comprehension datasets – questions asked by crowd-workers have different distribution compared to that of questions asked by actual users of intelligent agents, answers being restricted to span from the reading text rather than requiring reasoning across multiple pieces of text/passages.': 0.6465103626251221, '2. MS MARCO dataset has novel useful characteristics compared to existing reading comprehension datasets – questions are sampled from user queries, answers are generated by humans.': 1.0986123085021973, '3. The experimental evaluation of the existing baseline models on the MS MARCO dataset is satisfactory.': 1.0986123085021973, 'Weaknesses/Suggestions:': 1.0986123085021973, '1. The paper does not report human performance on the dataset. Human performance should be reported to estimate the difficulty of the dataset. The degree of inter-human agreement will also reflect how well the metric (being used to compute inter-human agreement and accuracies of the baseline models) can deal with variance in the sentence structure with similar semantics.': 1.0986123085021973, '2. I would like to see the comparison between the answer type distribution in the MS MARCO dataset and that in existing reading comprehension datasets such as SQuAD. This would ground the claim made in the paper the distributions of questions asked by crowd-workers is different from that of user queries.': 1.0986123085021973, '3. The paper uses automatic metrics such as ROUGE, BLEU for evaluating natural language answers. However, it is known that such metrics poorly correlate with human judgement for tasks such as image caption evaluation (Chen et al., Microsoft COCO Captions: Data Collection and Evaluation Server, CoRR abs/1504.00325 (2015)). So, I wonder how authors justify using such metrics for evaluating open-ended natural language answers.': 1.0986123085021973, '4. The paper mentions that a classifier was used to filter answer seeking queries from all Bing queries. It would be good to mention the accuracy of this classifier. This will provide insights into what percentage of the MS MARCO questions are answer seeking queries. Similarly, what is the accuracy of the information retrieval based system used to retrieve passages for filtered queries?': 1.0986123085021973, '5. Please include the description of the best passage baseline in the paper.': 1.0986123085021973, '6. Fix opening quotes, i.e. ” -> “ (for instance, on page 5, ”what” -> “what”).': 1.0986123085021973, 'Review Summary: The paper is well motivated, the use of user queries and human generated answers makes the dataset different from existing datasets.': 1.0986123085021973, 'However, I would like to see the human performance on the dataset and quantitative comparison between the distribution of questions obtained from user queries and that of crowd-sourced questions.': 1.0986123085021973, 'I would also like the authors to comment on the use of automatic metrics (such as ROUGE, BLEU) in the light of the fact that such metrics do not correlate well with human judgements for tasks such as image caption evaluation.': 1.0986123085021973}"
186,https://openreview.net/forum?id=Hk1l9Xqxe,"{'This paper applies HDP-HMM to challenging bioacoustics segmentation problems including humpback whole sound and bird sound segmentation.': 1.0986123085021973, 'Although the technique itself is not novel, the application of this data-driven method to bioacoustics segmentation is quite challenging, and may yield some scientific findings, and this is a valuable contribution to the bioacoustics field.': 1.098611831665039, 'My concern for this paper is that it does not have fair comparison of the other simple methods including BIC and AIC, and it is better to provide such comparisons.': 1.0986123085021973, 'Especially, as the authors pointed out, the computational cost of HDP-HMM is a big issue, and the other simple methods may solve this issue.': 1.0986109972000122, 'This paper presented an unsupervised approach for the automatic segmentation of bioacoustic data.': 0.4623827636241913, 'The authors applied an existing approach (Hierarchical Dirichlet Process Hidden Markov Models) to their task.': 0.41926684975624084, 'The originality of their work is the investigation of this approach on a new task, which they argue is more difficult, namely bioacoustic segmentation.': 1.0954818725585938, ""They provide evidence that this is a difficult task by explaining that there doesn't exist a consensus among human experts on how this should be done."": 1.0905205011367798, 'However, they do not provide convincing results that their approach is successful, as it fails in many cases to replicate the correct segmentations as defined by their baseline: human experts.': 1.0986121892929077, 'In addition, the clarity of the writing is extremely poor, including many grammatical errors and awkward sentences.': 1.094357967376709, 'The paper is a novel application for the sticky HDP-HMM, focused on correctly identifying the number of components in bird and whale song across a variety of datasets.': 1.09861159324646, ""It's nice to see the model applied to an interesting dataset."": 1.0596851110458374, 'My main issues with the paper have to do with structure and the choice of representation used in the model.': 1.0984467267990112, 'Namely:': 1.0986123085021973, 'The organization of the paper could be significantly improved.': 1.0377066135406494, 'There is a lot of repetitive introduction that adds little to the paper.': 1.0140597820281982, 'The first and last two sentences of the abstract could be cut.': 1.0984066724777222, 'Many other parts of the abstract basically repeat the introduction.': 1.0842450857162476, ""The second paragraph of section 2.3 also repeats your introduction - by now we know what you're doing."": 1.0986123085021973, 'I think most people reading this will have no idea what Kershenbaum (2014) is.': 0.40474987030029297, 'The description of the data should go in the experiments section.': 0.47735652327537537, '""Different hypotheses for the songs were emitted"" in the introduction is odd phrasing.': 0.7963231801986694, 'Figure 4 should be the first figure and go in the introduction.': 1.097890853881836, 'Figure 5 should be in the methods section.': 1.0986123085021973, 'A summary of Table 1 should be in the experiments section.': 1.0983587503433228, 'Generally the writing could be tightened quite a bit, which would make space for these figures.': 1.0986067056655884, 'The description of the HDP-HMM, which mostly follows the existing literature, is well done.': 1.0972529649734497, 'Some general questions about the methods used:': 1.0986090898513794, ""If you're interested in scalable inference, why use Gibbs sampling?"": 0.46457818150520325, 'Why not the beam sampler (van Gael 2008), which at least recently was the state of the art for MCMC inference in the HDP-HMM?': 0.4480193555355072, 'More generally, why use MCMC at all?': 1.0973765850067139, 'For very large datasets, most of the Bayesian ML community has converged on stochastic variational inference as the most practical method (eg Wang, Paisley and Blei 2011).': 1.0930650234222412, 'If your interest is mainly in the number of clusters, how would you address the fact that DP mixture models are known not to be consistent for estimating the true number of clusters (Miller and Harrison 2013)?': 1.09656822681427, 'MFCC features are calibrated to the human auditory system, not bird or whale auditory systems.': 0.9299113154411316, 'In your data, do you calibrate the MFCC scale to be closer to the auditory systems of the animals that generated the song?': 1.0691604614257812, 'And a final suggestion for future work, which could use the results presented here as a baseline:': 0.9515974521636963, 'Given the success of LSTMs in speech recognition in recent years, it may be the case that deep learned representations are superior to linear features (like the means of each cluster in an HDP-HMM) for animal song as well.': 0.5995790362358093, 'Have you considered a hybrid model, similar to recent work combining autoencoders and graphical models (Johnson, Duvenaud, Wiltschko, Datta and Adams 2016)?': 1.045815348625183}"
187,https://openreview.net/forum?id=Hk3mPK5gg,"{'The paper describes approaches taken to train learning agents for the 3D game Doom.': 1.0986123085021973, 'The authors propose a number of performance enhancements (curriculum learning, attention (zoomed-in centered) frames, reward shaping, game variables, post-training rules) inspired by domain knowledge.': 1.0986123085021973, 'The enhancements together lead to a clear win as demonstrated by the competition results.': 1.08077073097229, 'From Fig 4, the curriculum learning clearly helps with learning over increasingly difficult settings.': 1.0986123085021973, 'A nice result is that there is no overfitting to the harder classes once they have learned (probably because the curriculum is health and speed).': 1.0986123085021973, 'The authors conclude from Fig 5 that the adaptive curriculum is better and more stable that pure A3C; however, this is a bit of a stretch given that graph.': 1.0986123085021973, ""They go on to say that Pure A3C doesn't learn at all in the harder map but then show no result/graph to back this claim."": 1.0986114740371704, 'Tbl 5 shows a clear benefit of the post-training rules.': 1.0986123085021973, 'If the goal is to solve problems like these (3D shooters), then this paper makes a significant contribution in that it shows which techniques are practical for solving the problem and ultimately improving performance in these kinds of tasks.': 1.0986123085021973, 'Still, I am just not excited about this paper, mainly because it relies so heavily of many sources of domain knowledge, it is quite far from the pure reinforcement learning problem.': 1.0986123085021973, 'The results are relatively unsurprising.': 0.16194197535514832, 'Maybe they are novel for this problem, though.': 1.0559042692184448, ""I'm not sure we can realistically draw any conclusions about Figure 6 in the paper's current form."": 0.6939882636070251, 'I recommend the authors increase the resolution or run some actual metrics to determine the fuzziness/clarity of each row/image: something more concrete than an arrow of already low-resolution images.': 1.0986123085021973, 'Added after rebuttal:': 1.098610281944275, 'I still do not see any high-res images for Figure 6 or any link to them, but I trust that the authors will add them if accepted.': 1.0986113548278809, 'This paper basically applies A3C to 3D spatial navigation tasks.': 1.0986123085021973, 'This is not the first time A3C has been applied to 3D navigation.': 1.0986123085021973, 'In fact the original paper reported these experiments.': 1.0986123085021973, 'Although the experimental results are great, I am not sure if this paper has any additional insights to warrant itself as a conference paper.': 1.0986123085021973, 'It might make more sense as a workshop paper': 1.098606824874878, 'Are the graphs in Fig 5 constructed using a single hyper-parameter sweep?': 1.0986123085021973, 'I think the authors should report results with many random initializations to make the comparisons more robust': 1.0986123085021973, 'Overall the two main ideas in this paper': 1.0771287679672241, 'A3C and curriculums': 0.7341240048408508, 'are not really novel but the authors do make use of them in a real system.': 1.073796272277832, 'This is a solid paper that applies A3C to Doom, enhancing it with a collection of tricks so as to win one of the VizDoom competitions.': 1.0986123085021973, 'I think it is fair to expect the competition aspect to overshadow the more scientific approach of justifying every design decision in isolation, but in fact the authors do a decent job at the latter.': 1.0986123085021973, 'Two of my concerns have remained unanswered (see AnonReviewer2, below).': 1.0986088514328003, 'In addition, the citation list is rather thin, for example reward shaping has a rich literature, as do incrementally more difficult task setups, dating back at least to Mark Ring’s work in the 1990s.': 1.0986099243164062, 'There has also been a lot of complementary work on other FPS games.': 1.0984580516815186, 'I’m not asking that the authors do any direct comparisons, but to give the reader a sense of context in which to place this.': 1.0985755920410156}"
188,https://openreview.net/forum?id=Hk4_qw5xe,"{'This paper makes a valuable contribution to provide a more clear understanding of generative adversarial network (GAN) training procedure.': 1.0986123085021973, 'With the new insight of the training dynamics of GAN, as well as its variant, the authors reveal the reason that why the gradient is either vanishing in original GAN or unstable in its variant.': 1.0986123085021973, 'More importantly, they also provide a way to avoid such difficulties by introducing perturbation.': 1.0986123085021973, 'I believe this paper will inspire more principled research in this direction.': 1.0986123085021973, 'I am very interested in the perturbation trick to avoid the gradient instability and vanishment.': 1.0986121892929077, 'In fact, this is quite related to dropout trick in where the perturbation can be viewed as Bernoulli distribution.': 1.0986123085021973, 'It will be great if the connection can be discussed.': 1.0964312553405762, 'Besides the theoretical analysis, is there any empirical study to justify this trick?': 1.0986123085021973, 'Could you please add some experiments like Fig 2 and 3 for the perturbated GAN for comparison?': 1.0986123085021973, 'This is a strong submission regarding one of the most important and recently introduced methods in neural networks - generative adversarial networks.': 1.0985603332519531, 'The authors analyze theoretically the convergence of GANs and discuss the stability of GANs.': 1.0985333919525146, 'Both are very important.': 1.0986123085021973, 'To the best of my knowledge, this is one of the first theoretical papers about GANs and the paper, contrary to most of the submissions in the field, actually provides deep theoretical insight into this architecture.': 1.0986119508743286, 'The stability issues regarding GANs are extremely important since the first proposed versions of GANs architecture were very unstable and did not work well in practice.': 1.098598837852478, 'Theorems 2.4-2.6 are novel and introduces mathematical techniques are interesting.': 1.0986123085021973, 'I have some technical questions regarding the proof of Theorem 2.5 but these are pretty minor.': 1.0986123085021973, 'SUMMARY': 1.0986123085021973, 'This paper addresses important questions about the difficulties in training generative adversarial networks.': 1.0986123085021973, 'It discusses consequences of using an asymmetric divergence function and sources of instability in training GANs.': 1.0986123085021973, 'Then it proposes an alternative using a smoothening approach.': 1.0986123085021973, 'PROS': 1.0986123085021973, 'Theory, good questions, nice answers.': 1.0986123085021973, 'Makes an interesting use of concepts form analysis and differential topology.': 1.0986123085021973, 'Proposes avenues to avoid instability in GANs.': 1.0986123085021973, 'CONS': 1.0986123085021973, 'A bit too long, technical.': 1.0986123085021973, 'Some parts and consequences still need to be further developed (which is perfectly fine for future work).': 1.0986123085021973, 'MINOR COMMENTS': 1.0986123085021973, 'Section 2.1 Maybe shorten this section a bit.': 1.0986123085021973, 'E.g., move all proofs to the appendix.': 1.0986123085021973, 'Section 3 provides a nice, intuitive, simple solution.': 1.0986101627349854, 'On page 2 second bullet.': 1.0986123085021973, 'This also means that P_g is smaller than the data distribution in some other x, which in turn will make the KL divergence non zero.': 1.0986123085021973, ""On page 2, ``for not generating plausibly looking pictures'' should be ``for generating not plausibly looking pictures''."": 1.0986123085021973, 'Lemma 1 would also hold in more generality.': 1.0986123085021973, 'Theorem 2.1 seems to be basic analysis.': 1.0986123085021973, '(In other words, a reference could spare the proof).': 0.5748640894889832, 'In Theorem 2.4, it would be good to remind the reader about p(z).': 1.0975180864334106, 'Lemma 2 seems to be basic analysis.': 1.096807599067688, 'Specify the domain of the random variables.': 1.0969630479812622, 'relly - > rely': 0.5796812772750854, 'Theorem 2.2 the closed manifolds have boundary or not?': 1.098605990409851, '(already in the questions)': 0.8331751823425293, ""Corollary 2.1, ``assumptions of Theorem 1.3''."": 1.0321862697601318, 'I could not find Theorem 1.3.': 1.0986117124557495, ""Theorem 2.5 ``Therefore'' -> `Then'?"": 1.0936641693115234, ""Theorem 2.6 ``Is a... ''"": 0.986049234867096, ""-> `is a' ?"": 1.0986123085021973, 'The number of the theorems is confusing.': 1.0986123085021973}"
189,https://openreview.net/forum?id=Hk4kQHceg,"{'* Brief Summary:': 1.0986123085021973, 'This paper explores an extension of multiplicative RNNs to the LSTM type of models.': 1.0986123085021973, 'The resulting proposal is very similar to [1].': 1.0986123085021973, 'Authors show experimental results on character-level language modeling tasks.': 1.0986123085021973, 'In general, I think the paper is well-written and the explanations are quite clear.': 1.0986123085021973, '* Criticisms:': 1.0986123085021973, 'In terms of contributions, the paper is weak.': 1.0986123085021973, 'The motivation makes sense, however, very similar work has been done in [1] and already an extension over [2].': 1.0986123085021973, 'Because of that this paper mainly stands as an application paper.': 1.0986123085021973, 'The results are encouraging.': 1.0986123085021973, 'On the other hand, they are still behind the state of art without using dynamic evaluation.': 1.0986123085021973, 'There are some non-standard choices on modifications on the standard algorithms, such as ""l"" parameter of RMSProp and multiplying output gate before the nonlinearity.': 1.0986123085021973, 'The experimental results are only limited to character-level language modeling only.': 1.0986123085021973, '*': 1.0986123085021973, 'An Overview of the Review:': 1.0986123085021973, 'Pros:': 1.0986123085021973, 'A simple modification that seems to reasonably well in practice.': 1.0986123085021973, 'Well-written.': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'Lack of good enough experimental results.': 1.0986123085021973, 'Not enough contributions (almost trivial extension over existing algorithms).': 1.0986123085021973, 'Non-standard modifications over the existing algorithms.': 1.0986123085021973, '[1] Wu Y, Zhang S, Zhang Y, Bengio Y, Salakhutdinov RR.': 1.0986123085021973, 'On multiplicative integration with recurrent neural networks.': 1.0986123085021973, 'InAdvances in Neural Information Processing Systems 2016 (pp. 2856-2864).': 1.0986123085021973, '[2] Sutskever I, Martens J, Hinton GE.': 1.0986123085021973, 'Generating text with recurrent neural networks.': 1.0986123085021973, 'InProceedings of the 28th International Conference on Machine Learning (ICML-11) 2011 (pp. 1017-1024).': 1.0986123085021973, 'This paper proposes an extension of the multiplicative RNN': 1.0986123085021973, '[1] where the authors apply the same reparametrization trick to the weight matrices of the LSTM.': 1.0986123085021973, 'The paper proposes some interesting tricks, but none of them seems to be very crucial.': 1.0982130765914917, 'For instance, in Eq.': 1.0986123085021973, '(16), the authors propose to multiply the output gate inside the activation function in order to alleviate the saturation problem in logistic sigmoid or hyperbolic tangent.': 1.0986123085021973, 'Also, the authors share m_t across the inference of different gating units and cell-state candidates, at the end this brings only 1.25 times increase on the number of model parameters.': 1.0986123085021973, 'Lastly, the authors use a variant of RMSProp where they add an additional hyper-parameter  and schedule it across the training time.': 1.0986123085021973, 'It would be nicer to apply the same tricks to other baseline models and show the improvement with regard to each trick.': 1.0986123085021973, 'With the new architectural modification to the LSTM and all the tricks combined, the performance is not as great as we would expect.': 1.0986123085021973, 'Why didn’t the authors apply batch normalization, layer normalization or zoneout to their models?': 1.0986123085021973, 'Was there any issue with applying one of those regularization or optimization techniques?': 1.0986123085021973, 'At the fourth paragraph of Section 4.4, where the authors connect dynamic evaluation with fast weights is misleading.': 1.0986123085021973, 'I find it a bit hard to connect dynamic evaluation as a variant of fast weights.': 1.0986123085021973, 'Fast weights do not use test error signal.': 1.098453402519226, 'In the paper, the authors claim that “dynamic evaluation uses the error signal and gradients to update the weights, which potentially increases its effectiveness, but also limits its scope to conditional generative modelling, when the outputs can be observed after they are predicted”, and I am afraid to tell that this assumption is very misleading.': 1.0986123085021973, 'We should never assume that test label information is given at the inference time.': 1.0986123085021973, 'The test label information is there to evaluate the generalization performance of the model.': 1.097588062286377, 'In some applications, we may get the label information at test time, e.g., stock prediction, weather forecasting, however, in many other applications, we don’t.': 1.098607063293457, ""For instance, in machine translation, we don't know what's the best translation at the end, unlike weather forecasting."": 1.0986123085021973, 'Also, it would be fair to apply dynamic evaluation to all the other baseline models as well to compare with the BPC score 1.19 achieved by the proposed mLSTM.': 1.0986123085021973, 'The quality of the work is not that bad, but the novelty of the paper is not that good either.': 1.0986123085021973, 'The performance of the proposed model is oftentime worse than other methods, and it is only better when dynamic evaluation is coupled together.': 1.0986123085021973, 'However, dynamic evaluation can improve the other methods as well.': 1.0986123085021973, '[1] Ilya et al., “Generating Text with Recurrent Neural Networks”, ICML’11': 1.0986123085021973, '* Clearly written.': 1.0986123085021973, '* New model mLSTM which seems to be useful according to the results.': 1.0986123085021973, 'Some interesting experiments on big data.': 1.0986123085021973, '* Number of parameters in comparisons of different models is missing.': 1.0986123085021973, '* mLSTM is behind some other models in most tasks.': 1.0986123085021973}"
190,https://openreview.net/forum?id=Hk6a8N5xe,"{'This paper presents two models for extractive document summarization: the classifier architecture and the selector architecture.': 1.077178716659546, 'These two models basically use either classification or ranking in a sequential order to pick the candidate sentences for summarization.': 1.0971046686172485, 'Experiments in this paper show the results are either better or close to the SOTA.': 1.0985239744186401, 'Technical comments:': 1.0986123085021973, 'In equation (1), there is a position-relevant component call ""positional importance"".': 1.0986123085021973, 'I am wondering how important this component is?': 0.9483209252357483, 'Is it possible to show the performance without this component?': 1.0861130952835083, 'Especially, for the discussion on impact of document structure, when the model is trained on the shuffled order but tested on the original order.': 1.09855318069458, 'A similar question about equation (1), is the content-richness component really necessary?': 1.0986119508743286, 'Since the score function already has salience part, which could measure how important of  with respect to the whole document.': 1.0986123085021973, 'For the dynamic summary representation in equation (3), why not use the same updating equation for both training and test procedures?': 1.0986119508743286, 'During test time, the model actually knows the decisions that have been made so far by the decoder.': 1.0986123085021973, 'In this way, the model will be more consistent during training and test.': 1.0974191427230835, 'I think section 5 is the most interesting part of this paper, and it is also convincing on the difference between the two architectures.': 1.0986114740371704, 'It is a little disappointing that the decoding algorithm used in this paper is too simple.': 1.0958223342895508, 'In a minimal case, both of them could use beam search and the results could be better.': 1.0984723567962646, 'This paper presents two RNN architectures for extractive document summarization.': 0.5343844890594482, 'The first one, Classifier, takes into account the order in which sentences appear in the original document, whereas the second one, Selector, chooses sentences in an arbitrary order.': 1.096483588218689, 'For each architecture, the concatenated RNN hidden state from a sentence forward and backward pass  is used as features to compute a score that captures content richness, salience, positional importance, and redundancy.': 1.0986121892929077, 'Both models are trained in a supervised manner, so the authors used ""pseudo-ground truth generation"" to create training data from abstractive summaries.': 1.0986123085021973, 'Experiments show that the Classifier model performs better, and it achieves near state-of-the-art performance for some evaluation metrics.': 1.0986120700836182, 'The proposed model is in general an extension of Cheng and Lapata, 2016.': 1.0891722440719604, 'Unfortunately, the performance is only slightly better or sometimes even worse.': 1.0982919931411743, 'The authors mentioned that one key difference how they transform abstractive summaries to become gold labels for the supervised method.': 1.0986123085021973, 'However, in the experiment results, the authors described that one potential reason their models do not consistently outperform the extractive model of Cheng & Lapata, 2016 is that the unsupervised greedy approximation may generate noisier ground truth labels than Cheng & Lapata.': 1.0986123085021973, 'Is there a reason to construct the training data similar to Cheng & Lapata, if that turns out to be a better method?': 1.098606824874878, ""In order for the proposed models to be convincing, they need to outperform this baseline that's very similar to the proposed methods more consistently, since the main contribution is improved neural architectures for extractive document summarization."": 1.0063611268997192, 'This paper provides two RNN-based architectures for extractive document summarization.': 1.0986123085021973, 'The first, ""Classify"", reads in the whole document and traverses the sentences a second time to decide whether to include them or not (0/1 decisions).': 1.0986123085021973, 'The second, ""Select"",  reads in the whole document and picks the most relevant sentence one at the time.': 1.0986123085021973, 'The models assume that oracle extractive summaries exist, and a pseudo ground-truth generation procedure is used, which mimics Svore et al. (2007) among others.': 1.0986123085021973, 'Overall, this paper seems a small increment over Cheng & Lapata (2016) and performance is similar or worse to that paper.': 1.0986123085021973, 'The problem of single document extractive summarization is not particularly exciting since in DUC 2002 (14 years ago) existing models could not beat the lead baseline (which selects the first sentences of the document).': 1.0986123085021973, ""It's a pity that this paper doesn't address the most interesting problems of abstractive summarization or apply the proposed approach to multi-document summarization."": 1.0986123085021973, ""It's also a little disappointing that the maximum sentence length had to be capped to 50, which suggests the model has some trouble to scale."": 1.0986123085021973}"
191,https://openreview.net/forum?id=Hk85q85ee,"{'The paper proposes a convergence analysis of some two-layer NNs with ReLUs.': 1.0986123085021973, 'It is not the first such analysis, but maybe it is novel on the assumptions used in the analysis, and the focus on ReLU nonlinearity that is pretty popular in practice.': 1.0986123085021973, 'The paper is quite hard to read, with many English mistakes and typos.': 1.0986123085021973, 'Nevertheless, the analysis seems to be generally correct.': 1.0986123085021973, 'The novelty and the key insights are however not always well motivated or presented.': 1.0986123085021973, 'And the argument that the work uses realistic assumptions (Gaussian inputs for example) as opposed to other works, is quite debatable actually.': 1.0986123085021973, 'Overall, the paper looks like a correct analysis work, but its form is really suboptimal in terms of writing/presentation, and the novelty and relevance of the results are not always very clear, unfortunately.': 1.0986123085021973, 'The main results and intuition should be more clearly presented, and details could be moved to appendices for example - that could only help to improve the visibility and impact of these interesting results.': 1.0986123085021973, 'This work analyzes the continuous-time dynamics of gradient descent when training two-layer ReLU networks (one input, one output, thus only one layer of ReLU units).': 0.4835859537124634, 'The work is interesting in the sense that it does not involve some unrealistic assumptions used by previous works with similar goal.': 1.0986123085021973, 'Most importantly, this work does not assume independence between input and activations, and it does not rely on noise injection (which can simplify the analysis).': 1.0983757972717285, 'Nonetheless, removing these simplifying assumptions comes at the expense of limiting the analysis to:': 1.0986123085021973, '1. Only one layer of nonlinear units': 1.0986123085021973, '2. Discarding the bias term in ReLU while keeping the input Gaussian (thus constant input trick cannot be used to simulate the bias term).': 1.0986123085021973, '3. Imposing strong assumption on the representation on the input/output via (bias-less) ReLU networks: existence of orthonormal bases to represent this relationships.': 1.0986123085021973, 'Having that said, as far as I can tell, the paper presents original analysis in this new setting, which is interesting and valuable.': 1.0955480337142944, 'For example, by exploiting the symmetry in the problem under the assumption 3': 1.0986123085021973, 'I listed above, the authors are able to reduce the high-dimensional dynamics of the gradient descent to a bivariate dynamics (instead of dealing with original size of the parameters).': 0.6678925156593323, 'Such reduction to 2D allows the author to rigorously analyze the behavior of the dynamics (e.g. convergence to a saddle point in symmetric case, or to the optimum in non-symmetric case).': 1.0986123085021973, 'Clarification Needed: first paragraph of page 2.': 1.0986123085021973, 'Near the end of the paragraph you say ""Initialization can be arbitrarily close to origin"", but at the beginning of the same paragraph you state ""initialized randomly with standard deviation of order 1/sqrt(d)"".': 1.0986123085021973, ""Aren't these inconsistent?"": 1.0986123085021973, 'Some minor comments about the draft:': 1.0986123085021973, '1. In section 1, 2nd paragraph: ""We assume x is Gaussian and thus the network is bias free"". Do you mean ""zero-mean"" Gaussian then?': 1.0986123085021973, '2. ""standard deviation"" is spelled ""standard derivation"" multiple times in the paper.': 1.0986123085021973, '3. Page 6, last paragraph, first line: Corollary 4.1 should be Corollary 4.2': 1.0986123085021973, 'In this paper, the author analyzes the convergence dynamics of a single layer non-linear network under Gaussian iid input assumptions.': 1.0986123085021973, 'The first half of the paper, dealing with a single hidden node, was somewhat clear, although I have some specific questions below.': 1.0986123085021973, 'The second half, dealing with multiple hidden nodes, was very difficult for me to understand, and the final ""punchline"" is quite unclear.': 1.0986117124557495, 'I think the author should focus on intuition and hide detailed derivations and symbols in an appendix.': 1.0986123085021973, 'In terms of significance, it is very hard for me to be sure how generalizable these results are: the Gaussian assumption is a very strong one, and so is the assumption of iid inputs.': 1.0984491109848022, 'Real-world feature inputs are highly correlated and are probably not Gaussian.': 1.0986123085021973, 'Such assumptions are not made (as far as I can tell) in recent papers analyzing the convergence of deep networks e.g. Kawaguchi, NIPS 2016.': 1.0982246398925781, 'Although the author says the no assumption is made on the independence of activations, this assumption is shifted to the input instead.': 0.6280017495155334, 'I think this means that the activations are combinations of iid random variables, and are probably Gaussian like, right?': 1.0849336385726929, ""So I'm not sure where this leaves us."": 1.0985636711120605, 'Specific comments:': 1.0986123085021973, '1. Please use D_w instead of D to show that D is a function of w, and not a constant. This gets particularly confusing when switching to D(w) and D(e) in Section 3. In general, notation in the paper is hard to follow and should be clearly introduced.': 1.0978153944015503, '2. Section 3, statement that says ""when the neuron is cut off at sample l, then (D^(t))_u"" what is the relationship between l and u? Also, this is another example of notational inconsistency that causes problems to the reader.': 0.5993503332138062, '3. Section 3.1, what is F(e, w) and why is D(e) introduced? This was unclear to me.': 1.0693806409835815, '4. Theorem 3.3 suggests that (if \\epsilon is > 0), then to have the maximal probability of convergence, \\epsilon should be very close to 0, which means that the ball B_r has radius r -> 0? This seems contradictory from Figure 2.': 0.8690533638000488, '5. Section 4 was really unclear and I still do not understand what the symmetry group really represents. Is there an intuitive explanation why this is important?': 1.0930150747299194, '6. Figure 5: what is a_j ?': 1.0986123085021973, 'I encourage the author to rewrite this paper for clarity.': 1.0986123085021973, ""In it's present form, it would be very difficult to understand the takeaways from the paper."": 1.097477912902832}"
192,https://openreview.net/forum?id=Hk8N3Sclg,"{'Thank you for an interesting read.': 1.0986123085021973, 'Pros': 1.0986123085021973, 'This paper tackles a very crucial problem of understanding communications between 2 agents.': 1.0986123085021973, 'As more and more applications of reinforcement learning are being explored, this approach brings us back to a basic question.': 0.5887700319290161, 'Is the problem solving approach of machines similar to that of humans.': 1.0986123085021973, 'The task is simple enough to make the post learning analysis intuitive.': 1.0986123085021973, 'It was interesting to see how informed agents made use of multiple symbols to transmit the message, where as agnostic agents relied only on 2 symbols.': 1.098341464996338, 'Cons': 1.0986123085021973, 'The task effectively boils down to image classification, if the 2 images sent are from different categories.': 0.48502016067504883, 'The symbols used are effectively the image class which the second agent learns to assign to either of the images.': 1.0985978841781616, 'By all means, this approach boils down to a transfer learning problem which could probably be trained much faster than a reinforcement learning algorithm.': 1.0984673500061035, 'To train natural language systems by putting multiple agents within an interactive referential communication game is very nice.': 1.0986123085021973, 'As the authors mention, there has been some (although seemingly not much) previous work on using multi-agent games to teach communication, and it certainly seems like a direction worth pursuing.': 1.0986123085021973, 'Moreover, the approach of switching between these games and some supervised learning, as in the experiment described in Section 5 and suggested in Section 6, seems particularly fruitful.': 1.0986123085021973, 'Note: For “clarity”, I believe some of the network connections in Fig 1 have been omitted.': 1.0986123085021973, 'However, given the rather highly-customized architecture and the slightly hard-to-follow description in Section 3, the shorthand diagram only adds to the confusion.': 1.0986123085021973, 'The diagram probably needs to be fine-tuned, but at the very least (especially if I am misunderstanding it!), a caption must [still] be added to help the reader interpret the figure.': 1.0986123085021973, 'Overall, the framework (Section 2) is great and seems quite effective/useful in various ways, the results are reasonable, and I expect there will be some interesting future variations on this work as well.': 1.0986123085021973, 'Caveat: While I am quite confident I understood the paper (as per confidence score below), I do not feel I am sufficiently familiar with the most closely related literature to accurately assess the place of this work within that context.': 1.098577857017517, 'In this paper, a referential game is proposed between two agents.': 0.9617531895637512, 'Both agents observe two images.': 1.0984658002853394, 'The first agent, called the sender, receive a binary target variable (t) and must send a symbol (message) to the second agent, called the receiver, such that this agent can recover the target.': 1.0986034870147705, 'The agents both get a reward, if the receiver agent can predict the target.': 1.0986084938049316, 'The paper proposes to parametrize the agents as neural networks - with pretrained representations of the images as feature vectors - and train them using REINFORCE.': 1.0983829498291016, 'In this setting, it is shown that the agents converge to  optimal policies and that their learned communications (e.g. the symbolic code transmitted from the sender to the receiver) have some meaningful concepts.': 1.0986123085021973, 'In addition to this, the paper presents experiments on a variant of the game grounded on different image classes.': 1.0986123085021973, 'In this setting, the agents appear to learn even more meaningful concepts.': 1.0982025861740112, 'Finally, multi-game setup is proposed, where the sender agent is alternating between playing the game before and playing a supervised learning task (classifying images).': 1.0986123085021973, 'Not surprisingly, when anchored to the supervised learning task, the symbolic communications have even more meaningful concepts.': 1.084200382232666, 'Learning shared representations for communication in a multi-agent setup is an interesting research direction to explore.': 1.0985996723175049, 'This is a much harder task compared to standard supervised learning or single-agent reinforcement learning tasks, which justifies starting with a relatively simple task.': 1.0986123085021973, 'To the best of my knowledge, the approach of first learning communication between two agents and then grounding this communication in human language is novel.': 1.0986042022705078, 'As the authors remark, this may be an alternative paradigm to standard sequence-to-sequence models which tend to focus on statistical properties of language rather than their functional aspects.': 1.096292495727539, 'I believe the contributions of the proposed task and framework, and the analysis and visualization of what the communicated tokens represent is a useful stepping stone for future work.': 1.0557445287704468, 'For this reason, I think the paper should be accepted.': 1.0986123085021973, 'Other comments:': 1.0986123085021973, 'How is the target (t) incorporated into the sender networks?': 1.0986123085021973, 'Please clarify this.': 1.0986123085021973, 'Table 1 and Table 2 use percentage (%) values differently.': 1.0986123085021973, 'In the first, percentages seem to be written in the interval': 1.0986123085021973, '[0, 100], and in the second in the interval [0, 1].': 1.0986123085021973, 'Please correct this.': 1.0986123085021973, 'Perhaps related to this, in Table 1, the column ""obs-chance purity"" seems to have extremely small values.': 1.0986123085021973, 'I assume this was mistake?': 1.0986123085021973, '""assest"" -> ""assess""': 1.0986123085021973, '""usufal"" -> ""usual""': 1.0986123085021973}"
193,https://openreview.net/forum?id=Hk8TGSKlg,"{'First I would like to apologize for the delay in reviewing.': 1.098297357559204, 'Summary : this work introduces a novel memory based artificial neural network for reading comprehension. Experiments show improvement on state of the art.': 1.0986096858978271, 'The originality of the approach seems to be on the implementation of an iterative procedure with a loop testing that the current answer is the correct one.': 1.098610281944275, 'In order to get a better sense of the reason for improvement it would be interesting to have a complexity and/or a time analysis of the algorithm.': 1.0986077785491943, 'I might be mistaken': 0.9490056037902832, ""but I don't see you reporting anything on the actual number of loops necessary in the reported experiments."": 1.0986123085021973, 'The dataset description in section 2.2, should be moved to section 4 where the other datasets are described.': 1.0986123085021973, 'Thie paper proposed an iterative memory updating model for cloze-style question-answering task.': 1.0985612869262695, 'The approach is interesting, and result is good.': 1.0986119508743286, 'For the paper, I have some comments:': 0.6004074811935425, '1. Actually the model in the paper is not single model, it proposed two models. One consists of ""reading"", ""writing"", ""adaptive computation"" and "" Answer module 2"", the other one is ""reading"", ""composing"", ""writing"", ""gate querying"" and ""Answer module 1"". Based on the method section and the experiment, it seems the ""adaptive computation"" model is simpler and performs better. And without two time memory update in single iteration and composing module, the model is similar to neural turing machine.': 0.7932330369949341, '2. What is the MLP setting in the composing module?': 1.025960922241211, '3. This paper tested different size of hidden state:[256, 368, 436, 512], I do not find any relation between those numbers, how could you find 436? Is there any tricks helping you find those numbers?': 0.7137731313705444, '4. It needs more ablation study about using different T such as T=1,2..': 1.0921841859817505, '5. According to my understanding, for the adaptive computation,  it would stop when the P_T <0. So what is the distribution of T in the testing data?': 0.006532453000545502, 'This paper proposed an iterative query updating mechanism for cloze-style QA.': 1.0986123085021973, 'The approach is novel and interesting and while it is only verified in the paper for two Cloze-style tasks (CBT and WDW), the concept of read/compose/write operations seem to be more general and can be potentially applied to other reasoning tasks beyond Cloze-style QA.': 1.0985746383666992, 'Another advantage of the proposed model is to learn when to terminate the iteration by the so-called adaptive computation model, such that it avoids the issue of treating the number of iterations as another hyper-parameter, which is a common practice of iterative models/multi-hop reasoning in previous papers.': 1.0986123085021973, 'There are a couple places that this paper can improve.': 1.098610758781433, 'First, I would like to see the results from CNN/Daily Mail as well to have a more comprehensive comparison.': 1.0882785320281982, 'Secondly, it will be useful to visualize the entire M^q sequence over time t (not just z or the query gating) to help understand better the query regression and if it is human interpretable.': 1.0986123085021973}"
194,https://openreview.net/forum?id=Hk8rlUqge,"{'The paper introduces the joint multimodal variational autoencoder, a directed graphical model for modeling multimodal data with latent variable.': 1.0986123085021973, 'the model is rather straightforward extension of standard VAE where two data modalities are generated from a shared latent representation independently.': 0.8933824896812439, 'In order to deal with missing input modalities or bi-directional inference between two modalities the paper introduces modality-specific encoder that is trained to minimize the KL divergence of latent variable distributions between joint and modality-specific recognition networks.': 1.0986123085021973, 'The paper demonstrates its effectiveness on MNIST and CelebA datasets, both in terms of test log-likelihoods and the conditional image generation and editing.': 1.0986123085021973, 'The proposed method is rather straightforward extension of VAE and therefore the model should inherent the probabilistic inference methods of VAE.': 1.0986123085021973, 'For example, for missing data modalities, the model should be able to infer joint representation as well as filling in the missing modalities via iterative sampling as introduced by Rezende et al. (2014).': 1.0986123085021973, 'Given marginal improvement, I am not convinced by the contribution of modality-specific encoders in Section 3.3.': 1.0986123085021973, 'In addition, the inference methods introduced for generating Figure 5 looks somewhat unprincipled; I am wondering the conditional image generation results by following more principled approach (e.g., iterative sampling).': 1.0986123085021973, 'Experimental results on joint image-attribute generation is also missing.': 1.098598599433899, 'The proposed method of modeling multimodal datasets is a VAE with an inference network for every combination of missing and present modalities.': 1.0985651016235352, 'The method is evaluated on modeling MNIST and CelebA datasets.': 0.7815391421318054, 'MNIST is hardly a multimodal dataset.': 1.0986117124557495, 'The authors propose to use the labels as a separate modality that gets modeled with a variational autoencoder.': 1.0984303951263428, 'The reviewer finds this choice perplexing.': 1.0957179069519043, 'Even then the modalities are never actually missing, so the applicability of the suggested method is questionable.': 1.0986123085021973, 'In addition the differences in log-likelihoods between different models are tiny, and likely to be due to noise.': 1.0986100435256958, 'The other experiment reports log-likelihood of models that were not trained to maximize log-likelihood.': 1.0986114740371704, 'It is not clear what conclusions can be drawn from such comparison.': 1.0986056327819824, 'This paper proposes an alternative to Conditional Variational Auto-Encoders and Conditional MultiModal Auto-Encoders to perform inference of missing modalities in dataset with multiple modalities.': 1.0982357263565063, 'The proposed approach is a Variational Auto-Encoder jointly on all the modalities  with additional KL divergence penalties between the approximate posterior given all the modalities and the approximate posterior given a subset of the modalities.': 0.511315643787384, 'The approach is named Joint Multimodal Variational Auto-Encoder.': 1.085733413696289, 'The authors make a connection between this approach and the Variation of Information.': 1.0979580879211426, 'It is unclear why the authors chose the JMVAE approach instead of a more elegant Variation of Information approach.': 0.5579404830932617, 'Another unaddressed issue is the scalability of the method.': 0.904979944229126, 'As far as I can tell (given that no code is provided and the specification of the encoder is missing), this approach requires a new encoder per subset of missing modalities.': 1.0986121892929077, 'Right now this approach seems to scale since there are only two modalities.': 1.0986123085021973, 'The fact that the estimating the log-likelihood log(p(x)) using multiple modalities provide a lower value than with just one in Table 1 is a bit odd.': 1.098609209060669, 'Could you explain that ?': 1.0986123085021973, 'The comparison with between the representation learned by JMVAE and CVAE might be unfair given that the representation of CVAE is learned conditionally, on the label in the case of MNIST, and should therefore not consider the label in this representation.': 1.0953510999679565, 'Intuitively, this representation could represent ""style"" as shown in (Kingma et al., 2014) in their conditional generation figure.': 1.0986002683639526, 'For CelebA, comparing log-likelihood on models that use GANs is probably not significant since GAN does not optimizes log-likelihood.': 0.4055570363998413, 'Overall this is an interesting problem and there are also interesting ideas worth exploring further, but the execution of the paper requires more work.': 1.0985838174819946}"
195,https://openreview.net/forum?id=Hk95PK9le,"{'The paper proposes a new function for computing arc score between two words in a sentence for dependency parsing.': 1.0986123085021973, ""The proposed function is biaffine in the sense that it's a combination of a bilinear score function and a bias term playing a role as prior."": 1.0986123085021973, 'The paper reports new state-of-the-art dependency parsing performances on both English PTB and Chinese TB.': 1.0986121892929077, 'The paper is very well written with impressive experimental results and analysis.': 1.0986123085021973, 'However, the idea is hardly novel regarding to the theme of the conference: the framework that the paper uses is from Kiperwasser & Goldberg (2016), the use of bilinear score function for attention is from Luong et al (2015).': 1.0985214710235596, 'Projecting BiLSTM outputs into different spaces using MLPs is a trivial step to make the model ""deeper"", whereas adding linear bias terms isn\'t confirmed to work in the experiments (table 2 shows that diag bilinear has a close performance to biaffine).': 1.0986117124557495, 'I think that this paper is more proper for NLP conferences.': 1.0985950231552124, 'The paper brings the new STOA in PTB dependency parsing.': 0.9808926582336426, 'The numbers are very impressive.': 1.0299897193908691, 'Built upon the framework of K&G parser, this improvement is achieved by mainly two things': 1.0985996723175049, '(1) the paper replace the original scorer using bilinear scorer and make a difference between the head of modifier representation (2) the hyperparameter tuning in the ADAM trainer.': 1.098397135734558, ""Although I think the bilinear modification make some sense intuitively, I don't think this contribution alone is strong enough for a conference publication."": 1.0980778932571411, 'The authors did not show a good explanation of why this approach works better in this case nor did the author show this modification is generally applicable in any other tasks.': 1.0986117124557495, 'This is primarily an engineering paper.': 1.0969831943511963, 'The authors find a small architectural modification to prior work and some hyperparameter tuning which pushes up the state-of-the-art in dependency parsing in two languages.': 1.0986123085021973, 'The architecture modification is a biaffine attention mechanism, which was inspired work in neural machine translation by Luong et al. (2015).': 1.0986123085021973, 'The proposed attention model appears to be a win-win: better accuracy, reduced memory requirements, and fewer parameters.': 1.0986119508743286, 'The performance of the model is impressive, but how the performance is achieved is not very impressive.': 1.098322868347168, 'I do not believe that there are novel insights in the paper that will generalize to other tasks, nor does the paper shed light on the dependency parsing tasks (e.g., does biaffine attention have a linguistic interpretation?).': 1.0986123085021973}"
196,https://openreview.net/forum?id=HkCjNI5ex,"{'The paper experimentally investigates a slightly modified version of label smoothing technique for neural network training, and reports results on various tasks.': 0.9630469083786011, 'Such smoothing idea is not new, but was not investigated previously in wide range of machine learning tasks.': 0.8780773282051086, 'Comments:': 1.0986123085021973, 'The paper should report the state-of-the-art results for speech recognition tasks (TIMIT, WSJ), even if models are not directly comparable.': 0.6582548022270203, 'The error back-propagation of label smoothing through softmax is straightforward and efficient.': 0.704073429107666, 'Is there an efficient solution for BP of the entropy smoothing through softmax?': 1.09811270236969, 'Although the classification accuracy could remain the same, the model will not estimate the true posterior distribution with this kind of smoothing.': 0.5294056534767151, 'This might be an issue in complex machine learning problems where the decision is made on higher level and based on the posterior estimations, e.g. language models in speech recognition.': 0.5525855422019958, 'More motivation is necessary for the proposed smoothing.': 1.098608136177063, 'Specifically, this paper suggests regularizing the estimator of a probability distribution to prefer high-entropy distributions.': 1.0986123085021973, 'This avoids overfitting.': 1.0986123085021973, 'I generally like this idea.': 1.0986123085021973, 'Regularizing the behavior of the model often makes more sense than regularizing its parameters.': 1.0986123085021973, 'After all, the behavior is interpretable, whereas the parameters are uninterpretable and work together in mysterious ways to produce the behavior.': 1.0986123085021973, 'So one might be able to choose a more sensible prior over the behavior.': 1.0986123085021973, 'In other words, prefer parameters not because they are individually close to 0': 1.0986123085021973, 'but because they jointly lead to a distribution that is plausible or low-risk a priori.': 1.0986123085021973, 'Pro: I believe that the idea is natural and sound (that is, I do not share the doubts of AnonReviewer5).': 1.0986123085021973, ""Pro: It's possible that this hasn't been well-explored yet in neural networks (not sure)."": 1.0986114740371704, 'Pro: The experimental results look good.': 1.0980627536773682, 'So maybe everyone should use this kind of regularizer.': 1.0986123085021973, 'Con: It is a kind of pollution of the scientific literature to introduce this idea to the community as if it were unconnected to (almost) anything else in machine learning.': 1.098577618598938, 'There are many, many papers that include a scaled entropy term in the optimization objective!': 1.0986123085021973, ""It's not just for reinforcement learning."": 0.9100069403648376, 'Please see the long list of connections in my pre-review questions / comments.': 1.0986123085021973, 'Con: Experimental results should always be accompanied by significance tests and error analysis.': 1.0986123085021973, 'Is your trained model actually doing better on the distribution of test data, or was your test set too small to tell?': 1.0986121892929077, 'Are the improvements robust across many different training sets?': 1.0912749767303467, 'What errors does your model fix, and what errors does it introduce?': 1.0986090898513794, ""Summary recommendation: Revise and resubmit.  ICLR has lots of submissions.  I would prefer to reward authors who not only tried something, but who properly contextualized it and carefully evaluated it.  Otherwise, there's a race to the bottom where everyone wants to be the first to try something, so that readers are confronted with a confusing sea of slapdash papers with unclear relationships."": 1.0986015796661377, 'The authors propose a simple idea.': 1.0986123085021973, 'They penalize confident predictions by using the entropy of the predictive distribution as a regularizer.': 1.0986123085021973, 'The authors consider two variations on this idea.': 1.0986123085021973, 'In one, they penalize the divergence from the uniform distribution.': 1.0986123085021973, 'In the other variation, they penalize distance from the base rates.': 1.0986123085021973, 'They term this variation ""unigram"" but I find the name odd as I\'ve never seen multi-class labels described as unigrams before.': 1.0986123085021973, 'What would a bigram be?': 1.0986123085021973, ""The idea is simple,  and while it's been used in the context of reinforcement learning, it hasn't been popularized as a regularizer for improving generalization in supervised learning."": 1.0986123085021973, 'The justifications for the idea still lacks analysis.': 1.0986123085021973, 'And the author responses comparing it to L2 regularization have some holes.': 1.0986123085021973, 'A simple number line example with polynomial regression makes clear how L2 regularization could prevent a model from badly overfitting to accommodate every data point.': 1.0986123085021973, 'In contrast, it seems trivial to fit every data point and satisfy arbitrarily high entropy.': 1.0986114740371704, 'Of course, the un-regularized optimization is to maximize log likelihood, not simply to maximize accuracy.': 1.0909075736999512, 'And perhaps something interesting may be happening at the interplay between the log likelihood objective and the regularization objective.': 1.0978271961212158, ""But the paper doesn't indicate precisely what."": 1.098604440689087, 'I could imagine the following scenario: when the network outputs probabilities near 0, it can get high loss (if the label is 1).': 1.0933488607406616, 'The entropy regularization could be stabilizing the gradient, preventing sharp loss on outlier examples.': 1.0986121892929077, 'The regularization then might owe mainly to faster convergence.': 1.0986123085021973, 'Could the authors analyze the effect empirically, on the distribution of the gradient norms?': 1.0985702276229858, 'The strength of this paper is its empirical rigor.': 0.7703943848609924, 'The authors take their idea and put it through its paces on a host of popular and classic benchmarks spanning CNNs and RNNs.': 1.0986123085021973, 'It appears that on some datasets, especially language modeling, the confidence penalty outperforms label smoothing.': 1.0986123085021973, ""At present, I rate this paper as a borderline contribution but I'm open to revising my review pending further modifications."": 1.0986123085021973, 'Typo:': 1.0986123085021973, 'In related work: ""Penalizing entropy"" - you mean penalizing low entropy': 1.0986123085021973}"
197,https://openreview.net/forum?id=HkE0Nvqlg,"{'This is a very nice paper.': 1.0986123085021973, 'The writing of the paper is clear.': 1.0986123085021973, 'It starts from the traditional attention mechanism case.': 1.0986123085021973, 'By interpreting the attention variable z as a distribution conditioned on the input x and query q, the proposed method naturally treat them as latent variables in graphical models.': 1.0986123085021973, 'The potentials are computed using the neural network.': 1.0986123085021973, 'Under this view, the paper shows traditional dependencies between variables (i.e. structures) can be modeled explicitly into attentions.': 1.0986123085021973, 'This enables the use of classical graphical models such as CRF and semi-markov CRF in the attention mechanism to capture the dependencies naturally inherit in the linguistic structures.': 1.0986123085021973, 'The experiments of the paper prove the usefulness of the model in various level — seq2seq and tree structure etc.': 1.0986123085021973, 'I think it’s solid and the experiments are carefully done.': 1.0986123085021973, 'It also includes careful engineering such as normalizing the marginals in the model.': 1.0986123085021973, 'In sum, I think this is a solid contribution and the approach will benefit the research in other problems.': 1.0986123085021973, 'This is a solid paper that proposes to endow attention mechanisms with structure (the attention posterior probabilities becoming structured latent variables).': 1.0986123085021973, 'Experiments are shown with segmental atention (as in semi-Markov models) and syntactic attention (as in projective dependency parsing), both in a synthetic task (tree transduction) and real world tasks (neural machine translation and natural language inference).': 1.0986123085021973, 'There is a small gain in using structured attention over simple attention in the latter tasks.': 1.0986123085021973, 'A clear accept.': 1.0986123085021973, 'The paper is very clear, the approach is novel and interesting, and the experiments seem to give a good proof of concept.': 1.0986123085021973, ""However, the use of structured attention in neural MT seems doesn't seem to be fully exploited here: segmental attention could be a way of approaching neural phrase-based MT, and syntactic attention offers a way of incorporating latent syntax in MT"": 1.0986123085021973, 'these seem very promising directions.': 1.0986123085021973, 'In particular it would be interesting to try to add some (semi-)supervision on these attention mechanisms (e.g. posterior marginals computed by an external parser) to see if that helps learning the attention components of the network, or at least help initializing them.': 1.0986123085021973, 'This seems to be the first interesting use of the backprop of forward-backward/inside-outside (Stoyanov et al. 2011).': 1.0986123085021973, 'As stated in sec 3.3., for general probabilistic models the forward step over structured attention corresponds to the computation of first-order moments (posterior marginals) while the backprop step corresponds to second-order moments (gradients of marginals wrt log-potentials, i.e., Hessian of log-partition function).': 1.0986123085021973, 'This extends the applicability of the proposed approach to arbitrary graphical models where these quantities can be computed efficiently.': 1.0986123085021973, 'E.g. is there a generalized matrix-tree formula that allows to do backprop for non-projective syntax?': 1.0986123085021973, 'On the negative side, I suspect the need for second-order statistics may bring some numerical instability in some problems, caused by the use of the signed log-space field.': 1.0986123085021973, 'Was this seen in practice?': 1.0986123085021973, 'Minor comments/typos:': 1.0986123085021973, 'last paragraph of sec 1: ""standard attention attention""': 1.0986123085021973, 'third paragraph of sec 3.2: ""the on log-potentials""': 1.0986123085021973, 'sec 4.1, Results: ""... as it has no information about the source ordering""': 1.0986123085021973, 'what do you mean here?': 1.0986123085021973, 'The authors propose to extend the “standard” attention mechanism, by extending it to consider a distribution over latent structures (e.g., alignments, syntactic parse trees, etc.).': 1.0986123085021973, 'These latent variables are modeled as a graphical model with potentials derived from a neural network.': 1.0986123085021973, 'The paper is well-written and clear to understand.': 0.48774412274360657, 'The proposed methods are evaluated on various problems, and in each case the “structured attention” models outperform baseline models (either one without attention, or using simple attention).': 1.0986062288284302, 'For the two real-world tasks, the improvements obtained from the proposed approach are relatively small compared to the “simple” attention models, but the techniques are nonetheless interesting.': 0.8638090491294861, 'Main comments:': 1.0986123085021973, '1. In the Japanese-English Machine Translation example, the relative difference in performance between the Sigmoid attention model, and the Structured attention model appears to be relatively small. In this case, I’m curious if the authors analyzed the attention alignments to determine whether the structured models resulted in better alignments. In other words, if ground-truth alignments are available for the dataset, or if they can be human-annotated for some test examples, it would be interesting to measure the quality of the alignments in addition to the BLEU metric.': 0.9910610914230347, '2. In the final experiment on natural language inference, I thought it was a bit surprising that using pretrained syntactic attention layers did not appear to improve model performance, but instead appear to degrade performance. I was curious if the authors have any hypotheses for why this is the case?': 0.5699227452278137, 'Minor comments:': 1.0986123085021973, '1. Typographical error: Equation 1: “p(z | x, q” → “p(z | x, q)”': 1.0782912969589233, '2. Section 3.3: “Past work has demonstrated that the techniques necessary for this approach, … ” →  “Past work has demonstrated the techniques necessary for this approach, … ”': 0.19325889647006989}"
198,https://openreview.net/forum?id=HkEI22jeg,"{'This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models.': 1.0986100435256958, 'A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images.': 1.0986123085021973, 'The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models.': 1.0986058712005615, 'This work is an important step in understanding the nonlinear response properties of visual neurons.': 1.098383903503418, 'Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models.': 1.0547327995300293, 'So this presents an important challenge to the field.': 1.0591769218444824, 'If we even had *a* model that works, it would be a starting point.': 1.0986123085021973, 'So this work should be seen in that light.': 1.0608190298080444, 'The challenge now of course is to tease apart what the rnn is doing.': 0.5250386595726013, 'Perhaps it could now be pruned and simplified to see what parts are critical to performance.': 1.0948213338851929, 'It would have been nice to see such an analysis.': 0.6385853290557861, 'Nevertheless this result is a good first start': 1.0878078937530518, 'and I think important for people to know about.': 0.23187795281410217, 'I am a bit confused about what is being called a ""movie.""': 0.803020179271698, 'My understanding is that it is essentially a sequence of unrelated images shown for 1 sec.': 0.3680517375469208, 'each.': 1.0986123085021973, 'But then it is stated that the ""frame rate"" is 1/8.33 ms.': 1.097456693649292, 'I think this must refer to the refresh rate of the monitor, right?': 0.4233783781528473, 'I would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies.': 1.0986047983169556, 'Here I would expect the rnn to have an even more profound effect, and potentially be much more informative.': 1.098610758781433, 'This is a clearly written paper with a nice, if straightforward, result: RNNs can be good predictive models of neuron firing rates in the retina.': 1.0986123085021973, 'On the one hand, the primary scientific contribution seems to just be to confirm that this approach works.': 0.994606077671051, ""On this particular stimulus locked task the gains from using the RNN seemed relatively modest, and it hasn't yet taught us anything new about the biology."": 1.0933330059051514, ""On the other hand, this (along with the concurrent work of McIntosh et al.) is introducing neural network modeling to a field that isn't currently using it, and where it should prove very effective."": 1.0986123085021973, 'I think it would be very interesting to see the results of applying a framework like this one with LFP and other neurons as input and on a shorter discretization time scale.': 1.09861159324646, 'I suspect followup work building on this proof of concept will be increasingly exciting.': 1.0986123085021973, 'Minor comments:': 1.0782824754714966, 'Sec 3.2:': 1.0985057353973389, ""I didn't understand the role of the 0.833 ms bins."": 0.4945026636123657, 'Use ""epoch"" throughout, rather than alternating between ""epoch"" and ""pass through data"".': 1.0986123085021973, 'Fig. 4 would be better with the x-axis on a log scale.': 1.065507411956787, 'This paper fits models to spike trains of retinal ganglion cells that are driven by natural images.': 1.095062255859375, 'I think the title should thus include the word “activity” at the end for otherwise it is actually formally incorrect.': 0.7942866086959839, 'Anyhow, this paper proposes more specifically a recurrent network for this time series prediction and compares it to what seems to be the previous approach of a generalized linear model.': 0.3492027521133423, 'Overall the stated paradigm is that when one can predict the spikes well then one can look into the model and learn how nature does it.': 1.0913310050964355, 'In general the paper sounds plausible, though I am not convinced that I learned a lot.': 1.0985971689224243, 'The results in figure 2 show that the RNN model can predict the spikes a bit better.': 1.0986078977584839, 'So this is nice.': 0.747972309589386, 'But now what?': 1.0399212837219238, 'You have shown that a more complicated model can produce better fits to the data, though there are of course still some variations to the real data.': 1.093556523323059, 'Your initial outline was that a better predictive model helps you to better understand the neural processing in the retina.': 1.0985695123672485, 'So tell us what you learned.': 1.0819212198257446, 'I am not a specialist of the retina, but I know that there are several layers and recurrencies in the retina, so I am not so surprised that the new model is better than the GLM.': 1.0750093460083008, 'It seems that more complicated recurrent models such as LSTM do not improve the performance according to a statement in the paper.': 1.0566352605819702, 'However, comparisons on this level are also difficult as a more complex models needs more data.': 1.098604679107666, 'Hence, I would actually expect that more layers and even a more detailed model of the retina could eventually improve the prediction even further.': 1.0888886451721191, 'I was also a bit puzzled that all the neurons in the network share all the same parameters (weights).': 0.8935308456420898, 'While the results show that these simplified models can capture a lot of the spike train characteristics, couldn’t a model with free parameters eventually outperform this one (with correspondingly more training data)?': 1.0687859058380127}"
199,https://openreview.net/forum?id=HkIQH7qel,"{'This paper proposes RaSoR, a method to efficiently representing and scoring all possible spans in an extractive QA task.': 1.0986123085021973, 'While the test set results on SQuAD have not been released, it looks likely that they are not going to be state-of-the-art; with that said, the idea of enumerating all possible spans proposed in this paper could potentially improve many architectures.': 1.0986123085021973, 'The paper is very well-written and the analysis/ablations in the final sections are mostly interesting (especially Figure 2, which confirms what we would intuitively believe).': 1.0986123085021973, 'Based on its potential to positively impact other researchers working on SQuAD, I recommend that the paper is accepted.': 1.0986119508743286, 'This paper presents an architecture for answer extraction task and evaluates on the SQUAD dataset.': 1.0986123085021973, 'The proposed model builds fixed length representations of all spans in the answer document based on recurrent neural network.': 1.0986100435256958, 'It outperforms a few baselines in exact match and F1 on SQUAD.': 1.0985360145568848, 'It is unfortunate that the blind test results are not obtained yet due to the copyright issue.': 1.0986120700836182, 'There are quite a few other systems/submissions on the SQUAD leader board that were available for comparison.': 1.0986114740371704, ""Given that there's no result on the test set reported, the grid search for hyperparameters on the dev set directly is also a concern, even though the authors did cross validation experiments."": 1.0986123085021973, 'The authors proposed RASOR to address the problem of finding the best answer span according to a given question.': 1.0984539985656738, 'The focus of the paper is mainly on how to model the relationship between question and the answer spans.': 1.0986087322235107, 'The idea proposed by this paper is reasonable, but not ground breaking.': 1.0986123085021973, 'The analysis is interesting and potentially useful.': 1.0986123085021973, 'I would hope the authors can go extra miles to analyze different choices of boundary prediction models and make a more convincing case for the necessity of modeling the score of the span globally.': 1.0986123085021973, 'The main idea behind RASOR is to globally normalize and rank the scores of the possible answer spans.': 1.0986123085021973, 'RASOR is able to achieve this by first modeling the hidden vectors of all words with LSTMs.': 1.0986123085021973, 'Then, the representation of a text span is formed by concatenating the corresponding hidden vectors of the start and the end word of the corresponding chunk.': 1.0986123085021973, 'The approach is reasonable, but not earth shattering.': 1.0986123085021973, 'Also, the table 6 shows that the improvement over end-prediction point is not very large.': 1.0986123085021973, 'I appreciate the fact that authors conduct several analysis experiments as some of them are quite interesting.': 1.0986123085021973, 'For example, it seems that question independent representation is also very import to the performance.': 1.0986123085021973, 'In addition to the current analysis, I also want to get a clear idea on what makes the current model be better than the Match-LSTM.': 1.0986123085021973, 'Is it hyper-parameter tuning?': 1.0986123085021973, 'Or it is due to the use of the question independent representation?': 1.0986123085021973, 'Another good thing about the proposed model is that it is relatively simple, so there is a chance that the proposed techniques can be combined with other newly proposed ones.': 1.0986123085021973}"
200,https://openreview.net/forum?id=HkJq1Ocxl,"{'This paper presents an approach to make a programming language (Forth) interpreter differentiable such that it can learn the implementation of high-level instruction from provided examples.': 1.0986123085021973, 'The paper is well-written and the research is well-motivated.': 1.0986123085021973, 'Overall, I find this paper is interesting and pleasure to read.': 1.0986123085021973, 'However, the experiments only serve as proof of concept.': 1.0986123085021973, 'A more detailed empirical studies can strength the paper.': 1.0986031293869019, 'Comments:': 1.0986123085021973, 'To my knowledge, the proposed approach is novel and nicely bridge programming by example and sketches by programmers.': 1.0986123085021973, 'The proposed approach borrow some ideas from probabilistic programming and Neural Turing Machine, but it is significantly different from these methods.': 1.0986123085021973, 'It also presents optimisations of the interpreter to speed-up the training.': 1.0986123085021973, 'It would be interesting to present results on different types of programming problems and see how complex of low-level code can be generated.': 1.0986123085021973, 'This paper presents an approach to do (structured) program induction based on program sketches in Forth (a simple stack based language).': 1.0986123085021973, 'They turn the overall too open problem of program induction into a slot filling problem, with a differentiable Forth interpreter, for which one can backprop through  the slots (as they are random variables).': 1.0986123085021973, 'The point of having sketches/partial programs is that one can learn more complex programs than starting from scratch (with no prior information).': 1.0986123085021973, 'The loss that they optimize (end to end through the program flow) is a L2 (RMSE) of the program memory (at targeted/non-masked adresses) and the desired output.': 1.0986123085021973, 'They show that they can learn addition, and bubble sort, both with a Permute (3-way) sketch and with a Compare (2-way) sketch.': 1.0986123085021973, 'The idea of making a language fully differentiable to write partial programs (sketches) and have them completed was previously explored in the  probabilistic programming community and more recently with TerpreT. I think': 1.0986043214797974, 'that using Forth (a very simple stack-based language) as the sketch definition language is interesting in itself, as it is between machine code (Neural Turing Machine, Stack RNN, Neural RAM approaches...) and higher level languages (Church, TerpreT, ProbLog...).': 0.4410967230796814, 'Section 3.3.1 (and Figure 2) could be made clearer (explain the color code, explain the parallel between D and the input list).': 1.0986123085021973, 'The experimental section is quite sparse, even for learning to sort, there is only one experimental setting (train on length 3 and test on length 8), and .e.g': 1.0986123085021973, 'no study of the length at which the generalization breaks (it seems that it possibly does not), no study of the ""relative runtime improvement"" w.r.t.': 1.0986123085021973, 'the training set (in size and length of input sequences).': 1.0986123085021973, 'There are no baselines (not even at least exhaustive search, one of the neural approaches would be a plus) to compare to.': 1.0986123085021973, 'Similarly, the ""addition"" experiment (section 4.2) is very shortly described, and there are no baselines either (whereas this is a staple of ""neural approaches"" to program induction).': 1.0986123085021973, 'Does ""The presented sketch, when trained on single-digit addition examples, successfully learns the addition, and generalises to longer sequences.""': 1.0986123085021973, 'mean that it generalizes to three digits or more?': 1.0986123085021973, 'Overall, the paper is very interesting, but it seems to me like the experiments do not support the claims, nor the usefulness, enough.': 1.0986123085021973, 'This paper develops a differentiable interpreter for the Forth programming': 1.0986123085021973, 'language.': 1.0986123085021973, 'This enables writing a program ""sketch"" (a program with parts left': 1.0986123085021973, 'out), with a hole to be filled in based upon learning from input-output': 1.0986123085021973, 'examples.': 1.0986123085021973, 'The main technical development is to start with an abstract machine': 1.0986123085021973, 'for the Forth language, and then to make all of the operations differentiable.': 1.0986123085021973, 'The technique for making operations differentiable is analogous to what is done': 1.0986123085021973, 'in models like Neural Turing Machine and Stack RNN.': 1.0986123085021973, 'Special syntax is developed': 1.0986123085021973, 'for specifying holes, which gives the pattern about what data should be read': 1.0873206853866577, 'when filling in the hole, which data should be written, and what the rough': 1.0867021083831787, 'structure of the model that fills the hole should be.': 1.097617268562317, 'Motivation for why one': 1.0985937118530273, 'should want to do this is that it enables composing program sketches with other': 1.058067798614502, 'differentiable models like standard neural networks, but the experiments focus': 1.0867151021957397, 'on sorting and addition tasks with relatively small degrees of freedom for how': 1.0935903787612915, 'to fill in the holes.': 1.0985978841781616, 'Experimentally, result show that sorting and addition can be learned given': 1.0986121892929077, 'strong sketches.': 0.6169121265411377, 'The aim of this paper is very ambitious: convert a full programming language to': 1.098611831665039, 'be differentiable, and I admire this ambition.': 1.09519362449646, 'The idea is provocative and I': 1.057754397392273, 'think will inspire people in the ICLR community.': 0.4134911894798279, 'The main weakness is that the experiments are somewhat trivial and there are no': 1.0828977823257446, 'baselines.': 0.9575667381286621, 'I believe that simply enumerating possible values to fill in the': 1.089464783668518, ""holes would work better, and if that is possible, then it's not clear to me"": 0.7671260237693787, 'what': 1.0986123085021973, 'is practically gained from this formulation.': 0.4825475215911865, '(The authors argue that the point': 0.41913285851478577, 'is to compose differentiable Forth sketches with neural networks sitting below,': 1.011596918106079, 'but if the holes can be filled by brute force, then could the underlying neural': 0.8384601473808289, 'network not be separately trained to maximize the probability assigned to any': 0.7580769062042236, 'filling of the hole that produces the correct input-output behavior?)': 0.8568882942199707, 'Related, one thing that is missing, in my opinion, is a more nuanced outlook of': 0.964203953742981, 'where the authors believe this work is going.': 1.0983864068984985, 'Based on the small scale of the': 0.8738539814949036, 'experiments and from reading other related papers in the area, I sense that it': 1.0986123085021973, 'is hard to scale up differentiable forth to large real-world problems.': 1.0986123085021973, 'It': 1.0986123085021973, 'would be nice to have more discussion about this, and perhaps even an experiment': 1.0986123085021973, 'that demonstrates a failure case.': 1.0986123085021973, 'Is there a problem that is somewhat more': 1.0986123085021973, 'complex than the ones that appear in the paper where the approach does not work?': 1.0986123085021973, 'What has been tried to make it work?': 1.0986123085021973, 'What are the failure modes?': 1.0986123085021973, 'What are the': 1.0986123085021973, 'challenges that the authors believe need to be overcome to make this work.': 1.0986123085021973, 'Overall, I think this paper deserves consideration for being provocative.': 1.0986123085021973, ""However, I'm hesitant to strongly recommend acceptance because the experiments"": 1.0986123085021973, 'are weak.': 1.0986123085021973}"
201,https://openreview.net/forum?id=HkLXCE9lx,"{'The authors try to address the issue of data efficiency in deep reinforcement learning by meta-learning a reinforcement learning algorithm using a hand-designed reinforcement learning algorithm (TRPO in this case).': 1.0986123085021973, 'The experiments suggest comparable performance to models with prior knowledge of the distribution over environments for bandit tasks, and experiments on random maze navigation from vision is shown as well, though the random maze experiments would benefit from a clearer explanation.': 1.0986123085021973, 'It was not obvious from the text how their experiments supported the thesis of the paper that the learned RL algorithm was effectively performing one-shot learning.': 1.0986123085021973, 'The subject of the paper is also strikingly similar to the recently-posted paper Learning to Reinforcement Learn (https://arxiv.org/pdf/1611.05763.pdf), and while this paper was posted after the ICLR deadline, the authors should probably update the text to reflect the state of this rapidly-advancing field.': 1.0986123085021973, 'The problem of maximising total discounted reward across multiple trials of an unknown MDP (but sampled from a known distribution, e.g. multi-arm bandit) can be formulated as a POMDP problem.': 1.0986123085021973, 'A single episode of the POMDP consists of multiple episodes of interaction with the underlying MDP during which the agent should efficiently explore and integrate information about the MDP to minimize reward across the whole trial.': 1.098611831665039, 'Using this observation (which is not original to this work), the authors compare using an existing RL algorithm (TRPO) to solve POMDPs against classic regret minimization methods on two classic tasks: multi-armed bandits and tabular MDPs.': 1.0986123085021973, 'Additionally, they also demonstrate their approach can scale to a visual navigation task.': 1.0985805988311768, 'While the comparison with classic regret minimization problems is useful, this paper has several weaknesses.': 1.0986123085021973, 'It seems very confusing to introduce a new name (RL^2) for an existing class of algorithms (essentially any RL method for solving POMDPs).': 1.0986013412475586, 'This terminology and the paper structure obscures to relationship between this and prior work.': 1.0985854864120483, 'The comparison with prior work training RNNs is, while improved from the previous version, still lacking.': 1.0591038465499878, 'The distinction the authors make, that prior work “focussed on memory aspect instead of fast RL”, seems somewhat arbitrary.': 1.0986123085021973, 'The visual navigation task is conceptually identical to the water maze experiment': 1.0986123085021973, '[Heess et al, 2015] or Labyrinth navigation': 1.0986123085021973, '[Mnih et al, 2016].': 1.0986123085021973, 'These prior tasks require more than just memory, the also requires meta-learning such as exploration and demonstrate “fast RL” with the agent able to improve dramatically after a single episode.': 1.0986123085021973, 'The introduction mentions the need for the use of priors on the environment to create agents which learn quickly and suggests that prior work in DeepRL is data inefficient.': 1.0986123085021973, 'Yet, the recently prior work (e.g. previous paragraph) focussed on POMDPs demonstrated one-shot learning once trained (in the “fast RL” task to use the author’s terminology).': 1.0986123085021973, 'Although the discussion highlights the potential for new algorithms and architectures which are structured to improve performance at these “multi-episode” tasks, no new algorithms or architectures are introduced.': 1.0986123085021973, 'Unfortunately, because of the limited contribution, poor comparisons with prior work and confusing terminology this paper is not suitable for ICLR without substantial revision.': 1.0986123085021973, 'The paper proposes to use RL methods on sequences of episodes instead of single episodes.': 1.0986123085021973, ""The underlying idea is the problem of 'learning to learn', and the experimental protocol proposed here allows one to understand how a neural network-based RL model can keep memory of past episodes in order to improve its ability to solve a particular problem."": 1.0986123085021973, 'Experiments are made on bandit problems, but also on maze problems and show the interesting properties of such an approach, particularly on the maze problem where the agent seems to learn to first explore the maze, and then to exploit its knowledge to quickly find the goal.': 1.0986123085021973, 'The paper is based on a very simple and natural idea which is acutally a good point.': 1.0986123085021973, 'I really like the idea, and also the experiment on the maze which is very interesting.': 1.0986123085021973, 'Experiments on bandits problem are less interesting since meta-learning models have been already proposed in the bandit problem with interesting results and the proposed model does not really bring additionnal information.': 1.0986123085021973, 'My main concerns is  based on the fact that the paper never clearly formally defines the problem that it attempts to solve.': 1.0986123085021973, 'So, between the intuitive idea and the experimental results, the reader does not understand what  exactly the learning problem is, what is its impact and/or to which concrete application it belongs to.': 1.0986123085021973, 'From my point of view, the article clearly lacks of maturity and does not bring yet a strong contribution to the field.': 1.0986123085021973, 'Good:': 1.0986123085021973, '* Interesting experimental setting': 1.0986123085021973, '* Simple and natural idea': 1.0986123085021973, '*': 1.0986123085021973, 'Nice maze experiments and model behaviour': 1.0986123085021973, 'Bad:': 1.0986123085021973, '* No real problem defined, only an intuition is given.': 1.0986123085021973, 'Is it really useful ?': 1.0986123085021973, 'For which problems ?': 1.0986123085021973, 'What is the performance criterion one wants to optimize ? ...': 1.0986123085021973, '* Bandit experiments do not really bring relevant informations': 1.0986123085021973}"
202,https://openreview.net/forum?id=HkNEuToge,"{'The paper introduces an efficient variant of sparse coding and uses it as a building block in CNNs for image classification.': 1.0986123085021973, 'The coding method incorporates both the input signal reconstruction objective as well as top down information from a class label.': 1.0986123085021973, 'The proposed block is evaluated against the recently proposed CReLU activation block.': 1.0986123085021973, 'Positives:': 1.0865570306777954, 'The proposed method seems technically sound, and it introduces a new way to efficiently train a CNN layer-wise by combining reconstruction and discriminative objectives.': 1.0986123085021973, 'Negatives:': 1.0894209146499634, 'The performance gain (in terms of classification accuracy) over the previous state-of-the-art is not clear.': 1.0986123085021973, 'Using only one dataset (CIFAR-10), the proposed method performs slightly better than the CRelu baseline, but the improvement is quite small (0.5% in the test set).': 1.0986084938049316, 'The paper can be strengthened if the authors can demonstrate that the proposed method can be generally applicable to various CNN architectures and datasets with clear and consistent performance gains over strong CNN baselines.': 1.0986123085021973, 'Without such results, the practical significance of this work seems unclear.': 1.0985908508300781, 'This paper proposes sparse coding problem with cosine-loss and integrated it as a feed-forward layer in a neural network as an energy based learning approach.': 1.0986123085021973, 'The bi-directional extension makes the proximal operator equivalent to a certain non-linearity (CReLu, although unnecessary).': 1.0986123085021973, 'The experiments do not show significant improvement against baselines.': 1.0985946655273438, 'Pros:': 1.0986088514328003, 'Minimizing the cosine-distance seems useful in many settings where compute inner-product between features are required.': 1.0986123085021973, 'The findings that the bidirectional sparse coding is corresponding to a feed-forward net with CReLu non-linearity.': 1.0986123085021973, 'Cons:': 1.098608374595642, 'Unrolling sparse coding inference as a feed-foward network is not new.': 1.0986123085021973, 'The class-wise encoding makes the algorithm unpractical in multi-class cases, due to the requirement of sparse coding net for each class.': 1.0986123085021973, 'It does not show the proposed method could outperform baseslines in real-world tasks.': 1.0986117124557495, ""First, I'd like to thank the authors for their answers and clarifications."": 1.0986123085021973, 'I find, the presentation of the multi-stage version of the model much clearer now.': 1.0977287292480469, '+': 1.0882550477981567, 'The paper states a sparse coding problem using cosine loss, which allows to solve the problem in a single pass.': 0.8706128597259521, 'The energy-based formulation allows bi-directional coding that incorporates top-down and bottom-up information in the feature extraction process.': 1.0986123085021973, 'The cost of running the evaluation could be large in the  multi-class setting, rendering the approach less attractive and the computational cost comparable to recurrent architectures.': 1.0986114740371704, '+ While the model is competitive and improves over the baseline, the paper would be more convincing with other comparisons (see text).': 1.047817587852478, 'The experimental evaluation is limited (a single database and a single baseline)': 1.0986117124557495, 'The motivation of the sparse coding scheme is to perform inference in a feed forward manner.': 1.0706998109817505, 'This property does not hold in the multi stage setting, thus optimization would be required (as clarified by the authors).': 1.0985995531082153, 'Having an efficient way of performing a bi-directional coding scheme is very interesting.': 1.0968178510665894, 'As the authors clarified, this could not necessarily be the case, as the model needs to be evaluated many times for performing a classification.': 1.0985260009765625, 'Maybe an interesting combination would be to run the model without any class-specific bias, and evaluation only the top K predictions with the energy-based setting.': 1.0986121892929077, 'Having said this, it would be good to include a discussion (if not direct comparisons) of the trade-offs of using a model as the one proposed by Cao et al.': 1.0976842641830444, 'Eg. computational costs, performance.': 0.8603404760360718, 'Using the bidirectional coding only on the top layers seems reasonable: one can get a good low level representation in a class agnostic way.': 1.0985896587371826, 'This, however could be studied in more detail, for instance showing empirically the trade offs.': 1.0939218997955322, 'If I understand correctly, now only one setting is being reported.': 1.0985969305038452, 'Finally, the authors mention that one benefit of using the architecture derived from the proposed coding method is the spherical normalization scheme, which can lead to smoother optimization dynamics.': 1.058921456336975, 'Does the baseline (or model) use batch-normalization?': 1.084910273551941, 'If not, seems relevant to test.': 1.0986123085021973, 'Minor comments:': 1.0986123085021973, 'I find figure 2 (d) confusing.': 1.0986123085021973, 'I would not plot this setting as it does not lead to a function (as the authors state in the text).': 1.0986123085021973}"
203,https://openreview.net/forum?id=HkNKFiGex,"{'The paper presents two main contributions:': 1.0986123085021973, '(1) A novel model visualization and photo manipulation technique that allows to transform an image using a paintbrush, much like in an image editing software.': 1.0985784530639648, '(2) A hybridization of GANs and VAEs called Introspective Adversarial Network.': 1.0204029083251953, 'The main problem I have with the paper is that it feels very much like two papers in one with a very loose story tying the two together.': 1.0985945463180542, 'On one hand, the neural photo editing technique is presented in sufficient detail to be reproducible and it is shown to be effective.': 1.0986123085021973, 'I personally find the idea exciting, but in order for it to be of interest to the ICLR community I think more emphasis should be put on what insights such a technique allows to gain on trained models.': 0.8544123768806458, 'On the other hand, the IAF model is introduced, along with multiple network architecture modifications for improving its performance.': 1.0986123085021973, 'One criticism that I have regarding the presentation is that it makes it hard to assign credit to individual ideas when they are presented in a ""list of things to make it work"" fashion.': 1.0986077785491943, 'I would like to see more empirical results in that direction to help clear up things.': 1.0986123085021973, 'Overall I think the paper proposes interesting ideas, but given its lack of focus on a single, cohesive story I think it is not yet ready for publication.': 1.0983096361160278, 'UPDATE:': 0.5119421482086182, ""The rating has been updated to a 6 following the authors' reply."": 1.0986123085021973, 'The rewritten paper is more focused and precise than the previous version.': 1.0222740173339844, 'The ablation studies and improved evaluation of the IAN model help to the make clear the relative contributions of the proposed MDC and orthogonal regularization.': 1.0986111164093018, 'Though the paper is much improved, in my opinion there is still too much emphasis on the photo-editing interface.': 1.0986111164093018, 'In addition, the MDC blocks are used in the generator of the model but their efficacy is measured via discriminative experiments.': 1.0986123085021973, 'All-in-all I am updating my score to a 5.': 1.0986123085021973, '==========': 1.0954856872558594, 'This paper proposes a hybridization of a VAE and a GAN whereby the generator both generates random samples and produces reconstructions of the real data, and the discriminator attempts to classify each true data point, sample, and reconstruction as being real, fake, or reconstructed.': 1.0986075401306152, 'It also proposes a user-facing interface with an interactive image editing algorithm along with various modifications to standard generative modeling architectures.': 1.0974228382110596, 'Pros:': 0.8408709764480591, '+ The IAN model itself is interesting as standard GAN-based approaches do not simultaneously train an autoencoder.': 0.5312457084655762, 'Cons:': 1.098609447479248, 'The writing is unclear at times and the mathematical formulation of the IAN is not very precise.': 1.0255029201507568, 'Many different ideas are proposed in the paper without sufficient empirical validation to characterize their individual contributions.': 1.0986123085021973, 'The photo editing interface, though interesting, is probably better suited for a conference with more of an HCI focus.': 1.0261491537094116, '* Section 2: The gradient descent step procedure seems to be quite similar to the approach proposed by [2].': 1.0986098051071167, 'More elaboration on the differences would be helpful.': 0.25204238295555115, '* Section 3: It is unclear whether the discriminator has three binary outputs or if there is a softmax over the three possible labels.': 1.0925278663635254, 'The paper does not mention whether L_G and L_D are minimized or maximized.': 1.0217152833938599, 'Presumably they are both minimized, but in that case it is counter-intuitive that the generator attempts to decrease the probability that the discriminator assigns the ""real"" label to the generated samples and reconstructions.': 1.0975897312164307, 'In addition, in the minimization scenario L_D maximizes the probability of the correct labels being assigned to X_gen and \\hat{X} but minimizes the probability of the ""real"" label being assigned to X.': 1.0983161926269531, '* Section 3.2: It is odd that not training MADE leads to better performance, as training MADE should lead to a better variational approximation to the true posterior.': 1.0971086025238037, 'More exploration seems warranted here.': 1.0986120700836182, '* Section 3.3.2: One drawback of autoregressive approaches is that sampling is slow.': 1.0977675914764404, 'How do you reconcile this with its use in an interactive application, where speed is important?': 0.5822632312774658, '* Figure 7: The Inception score is typically expressed as an exponentiated KL-divergence.': 1.098610758781433, 'It is odd that the scores are being presented here as percents.': 0.7692091464996338, ""* Section 4.1: The Inception score's direct application to non-natural images is indeed problematic."": 1.0985809564590454, 'One potential workaround is to compute exponentiated KL for a discriminative net trained specifically for the dataset, e.g. to predict binary attributes on CelebA.': 0.9897847175598145, 'Overall, the paper attempts to simultaneously do too many things.': 1.0962573289871216, 'It could be made much stronger by focusing on the primary contribution, the IAN, and performing a comparison against similar approaches such as [1].': 1.0697847604751587, 'The other techniques, such as IAF with randomized MADE, MDC blocks, and orthogonal regularization are potentially interesting in their own right but the current results are not conclusive as to their specific benefits.': 1.0558890104293823, '[1] Larsen, Anders Boesen Lindbo, Søren Kaae Sønderby, and Ole Winther.': 0.9008662700653076, '""Autoencoding beyond pixels using a learned similarity metric.""': 1.0986123085021973, 'arXiv preprint arXiv:1512.09300 (2015).': 1.0985945463180542, '[2] J.-Y. Zhu, P. Krähenbühl, E. Shechtman, and A. A. Efros, “Generative Visual Manipulation on the Natural Image Manifold,” ECCV 2016.': 0.42572131752967834, 'After rebuttal:': 1.0023119449615479, 'I think the presentation improved in the revised version (although still quite cluttered and confusing), and new quantitative results look quite convincing.': 1.0986123085021973, 'Therefore I raise my rating.': 0.6085931658744812, 'Still, the paper could use polishing.': 1.0986123085021973, 'If written in a better way, it would be a definite accept in my opinion.': 1.098610520362854, 'Initial review:': 1.0553431510925293, 'The paper presents a tool for exploring latent spaces of generative models, and ""introspective adversarial network"" model - a new hybrid of a generative adversarial network (GAN) and a variational autoencoder (VAE).': 1.098609447479248, 'On the plus side, the presented tool is interesting and may be useful for analysis of generative models, and the proposed architecture seems to perform well.': 1.0986123085021973, 'On the downside, experimental evaluation does not allow for confident conclusions, and a recent closely related work by Zhu et al.': 1.0976910591125488, '[1] is not discussed in enough detail.': 1.0257688760757446, 'Overall, I am in the borderline mode, and may change my opinion depending on how the discussion phase goes.': 1.0981346368789673, 'Detailed comments:': 1.0923497676849365, '1) The presented model combines elements of a large number of existing techniques: GAN, VAE, VAE/GAN (Lamb et al. 2016), inverse autoregressive flow (IAF), PixelRNN, ResNet, dilated convolutions.': 0.4697009027004242, 'In addition, the authors propose new modifications: orthogonal regularization (inspired by Saxe et al.), ternary discriminator in a GAN.': 0.9927114248275757, 'This makes the overall architecture complicated.': 0.9972898960113525, 'An extensive ablation study could allow to judge about the effect of different components, but the ablation study presented in the paper is somewhat restricted.': 1.097660779953003, 'What do we learn from the proposed architecture?': 1.0986123085021973, 'Can other researchers gain any new insights?': 1.0986123085021973, 'What is important, what is not?': 1.091884732246399, 'Answering these question would significantly raise the potential impact of this paper.': 1.0957858562469482, '2) Related to the previous point, proper analysis requires adequate measures of performance.': 1.006378173828125, 'Qualitative results are nice, but with the current surge of interest in generative models it gets very difficult to rely on qualitative evaluations: many methods produce visually similar results, and unless there is an obvious large jump in the quality of the produced images, it is unclear how to compare these.': 1.098601222038269, 'I do appreciate the effort authors have already put into evaluating the model: especially the keypoint error is interesting.': 1.0446715354919434, 'Unfortunately, none of the presented measures evaluates visual quality of the images.': 1.0604686737060547, 'A user study would be useful - I realize it is additional effort, but what is so restrictively difficult about it?': 1.0978866815567017, 'Perhaps not with AMT, but with some fellow researchers/students.': 1.0846811532974243, '3) Work': 1.098610520362854, '[1] looks very related to the proposed visualization tool and deserves a more thorough discussion than a single sentence in the related work section.': 0.7403985857963562, 'The paper by Zhu et al. appeared on arxiv more than 1,5 months before the ICLR deadline, and, more importantly, it has been published at ECCV before the ICLR deadline.': 0.532558798789978, 'This is unfortunate for the authors, but I think this makes the paper count as prior work, not concurrent.': 0.6423312425613403, 'In the end this is up to ACs and PCs to decide.': 0.6925270557403564, 'Anyway, I strongly suggest the authors to add a detailed discussion of differences of the two approaches, their capabilities, strengths and weaknesses.': 0.7980276346206665, 'The authors could also try to directly compare to the approach of Zhu et al. or explain why it is impossible.': 0.6407403349876404, '4) May be a good idea to extend Appendix A with more approaches (VAE and DCGAN are not state of the art, are they?': 0.7690737247467041, 'why not show at least VAE/GAN?) and more datasets.': 1.0106775760650635, 'If this is not possible, please explain why.': 0.9452958106994629, 'Samples of faces from IAN do look very impressive, but a fair comparison with SOTA would strengthen your point.': 1.0978703498840332, 'By the way, I assume the samples are random, not cherry-picked?': 0.7413678765296936, 'Please mention it in the paper.': 0.893521249294281, '5) The analysis capabilities of the proposed tool are not fully explored.': 1.0984210968017578, 'What does it teach us about generative models?': 1.0915149450302124, 'Does it work on non-face datasets?': 1.0966622829437256, 'Overall, it seems that since the paper includes two largely disjoint contributions (a tool and a generative model), none of the two gets analyzed in depth, which makes the paper look somewhat incomplete.': 1.0933401584625244, 'Small remarks:': 1.0859817266464233, '1) Why not include 8% result of IAN on SVHN into the table?': 1.0724321603775024, 'It is easy to miss otherwise.': 1.0986104011535645, 'From the table it is absolutely unclear that some of the results are not comparable.': 0.8993796110153198, 'The table should be more self-explanatory.': 0.6112174391746521, '[1] Zhu et al., ""Generative Visual Manipulation on the Natural Image Manifold"", ECCV 2016, https://arxiv.org/pdf/1609.03552v2.pdf': 0.4367520809173584}"
204,https://openreview.net/forum?id=HkNRsU5ge,"{'This paper presented a method of improving the efficiency of deep networks acting on a sequence of correlated inputs, by only performing the computations required to capture changes between adjacent inputs.': 1.0986123085021973, ""The paper was clearly written, the approach is clever, and it's neat to see a practical algorithm driven by what is essentially a spiking network."": 1.098610281944275, 'The benefits of this approach are still more theoretical than practical': 1.0985597372055054, 'it seems unlikely to be worthwhile to do this on current hardware.': 0.26487380266189575, 'I strongly suspect that if deep networks were trained with an appropriate sparse slowness penalty, the reduction in computation would be much larger.': 1.0986123085021973, 'The paper presents a method to improve the efficiency of CNNs that encode sequential inputs in a ‘slow’ fashion such that there is only a small change between the representation of adjacent steps in the sequence.': 1.0986064672470093, 'It demonstrates theoretical performance improvements for toy video data (temporal mnist) and natural movies with a powerful Deep CNN (VGG).': 1.0986123085021973, 'The improvement is naturally limited by the ‘slowness’ of the CNN representation that is transformed into a sigma-delta network: CNNs that are specifically designed to have ‘slow’ representations will benefit most.': 1.0986121892929077, 'Also, it is likely that only specialised hardware can fully harness the improved efficiency achieved by the proposed method.': 1.0967122316360474, 'Thus as of now, the full potential of the method cannot be thoroughly evaluated.': 1.0982578992843628, 'However, since the processing of sequential data seems to be a broad and general area of application, it is conceivable that this work will be useful in the design and application of future CNNs.': 1.098611831665039, 'All in all, this paper introduces an interesting idea to address an important topic.': 0.8315442800521851, 'It shows promising initial results, but the demonstration of the actual usefulness and relevance of the presented method relies on future work.': 1.0986121892929077, 'This is an interesting paper about quantized networks that work on temporal difference inputs.': 1.058738112449646, 'The basic idea is that when a network has only to process differences then this is computational much more efficient specifically with natural video data since large parts of an image would be fairly constant so that the network only has to process the informative sections of the image (video stream).': 1.0986121892929077, 'This is of course how the human visual system works, and it is hence of interest even beyond the core machine learning community.': 1.0985546112060547, 'As an aside, there is a strong community interested in event-based vision such as the group of Tobi Delbrück, and it might be interesting to connect to this community.': 1.0986123085021973, 'This might even provide a reference for your comments on page 1.': 1.0986123085021973, 'I guess the biggest novel contribution is that a rounding network can be replaced by a sigma-delta network, but that the order of discretization and summation doe make some difference in the actual processing load.': 1.0986123085021973, 'I think I followed the steps and': 1.0979465246200562, 'Most of my questions have already been answers in the pre-review period.': 1.0986123085021973, 'My only question remaining is on page 3, “It should be noted that when we refer to “temporal differences”, we refer not to the change in the signal over time, but in the change between two inputs presented sequentially.': 1.0970702171325684, 'The output of our network only depends on the value and order of inputs, not on the temporal spacing between them.”': 1.0986008644104004, 'This does not make sense to me.': 1.0733742713928223, 'As I understand you just take the difference between two frames regardless if you call this temporal or not it is a change in one frame.': 1.0977870225906372, 'So this statement rather confuses me and maybe should be dropped unless I do miss something here, in which case some more explanation would be necessary.': 1.0976862907409668, 'Figure 1 should be made bigger.': 1.0983253717422485, 'An improvement of the paper that I could think about is a better discussion of the relevance of the findings.': 1.0983513593673706, 'Yes, you do show that your sigma-delta network save some operation compared to threshold, but is this difference essential for a specific task, or does your solution has relevance for neuroscience?': 1.0986120700836182}"
205,https://openreview.net/forum?id=HkSOlP9lg,"{'This paper proposes the RIMs that unrolls variational inference procedure.': 1.0986123085021973, 'The author claims that the novelty lies in the separation of the model and inference procedure, making the MAP inference as an end-to-end approach.': 1.0967381000518799, 'The effectiveness is shown in image restoration experiments.': 1.0986123085021973, ""While unrolling the inference is not new, the author does raise an interesting perspective towards the `model-free' configuration, where model and inference are not separable and can be learnt jointly."": 1.0982080698013306, ""However I do not quite agree the authors' argument regarding [1] and"": 1.0986123085021973, '[2].': 1.0986123085021973, 'Although both [1] and [2] have pre-defined MAP inference problem.': 1.0986123085021973, 'It is not necessarily that a separate step is required.': 1.0986123085021973, 'In fact, both do not have either a pre-defined prior model or an explicit prior evaluation step as shown in Fig. 1(a).': 1.0943841934204102, 'I believe that the implementation of both follows the same procedure as the proposed, that could be explained through Fig. 1(c).': 1.0986123085021973, 'That is to say, the whole inference procedure eventually becomes a learnable neural network and the energy is implicitly defined through learning the parameters.': 1.0653948783874512, 'Moreover, the RNN block architecture (GRU) and non-linearity (tanh) restrict the flexibility and implicitly form the inherent family of variational energy and inference algorithm.': 0.7840722799301147, 'This is also similar with [1] and': 1.0986123085021973, 'Based on that fact, I have the similar feeling with R1 that the novelty is somewhat limited.': 1.0986123085021973, 'Also some discussions should be added in terms of the architecture and nonlinearity that you have chosen.': 1.0986123085021973, ""Unfortunately, even after reading the authors' response to my pre-review question, I feel this paper in its current form lacks sufficient novelty to be accepted to ICLR."": 1.0986123085021973, 'Fundamentally, the paper suggests that traditional iterative algorithms for specific class of problems (ill-posed image inverse problems) can be replaced by discriminatively trained recurrent networks.': 1.0986123085021973, ""As R3 also notes, un-rolled networks for iterative inference aren't new: they've been used to replace CRF-type inference, and _also_ to solve image inverse problems (my refs"": 1.0986123085021973, '[1-3]).': 1.0986123085021973, ""Therefore, I'd argue that the fundamental idea proposed by the paper isn't new"": 1.0983136892318726, ""it is just that the paper seeks to 'formalize' it as an approach for inverse problems (although, there is nothing specific about the analysis that ties it to inverse problems: the paper only shows that the RIM can express gradient descent over prior + likelihood objective)."": 0.9328904151916504, 'I also did not find the claims about benefits over prior approaches very compelling.': 1.097785472869873, 'The comment about parameter sharing works both ways': 1.0746310949325562, ""it is possible that untying the parameters leads to better performance over a fewer number of 'iterations', and given that the 'training set' is synthetically generated, learning a larger number of parameters doesn't seem to be an issue."": 0.8212200403213501, ""Also, I'd argue that sharing the parameters is the 'obvious' approach, and the prior methods choose to not tie the parameters to get better accuracy."": 1.0986109972000122, 'The same holds for being able to handle different noise levels / scale sizes.': 1.0986090898513794, 'A single model can always be trained to handle multiple forms of degradation': 0.4185202419757843, ""its just that its likely to do better when it's trained for specific degradation model/level."": 0.5754991769790649, 'But more importantly, there is no evidence in the current set of experiments that shows that this is a property of the RIM architecture.': 0.9495264887809753, '(Moreover, this claim goes against one of the motivations of the paper of not training a single prior for different observation models ... but to train the entire inference architecture end-to-end).': 0.7572999596595764, 'It is possible that the proposed method does offer practical benefits beyond prior work': 1.0966646671295166, ""but these benefits don't come from the idea of simply unrolling iterations, which is not novel."": 0.23523880541324615, 'I would strongly recommend that the authors consider a significant re-write of the paper': 1.0986123085021973, 'with a detailed discussion of prior work mentioned in the comments that highlights, with experiments, the specific aspects of their recurrent architecture that enables better recovery for inverse problems.': 1.0986123085021973, ""I would also suggest that to claim the mantle of 'solving inverse problems', the paper consider a broader set of inverse tasks"": 1.0986123085021973, 'in-painting, deconvolution, different noise models, and possibly working with multiple observations (like for HDR).': 1.0986123085021973, 'This paper presents a method to learn both a model and inference procedure at the same time with recurrent neural networks in the context of inverse problems.': 1.0986123085021973, 'The proposed method is interesting and results are quite good.': 1.0986123085021973, 'The paper is also nicely presented.': 1.0986123085021973, 'I would be happy to see some discussion about what the network learns in practice about natural images in the case of denoising.': 1.0986123085021973, 'What are the filters like?': 1.0986123085021973, 'Is it particularly sensitive to different structures in images?': 1.0986123085021973, 'edges?': 1.0986123085021973, 'Also, what is the state in the recurrent unit used for?': 1.0986123085021973, 'when are the gates open etc.': 1.0986123085021973, 'Nevertheless, I think this is nice work which should be accepted.': 1.0986123085021973}"
206,https://openreview.net/forum?id=HkYhZDqxg,"{'This paper proposes a variant of a recurrent neural network that has two orthogonal temporal dimensions that can be used as a decoder to generate tree structures (including the topology) in an encoder-decoder setting.': 1.0986123085021973, ""The architecture is well motivated and I can see several applications (in addition to what's presented in the paper) that need to generate tree structures given an unstructured data."": 1.0986123085021973, 'One weakness of the paper is the limitation of experiments.': 1.0983383655548096, 'IFTTT dataset seems to be an interesting appropriate application, and there is also a synthetic dataset, however it would be more interesting to see more natural language applications with syntactic tree structures.': 1.0986123085021973, 'Still, I consider the experiments sufficient as a first step to showcase a novel architecture.': 1.0986123085021973, 'A strength is that the authors experiment with different design decisions when building the topology predictor components of the architecture, about when / how to decide to terminate, as opposed to making a single arbitrary choice.': 1.0986123085021973, 'I see future applications of this architecture and it seems to have interesting directions for future work': 1.0986123085021973, 'so I suggest its acceptance as a conference contribution.': 1.0955363512039185, 'The paper propose DRNN as a neural decoder for tree structures.': 1.0729424953460693, 'I like the model architecture since it has two clear improvements over traditional approaches — (1) the information flows in two directions, both from the parent and from siblings, which is desirable in tree structures (2) the model use a probability distribution to model the tree boundary (i.e. the last sibling or the leaf).': 1.0986123085021973, 'This avoids the use of special ending symbols which is larger in size and putting more things to learn for the parameters (shared with other symbols).': 1.0986123085021973, 'The authors test the DRNN using the tasks of recovering the synthetic trees and recovering functional programs.': 1.0986123085021973, 'The model did better than traditional methods like seq2seq models.': 1.0986123085021973, 'I think the recovering synthetic tree task is not very satisfying for two reasons — (1) the surface form itself already containing some of the topological information which makes the task easier than it should be (2) as we can see from figure 3, when the number of nodes grows (even to a number not very large), the performance of the model drops dramatically, I am not sure if a simple baseline only captures the topological information in the surface string would be much worse than this.': 1.0986120700836182, 'And DRNN in this case, seems can’t show its full potentials since the length of the information flow in the model won’t be very long.': 1.0986123085021973, 'I think the experiments are interesting.': 1.0986123085021973, 'But I think there are some other tasks which are more difficult and the tree structure information are more important in such tasks.': 1.0986123085021973, 'For example, we have the seq2seq parsing model (Vinyals et al, 2014), is it possible to use the DRNN proposed here on the decoder side?': 1.0986123085021973, 'I think tasks like this can show more potentials of the DRNN and can be very convincing that model architectures like this are better than traditional alternatives.': 1.0986123085021973, ""Authors' response well answered my questions."": 1.0986123085021973, 'Thanks.': 1.0986123085021973, 'Evaluation not changed.': 1.0986123085021973, '###': 1.0986123085021973, 'This paper proposes a neural model for generating tree structure output from scratch.': 1.0986123085021973, 'The model does 1) separate the recurrence between depths and siblings; 2) separate the topology and label generation, and outperforms previous methods on a benchmark IFTTT dataset.': 1.0986123085021973, 'Compared to previous tree-decoding methods, the model avoids manually annotating subtrees with special tokens, and thus is a very good alternative to such problems.': 1.0986123085021973, 'The paper does solid experiments on one synthetic dataset, and outperforms alternative methods on one real-world IFTTT dataset.': 1.0986123085021973, 'There are couple of interesting results in the paper that I believe is worth further investigation.': 1.0986123085021973, 'Firstly, on the synthetic dataset, the precision drops rapidly with the number of nodes.': 1.0986123085021973, 'Is it because that the vector representation of the sequential encoder fails to provide sufficient information of long sequences, such that the tree decoder can not do a good job?': 1.0986123085021973, 'Or is it because that such tree decoder is not tolerant to the long sequence input, i.e., large tree structure?': 1.0986123085021973, 'I believe that it is important to understand this before a better model can be developed.': 1.0986123085021973, 'For example, if it is the fault of encoder, maybe an attention layer can be added, as in a seq-to-seq model, to preserve more information of the input sequence.': 1.0986123085021973, 'Moreover, besides only showing how the precision changes with the number of nodes in the tree, it might be interesting to investigate how it goes with 1) number of depths; 2) number of widths; 3) symmetricity; etc.': 1.0986123085021973, 'Moreover, as greedy search is used in decoding, it might be interesting to see how it helps, if it does, to use beam-search in tree decoding.': 1.0986123085021973, 'On the IFTTT dataset, listing more statistics about this dataset might be helpful for better understanding the difficulty of this task.': 1.0986123085021973, 'How deep are the trees?': 1.0986123085021973, 'How large are the vocabularies on both language and program sides?': 1.0986123085021973, 'The paper is well written, except for minor typo as mentioned in my pre-review questions.': 1.0986123085021973, 'In general, I believe this is a solid paper, and more can be explored in this direction.': 1.0986123085021973, 'So I tend to accept it.': 1.0986123085021973}"
207,https://openreview.net/forum?id=HkcdHtqlx,"{'Summary:': 1.0986123085021973, 'The authors propose a multi-hop ""gated attention"" model, which models the interactions between query and document representations, for answering cloze-style questions.': 1.095131516456604, 'The document representation is attended to sequentially over multiple-hops using similarity with the query representation (using a dot-product) as the scoring/attention function.': 1.0986090898513794, 'The proposed method improves upon (CNN, Daily Mail, Who-Did-What datasets) or is comparable to (CBT dataset) the state-of-the-art results.': 0.9485890865325928, 'Pros:': 1.0986123085021973, '1. Nice idea on heirarchical attention for modulating the context (document) representation by the task-specific (query) representation.': 1.0984880924224854, '2. The presentation is clear with thorough experimental comparison with the latest results.': 1.0986123085021973, 'Comments:': 1.0986123085021973, '1. The overall system presents a number of architectural elements: (1) attention at multiple layers (multi-hop), (2) query based attention for the context (or gated attention), (3) encoding the query vector at each layer independently.': 0.7627810835838318, 'It is important to breakdown the gain in performance due to the above factors: the ablation study presented in section 4.4 helps establish the importance of Gated Attention (#2 above).': 1.0879676342010498, 'However, it is not clear:': 1.0986123085021973, '(1) how much multiple-hops of gated-attention contribute to the performance.': 0.8199602365493774, '(2) how important is it to have a specialized query encoder for each layer.': 1.0986123085021973, 'Understanding the above better, will help simplify the architecture.': 1.0986123085021973, '2. The tokens are represented using L(w) and C(w). It is not clear if C(w) is crucial for the performance of the proposed method.': 1.0985532999038696, 'There is a significant performance drop when C(w) is absent (e.g. in ""GA Reader': 1.053504228591919, '""; although there are other changes in ""GA Reader': 1.0986123085021973, '"" which could affect the performance).': 1.0986123085021973, 'Hence, it is not clear how much does the main idea, i.e., gated attention contributes towards the superior performance of the proposed method.': 1.0966227054595947, 'This paper presents an interesting idea for iteratively re-weighting the word representations in a document (hence the GRU-coded doc representation as well) with a simple multiplication operation.': 1.0986123085021973, 'As the authors correctly pointed out, such an operation serves as a ""filter"" to reduce the attentions to less relevant parts in the document, hence leading to better performance of the modeling.': 1.0986123085021973, 'The results are or close to the state-of-the-art for a few Cloze-style QA tasks.': 1.0986119508743286, 'This paper would deserve an even higher score, if the following limitations could be addressed better:': 1.0986123085021973, '1. While interesting and conceptually simple (though with significant increased computational overheads), the architecture proposed in the paper is for a very specific task.': 0.38610216975212097, '2. The improvement of the main idea of this paper (gated attention) is less significant, comparing GA Reader': 0.627576470375061, 'vs. GA Reader, while the latter includes a number of engineering tricks such as adding character embedding and using a word embedding trained from larger corpus (GloVe), as well as some small improvements on the modeling by using token-specific attention in (5).': 0.6232965588569641, ""3. I also wish the authors can shed more lights on what a role the K (number of hops) plays, both intuitively and empirically. I feel more insights could be obtained if we do more deeper analysis of K's impacts to different types of questions for example."": 0.6726734638214111, 'SUMMARY.': 1.0986123085021973, 'The paper proposes a machine reading approach for cloze-style question answering.': 1.0565879344940186, 'The proposed system first encodes the query and the document using a bidirectional gru.': 1.098609447479248, 'These two representations are combined together using a Gated Attention (GA).': 1.0980044603347778, 'GA calculates the compatibility of each word in the document and the query as a probability distribution.': 0.7497819066047668, 'For each word in the document a gate is calculated weighting the query representation according to the word compatibility.': 1.0985655784606934, 'Ultimately, the gate is applied to the gru-encoded document word.': 1.0986123085021973, 'The resulting word vectors are re-encoded with a bidirectional GRU.': 1.0986121892929077, 'This process is performed for multiple hops.': 1.0986077785491943, 'After k hops, the probability of a word to be part of the answer is calculated by a log-linear model that take as input the last word representations, and the concatenation of the last query representation before and after the cloze token.': 1.0986123085021973, 'The probability of a candidate being the answer to the question is given by a linear combination of the single word probabilities.': 1.0985896587371826, 'The proposed model is tested on 4 different dataset.': 1.0984164476394653, 'The authors shown that the proposed model works well (state-of-the-art performance) for 3 out of 4 benchmarks.': 1.0985242128372192, 'OVERALL JUDGMENT': 1.0969332456588745, 'The main contribution of the paper is the gated attention mechanism, that in my opinion, is a simple and interesting idea.': 1.0985658168792725, 'The paper is well thought, and the ablation study on the benefits given by the gated attention are convincing.': 0.936509370803833, 'The GA reader as whole model outperforms previous state-of-the-art models on 3 benchmarks and seems very promising also on the CBT dataset.': 1.098471999168396, 'I would have liked to see some discussion on why the model works less well on the CBT dataset, though.': 1.0986076593399048, 'DETAILED COMMENTS': 0.8560001850128174, 'minor. In the introduction, Weston et al., 2014 do not use any attention mechanism.': 1.095554232597351}"
208,https://openreview.net/forum?id=Hkg4TI9xl,"{'The paper address the problem of detecting if an example is misclassified or out-of-distribution.': 1.0986117124557495, 'This is an very important topic and the study provides a good baseline.': 1.0986026525497437, 'Although it misses strong novel methods for the task, the study contributes to the community.': 1.0986123085021973, 'The authors propose the use of statistics of softmax outputs to estimate the probability of error and probability of a test sample being out-of-domain.': 1.0986123085021973, 'They contrast the performance of the proposed method with directly using the softmax output probabilities, and not their statistics, as a measure of confidence.': 1.0986123085021973, 'It would be great if the authors elaborate on the idea of ignoring the logit of the blank symbol.': 1.0986123085021973, 'It would be interesting to see the performance of the proposed methods in more confusable settings, ie., in cases where the out-of-domain examples are more similar to the in-domain examples.': 1.0985965728759766, ""e.g., in the case of speech recognition this might correspond to using a different language's speech with an ASR system trained in a particular language."": 0.6245821714401245, 'Here the acoustic characteristics of the speech signals from two different languages might be more similar as compared to the noisy and clean speech signals from the same language.': 1.0986123085021973, 'In section 4, the description of the auxiliary decoder setup might benefit from more detail.': 1.0986123085021973, 'There has been recent work on performance monitoring and accuracy prediction in the area of speech recognition, some of this work is listed below.': 1.0986123085021973, '1. Ogawa, Tetsuji, et al. ""Delta-M measure for accuracy prediction and its application to multi-stream based unsupervised adaptation."" Proceedings of ICASSP. 2015.': 0.9919760227203369, '2. Hermansky, Hynek, et al. ""Towards machines that know when they do not know."" Proceedings of ICASSP, 2015.': 0.3237725496292114, '3. Variani, Ehsan et al. ""Multi-stream recognition of noisy speech with performance monitoring."" INTERSPEECH. 2013.': 0.6031071543693542, 'The authors present results on a number of different tasks where the goal is to determine whether a given test example is out-of-domain or likely to be mis-classified.': 1.0790516138076782, 'This is accomplished by examining statistics for the softmax probability for the most likely class; although the score by itself is not a particularly good measure of confidence, the statistics for out-of-domain examples are different enough from in-domain examples to allow these to be identified with some certainty.': 1.0986121892929077, 'My comments appear below:': 1.0986123085021973, '1. As the authors point out, the AUROC/AUPR criterion is threshold independent. As a result, it is not obvious whether the thresholds that would correspond to a certain operating point (say a true positive rate of 10%) would be similar across different data sets. In other words, it would be interesting to know how sensitive the thresholds are to different test sets (or different splits of the test set). This is important if we want to use the thresholds determined on a given held-out set during evaluation on unseen data (where we would need to select a threshold).': 0.550871729850769, '2. Performance is reported in terms of AUROC/AUPR and models are compared against a random baseline. I think it’s a little hard to look at the differences in AUC/AUPR to get a sense for how much better the proposed classifier is than the random baseline. It would be useful, for example, if the authors could also report how strongly statistically significant some of these differences are (although admittedly they look to be pretty large in most cases).': 1.0425435304641724, '3. In the experiments on speech recognition presented in Section 3.3, I was not entirely clear on how the model was evaluated. In Table 9, for example, is an “example” the entire utterance or just a single (stacked?) speech frame. Assuming that each “example” is an utterance, are the softmax probabilities the probability of the entire phone sequence (obtained by multiplying the local probability estimates from a Viterbi decoding?)': 1.0453879833221436, '4. I’m curious about the decision to ignore the blank symbol’s logit in Section 3.3. Why is this required?': 1.0986123085021973, '5. As I mentioned in the pre-review question, at least in the speech recognition case, it would have been interesting to compare performance obtained using a simple generative baseline (e.g., GMM-HMM). I think that would serve as a good indication of the ability of the proposed model to detect out-of-domain examples over the baseline.': 1.0986123085021973}"
209,https://openreview.net/forum?id=Hkg8bDqee,"{'In this paper, the authors use a separate introspection neural network to predict the future value of the weights directly from their past history.': 1.0986123085021973, 'The introspection network is trained on the parameter progressions collected from training separate set of meta learning models using a typical optimizer, e.g. SGD.': 1.0986120700836182, 'Pros:': 1.0986123085021973, '+': 1.0986123085021973, 'The organization is generally very clear': 1.0986123085021973, '+ Novel meta-learning approach that is different than the previous learning to learn approach': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'The paper will benefit from more thorough experiments on other neural network architectures where the geometry of the parameter space are sufficiently different than CNNs such as fully connected and recurrent neural networks.': 1.0986123085021973, 'Neither MNIST nor CIFAR experimental section explained the architectural details': 1.0986123085021973, 'Mini-batch size for the experiments were not included in the paper': 1.0986123085021973, 'Comparison with different baseline optimizer such as Adam would be a strong addition or at least explain how the hyper-parameters, such as learning rate and momentum, are chosen for the baseline SGD method.': 1.0986123085021973, 'Overall, due to the omission of the experimental details in the current revision, it is hard to draw any conclusive insight about the proposed method.': 1.0986123085021973, 'The paper reads well and the idea is new.': 1.0986123085021973, 'Sadly, many details needed for replicating the results (such as layer sizes of the CNNs, learning rates) are missing.': 1.068624496459961, 'The training of the introspection network could have been described in more detail.': 1.0986123085021973, 'Also, I think that a model, which is closer to the current state-of-the-art should have been used in the ImageNet experiments.': 1.020524263381958, 'That would have made the results more convincing.': 1.0986123085021973, 'Due to the novelty of the idea, I recommend the paper.': 1.0986121892929077, 'I would increase the rating if an updated draft addresses the mentioned issues.': 1.0986109972000122, 'EDIT: Updated score.': 1.0986123085021973, 'See additional comment.': 1.0986123085021973, 'I quite like the main idea of the paper, which is based on the observation in Sec. 3.0 - that the authors find many predictable patterns in the independent evolution of weights during neural network training.': 1.0986109972000122, 'It is very encouraging that a simple neural network can be used to speed up training by directly predicting weights.': 1.0528621673583984, 'However the technical quality of the current paper leaves much to be desired, and I encourage the authors to do more rigorous analysis of the approach.': 1.025890588760376, 'Here are some concrete suggestions:': 1.0986123085021973, 'The findings in Section 3.0 which motivate the approach, should be clearly presented in the paper.': 0.6338050365447998, 'Presently they are stated as anecdotes.': 1.0986123085021973, 'A central issue with the paper is that the training of the Introspection network I is completely glossed over.': 1.0671087503433228, 'How well did the training work, in terms of training, validation/test losses?': 0.5056248307228088, 'How well does it need to work in order to be useful for speeding up training?': 1.0986121892929077, 'These are important questions for anyone interested in this approach.': 1.0986123085021973, 'An additional important issue is that of baselines.': 1.0986123085021973, 'Would a simple linear/quadratic model also work instead of a neural network?': 1.0986123085021973, 'What about a simple heuristic rule to increase/decrease weights?': 1.0985839366912842, ""I think it's important to compare to such baselines to understand the complexity of the weight evolution learned by the neural network."": 1.0986123085021973, 'I do not think that default tensorflow example hyperparameters should be used, as mentioned by authors on OpenReview.': 1.0975967645645142, 'There is no scientific basis for using them.': 1.0787142515182495, 'Instead, first hyperparameters which produce good results in a reasonable time should be selected as the baseline, and then added the benefit of the introspection network to speed up training (and reaching a similar result) should be shown.': 1.0986123085021973, ""The authors state in the discussion on OpenReview that they also tried RNNs as the introspection network but it didn't work with small state size."": 1.0979665517807007, 'What does ""didn\'t work"" mean in this context?': 0.5377038717269897, 'Did it underfit?': 0.9124126434326172, 'I find it hard to imagine that a large state size would be required for this task.': 1.0985831022262573, ""Even if it is, that doesn't rule out evaluation due to memory issues because the RNN can be run on the weights in 'mini-batch' mode."": 1.0638105869293213, 'In general, I think other baselines are more important than RNN.': 0.5320786833763123, 'A question about jump points:': 1.0985314846038818, 'The I is trained on SGD trajectories.': 1.0986123085021973, 'While using I to speed up training at several jump points, if the input weights cross previous jump points, then I gets input data from a weight evolution which is not from SGD (it has been altered by I).': 1.0958634614944458, ""This seems problematic but doesn't seem to affect your experiments."": 1.0986123085021973, 'I feel that this again highlights the importance of the baselines.': 1.098606824874878, 'Perhaps I is doing something extremely simple that is not affected by this issue.': 1.0986111164093018, 'Since the main idea is very interesting, I will be happy to update my score if the above concerns are addressed.': 1.098531723022461}"
210,https://openreview.net/forum?id=HkljfjFee,"{'The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding.': 1.0986123085021973, 'Namely, two data samples that are similar or neighbours, should have a sparse code that is similar (in terms of support).': 1.0986123085021973, 'The general idea is not unique, but it is an interesting one (if one admits that the adjacency matrix A is known a priori), and the novelty mostly lies on the definition of the regularisation term that is an l1-norm (while other techniques would mostly use l2 regularisation).': 1.0986123085021973, 'Based on this idea, the authors develop a new SRSC algorithm, which is analysed in detail and shown to perform better than its competitors based on l2 sparse coding regularisation and other schemes in terms of clustering performance.': 1.0986123085021973, 'Inspired by LISTA, the authors then propose an approximate solution to the SRSC problem, called Deep-SRSC, that acts as a sort of fast encoder.': 1.0986121892929077, 'Here too, the idea is interesting and seems to be quite efficient from experiments on USPS data, even if the framework seems to be strongly inspired from LISTA.': 1.0986123085021973, 'That scheme should however be better motivated, by the limitations of SRSC that should be presented more clearly.': 1.0986123085021973, 'Overall, the paper is well written, and pretty complete.': 1.0986123085021973, 'It is not extremely original in its main ideas though, but the actual algorithm and implementation seem new and effective.': 1.0986109972000122, 'In this paper the authors propose a method to explicitly regularize sparse coding to encode neighbouring datapoints with similar sets of atoms from the dictionary by clustering training examples with KNN in input space.': 1.0986123085021973, 'The resulting algorithm is relatively complex and computationally relatively expensive, but the authors provide detailed derivations and use arguments from proximal gradient descent methods to prove convergence (I did not follow all the derivations, only some).': 1.0986123085021973, 'In general the paper is well written and the authors explain the motivation behind the algorithms design in detail.': 1.098435640335083, 'In the abstract the authors mention “extensive experimental results …”, but I find the experiments not very convincing: With experiments on the USPS handwritten digits dataset (why not MNIST?), COIL-20 and COIL-100 and UCI, the datasets are all relatively small and the algorithm is run with dictionary sizes between p=100 to p=500.': 1.0981848239898682, 'This seems surprising because the authors state that they implemented SRSC in “CUDA C++ with extreme efficiency” (page 10).': 1.0986123085021973, 'But more importantly, I find it hard to interpret and compare the results: The paper reports accuracy and and normalized mutual information for a image retrieval / clustering task where the proposed SRSC is used as a feature extractor.': 1.0986123085021973, 'The improvements relative to standard Sparse Coding seem very small  (often < 1% in terms of NMI; it looks more promising in terms of accuracy) and if I understand the description on page 11 correctly, than the test set was used to select some hyperparameters (the best similarity measure for clustering step)?': 1.0986123085021973, 'There is no comparisons to other baselines / state of the art image clustering methods.': 1.0940871238708496, 'Besides of providing features for a small scale image clustering system, are there maybe ways to more directly evaluate the properties and qualities of  a sparse coding approach?': 1.0986123085021973, 'E.g. reconstruction error / sparsity; maybe even denoising performance?': 1.0986123085021973, 'In summary, I think in it current form the paper lacks the evaluation and experimental results for an ICLR publication.': 1.0983679294586182, 'Intuitively, I agree with the authors that the proposed regularization is an interesting direction, but I don’t see experiments that directly show that the regularization has the desired effect; and the improvements in the clustering task where SRSC is used as a feature extractor are very modest.': 1.0956127643585205, ""I'd like to thank the authors for their detailed response to my questions."": 1.0985437631607056, 'The paper proposes a support regularized version of sparse coding that takes into account the underlying manifold structure of the data.': 1.0986123085021973, 'For this purpose, the authors augment the classic sparse coding loss with a term that encourages near by points to have similar active set.': 1.0986123085021973, 'Convergence guarantees for the optimization procedure are presented.': 1.0986123085021973, 'Experimental evaluation on clustering and semi-supervised learning shows the benefits of the proposed approach.': 1.0986123085021973, 'The paper is well written and a nice read.': 1.0986123085021973, 'The most relevant contribution of this work is to including (and optimizing) the regularization function, and not an approximation or surrogate.': 1.0986123085021973, 'The authors derive a a PGD-styple iterative method and present convergence analysis for it.': 1.0986123085021973, 'Thanks for the clarifications regarding the assumptions used in Section 3.': 1.0986123085021973, 'It would be nice to include some of that in the manuscript.': 1.0986123085021973, 'The authors also propose a fast encoding scheme for their proposed method.': 1.0986123085021973, 'The authors included a new experiment in semi-supervised consists of a very interesting use (of the method and the fast approximation).': 1.0986123085021973, 'While this is an interesting addition, I think that using fast encoders is not particularly novel or the main part of the work.': 1.0986123085021973, '""Converting"" iterative optimization algorithms into feed-forward nets for accelerating the inference process has been done in the past (several times with quite similar problems).': 1.0986123085021973, 'Is natural that this can be done, and not very surprising.': 1.0986123085021973, 'Maybe would be interesting to evaluate how important is to have an architecture matching the optimization algorithm, compared to a generic network (though some of this analysis has also been performed in the past).': 1.0986123085021973}"
211,https://openreview.net/forum?id=HkpLeH9el,"{'The authors talk about design choice recommendations for performing program induction via gradient descent, basically advocating reasonable programming language practice (immutable data, higher-order language constructs, etc.).': 1.6094379425048828, 'As mentioned in the comments I feel fairly strongly that this is a marginal at best contribution beyond TerpreT, an already published system with extensive experimentation and theoretical grounding.': 1.6094379425048828, 'To be clear I think the TerpreT paper deserves a large amount of attention.': 1.6094379425048828, 'It is truly inspiring.': 1.6094379425048828, ""This paper contradicts one of the key findings in the original paper but doesn't provide convincing evidence that gradient-based evaluators for TerpreT are superior or even, frankly, appropriate for program induction."": 1.6094379425048828, ""This is uncomfortable for me and makes me wonder why gradient-based methods weren't more carefully vetted in the first place or why more extensive comparisons to already implemented alternatives weren't included in this paper."": 1.6094379425048828, 'My opinion: if we want to give the original TerpreT paper more attention, which I think it deserves, then this paper is above threshold.': 1.6094379425048828, ""On the other hand it's basically unreadable, actually contradicts its mother-paper in not well-defended ways, and is irreproducible without the same"": 1.6094379425048828, ""so I think, unfortunately, it's below threshold."": 1.6094379425048828, 'The paper discusses a range of modelling choices for designing differentiable programming languages.': 1.6094379425048828, 'Authors propose 4 recommendations that are then tested on a set of 13 algorithmic tasks for lists, such as ""length of the list"", ""return k-th element from the list"", etc.': 1.6094379425048828, 'The solutions are learnt from input/output example pairs (5 for training, 25 for test).': 1.6094379425048828, 'The main difference between this work and differentiable architectures, like NTM, Neural GPU, NRAM, etc. is the fact that here the authors aim at automatically producing code that solves the given task.': 1.6094379425048828, 'My main concern are experiments - it would be nice to see a comparison to some of the neural networks mentioned in related work.': 1.6094379425048828, 'Also, it would be useful to see how this model is doing on typical problems used by mentioned neural architectures (problems such as ""sorting"", ""merging"", ""adding"").': 1.6094379425048828, ""I'm wondering how this is going to generalize to other types of programs that can't be solved with prefix-loop-suffix structure."": 1.6094379425048828, 'It is also concerning that although  1) the tasks are simple, 2) the structure of the solution is very restricted and 3) model is using extensions doing most of the work, the proposed model still fails to find solutions (example: A+L model that has “loop” fails to solve “list length” task in 84% of the runs).': 1.5881738662719727, 'Pro:': 1.6094379425048828, 'generates code rather than black-box neural architecture': 1.6094379425048828, 'nice that it can learn from very few examples': 1.6094379425048828, 'Cons:': 1.6094379425048828, 'weak results, works only for very simple tasks, missing comparison to neural architectures': 1.6094379425048828, 'This paper presents design decisions of TerpreT': 1.6094379425048828, '[1] and experiments about learning simple loop programs and list manipulation tasks.': 1.6094379425048828, 'The TerpreT line of work (is one of those which) bridges the gap between the programming languages (PL) and machine learning (ML) communities.': 1.4269194602966309, 'Contrasted to the recent interest of the ML community for program induction, the focus here is on using the design of the programming language to reduce the search space.': 1.6060187816619873, 'Namely, here, they used the structure of the control flow (if-then-else, foreach, zipWithi, and foldli ""templates""), immutable data (no reuse of a ""neural"" memory), and types (they tried penalizing ill-typedness, and restricting the search only to well-typed programs, which works better).': 1.0186443328857422, 'My bird eye view would be that this stands in between ""make everything continuous and perform gradient descent"" (ML) and ""discretize all the things and perform structured and heuristics-guided combinatorial search"" (PL).': 1.5738296508789062, 'I liked that they have a relevant baseline (\\lambda^2), but I wished that they also included a fully neural network program synthesis baseline.': 1.5690033435821533, 'Admittedly, it would not succeed except on the simplest tasks, but I think some of their experimental tasks are simple enough for ""non-generating code "" NNs to succeed on.': 1.490352749824524, 'I wished that TerpreT was available, and the code to reproduce these experiments too.': 1.6082044839859009, 'I wonder if/how the (otherwise very interesting!)': 0.9501369595527649, 'recommendations for the design of programming languages to perform gradient descent based-inductive programming would hold/perform on harder task than these loops.': 1.6089439392089844, 'Even though these tasks are already interesting and challenging, I wonder how much of these tasks biased the search for good subset of constraints (e.g. those for structuring the control flow).': 1.6093451976776123, 'Overall, I think that the paper is good enough to appear at ICLR, but I am no expert in program induction / synthesis.': 1.5163418054580688, 'Writing:': 1.6094379425048828, '- The paper is at times hard to follow. For instance, the naming scheme of the model variants could be summarized in a table (with boolean information about the features it embeds).': 1.6054853200912476, '- Introduction: ""basis modern computing"" -> of': 1.6061397790908813, '- Page 3, training objective: ""minimize the cross-entropy between the distribution in the output register r_R^{(T)} and a point distribution with all probability mass on the correct output value"" -> if you want to cater to the ML community at large, I think that it is better to say that you treat the output of r_R^{(T)} as a classification problem with the correct output value (you can give details and say exactly which type of criterion/loss, cross-entropy, you use).': 1.3133020401000977, '[1] ""TerpreT: A Probabilistic Programming Language for Program Induction"", Gaunt et al. 2016': 1.584106206893921, 'The paper proposes a set of recommendations for the design of differentiable programming languages, based on what made gradient descent more successful in experiments.': 1.6094379425048828, 'I must say i’m no expert in program induction.': 1.6094379425048828, 'While i understand there is value in exploring what the paper set out to explore': 1.6094379425048828, 'making program learning easier': 1.6094379425048828, 'i did not find the paper too engaging.': 1.6094379425048828, 'First everything is built on top of Terpret, which isn’t yet publicly available.': 1.6094379425048828, 'Also most of the discussion is very detailed on the programming language side and less so on the learning side.': 1.6094379425048828, 'It is conceivable that it would be best received on a programming language conference.': 1.6094379425048828, 'A comparison with alternatives not generating code would be valuable in my opinion, to motivate for the overall setup.': 1.6094379425048828, 'Pros:': 1.6094379425048828, 'Useful, well executed, novel study.': 1.6094379425048828, 'Low on learning-specific contributions, more into domain-related constraints.': 1.6094379425048828, 'Not sure a great fit to ICLR.': 1.6094379425048828, 'This paper presents small but important modifications which can be made to differentiable programs to improve learning on them.': 1.6094379425048828, 'Overall these modifications seem to substantially improve convergence of the optimization problems involved in learning programs by gradient descent.': 1.6094379425048828, 'That said, the set of programs which can be learned is still small, and unlikely to be directly useful.': 1.6094379425048828}"
212,https://openreview.net/forum?id=HkpbnH9lx,"{'This paper presents a clever way of training a generative model which allows for exact inference, sampling and log likelihood evaluation.': 1.0986123085021973, 'The main idea here is to make the Jacobian that comes when using the change of variables formula (from data to latents) triangular - this makes the determinant easy to calculate and hence learning possible.': 1.0986123085021973, 'The paper nicely presents this core idea and a way to achieve this - by choosing special ""routings"" between the latents and data such that part of the transformation is identity and part some complex function of the input (a deep net, for example)': 1.0986123085021973, 'the resulting Jacobian has a tractable structure.': 1.0986123085021973, 'This routing can be cascaded to achieve even more complex transformation.': 1.0986123085021973, 'On the experimental side, the model is trained on several datasets and the results are quite convincing, both in sample quality and quantitive measures.': 1.0986123085021973, 'I would be very happy to see if this model is useful with other types of tasks and if the resulting latent representation help with classification or inference such as image restoration.': 1.0986123085021973, ""In summary - the paper is nicely written, results are quite good and the model is interesting - I'm happy to recommend acceptance."": 1.0986123085021973, 'This paper proposes a new generative model that uses real-valued non-volume preserving transformations in order to achieve efficient and exact inference and sampling of data points.': 1.0715885162353516, 'The authors use the change-of-variable technique to obtain a model distribution of the data from a simple prior distribution on a latent variable.': 1.070988655090332, 'By carefully designing the bijective function used in the change-of-variable technique, they obtain a Jacobian that is triangular and allows for efficient computation.': 1.0941390991210938, 'Generative models with tractable inference and efficient sampling are an active research area and this paper definitely contributes to this field.': 1.0986123085021973, 'While not achieving state-of-the-art, they are not far behind.': 1.0986123085021973, ""This doesn't change the fact that the proposed method is innovative and worth exploring as it tries to bridge the gap between auto-regressive models, variational autoencoders and generative adversarial networks."": 1.0984997749328613, 'The authors clearly mention the difference and similarities with other types of generative models that are being actively researched.': 1.0985546112060547, 'Compared to autoregressive models, the proposed approach offers fast sampling.': 1.0986123085021973, 'Compared to generative adversarial networks, Real NVP offers a tractable log-likelihood evaluation.': 1.0986121892929077, 'Compared to variational autoencoders, the inference is exact.': 1.0986123085021973, 'Compared to deep Boltzmann machines, the learning of the proposed method is tractable.': 1.0986123085021973, 'It is clear that Real NVP goal is to bridge the gap between existing and popular generative models.': 1.0986123085021973, 'The paper presents a lot of interesting experiments showing the capabilities of the proposed technique.': 1.0986123085021973, 'Making the code available online will certainly contribute to the field.': 1.0986123085021973, 'Is there any intention of releasing the code?': 1.0986123085021973, 'Typo: (Section 3.7)': 1.0986123085021973, 'We also ""use apply"" batch normalization': 1.0986123085021973, 'Building on earlier work on a model called NICE, this paper presents an approach to constructing deep feed-forward generative models.': 1.0789780616760254, 'The model is evaluated on several datasets.': 1.0986123085021973, 'While it does not achieve state-of-the-art performance, it advances an interesting class of models.': 1.0984772443771362, 'The paper is mostly well written and clear.': 1.0986123085021973, 'Given that inference and generation are both efficient and exact, and given that this represents a main advantage over other models, it would be great if the authors could provide some motivating example applications where this is needed/would be useful.': 1.0642684698104858, 'The authors claim that “unlike both variational autoencoders and GANs, our technique is able to learn a semantically meaningful latent space which is as high dimensional as the input space.”': 0.8053548336029053, 'Where is the evidence for this claim?': 1.0986123085021973, 'I didn’t see any analysis of the semantic meaningfulness of the latent space learned by real NVP.': 0.440600723028183, 'Stronger evidence that the learned representations are actually useful for downstream tasks would be nice.': 1.098350167274475, 'I still think the author’s intuitions around the “fixed reconstruction cost of L2” are very vague.': 1.098507046699524, 'The factorial Gaussian assumption itself does not limit the generative model, it merely smoothes an otherwise arbitrary distribution, and to a degree which can be arbitrarily small, p(x) = \\int p(z) N(x | f(z), \\sigma^2) dz.': 1.0810426473617554, 'How a lose lower bound plays into this is not clear from the paper.': 1.0986120700836182}"
213,https://openreview.net/forum?id=HksioDcxl,"{'This paper proposes an RNN-based model for recommendation which takes into account temporal dynamics in user ratings and reviews.': 1.0986121892929077, 'Interestingly, the model infers time-dependant user/item vectors by applying an RNN to previous histories.': 1.0986123085021973, 'Those vectors are used to predict ratings, in a similar fashion to standard matrix factorization methods, and also bias a conditional RNN language model for reviews.': 1.0986123085021973, 'The paper is well written and the architectural choices make sense.': 1.0986123085021973, 'The main shortcomings of the paper are in the experiments:': 0.9208556413650513, '1) The full model (rating+text) is only applied to one and relatively small dataset.': 0.8801802396774292, 'Applying the model on multiple datasets with more data, e.g. Amazon reviews dataset (https://snap.stanford.edu/data/web-Amazon.html) would be more convincing.': 1.0986123085021973, '2) While modelling order in review text seems like the right choice, previous papers (e.g. Almahairi et al. 2015) have shown that for rating prediction, modelling order in reviews might not be useful.': 1.053101897239685, 'A comparison with a similar model, but with bag-of-words reviews model would be nice in order to show the importance of the RNN-based review model, especially given previous literature.': 1.0986123085021973, 'Finally, this paper is an application paper applying well-established deep learning techniques, and I do not feel the paper offers new insights which the ICLR community in general would benefit from.': 1.0986123085021973, 'This is not to undermine the importance of this paper, but I would like the authors to comment on why they think ICLR is a good avenue for their work.': 1.0975703001022339, 'This paper proposed a joint model for rate prediction and text generation.': 1.0986121892929077, 'The author compared the methods on a more realistic time based split setting, which requires “predict into the future.”': 1.0985100269317627, 'One major flaw of the paper is that it does not address the impact of BOW vs the RNN based text model, specifically RRN(rating+text) already uses RNN for text modeling, so it is unclear whether the improvement comes from RNN(as opposed to BOW) or application of text information.': 1.0985809564590454, 'A more clear study on impact of each component could make it more clear and benefit the readers.': 1.0986123085021973, 'Another potential improvement direction of the paper is to support ranking objectives, as opposed to rate prediction, which is more realistic for recommendation settings.': 1.0983966588974, 'The overall technique is intuitive and novel, but can be improved to give more insights to the reader,.': 1.0985063314437866, 'The paper seeks to jointly model ratings and reviews in addition to temporal patterns.': 0.869520366191864, ""Existing papers have captured these aspects previously, but what's novel here is the particular combination of parts and choice of techniques."": 1.0986123085021973, 'In particular, the use of LSTMs as opposed to ""bag-of-words"" models that have previously been used when combining the same components.': 1.0860925912857056, 'A criticism is made of existing models that use bag-of-words features as being too ""coarse"" to capture the real dynamics of reviews.': 1.0986123085021973, ""This seems a valid criticsm, though it's not clear exactly what features those existing models miss."": 1.0986123085021973, 'Something missing from this paper is any interpretation of what the model ""learns"" that may explain its better performance.': 1.0986123085021973, 'I also don\'t know about the significance of predicting ""future"" ratings as opposed to random splitting of the data.': 1.0986123085021973, 'Lots of work on temporal recommender systems uses a variety of hold-out strategies besides random sampling, e.g. holding out the final ratings.': 1.0986123085021973, 'Is there a methodological contribution associated with this change?': 1.0986123085021973, 'The experiments evaluate the extent to which the model achieves good performance due to its ability to capture evolving temporal patterns at the level of items, and the ability of the model to capture additional dynamics from text.': 1.0986123085021973, 'Text makes a small but significant contribution; I was surprised by how large an improvement is achieved given how little text was included.': 1.0986123085021973, 'Overall this is a reasonably strong experimental comparison, though could be improved in two dimensions:': 1.0986123085021973, ""(a) There's no comparison to models that use ratings + text (e.g. CoBaFi, JMARS, etc.)."": 1.0986123085021973, 'These methods have reported substantial improvements over rating-only models in the past, perhaps even larger improvements than what is reported here.': 1.0986123085021973, 'Such an experiment is important given the claim that existing models (in particular bag-of-words models) are too coarse to capture the dynamics of text.': 1.0986123085021973, '(b) The restriction to movie datasets is okay, but these datasets are somewhat outliers when it comes to recommendation.': 1.0986123085021973, ""In particular, they're extremely dense datasets that support very parameter-rich models."": 1.0986123085021973, ""I'd question whether the results would hold on sparser data (though in fact I'd suspect they would hold, since on a sparser dataset the contribution due to using reviews ought to be larger)."": 1.0986123085021973, 'Otherwise the experiments are fine.': 1.0986123085021973, ""Perplexity results are nice but essentially what we'd expect."": 1.0986123085021973, ""It is a shame that unlike other papers that use text to inform recommender systems there's no high-level analysis here of what the model has uncovered."": 1.0986123085021973}"
214,https://openreview.net/forum?id=Hku9NK5lx,"{'The method proposes to compress the weight matrices of deep networks using a new density-diversity penalty together with a computing trick (sorting weights) to make computation affordable and a strategy of tying weights.': 1.0777002573013306, 'This density-diversity penalty consists of an added cost corresponding to the l2-norm of the weights (density) and the l1-norm of all the pairwise differences in a layer.': 0.46163704991340637, 'Regularly, the most frequent value in the weight matrix is set to zero to encourage sparsity.': 0.9559071660041809, 'As weights collapse to the same values with the diversity penalty, they are tied together and then updated using the averaged gradient.': 1.0986112356185913, 'The training process then alternates between training with 1.': 1.0985628366470337, 'the density-diversity penalty and untied weights, and 2. training without this penalty but with tied weights.': 0.6280590295791626, 'The experiments on two datasets (MNIST for vision and TIMIT for speech) shows that the method achieves very good compression rates without loss of performance.': 1.0979608297348022, 'The paper is presented very clearly,  presents very interesting ideas and seems to be state of the art for compression.': 1.0981487035751343, 'The approach opens many new avenues of research and the strategy of weight-tying may be of great interest outside of the compression domain to learn regularities in data.': 1.0986123085021973, 'The result tables are a bit confusing unfortunately.': 0.6010217666625977, 'minor issues:': 1.0986123085021973, 'p1': 1.0986104011535645, 'english mistake: “while networks *that* consist of convolutional layers”.': 0.656245231628418, 'p6-p7': 1.0934559106826782, 'Table 1,2,3 are confusing.': 0.8143857717514038, 'Compared to the baseline (DC), your method (DP) seems to perform worse:': 0.32765355706214905, 'In Table 1 overall, Table 2 overall FC, Table 3 overall, DP is less sparse and more diverse than the DC baseline.': 0.9365478754043579, 'This would suggest a worse compression rate for DP and is inconsistent with the text which says they should be similar or better.': 0.22183862328529358, 'I assume the sparsity value is inverted and that you in fact report the number of non-modal values as a fraction of the total.': 0.9878649711608887, 'This work introduces a number of techniques to compress fully-connected neural networks while maintaining similar performance, including a density-diversity penalty and associated training algorithm.': 1.0986123085021973, 'The core technique of this paper is to explicitly penalize both the overall magnitude of the weights as well as diversity between weights.': 1.0986026525497437, 'This approach results in sparse weight matrices comprised of relatively few unique values.': 1.0986123085021973, 'Despite introducing a more efficient means of computing the gradient with respect to the diversity penalty, the authors still find it necessary to apply the penalty with some low probability (1-5%) per mini-batch.': 1.0986123085021973, 'The approach achieves impressive compression of fully connected layers with relatively little loss of accuracy.': 1.0984801054000854, 'I wonder if the cost of having to sort weights (even for only 1 or 2 out of 100 mini-batches) might make this method intractable for larger networks.': 1.0986123085021973, 'Perhaps the sparsity could help remove some of this cost?': 1.0986123085021973, 'I think the biggest fault this paper has is the number of different things going on in the approach that are not well explored independently.': 1.0986123085021973, 'Sparse initialization, weight tying, probabilistic application of density-diversity penalty and setting the mode to 0, and alternating schedule between weight tied standard training and diversity penalty training.': 1.0986123085021973, ""The authors don't provide enough discussion of the relative importance of these parts."": 1.0985915660858154, 'Furthermore, the only quantitative metric shown is the compression rate which is a function of both sparsity and diversity such that they cannot be compared on their own.': 1.0986123085021973, 'I would really like to see how each component of the algorithm affects diversity, sparsity, and overall compression.': 1.0984127521514893, 'A quick verification: Section 3.1 claims the density-diversity penalty is applied with a fixed probability per batch while 3.4 implies structured phases alternating between application of density-diversity and weight tied standard cross entropy.': 1.0986123085021973, 'Is this scheme in 3.4 only applying the density-diversity penalty probabilistically when it is in the density-diversity phase?': 1.0979163646697998, 'Preliminary rating:': 1.0986120700836182, 'I think this is an interesting paper but lacks sufficient empirical evaluation of its many components.': 1.0985757112503052, 'As a result, the algorithm has the appearance of a collection of tricks that in the end result in good performance without fully explaining why it is effective.': 0.9421907067298889, 'Minor notes:': 1.0985801219940186, 'Please resize equation 4 to fit within the margins (\\resizebox{\\columnwidth}{!}{ blah } works well in latex for this)': 1.0334552526474, 'The paper shows promising results but it is difficult to read and follow.': 1.0986061096191406, 'It presents different things closely related and it is difficult to asses the performance of each one.': 1.0976669788360596, 'Diversity, sparsity, regularization term, tying weights.': 1.0986123085021973, 'Anyway results are good.': 1.09223473072052}"
215,https://openreview.net/forum?id=HkuVu3ige,"{'This paper investigates the impact of orthogonal weight matrices on learning dynamics in RNNs.': 1.0986123085021973, 'The paper proposes a variety of interesting optimization formulations that enforce orthogonality in the recurrent weight matrix to varying degrees.': 1.0986123085021973, 'The experimental results demonstrate several conclusions: enforcing exact orthogonality does not help learning, while enforcing soft orthogonality or initializing to orthogonal weights can substantially improve learning.': 1.0986123085021973, 'While some of the optimization methods proposed currently require matrix inversion and are therefore slow in wall clock time, orthogonal initialization and some of the soft orthogonality constraints are relatively inexpensive and may find their way into practical use.': 1.0986123085021973, 'The experiments are generally done to a high standard and yield a variety of useful insights, and the writing is clear.': 1.0986123085021973, 'The experimental results are based on using a fixed learning rate for the different regularization strengths.': 1.0986123085021973, 'Learning speed might be highly dependent on this, and different strengths may admit different maximal stable learning rates.': 1.0986123085021973, 'It would be instructive to optimize the learning rate for each margin separately (maybe on one of the shorter sequence lengths) to see how soft orthogonality impacts the stability of the learning process.': 1.0986123085021973, 'Fig.': 1.0986123085021973, '5, for instance, shows that a sigmoid improves stability—but perhaps slightly reducing the learning rate for the non-sigmoid Gaussian prior RNN would make the learning well-behaved again for weightings less than 1.': 0.5037751793861389, '4 shows singular values converging around 1.05 rather than 1.': 1.0986121892929077, 'Does initializing to orthogonal matrices multiplied by 1.05 confer any noticeable advantage over standard orthogonal matrices?': 1.0981042385101318, 'Especially on the T=10K copy task?': 1.0985970497131348, '“Curiously, larger margins and even models without sigmoidal constraints on the spectrum (no margin) performed well as long as they were initialized to be orthogonal suggesting that evolution away from orthogonality is not a serious problem on this task.”': 1.0975241661071777, 'This is consistent with the analysis given in Saxe et al. 2013, where for deep linear nets, if a singular value is initialized to 1 but dies away during training, this is because it must be zero to implement the desired input-output map.': 1.098609209060669, 'More broadly, an open question has been whether orthogonality is useful as an initialization, as proposed by Saxe et al., where its role is mainly as a preconditioner which makes optimization proceed quickly but doesn’t fundamentally change the optimization problem; or whether it is useful as a regularizer, as proposed by Arjovsky et al. 2015 and Henaff et al. 2015, that is, as an additional constraint in the optimization problem (minimize loss subject to weights being orthogonal).': 1.0444852113723755, 'These experiments seem to show that mere initialization to orthogonal weights is enough to reap an optimization speed advantage, and that too much regularization begins to hurt performance—i.e., substantially changing the optimization problem is undesirable.': 1.0986123085021973, 'This point is also apparent in Fig.': 1.0986123085021973, '2: In terms of the training loss on MNIST (Fig. 2), no margin does almost indistinguishably from a margin of 1 or .1.': 1.0986123085021973, 'However in terms of accuracy, a margin of .1 is best.': 1.0986123085021973, 'This shows that large or nonexistent margins (i.e., orthogonal initializations) enable fast optimization of the training loss, but among models that attain similar training loss, the more nearly orthogonal weights perform better.': 1.0986123085021973, 'This starts to separate out the optimization speed advantage conferred by orthogonality from the regularization advantage it confers.': 1.0986123085021973, 'It may be useful to more explicitly discuss the initialization vs regularization dimension in the text.': 1.0986123085021973, 'Overall, this paper contributes a variety of techniques and intuitions which are likely to be useful in training RNNs.': 1.0986123085021973, 'The paper is well-motivated, and is part of a line of recent work investigating the use of orthogonal weight matrices within recurrent neural networks.': 1.0986123085021973, 'While using orthogonal weights addresses the issue of vanishing/exploding gradients, it is unclear whether anything is lost, either in representational power or in trainability, by enforcing orthogonality.': 1.0986123085021973, 'As such, an empirical investigation that examines how these properties are affected by deviation from orthogonality is a useful contribution.': 1.0986123085021973, 'The paper is clearly written, and the primary formulation for investigating soft orthogonality constraints (representing the weight matrices in their SVD factorized form, which gives explicit control over the singular values) is clean and natural, albeit not necessarily ideal from a practical computational standpoint (as it requires maintaining multiple orthogonal weight matrices each requiring an expensive update step).': 1.097190499305725, 'I am unaware of this approach being investigated previously.': 1.0986123085021973, 'The experimental side, however, is somewhat lacking.': 1.0986123085021973, 'The paper evaluates two tasks: a copy task, using an RNN architecture without transition non-linearities, and sequential/permuted sequential MNIST.': 1.0986123085021973, ""These are reasonable choices for an initial evaluation, but are both toy problems and don't shed much light on the practical aspects of the proposed approaches."": 1.0986123085021973, 'An evaluation in a more realistic setting would be valuable (e.g., a language modeling task).': 1.0986123085021973, ""Furthermore, while investigating pure RNN's makes sense for evaluating effects of orthogonality, it feels somewhat academic: LSTMs also provide a mechanism to capture longer-term dependencies, and in the tasks where the proposed approach was compared directly to an LSTM, it was significantly outperformed."": 1.0986123085021973, ""It would be very interesting to see the effects of the proposed soft orthogonality constraint in additional architectures (e.g., deep feed-forward architectures, or whether there's any benefit when embedded within an LSTM, although this seems doubtful)."": 1.0986123085021973, 'Overall, the paper addresses a clear-cut question with a well-motivated approach, and has interesting findings on some toy datasets.': 1.0986123085021973, 'As such I think it could provide a valuable contribution.': 1.0986123085021973, 'However, the significance of the work is restricted by the limited experimental settings (both datasets and network architectures).': 1.0986123085021973, 'Vanishing and exploding gradients makes the optimization of RNNs very challenging.': 1.0986123085021973, 'The issue becomes worse on tasks with long term dependencies that requires longer RNNs.': 1.0986123085021973, 'One of the suggested approaches to improve the optimization is to optimize in a way that the transfer matrix is almost orthogonal.': 1.0986123085021973, 'This paper investigate the role of orthogonality on the optimization and learning which is very important.': 1.0986123085021973, 'The writing is sound and clear and arguments are easy to follow.': 1.0986123085021973, 'The suggested optimization method is very interesting.': 1.0986123085021973, 'The main shortcoming of this paper is the experiments which I find very important and I hope authors can update the experiment section significantly.': 1.0986123085021973, 'Below I mention some comments on the experiment section:': 1.0986123085021973, '1- I think the experiments are not enough.': 1.0986123085021973, 'At the very least, report the result on the adding problem and language modeling task on Penn Treebank.': 1.0986123085021973, '2- I understand that the copying task becomes difficult with non-lineary.': 1.0986123085021973, 'However, removing non-linearity makes the optimization very different and therefore, it is very hard to conclude anything from the results on the copying task.': 1.0986123085021973, '3- I was not able to find the number of hidden units used for RNNs in different tasks.': 1.0986123085021973, '4- Please report the running time of your method in the paper for different numbers of hidden units, compare it with the SGD and mention the NN package you have used.': 1.0986123085021973, '5- The results on Table 1 and Table 2 might also suggest that the orthogonality is not really helpful since even without a margin, the numbers are very close compare to the case when you find the optimal margin.': 1.0986123085021973, 'Am I right?': 1.0986123085021973, '6- What do we learn from Figure 2?': 1.0986123085021973, 'It is left without any discussion.': 1.0986123085021973}"
216,https://openreview.net/forum?id=HkvS3Mqxe,"{'This paper proposes a simple randomized algorithm for selecting which weights in a ConvNet to prune in order to reduce theoretical FLOPs when evaluating a deep neural network.': 1.0986123085021973, 'The paper provides a nice taxonomy or pruning granularity from coarse (layer-wise) to fine (intra-kernel).': 1.0986123085021973, 'The pruning strategy is empirically driven and uses a validation set to select the best model from N randomly pruned models.': 1.0986123085021973, 'Makes claims in the intro about this being ""one shot"" and ""near optimal"" that cannot be supported: it is ""N-shot"" in the sense that N networks are generated and tested and there is no evidence or theory that the found solution is ""near optimal.""': 1.0986123085021973, 'Pros:': 1.0986123085021973, 'Nice taxonomy of pruning levels': 1.0986123085021973, 'Comparison to the recent weight-sum pruning method': 1.0986016988754272, 'Cons:': 1.0986123085021973, 'Experimental evaluation does not touch upon recent models (ResNets) and large scale datasets (ImageNet)': 1.098610281944275, 'Paper is somewhat hard to follow': 1.0904936790466309, 'Feature map pruning can obviously accelerate computation without specialized sparse implementations of convolution, but this is not the case for finer grained sparsity; since this paper considers fine-grained sparsity it should provide some evidence that introducing that sparsity can yield performance improvements': 0.6793181896209717, 'Another experimental downside is that the paper does not evaluate the impact of filter pruning on transfer learning.': 0.8948056101799011, 'For example, there is not much direct interest in the tasks of MNIST, CIFAR10, or even ImageNet.': 1.0986123085021973, 'Instead, a main interest in both academia and industry is the value of the learned representation for transferring to other tasks.': 1.0986123085021973, 'One might expect pruning to harm transfer learning.': 1.0986123085021973, ""It's possible that the while the main task has about the same performance, transfer learning is strongly hurt."": 1.0986123085021973, 'This paper has missed an opportunity to explore that direction.': 1.0986123085021973, 'In summary, the proposed method is simple, which is good, but the experimental evaluation is somewhat incomplete and does not cover recent models and larger scale datasets.': 1.0986074209213257, 'This paper proposes two pruning methods to reduce the computation of deep neural network.': 1.0986123085021973, 'In particular, whole feature maps and the kernel connections can be removed with not much decrease of classification accuracy.': 1.0930421352386475, 'However, this paper also has the following problems.': 1.0986123085021973, '1)': 1.0986123085021973, 'The method is somehow trivial, since the pruning masks are mainly chosen by simple random sampling.': 1.0986123085021973, 'The novelty and scalability are both limited.': 1.0986123085021973, '2) Experiment results are mainly focused on the classification rate and the ideal complexity.': 1.0986123085021973, 'As a paper on improving computation efficiency, it should include results on practical time consumption.': 1.0986123085021973, 'It is very common that reducing numbers of operations may not lead to reduced computational time on a highly parallel platform (e.g., GPU).': 0.6152694225311279, '3) It is more important to improve the computational efficiency on large-scale models (e.g., ImageNet classification network) than on small models (e.g., MNIST, CIFAR network).': 1.098567008972168, 'However, results on large-scale network is missing.': 1.0986123085021973, '4) (*Logical validity of the proposed method*)': 1.0986123085021973, 'For feature map pruning, what if just to train reduced-size network is trained from scratch without transfer any knowledge from the pretrained large network?': 1.0594582557678223, 'Is it possible to get the same accuracy?': 1.0986123085021973, 'If so, it will simply indicate the hyper-parameter is not optimal for the original network.': 1.0985913276672363, 'Experimental results are necessary to clarify the necessity of feature map pruning.': 1.0986123085021973, 'Note that I agree with that a smaller network may be more generalizable than a larger network.': 1.0986123085021973, ""Comments to the authors's response:"": 1.0986123085021973, 'Thanks for replying to my comments.': 1.0986123085021973, '1) I still believe that the proposed methods are trivial.': 1.0986123085021973, '2) It is nice to show GPU implementation.': 1.0986123085021973, 'Compared to existing toolboxes (e.g., Torch, Caffe, TensorFlow), is the implementation of convolution efficient enough?': 1.0986123085021973, '3) Experiments on Cifar-100 are helpful (better than cifar-10), but it is not really large-scale, where speed-up is not so critical.': 1.0986123085021973, 'ImageNet and Places datasets are examples of large-scale datasets.': 1.0986123085021973, '4)': 1.0986123085021973, 'The author did not reply to the question wrt the validity of the proposed methods.': 1.0986123085021973, 'This question is critical.': 1.0986123085021973, 'Summary: There are many different pruning techniques to reduce memory footprint of CNN models, and those techniques have different granularities (layer, maps, kernel or intra kernel), pruning ratio and sparsity of representation. The work proposes a method to choose the best pruning masks out to many trials. Tested on CIFAR-10, SVHN and MNIST.': 1.0536293983459473, 'Proposes a method to choose pruning mask out of N trials.': 1.097091555595398, 'Analysis on different pruning methods.': 1.0986123085021973, 'Cons & Questions:': 0.5040261745452881, '“The proposed strategy selects the best pruned network through N random pruning trials.': 1.0986109972000122, 'This approach enables one to select pruning mask in one shot and is simpler than the multi-step technique.”': 1.097688913345337, 'How can one get the best pruning mask in one shot if you ran N random pruning trials?': 1.0986123085021973, '(answered)': 1.0986123085021973, 'Missing tests of the approach with bigger CNN: like AlexNet, VGG, GoogLeNet or ResNet.': 1.0986077785491943, '(extended to VGG ok)': 1.09744131565094, 'Since reducing model size for embedded systems is the final goal, then showing how much memory space in MB is saved with the proposed technique compared with other approaches like Han et al. (2015) would be good.': 1.0885307788848877, 'Misc:': 1.0986123085021973, 'Typo in figure 6 a) caption: “Featuer” (corrected)': 1.0928632020950317}"
217,https://openreview.net/forum?id=HkwoSDPgg,"{'This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting.': 1.0949000120162964, 'I found the approach altogether plausible and very clearly explained by the authors.': 1.0986123085021973, 'Adding more discussion of the bound (and its tightness) from Theorem 1 itself would be appreciated.': 1.0986123085021973, 'A simple idea of adding perturbation error to the counts, known from differentially-private literature, is nicely re-used by the authors and elegantly applied in a much broader (non-convex setting) and practical context than in a number of differentially-private and other related papers.': 1.0986123085021973, 'The generality of the approach, clear improvement over predecessors, and clarity of the writing makes the method worth publishing.': 1.0986123085021973, 'This paper discusses how to guarantee privacy for training data.': 0.5693883895874023, ""In the proposed approach multiple models trained with disjoint datasets are used as ``teachers'' model, which will train a ``student'' model to predict an output chosen by noisy voting among all of the teachers."": 1.0981029272079468, 'The theoretical results are nice but also intuitive.': 0.8059889674186707, ""Since teachers' result are provided via noisy voting, the student model may not duplicate the teacher's behavior."": 1.0985952615737915, 'However, the probabilistic bound has quite a number of  empirical parameters, which makes me difficult to decide whether the security is 100% guaranteed or not.': 1.0986045598983765, 'The experiments on MNIST and SVHN are good.': 1.09534752368927, 'However, as the paper claims, the proposed approach may be mostly useful for sensitive data like medical histories, it will be nice to conduct one or two experiments on such applications.': 1.0986121892929077, 'Altogether a very good paper, a nice read, and interesting.': 1.097632646560669, 'The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.': 1.0985994338989258, 'One caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance.': 0.5910110473632812, 'Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions).': 1.093502163887024, 'Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures.': 0.42246222496032715, 'If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.': 1.0986123085021973, 'Another caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon.': 1.0986123085021973, 'However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.': 1.0986123085021973, 'Finally, as has been acknowledged in the paper, the present approach may not work on other natural data types.': 1.0986123085021973, 'Experiments on other data sets is strongly encouraged.': 1.0986123085021973, 'Also, please cite the data sets used.': 1.0986123085021973, 'Other comments:': 1.0986123085021973, 'Discussion of certain parts of the related work are thorough.': 1.0986123085021973, 'However, please add some survey/discussion of the related work on differentially-private semi-supervised learning.': 1.0986123085021973, 'For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as “teacher-learner”).': 1.0986123085021973, 'The only time the private labeled data is used is when learning the “primary ensemble.”': 1.0986123085021973, 'A ""secondary ensemble"" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.': 1.0986123085021973, 'G. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy.': 1.0986123085021973, 'Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.': 1.0986123085021973, 'Section C. does a nice comparison of approaches.': 1.0986123085021973, 'Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results.': 1.0986123085021973, 'The paper is extremely well-written, for the most part.': 1.0986123085021973, 'Some places needing clarification include:': 1.0986123085021973, 'Last paragraph of 3.1.': 1.0986123085021973, '“all teachers….get the same training data….”': 1.0986123085021973, 'This should be rephrased to make it clear that it is not the same w.r.t.': 1.0986123085021973, 'all the teachers, but w.r.t.': 1.0986123085021973, 'the same teacher on the neighboring database.': 1.0986123085021973, '4.1: The authors state: “The number n of teachers is limited by a trade-off between the classification task’s complexity and the available data.”': 1.0986123085021973, 'However, since this tradeoff is not formalized, the statement is imprecise.': 1.0986123085021973, 'In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution.': 1.0986123085021973, 'Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity.': 1.0986123085021973, 'In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption).': 1.0986123085021973, 'However later it is stated that the gap stays under 20%.': 1.0986123085021973, 'These sentences seem contradictory, which is likely not what was intended.': 1.0986123085021973}"
218,https://openreview.net/forum?id=HkxAAvcxx,"{'The paper proposes a method for future frame prediction based on transformation of previous frame rather than direct pixel prediction.': 1.0986123085021973, 'Many previous works have proposed similar methods.': 1.0986123085021973, 'The authors in their responses state that previous work is deterministic, yet the proposed model also does not handle multimodality.': 1.0986123085021973, 'Further, i asked if they could test their method using 2 RGB frames as input and predicting the transformation as output, to be able to quantify the importance of using transformations both as input and output, since this is the first work that uses transformations as input also.': 1.0986123085021973, 'The authors dismissed the suggestion by saying ""if we were to use RGB frames as input and ask the model to output future frames it would produce very blurry results"", that is, misunderstanding what the suggestion was.': 1.0986123085021973, 'So, currently, it does not seem to be a valid novel contribution in this work compared to previous works.': 1.0986123085021973, 'Paper Summary': 1.0986123085021973, 'This paper makes two contributions': 1.0986123085021973, '(1) A model for next step prediction, where the inputs and outputs are in the': 1.0986123085021973, 'space of affine transforms between adjacent frames.': 1.0986123085021973, '(2) An evaluation method in which the quality of the generated data is assessed': 1.0986123085021973, 'by measuring the reduction in performance of another model (such as a': 1.0986123085021973, 'classifier) when tested on the generated data.': 1.0986123085021973, 'The authors show that according to this metric, the proposed model works better': 1.0986123085021973, 'than other baseline models (including the recent work of Mathieu et al.': 1.0986123085021973, 'which': 1.0986123085021973, 'uses adversarial training).': 1.0986123085021973, 'Strengths': 1.0986123085021973, 'This paper attempts to solve a major problem in unsupervised learning': 1.0986123085021973, 'with videos, which is evaluating them.': 1.0986123085021973, 'The results show that using MSE in transform space does prevent the blurring': 1.0986123085021973, 'problem to a large extent (which is one of the main aims of this paper).': 1.0986123085021973, 'The results show that the generated data reduces the performance of the C3D': 1.0986123085021973, 'model on UCF-101 to a much less extent than other baselines.': 1.0986123085021973, 'The paper validates the assumption that videos can be approximated to quite a': 1.0986123085021973, 'few time steps by a sequence of affine transforms starting from an initial': 1.0986123085021973, 'frame.': 1.0986123085021973, 'Weaknesses': 1.0986123085021973, 'The proposed metric makes sense only if we truly just care about the performance': 1.0986123085021973, 'of a particular classifier on a given task.': 1.0986123085021973, 'This significantly narrows the': 1.0986123085021973, 'scope of applicability of this metric because arguably, one the important': 1.0986123085021973, 'reasons for doing unsupervised learning is to come up a representation that is': 1.0986123085021973, 'widely applicable across a variety of tasks.': 1.0986123085021973, 'The proposed metric would not help': 1.0986123085021973, 'evaluate generative models designed to achieve this objective.': 1.0986123085021973, 'It is possible that one of the generative models being compared will interact': 1.0979822874069214, 'with the idiosyncrasies of the chosen classifier in unintended ways.': 1.0986123085021973, 'Therefore, it would be hard to draw strong conclusions about the relative': 1.097652792930603, 'merits of generative models from the results of such experiments.': 0.8757922649383545, 'One way to': 1.028914451599121, 'ameliorate this would be to use several different classifiers (C3D,': 0.43340903520584106, 'dual-stream network, other state-of-the-art methods) and show that the ranking': 0.9138330817222595, 'of different generative models is consistent across the choice of classifier.': 0.5078977942466736, 'Adding such experiments would help increase certainty in the conclusions drawn': 1.0986121892929077, 'in this paper.': 1.0794181823730469, 'Using only 4 or 8 input frames sampled at 25fps seems like very little context': 1.0986123085021973, 'if we really expect the model to extrapolate the kind of motion seen in': 0.4004698395729065, 'UCF-101.': 1.0986123085021973, 'The idea of working in the space of affine transforms would be much': 1.0879342555999756, 'more appealing if the model can be shown to really generated non-trivial motion': 1.0706480741500854, 'patterns.': 1.0986123085021973, 'Currently, the motion patterns seem to be almost linear': 1.0869776010513306, 'extrapolations.': 0.8743417859077454, 'The model that predicts motion does not have access to content at all.': 1.0913844108581543, 'It only': 1.016686201095581, 'gets access to previous motion.': 0.4115716218948364, 'It seems that this might be a disadvantage': 1.0651936531066895, 'because the motion predictor cannot use any cues like object boundaries, or': 0.9063947796821594, 'decide what to do when two motion fields collide (it is probably easier to argue': 0.3364076316356659, 'about occlusions in content space).': 0.7347926497459412, 'Quality/Clarity': 1.0986123085021973, 'The paper is clearly written and easy to follow.': 1.0039451122283936, 'The assumptions are clearly': 1.0986123085021973, 'specified and validated.': 1.0985580682754517, 'Experimental details seem adequate.': 1.0986123085021973, 'Originality': 0.43207383155822754, 'The idea of generating videos by predicting motion has been used previously.': 0.5378281474113464, 'Several recent papers also use this idea.': 1.0747628211975098, 'However the exact implementation in': 1.0551759004592896, 'this paper is new.': 0.4887218475341797, 'The proposed evaluation protocol is novel.': 1.0939264297485352, 'Significance': 1.0985361337661743, 'The proposed evaluation method is an interesting alternative, especially if it': 1.0974448919296265, 'is extended to include multiple classifiers representative of different': 0.7715259790420532, 'state-of-the-art approaches.': 1.098179578781128, 'Given how hard it is to evaluate generative models': 1.0986121892929077, 'of videos, this paper could help start an effort to standardize on a benchmark': 1.0654526948928833, 'set.': 1.0701501369476318, 'Minor comments and suggestions': 1.0489872694015503, '(1) In the caption for Table 1: ``Each column shows the accuracy on the test set': 1.0986119508743286, 'when taking a different number of input frames as input"" - ``input"" here refers': 0.2241523265838623, 'to the input to the classifier (Output of the next step prediction model).': 0.03182012587785721, 'However': 0.05720019340515137, 'in the next sentence ``Our approach maps 16 \\times 16 patches into 8 \\times 8': 0.5573127865791321, 'with stride 4, and it takes 4 frames at the input"" - here ``input"" refers to': 0.6752297282218933, 'the input to the next step prediction model.': 0.4055188298225403, 'It might be a good idea to rephrase': 0.9004891514778137, 'these sentences to make the distinction clear.': 1.0986109972000122, '(2) In order to better understand the space of affine transform': 1.0975021123886108, 'parameters, it might help to include a histogram of these parameters in the': 1.0986028909683228, 'paper.': 1.0975078344345093, 'This can help us see at a glance, what is the typical range of these': 1.098577857017517, '6 parameters, should we expect a lot of outliers, etc.': 1.0739346742630005, '(3) In order to compare transforms A and B, instead of ||A - B||^2, one': 0.3965209722518921, 'could consider A^{-1}B being close to identity as the metric.': 1.007460355758667, 'Did the authors': 1.0986117124557495, 'try this ?': 1.0746055841445923, '(4) ""The performance of the classifier on ground truth data is an upper bound on': 1.0986084938049316, 'the performance of any generative model.""': 1.0986123085021973, 'This is not *strictly* true.': 1.0986123085021973, 'It is': 1.0986123085021973, 'possible (though highly unlikely) that a generative model might make the data': 1.0986123085021973, 'look cleaner, sharper, or highlight some aspect of it which could improve the': 1.0986123085021973, 'performance of the classifier (even compared to ground truth).': 1.0986123085021973, 'This is': 1.0986123085021973, 'especially true if the the generative model had access to the classifier,': 1.0986123085021973, 'it': 1.0986123085021973, 'could then see what makes the classifier fire and highlight those discriminative': 1.0986123085021973, 'features in the generated output.': 1.0986123085021973, 'Overall': 1.0986123085021973, 'This paper proposes future prediction in affine transform space.': 1.0986123085021973, 'This does': 1.0986123085021973, 'reduce blurriness and makes the videos look relatively realistic (at least to the': 1.0986123085021973, 'C3D classifier).': 1.0986123085021973, 'However, the paper can be improved by showing that the model can': 1.0986123085021973, 'predict more non-trivial motion flows and the experiments can be strengthened by': 1.0986123085021973, 'adding more classifiers besides than C3D.': 1.0986123085021973, 'This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames.': 1.0986123085021973, 'The approach is based on a CNN that, in contrast to most related papers, work in the space of affine transformations (instead of pixels or flow).': 1.0986123085021973, 'Said another way, the network takes as input a set of affine transforms that describe the motion of patches in the past frames, and likewise, outputs a set of affine transforms that predict future patch motion.': 1.0986123085021973, 'To that aim, the authors make a few simplifying hypotheses, namely, that a sequence of frames can be modeled accurately enough in their patch-affine framework.': 1.0986123085021973, 'This is not unreasonable.': 1.0986123085021973, 'A lot of papers in the optical flow community are based on similar hypotheses, i.e. model the flow as a smoothly varying affine field (for instance see ""Locally affine sparse-to-dense matching for motion and occlusion estimation"" by Leordeanu et al., ""EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow"" by Revaud et al., ""Optical Flow With Semantic Segmentation and Localized Layers"" by Sevilla-Lara et al.).': 1.0986123085021973, 'These methods are state of the art, which gives a hint about the validity of this kind of approach.': 1.0986123085021973, 'In addition, it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels.': 1.0986123085021973, 'Indeed, the (patch-affine) motion space is considerably smaller than the image space, making the problem much more tractable and amenable to high-resolution videos.': 1.0986123085021973, 'While I agree with the authors on these points, I also find that the paper suffer from important flaws.': 1.0986123085021973, 'Specifically:': 1.0986123085021973, '- the choice of not comparing with previous approaches in term of pixel prediction error seems very ""convenient"", to say the least. While it is clear that the evaluation metric is imperfect, it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on, e.g. the moving digits datasets (Figure 4), looks ok and can definitely be compared with other papers. Yet, the authors chose not to, which is suspicious.': 1.0986123085021973, '- The newly proposed metric poses several problems. First, action classification is evaluated with C3D, which is not a state-of-the-art approach at all for this task. Second, this metric actually *does not* evaluate what the network is claimed to do, that is, next frame prediction. Instead, it evaluates if another network, which was never trained to distinguish between real or synthetic frames by the way, can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition, it does not really make sense to train a network for something else that the final task it is evaluated for.': 1.0986123085021973, '- how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow, which is still an active subject of research. Therefore, an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available, it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input, I find it important to discuss this aspect. How do motion estimation failures impact the network? Also, the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.': 0.4162053167819977, '- Even ignoring the weird proxy-evaluation part, the network is still not trained end-to-end. That is, the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms, instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results, but other types of losses do exist, for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves, minimizing a loss in the transformation space, where affine parameters are harder to intepret, introduces unexpected artifacts. The motion is often largely underestimated, as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames.': 0.2543954849243164, '- The proposed approach is not sufficiently compared to previous work. In particular, the approach is closely related to ""SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY"" of Taraucean et al, ICLR\'15. This paper also output prediction in the motion space. Experimental results should compare against it.': 0.6890915632247925, '- The comparison with optical flow is unfair. First, the approach of Brox et al. is more than 10 years old. Second, it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall, the approach is not compared to very challenging baselines.': 1.0986123085021973, ""- I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0, X_1 ...} and predicted frames as {Y_1, Y_2, ...}. When asked if the videos at http://joo.st/ICLR/ReconstructionsFromGroundTruth are generated from a single groundtruth frame (X_0) which is warped several times for each next frame (X_0"": 1.0986123085021973, '> Y_1': 1.0986123085021973, '> Y_2': 1.0986123085021973, '> ...), they said that it was the case.': 1.0986123085021973, 'However, it is clear that the reconstruction is based on the previous groundtruth frame (X_t-1': 1.0986123085021973, '> Y_t) because it would be impossible to render objects that enter the camera field with motion alone (e.g. the dentist sequence, the top of the toothbrush enter the frame at some point).': 1.0986123085021973, 'Perhaps I misunderstood something or there was a misunderstanding between the reviewer and the authors.': 1.0986123085021973, 'Here is the original question:': 1.0986123085021973, '(1) In the reconstructions shown on this page - http://joo.st/ICLR/ReconstructionsFromGroundTruth, is the reconstruction at time t (say Y_t) created by applying the estimated affine transform on ground truth at t-1 (X_{t-1}), or by applying it on Y_{t-1} ?': 1.0986123085021973, 'In other words, do we start from ground truth X_0 and apply a sequence of transforms, or do we apply the transform to each ground truth frame ?': 1.0986123085021973, 'And answer:': 1.0986123085021973, '(1) We start from ground truth X_0 and apply a sequence of transforms.': 1.0986123085021973, 'In the end, this qualitative result is not convincing.': 1.0986123085021973, 'If the output was just trivally Y_t': 1.0986123085021973, '= X_t-1, then the two videos (X and Y) would render almost the same except for a negligible lag.': 1.0986123085021973, 'To conclude, this paper takes an interesting direction but suffers from important flaws.': 1.0986123085021973, 'The most important ones are about the experiments: qualitative experiments are not so convincing (Figure 4 is not as good as in the paper by Srivastava et al) and quantitative results are either missing or questionable.': 1.0986123085021973, 'In addition, the network is not really trained end-to-end w.r.t.': 1.0986123085021973, 'the final task.': 1.0986123085021973, 'Training for the affine loss is hard to intepret and seems to lead to unexpected artifacts.': 1.0986123085021973, 'Using a spatial transformer layer would make it possible to train in the image space, e.g. with the robust gradient loss of Mathieu et al.': 1.0986123085021973, 'Using per-patch affine spatial transformers is also possible, see the ""Universal Correspondence Network"" of Choy et al. (NIPS\'16).': 1.0986123085021973}"
219,https://openreview.net/forum?id=HkyYqU9lx,"{'The paper describes a recurrent transducer that uses hard monotonic alignments: at each step a discrete decision is taken either to emit the next symbol or to consume the next input token.': 1.0986123085021973, 'The model is moderately novel - similar architecture was proposed for speech recognition (https://arxiv.org/pdf/1608.01281v1.pdf).': 1.0986123085021973, 'Soft monotonic alignemts are also enforced by A. Graves in https://arxiv.org/abs/1308.0850.': 1.0986123085021973, 'The difficult part in training the proposed model is backpropagation through the discrete decisions.': 1.086772084236145, 'Typically, reinforcement learning techniques are used.': 0.35395047068595886, 'In this contribution, the authors side-step the issue by using a problem-dependent aligner to generate optimal decisions for which they train the model.': 1.097460150718689, ""The results indicate that such specially supervised model is better than the generic soft-attention model that doesn't require any problem-dependent external supervision."": 1.0986113548278809, 'However the authors did not attempt to work on regularizing the soft-attention model, which is not fair - the extra supervision by using the ground-truth alignment is a form of regularization and it could be used as e.g. an extra signal to the soft-attention model for a better comparison.': 1.0986120700836182, 'That being said the authors reash state-of-the-art results against other domain specific methods.': 1.0976110696792603, 'I believe the paper would more suit a NLP venue - it sound and properly written, but its applicability is limited to the considered NLP problem.': 1.0986123085021973, 'This paper proposes a sequence transduction model that first uses a traditional statistical alignment methods to provide alignments for an encoder-decoder type model.': 1.0986123085021973, 'The paper provides experiments on a number of morphological inflection generation datasets.': 1.0981297492980957, 'They shows an improvement over other models, although they have much smaller improvements over a soft attention model on some of their tasks.': 1.0963919162750244, 'I found this paper to be well-written and to have very thorough experiments/analysis, but I have concerns that this work isn\'t particularly different from previous approaches and thus has a more focused contribution that is limited to its application on this type of shorter input (the authors ""suggest"" that their approach is sufficient for shorter sequences, but don\'t compare against the approach of Chorowski et al. 2015 or Jailty el at 2016).': 1.0986123085021973, ""In summary, I found this paper to be well-executed/well-written, but it's novelty and scope too small."": 0.8948007822036743, 'That said, I feel this work would make a very good short paper.': 1.0986123085021973, 'The paper proposes an approach to sequence transduction for the case when a monotonic alignment between the input and the output is plausible.': 1.0986120700836182, 'It is assumed that the alignment can be provided as a part of training data, with Chinese Restaurant process being used in the actual experiments.': 0.41764652729034424, 'The idea makes sense, although its applicability is limited to the domains where a monotonic alignment is available.': 1.0983705520629883, 'But as discussed during the pre-review period, there has been a lot of strongly overlapping related work, such as probabilistic models with hard-alignment (Sequence Transduction With Recurrent Neural Network, Graves et al, 2012) and also attempts to use external alignments in end-to-end models (A Neural Transducer, Jaitly et al, 2015).': 1.0986123085021973, 'That said, I do not think the approach is sufficiently novel.': 1.0627741813659668, 'I also have a concern regarding the evaluation.': 1.091616153717041, 'I do not think it is fair to compare the proposed model that depends on external alignment with the vanilla soft-attention model that learns alignments from scratch.': 1.0986078977584839, 'In a control experiment soft-attention could be trained to match the external alignment.': 1.097168207168579, 'Such a pretraining could reduce overfitting on the small dataset, the one on which the proposed approach brings the most improvement.': 1.0986123085021973, 'On a larger dataset, especially SIGMORPHON, the improvements are not very big and are only obtained for a certain class of languages.': 1.098608136177063, 'To sum up, two main issues are (a) lack of novelty (b) the comparison of a model trained with external alignment and one without it.': 1.0985441207885742}"
220,https://openreview.net/forum?id=Hkz6aNqle,"{'The paper proposes a greedy supervised layer-wise initialization strategy for (deep) multi-layer perceptrons.': 1.0763176679611206, 'Layer weights are initialized by training linear SVMs for binary classification where the binary targets are constructed as error correcting codes (ECOC, including one-vs-all, one-vs-one and others).': 1.0787700414657593, 'The thus pertained model (together with a softmax output layer) is then globally fine-tuned by backdrop with dropout.': 0.4419797956943512, 'Note that as a heuristic greedy supervised layer-wise initialization strategy this work is very similar to the author’s other ICLR submission: « Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications ».': 1.0984041690826416, 'The two works differ in the supervised initialization strategy employed.': 0.9383639693260193, 'While layer-wise initialization strategies are worthy of further exploration, the paper doesn’t convey any insight as to what makes a better strategy.': 1.0986123085021973, 'Experimental results are not sufficiently convincing by themselves alone to support a mostly incremental work; in particular I remain unconvinced that competing methods received full proper hyper-parameter tuning of their own.': 1.0986123085021973, 'Results showing accuracies as bar graphs make it hard to read-off precise accuracies, and one cannot easily compare with known state-of-the-art performance references on benchmark problems (such as MNIST).': 1.0986113548278809, 'CIFAR10': 1.0617235898971558, 'performance seem far from state-of-the-art.': 1.098537564277649, 'Explanations are unnecessarily detailed for standard algorithms (e.g. SVMs) and not sufficiently for aspects specific to the approach such as lesser known ECOC schemes.': 0.40214312076568604, 'One important aspect remains unclear regarding the use of SVMs.': 0.8892378807067871, 'Did you use linear SVMs as stated in section 3.1 (« In order to take the probabilistic outputs of the base classifiers as new representations of data, we adopt linear support vector machines (linear SVMs) as the binary classifiers »)': 1.09861159324646, 'or kernel SVMs as mentioned later «': 1.0795457363128662, 'For all DeepECOC models, we used support vector machines (SVMs) with RBF kernel function».': 0.38384437561035156, 'In the latter case, the paper lacks a description of how learned RBF-kernel SVMs are transferred to a deep network layer (does it yield 2 layers the firrst of which would be a large RBF neural layer?)': 1.0986106395721436, 'Also in this case of kernel SVMs the computational cost is likely to skyrocket and the method will have scaling issues.': 1.0986086130142212, 'Is this the reason why the method is too expensive to use on CIFAR10 from scratch, and prompts doing LBP first?': 1.0703909397125244, 'If you used linear SVMs, did you use an efficient implementation specific to linear SVMS (as opposed to generic kernel SVM code with a linear kernel?).': 1.095829725265503, 'Finally for image datasets, a visual comparison of learned filters could help provide some qualitative insight.': 1.0986123085021973, 'Error correcting output coding is well established supervised learning approach.': 1.0986123085021973, 'Stacking several layers of ECOC seems a natural way of extending the framework to deep learning.': 1.0986123085021973, 'The main motivation of the authors here is to reduce the sample complexity of internal binary classifiers by using ternary coding and to learn discriminative representations at every layer thanks to the local supervision.': 1.0986123085021973, 'The main idea of this work is interesting.': 1.0986123085021973, 'However, the presentation can be improved and several sections (about SVMs for example) can be shortened.': 1.0986123085021973, 'I am not convinced by the advantage of this approach compared to end-to-end training of neural networks of other layer-wise training of deep architecture.': 1.0986123085021973, 'In particular, when the codes at every layer can are randomly generated the induced binary problems can be arbitrarily difficult and this could lead to learnability issues hence affecting the quality of the features.': 1.0986123085021973, 'While this problem is counterbalanced by the decoding scheme (and error-correction) at the prediction layer, it is not clear how it should be dealt with in the intermediate layers.': 1.0986123085021973, 'In addition, other approaches such as stacked RBMs or Autoencoders learn regularities capturing information about the distribution of the data at every layer.': 1.0986123085021973, 'In comparison, it is not clear what the learned representation by an ECOC layer is unless it is supervised (One versus Rest).': 1.0986123085021973, 'Overall, though interesting, I am not convinced by the proposed approach and the experimental section does not help mitigate the impression.': 1.0986121892929077, 'More insights on the learned internal representations, and experiments using standard datasets could help.': 1.0986123085021973, 'The paper proposes a strategy to pre-train the successive layers of a multi-layer perceptron for classification tasks.': 1.091744065284729, 'It consists in training successively each layer to predict an ECOC corresponding to the classification problem.': 1.0782248973846436, 'The first layer predicts this ECOC from the input data and each successive layer takes as output its preceding layer to predict another ECOC.': 0.5762104988098145, 'Different regularization strategies are used during training (input noise, Dropout).': 0.9523143768310547, 'The MLP is then fine-tuned using SGD.': 1.0986121892929077, 'Comparisons are performed with baselines on different datasets.': 1.097758412361145, 'This is a preliminary work.': 1.0986123085021973, 'The idea might be valuable but it should be pushed further.': 1.0986123085021973, 'Concerning the writing, sections concerning well known notions (e.g. SVM) could be suppressed.': 1.0986123085021973, 'Since ECOC are central to this contribution, a more detailed description of the different ECOC strategies would be helpful.': 1.0986123085021973, 'The experiments do not compare the proposed model with state of the art classifiers.': 1.0986123085021973, 'The experimental conditions should be made more precise, e.g., it is not clear whether the regularization strategies have been used for all the NN architectures or only for the ECOC based ones.': 1.0986121892929077, 'Finally note that there are several papers that try to learn intermediate representations for classification problems, see e.g. Samy Bengio, Jason Weston, David Grangier: Label Embedding Trees for Large Multi-Class Tasks.': 1.0986087322235107, 'NIPS 2010: 163-171': 1.0986123085021973}"
221,https://openreview.net/forum?id=HkzuKpLgg,"{'This paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU.': 0.6931300163269043, 'The paper provides both theoretical analysis and experiments.': 0.6931467652320862, 'Overall, the results presented in the paper are interesting, but the writing can be improved.': 0.693145215511322, 'Comments:': 0.6931471824645996, 'The authors compare their proposed approach with several alternative approaches and demonstrate strong performance of the proposed approaches.': 0.6931434869766235, 'But it is unclear if the improvement is from the proposed approach or from the implementation.': 0.6931470632553101, 'The paper is not easy to follow and the writing can be improved in many place (aside from typos and missing references).': 0.6931471824645996, 'Specifically, the authors should provide more intuitions of the proposed approach in the introduction and in Section 3.': 0.6931281089782715, 'The proposition and the analysis in Section 3.2 do not suggest the communication cost of linear pipeline is approximately 2x and log p faster than BE and MST, respectively, as claimed in many places in the paper.': 0.6931471824645996, 'Instead, it suggests LP *cannot* be faster than these methods by 2x and log p  times.': 0.6931471824645996, 'More specifically, Eq (2) shows T_broadcase_BE/ T_broadcase_LP < 2.': 0.6927157044410706, 'This does not provide an upper-bound of T_broadcase_LP and it can be arbitrary worse when comparing with T_broadcase_BE from this inequality.': 0.6931466460227966, 'Therefore, instead of showing T_broadcase_BE/ T_broadcase_LP < 2, the authors should state T_broadcase_BE/ T_broadcase_LP > 1 when n approaches infinity.': 0.6931470632553101, 'It would be interesting to emphasize more on the differences between designing parallel algorithms on CPU v.s. on GPU to motivate the paper.': 0.6929998397827148, 'This paper analyzes the ring-based AllReduce approach for multi-GPU data parallel training of deep net.': 0.6931471824645996, 'Comments': 0.6931471824645996, '1) The name linear pipeline is somewhat confusing to the readers, as the technique is usually referred as ring based approach in Allreduce literature.': 0.6909534335136414, 'The author should use the standard name to make the connection easier.': 0.6929469704627991, '2) The cost analysis of ring-based Allreduce is already provided in the existing literature.': 0.5921252369880676, 'This paper applied the analysis to the case of multi-GPU deep net training, and concluded that the scaling is invariant of number of GPUs.': 0.6930240988731384, '3)': 0.6931471824645996, 'The ring-based allreduce approach is already supported by NVidia’s NCCL library, although the authors claim that their implementation comes earlier than the NCCL implementation.': 0.6931384801864624, '4) The overlap of communication of computation is an already applied technique in systems such as TensorFlow and MXNet.': 0.6925868391990662, 'The schedule proposed by the authors exploits the overlap partially, doing backprop of t-1 while doing reduce.': 0.693121612071991, 'Note that the dependency pattern can be further exploited; with the forward of layer t depend on update of parameter of layer t in last iteration.': 0.6931377053260803, 'This can be done by a dependency scheduler.': 0.6918790936470032, '5) Since this paper is about analysis of Allreduce, it would be nice to include detailed analysis of tree-shape reduction, ring-based approach and all-to-all approach.': 0.6234421730041504, 'The discussion of all-to-all approach is missing in the current paper.': 0.6931471824645996, 'In summary, this is a paper discussed existing Allreduce techniques for data parallel multi-GPU training of deep net, with cost analysis based on existing results.': 0.6931471824645996, 'While I personally find the claimed result not surprising as it follows from existing analysis of Allreduce, the analysis might help some other readers.': 0.6931468844413757, 'I view this as a baseline paper.': 0.6931471824645996, 'The analysis of Allreduce could also been improved (see comment 5).': 0.6931195855140686}"
222,https://openreview.net/forum?id=Hy-2G6ile,"{'Paper proposes Gated Muiltimodal Unit, a building block for connectionist models capable of handling multiple modalities.': 1.0977649688720703, '(Figure 2)': 1.0986123085021973, 'The bimodal case returns weighted activation by gains of gating units, do you do anything special to keep multi-modal case weighted as well?': 1.0964782238006592, 'I.e. how the equation for h in section 3.1 would look like for multi-modal case.': 1.095595359802246, 'Also what’s the rationale for using tanh nonlinearity (over, say RELU), is it somehow experimentally optimised choice?': 1.0851738452911377, 'I would find interesting a discussion on a possibility of handling missing data in case one or more modalities are unavailable at test time.': 1.0980513095855713, 'Is this possible in the current model to back-off to fewer modalities?': 1.0725256204605103, 'Synthetic example may suggest that’s in fact possible.': 0.6719765067100525, 'Those numbers, perhaps, could be added to table 2.': 1.0982962846755981, 'In the synthetic experiment, you should compare MGU with the fully-connected MLP model really, with similar complexity - that is - at least two hidden units (as GMU has two such for each modality) followed by logistic regression.': 1.0981502532958984, 'At least in terms of capability of drawing decision boundary, those should be comparable.': 1.097605586051941, 'I think, broader discussion shall be written on the related work associated with mixture of experts models (which is fact are very similar conceptually) as well as multiplicative RNN models [1].': 1.0352238416671753, 'Also, gating unit in LSTM can, in principle, play very similar role when multiple modalities are spliced in the input.': 1.081769585609436, 'Overall, the paper is interesting, so is the associated (and to be released) dataset.': 1.0151867866516113, 'Minor comments/typos:': 1.0986121892929077, 'Sec. 3.3:  layers and a MLP (see Section 3.4) -> layers and an MLP': 1.0096428394317627, 'Apologies for unacceptably late review.': 1.0986117124557495, '[1] Multiplicative LSTM for sequence modelling B Krause, L Lu, I Murray, S Renals': 0.2036258578300476, 'The paper introduces Gated Multimodal Units GMUs, which use multiplicative weights to select the degree to which a hidden unit will consider different modalities in determining its activation.': 1.0985559225082397, 'The paper also introduces a new dataset, ""Multimodal IMDb,"" consisting of over 25k movie summaries, with their posters, and labeled genres.': 1.0986123085021973, 'GMUs are related to ""mixture of experts"" in that different examples will be classified by different parts of the model, (but rather than routing/gating entire examples, individual hidden units are gated separately).': 1.0986123085021973, 'They are related to attention models in that different parts of the input are weighted differently; there the emphasis is on gating modalities of input.': 1.0986123085021973, 'The dataset is a very nice contribution, and there are many experiments varying text representation and single-modality vs two-modality.': 1.0986123085021973, 'What the paper is lacking is a careful discussion, experimentation and analysis in comparison to other multiplicative gate models': 1.0986123085021973, 'which is the core intellectual contribution of the paper.': 1.0986123085021973, 'For example, I could imagine that a mixture of experts or attention models or other gated models might perform very well, and at the very least provide interesting scientific comparative analysis.': 1.0986123085021973, 'I encourage the authors to continue the work, and submit a revised paper when ready.': 1.0986123085021973, 'As is, I consider the paper to be a good workshop paper, but not ready for a major conference.': 1.0986123085021973, 'This paper proposed The Gated Multimodal Unit (GMU) model for information fusion.': 1.097854733467102, 'The GMU learns to decide how modalities influence the activation of the unit using multiplicative gates.': 1.0498247146606445, 'The paper collected a large genre dataset from IMDB and showed that GMU gets good performance.': 1.0982202291488647, 'The proposed approach seems quite interesting, and the audience may expect it can be used in general scenarios beyond movie genre prediction.': 1.0666855573654175, 'So it is quite straightforward that the paper should test the algorithm in other applications, which was not done yet.': 1.0343338251113892, 'That is the biggest shortcoming of this paper in my opinions.': 1.0973286628723145, 'Another concern lies in how to evaluate the performance of information fusion.': 1.0986121892929077, 'The abstract claims ""The model improves the macro f-score performance of single-modality models by 30% and 4% with respect to visual and textual information respectively"", however, such an improvement is off the key.': 1.098524808883667, 'If two modals are complementary to each other, the fusion results will always be higher.': 1.0366086959838867, 'The key fact is how much better than baselines the proposed GMU is.': 1.0985950231552124, 'There is a long list of techniques for fusions, so it is difficult to conduct an impressive comparison on only one real dataset.': 1.0986123085021973, 'I think GMU did a nice work on movie dataset, but I would also expect other techniques, including fine-tuning, dropout, distillation may help too.': 1.0986123085021973, 'It would be nice if the author could compare these techniques.': 1.0986123085021973, 'I also hope this paper could talk in more details the connection with mixture-of-expert (MoE) model.': 1.0986123085021973, 'Both models are based on the nonlinear gated functions, while both method may suffer from local minimum for optimization on small datasets.': 1.0986123085021973, 'I would like more in-depth discussion in their similarity and difference.': 1.0986123085021973, 'To gain more attention for GMU, I would encourage the author to open-source their code and try more datasets.': 1.0986123085021973}"
223,https://openreview.net/forum?id=Hy-lMNqex,"{'The authors present TARTAN, a derivative of the previously published DNN accelerator architecture: “DaDianNao”.': 1.6094379425048828, 'The key difference is that TARTAN’s compute units are bit-serial and unroll MAC operation over several cycles.': 1.6094379425048828, 'This enables the units to better exploit any reduction in precision of the input activations for improvement in performance and energy efficiency.': 1.6094379425048828, 'Comments:': 1.6094379425048828, '1. I second the earlier review requesting the authors to be present more details on the methodology used for estimating energy numbers for TARTAN. It is claimed that TARTAN gives only a 17% improvement in energy efficiency. However, I suspect that this small improvement is clearly within the margin of error ij energy estimation.': 1.609398365020752, ""2. TARTAN is a derivative of DaDianNao, and it heavily relies the overall architecture of DaDianNao. The only novel aspect of this contribution is the introduction of the bit-serial compute unit, which (unfortunately) turns out to incur a severe area overhead (of nearly 3x over DaDianNao's compute units)."": 1.6094003915786743, '3. Nonetheless, the idea of bit-serial computation is certainly quite interesting. I am of the opinion that it would be better appreciated (and perhaps be even more relevant) in a circuit design / architecture focused venue.': 1.6093536615371704, 'Summary:': 1.6094379425048828, 'The paper describes how the DaDianNao (DaDN) DNN accelerator can be improved by employing bit serial arithmetic.': 0.9574978947639465, 'They replace the bit-parallel multipliers in DaDN with multipliers that accept the weights in parallel but the activations serially (serial x parallel multipliers).': 1.4790548086166382, 'They increase the number of units keeping the total number of adders constant.': 1.60943603515625, 'This enables them to tailor the time and energy consumed to the number of bits used to represent activations.': 1.6094313859939575, 'They show how their configuration can be used to process both fully-connected and convolutional layers of DNNs.': 1.6094379425048828, 'Strengths:': 1.6094379425048828, 'Using variable precision for each layer of the network is useful - but was previously reported in Judd (2015)': 1.6094379425048828, 'Good evaluation including synthesis - but not place and route - of the units.': 1.6094379425048828, 'Also this evaluation is identical to that in Judd (2016b)': 1.609420657157898, 'Weaknesses:': 1.609433650970459, 'The idea of combining bit-serial arithmetic with the DaDN architecture is a small one.': 1.6094372272491455, 'The authors have already published almost everything that is in this paper at Micro 2016 in Judd (2016b).': 1.609427571296692, 'The increment here is the analysis of the architecture on fully-connected layers.': 1.6094375848770142, 'Everything else is in the previous publication.': 1.608549952507019, 'The energy gains are small - because the additional flip-flop energy of shifting the activations in almost offsets the energy saved on reducing the precision of the arithmetic.': 1.6094375848770142, 'The authors don’t compare to more conventional approaches to variable precision - using bit-parallel arithmetic units but data gating the LSBs so that only the relevant portion of the arithmetic units toggle.': 0.9705560207366943, 'This would not provide any speedup, but would likely provide better energy gains than the bit-serial x bit-parallel approach.': 1.5278573036193848, 'Overall:': 1.6094379425048828, 'The Tartan and Stripes architectures are interesting but the incremental contribution of this paper (adding support for fully-connected layers) over the three previous publications on this topic, and in particular Judd (2016b) is very small.': 1.4963570833206177, 'This idea is worth one good paper, not four.': 1.0491809844970703, ""This seems like a reasonable study, though it's not my area of expertise."": 1.6094378232955933, 'I found no fault with the work or presentation, but did not follow the details or know the comparable literature.': 1.6094304323196411, 'There seem to be real gains to be had through this technique, though they are only in terms of efficiency in hardware, not changing accuracy on a task.': 1.6094378232955933, 'The tasks chosen (Alexnet / VGG) seem reasonable.': 1.6094359159469604, 'The results are in simulation rather than in actual hardware.': 1.4566842317581177, 'The topic seems a little specialized for ICLR, since it does not describe any new advances in learning or representations, albeit that the CFP includes ""hardware"".': 1.6094365119934082, 'I think the appeal among attendees will be rather limited.': 1.6094379425048828, 'Please learn to use parenthetical references correctly.': 1.6094379425048828, 'As is your references make reading harder.': 1.6094379425048828, 'I do not feel very qualified to review this paper.': 1.5858899354934692, 'I studied digital logic back in university, that was it.': 1.227938175201416, 'I think the work deserves a reviewer with far more sophisticated background in this area.': 1.2578316926956177, 'It certainly seems useful.': 1.5501796007156372, 'My advice is also to submit it another venue.': 1.279586911201477, 'This paper proposed a hardware accelerator for DNN.': 1.0557746887207031, 'It utilized the fact that DNN are very tolerant to low precision inference and outperforms a state-of-the-art bit-parallel accelerator by 1.90x without any loss in accuracy while it is 1.17x more energy efficient.': 1.6094378232955933, 'TRT requires no network retraining.': 1.6093862056732178, 'It achieved super linear scales of performance with area.': 1.6094379425048828, ""The first concern is that this paper doesn't seem very well-suited to ICLR."": 1.6094377040863037, 'The circuit diagrams makes it more interesting for the hardware or circuit design community.': 0.9447126984596252, 'The second concern is the ""take-away for machine learning community"", seeing from the response, the take-away is using low-precision to make inference cheaper.': 1.6094379425048828, 'This is not novel enough.': 1.6094379425048828, ""In last year's ICLR, there were at least 4 papers discussing using low precision to make DNN more efficient."": 1.6094379425048828, ""These ideas have also been explored in the authors' previous papers."": 1.6094379425048828}"
224,https://openreview.net/forum?id=Hy0L4t5el,"{'The method overall seems to be a very interesting structural approach to variational autoencoders, however it seems to lack motivation as well as the application areas sufficient to prove its effectiveness.': 1.0986123085021973, 'I see the attractiveness of using structural information in this context and I find it more intuitive than using a flat sequence representation, especially when there is a clear structure in the data.': 1.0986123085021973, 'However experimental results seem to fail to be convincing in that regard.': 1.0986037254333496, 'One issue is the lack of a variety of applications in general, the experiments seem to be very limited in that regard, considering that the paper itself speaks about natural language applications.': 1.0986123085021973, 'It would be interesting to use the latent representations learned with the model for some other end task and see how much it impacts the success of that end task compared to various baselines.': 1.0986123085021973, 'In my opinion, the paper has a potentially strong idea however in needs stronger results (and possibly in a wider variety of applications) as a proof of concept.': 1.0986123085021973, 'The authors propose a variational autoencoder for a specific form of tree-generating model.': 1.0986106395721436, 'The generative model for trees seems reasonable but is not fully motivated.': 1.0986120700836182, 'If no previous references suggest this tree specification, then clear motivation for e.g. the extension beyond CFG should be given beyond the one sentence provided.': 0.9284291863441467, 'Given the tree model it may be natural to specify a tree model encoder, but the posterior distribution does not respect the structure of the prior (as the posterior distribution couples tree-distant variables), so there is in fact no good reason for this form, and a more general network could be compared with.': 1.0985647439956665, 'The approach provides sensible differentiable functions for encoding the network.': 1.0970048904418945, 'The tests are indicative, but the results are very similar to the tested approaches, and it is not clear what the best evaluation metric ought to be.': 1.0388948917388916, 'Significance: the work may well be significant in the future, but is currently somewhat preliminary, lacks motivation, chooses a tree structured encoder without particular motivation, and is lacking in wider comparisons.': 1.0986073017120361, 'There is also some lack of current motivation for the model, and no comparison with tractable models that do not need a variational autoencoder.': 1.0977580547332764, 'Originality: original, but at the moment it is not clear such originality is necessary.': 1.0885134935379028, 'Clarity: Good.': 1.0986123085021973, 'Experiments: Sensible, but not extensive or conclusive.': 1.0986123085021973, 'This paper introduces a novel extension of the variational autoencoder to arbitrary tree-structured outputs.': 1.0986123085021973, 'Experiments are conducted on a synthetic arithmetic expression dataset and a first-order logic proof clause dataset in order to evaluate its density modeling performance.': 1.0986123085021973, 'Pros:': 1.0986123085021973, '+': 1.0986123085021973, 'The paper is clear and well-written.': 1.0986123085021973, 'The tree-structure definition is sufficiently complete to capture a wide variety of tree types found in real-world situations.': 1.0986123085021973, 'The tree generation and encoding procedure is elegant and well-articulated.': 1.0986123085021973, 'The experiments, though limited in scope, are relatively thorough.': 1.0986123085021973, 'The use of IWAE to obtain a better estimate of log likelihoods is a particularly nice touch.': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'The performance gain over a baseline sequential model is marginal.': 1.0986123085021973, 'The experiments are limited in scope, both in the datasets considered and in the evaluation metrics used to compare the model with other approaches.': 1.0986123085021973, 'Specifically: (a) there is only one set of results on a real-world dataset and in that case the proposed model performs worse than the baseline, and (b) there is no evaluation of the learned latent representation with respect to other tasks such as classification.': 1.0986123085021973, 'The ability of the model to generate trees in time proportional to the depth of the tree is proposed as a benefit of the approach, though this is not empirically validated in the experiments.': 1.0986123085021973, 'The procedures to generate and encode trees are clever in their repeated use of common operations.': 1.0986123085021973, 'The weight sharing and gating operations seem important for this model to perform well': 1.0986123085021973, 'but it is difficult to assess their utility without an ablation (in Table 1 and 2 these modifications are not evaluated side-by-side).': 1.0986123085021973, 'Experiments in another domain (such as modeling source code, or parse trees conditioned on a sentence) would help in demonstrating the utility of this model.': 1.0986123085021973, 'Overall the model seems promising and applicable to a variety of data but the lack of breadth in the experiments is a concern.': 1.0986123085021973, '* Section 3.1: ""We distinguish three types"" => two': 1.0986123085021973, ""* Section 3.6: The exposition of the variable-sized latent state is slightly confusing because the issue of how many z's to generate is not discussed."": 1.0986088514328003, '* Section 4.2-4.3: When generating the datasets, did you verify that the test set is disjoint from the training set?': 1.0986120700836182, '* Table 1: Is there a particular reason why the variable latent results are missing for the depth 11 trees?': 1.0986123085021973}"
225,https://openreview.net/forum?id=Hy3_KuYxg,"{""I was holding off on this review hoping to get the missing details from the code at https://github.com/alexnowakvilla/DP, but at this time it's still missing."": 1.0986123085021973, ""After going over this paper couple of times I'm still missing the details necessary to reproduce the experiments."": 1.0986117124557495, 'I think this would be a common problem for readers of this paper, so the paper needs to be improved, perhaps with a toy example going through all the stages of learning.': 1.0986117124557495, 'As an example of the difficulty, take section 4.3.': 1.0729045867919922, 'It talks about training ""split block"" which is a function that can assign each element to either partition 0 or partition 1.': 1.0986123085021973, ""At this point I'm looking at it as a binary classification problem and looking for the parameters, loss, and how this loss is minimized."": 1.0986123085021973, 'Instead I get a lot of unexpected information, such as ""we must create artificial targets at every node of the generated tree from the available final target partition"".': 1.0986123085021973, 'What are these artificial targets, and how do they relate to the problem of training the splitter?': 1.0986123085021973, 'An example that explicitly goes through this construction would help with understanding.': 1.0986123085021973, 'The basic idea of this contribution is very nice and worth pursuing: how to use the powerful “divide and conquer” algorithm design strategy to learn better programs for tasks such as sorting or planar convex hull.': 1.0986123085021973, 'However, the execution of this idea is not convincing and needs polishing before acceptance.': 1.0986120700836182, 'As it is right now, the paper has a proof-of-concept feel that makes it great for a workshop contribution.': 1.0986123085021973, 'My main concern is that the method presented is currently not easily applicable to other tasks.': 1.0986121892929077, 'Typically, demonstrations of program induction from input-output examples on well known tasks serves the purpose of proving, that a generic learning machine is able to solve some well known tasks, and will be useful on other tasks due to its generality.': 1.0986123085021973, 'This contribution, however, presents a learning machine that is very hand-tailored to the two chosen tasks.': 1.0985989570617676, 'The paper essentially demonstrates that with enough engineering (hardcoding the recurrency structure, designing problem-specific rules of supervision at lower recurrency levels) one can get a partially trainable sorter or convex hull solver.': 1.0986123085021973, 'I found the contribution relatively hard to understand.': 0.8754440546035767, 'High level ideas are mixed with low-level tricks required to get the model to work and it is not clear either how the models operate, nor how much of them was actually learned, and how much was designed.': 1.0986123085021973, ""The answer to the questions did hep, nut didn't make it into the paper."": 1.0889737606048584, 'Mixing the descriptions of the tricks required to solve the two tasks makes things even more confusing.': 1.0563468933105469, 'I believe that the paper would be much more accessible if instead of promising a general solution it clearly stated the challenges faced by the authors and the possible solutions.': 1.0986123085021973, 'Highlights:': 1.0986090898513794, '+ Proof-of-concept of a partially-trainable implementation of the important “divide and conquer” paradigm': 0.9152611494064331, '++ Explicit reasoning about complexity of induced programs': 1.0752687454223633, 'The solution isn’t generic enough to be applicable to unknown problems - the networks require tricks specific to each problem': 1.0985444784164429, 'The writing style pictures the method as very general, but falls back on very low level details specific to each task': 1.0986073017120361, 'I find this paper extremely hard to read.': 1.098611831665039, 'The main promise of the paper is to train models for combinatorial search procedures, especially for dynamic programming to learn where to split and merge.': 1.0986123085021973, 'The present methodology is supposed to make use of some form of scale invariance property which is scarcely motivated for most problems this approach should be relevant for.': 1.0986123085021973, 'However, the general research direction is fruitful and important.': 1.02144455909729, 'The paper would be much more readable if it would start with a clear, formal problem formulation, followed by some schematic view on the overall flow and description on which parts are supervised, which parts are not.': 1.0986123085021973, 'Also a tabular form and sample of the various kinds problems solved by this method could be listed in the beginning as a motivation with some clear description on how they fit the central paradigm and motivate the rest of the paper in a more concrete manner.': 1.0986123085021973, 'Instead, the paper is quite chaotic, switching between low-level and high level details, problem formulations and their solutions in a somewhat random, hard to parse order.': 1.0985714197158813, 'Both split and merge phases seem to make a lot of discrete choices in a hierarchical manner during training.': 1.098610281944275, 'The paper does not explain how those discrete choices are backpropagated through the network in an unbiased manner, if that is the case at all.': 1.0986117124557495, 'In general, the direction this paper is exciting, but the paper itself is a frustrating read in its present form.': 1.0968984365463257, 'I have spent several hours on it without having to manage to achieve a clear mental image on how all the presented pieces fit together.': 1.0986123085021973, 'I would revise my score if the paper would be improved greatly from a readability perspective, but I think it would require a major rewrite.': 1.094999074935913}"
226,https://openreview.net/forum?id=Hy6b4Pqee,"{'The authors propose a new software package for probabilistic programming, taking advantage of recent successful tools used in the deep learning community.': 1.0986123085021973, 'The software looks very promising and has the potential to transform the way we work in the probabilistic modelling community, allowing us to perform rapid-prototyping to iterate through ideas quickly.': 1.0986123085021973, 'The composability principles are used insightfully, and the extension of inference to HMC for example, going beyond VI inference (which is simple to implement using existing deep learning tools), makes the software even more compelling.': 1.0986123085021973, 'However, the most important factor of any PPL is whether it is practical for real-world use cases.': 1.0986119508743286, 'This was not demonstrated sufficiently in the submission.': 1.0961748361587524, 'There are many example code snippets given in the paper, but most are not evaluated.': 1.0916378498077393, 'The Dirichlet process mixture model example (Figure 12) is an important one: do the proposed black-box inference tools really work for this snippet?': 1.0985718965530396, 'and will the GAN example (Figure 7) converge when optimised with real data?': 0.37633100152015686, 'To convince the community of the practicality of the package it will be necessary to demonstrate these empirically.': 1.098597764968872, 'Currently the only evaluated model is a VAE with various inference techniques, which are not difficult to implement using pure TF.': 1.0981658697128296, 'Presentation:': 1.0986123085021973, '* Paper presentation could be improved.': 1.0986123085021973, 'For example the authors could use more signalling for what is about to be explained.': 0.9300161004066467, 'On page 5 qbeta and qz are used without explanation - the authors could mention that an example will be given thereafter.': 1.0986123085021973, '* I would also suggest to the authors to explain in the preface how the layers are implemented, and how the KL is handled in VI for example.': 0.588386058807373, 'It will be useful to discuss what values are optimised and what values change as inference is performed (even before section 4.4).': 1.0986086130142212, 'This was not clear for the majority of the paper.': 1.096587061882019, 'Experiments:': 1.0986123085021973, '* Why is the run time not reported in table 1?': 1.0986123085021973, '* What are the ""difficulties around convergence"" encountered with the analytical entropies?': 1.0986123085021973, 'inference issues become more difficult to diagnose as inference is automated.': 1.0986123085021973, 'Are there tools to diagnose these with the provided toolbox?': 1.0986123085021973, '* Did HMC give sensible results in the experiment at the bottom of page 8?': 1.0986123085021973, 'only run time is reported.': 1.0986123085021973, ""* How difficult is it to get the inference to work (eg HMC) when we don't have full control over the computational graph structure and sampler?"": 1.0986123085021973, '* It would be extremely insightful to give a table comparing the performance (run time, predictive log likelihood, etc) of the various inference tools on more models.': 0.9373124241828918, '*': 1.0396772623062134, 'What benchmarks do you intend to use in the Model Zoo?': 1.0892447233200073, 'the difficulty with probabilistic modelling is that there are no set benchmarks over which we can evaluate and compare many models.': 0.4403257966041565, 'Model zoo is sensible for the Caffe ecosystem because there exist few benchmarks a large portion of the community was working on (ImageNet for example).': 0.837580680847168, 'What datasets would you use to compare the DPMM on for example?': 1.0314439535140991, 'Minor comments:': 1.0986123085021973, '* Table 1: I would suggest to compare to Li & Turner with alpha=0.5 (the equivalent of Hellinger distance) as they concluded this value performs best.': 0.4255538582801819, ""I'm not sure why alpha=-1 was chosen here."": 1.023390531539917, '* How do you handle discrete distributions (eg Figure 5)?': 1.049339771270752, '* x_real is not defined in Figure 7.': 0.6031797528266907, '* I would suggest highlighting M in Figure 8.': 1.0508674383163452, '* Comma instead of period after ""rized), In"" on page 8.': 0.9869574308395386, 'In conclusion I would say that the software developments presented here are quite exciting, and I\'m glad the authors are pushing towards practical and accessible ""inference for all"".': 1.0981842279434204, 'In its current form though I am forced to give the submission itself a score of 5.': 1.098464846611023, 'The paper introduces Edward, a probabilistic programming language': 1.0986123085021973, 'built over TensorFlow and Python, and supporting a broad range of most': 0.8168110847473145, 'popular contemporary methods in probabilistic machine learning.': 0.5385189652442932, 'Quality:': 1.0986123085021973, 'The Edward library provides an extremely impressive collection of': 1.0986123085021973, 'modern probabilistic inference methods in an easily usable form.': 1.0986121892929077, 'The paper provides a brief review of the most important techniques': 1.0985947847366333, 'especially from a representation learning perspective, combined with': 0.8593049645423889, 'two experiments on implementing various modern variational inference': 1.09861159324646, 'methods and GPU-accelerated HMC.': 1.027972936630249, 'The first experiment (variational inference) would be more valuable if': 1.0986123085021973, 'there was a clear link to complete code to reproduce the results': 1.0986123085021973, 'provided.': 1.0986123085021973, 'The HMC experiment looks OK, except the characterising Stan': 1.0986123085021973, 'as a hand-optimised implementation seems unfair as the code is clearly': 1.0986123085021973, 'not hand-optimised for this specific model and hardware configuration.': 1.0986123085021973, 'I do not think anyone doubts the quality of your implementation, so': 1.0986123085021973, 'please do not ruin the picture by unsubstantiated sensationalist': 0.41317862272262573, 'claims.': 1.098611831665039, 'Instead of current drama, I would suggest comparing': 1.0986123085021973, 'head-to-head against Stan on single core and separately reporting the': 0.4416986405849457, 'extra speedups you gain from parallelisation and GPU.': 0.9615749716758728, 'These numbers': 1.0986123085021973, 'would also help the readers to estimate the performance of the method': 0.43917572498321533, 'for other hardware configurations.': 1.0324064493179321, 'Clarity:': 0.8383582234382629, 'The paper is in general clearly written and easy to read.': 1.098570704460144, 'The numerous': 1.0986123085021973, 'code examples are helpful, but also difficult as it is sometimes': 0.8666359186172485, 'unclear what is missing.': 1.0923298597335815, 'It would be very helpful if the authors could': 1.0328792333602905, 'provide and clearly link to a machine-readable companion (a Jupyter': 0.08801356703042984, 'notebook would be great, but even text or HTML would be easier to': 0.551007091999054, 'copy-paste from than a pdf like the paper) with complete runnable code': 0.6396052241325378, 'for all the examples.': 1.09859299659729, 'Originality:': 1.0986120700836182, 'The Edward library is clearly a unique collection of probabilistic': 1.0863627195358276, 'inference methods.': 1.0984262228012085, 'In terms of the paper, the main threat to novelty': 1.0986121892929077, 'comes from previous publications of the same group.': 0.9111979603767395, 'The main paper': 1.0986123085021973, 'refers to Tran et al. (2016a) which covers a lot of similar material,': 0.2604130506515503, 'although from a different perspective.': 1.0591778755187988, 'It is unclear if the other': 1.0983953475952148, 'paper has been published or submitted somewhere and if so, where.': 0.6041009426116943, 'Significance:': 1.0986123085021973, 'It seems very likely Edward will have a profound impact on the field': 0.4130234122276306, 'of Bayesian machine learning and deep learning.': 1.0723447799682617, 'Other comments:': 1.098609209060669, 'In Sec. 2 you draw a clear distinction between specialised languages': 1.0986123085021973, '(including Stan) and Turing-complete languages such as Edward.': 1.0986123085021973, 'This': 1.0986123085021973, 'seems unfair as I believe Stan is also Turing complete.': 1.0986123085021973, 'Additionally': 1.0986123085021973, 'no proof is provided to support the Turing-completeness of Edward.': 1.0986123085021973, 'Thank you for an interesting read.': 1.0986123085021973, 'I found this paper very interesting.': 1.0986123085021973, ""Since I don't think (deterministic) approximate inference is separated from the modelling procedure (cf. exact inference), it is important to allow the users to select the inference method to suit their needs and constraints."": 1.0986123085021973, ""I'm not an expert of PPL, but to my knowledge this is the first package that I've seen which put more focus on compositional inference."": 1.0986123085021973, 'Leveraging tensorflow is also a plus, which allows flexible computation graph design as well as parallel computation using GPUs.': 1.0935285091400146, 'The only question I have is about the design of flexible objective functions to learn hyper-parameters (or in the paper those variables associated with delta q distributions).': 1.0986123085021973, 'It seems hyper-parameter learning is also specified as inference, which makes sense if using MAP.': 1.0986123085021973, 'However the authors also demonstrated other objective functions such as Renyi divergences, does that mean the user need to define a new class of inference method whenever they want to test an alternative loss function?': 1.0986123085021973}"
227,https://openreview.net/forum?id=Hy8X3aKee,"{'The paper compare three representation learning algorithms over symbolized sequences.': 0.9590718150138855, 'Experiments are executed on several prediction tasks.': 1.0978878736495972, 'The approach is potentially very important but the proposed algorithm is rather trivial.': 0.7681804895401001, 'Besides detailed analysis on hyper parameters are not described.': 1.0945942401885986, 'Because the authors provided no further responses to reviewer feedback, I maintained my original review score.': 1.0986123085021973, 'This paper takes a unique approach to the modeling of heterogeneous sequence data.': 1.0986123085021973, 'They first symbolize continuous inputs using a previously described approach (histograms or maximum entropy), the result being a multichannel discrete sequence (of symbolized time series or originally categorical data) of ""characters.""': 1.083176851272583, 'They then investigate three different approaches to learning an embedding of the characters at each time step (which can be thought of as a ""word""):': 1.0949690341949463, '1) Concatenate characters into a ""word"" and then apply standard lookup-based embeddings from language modeling (WDE)': 1.0269218683242798, '2) Embed each character independently and then sum over the embeddings (SCE)': 0.9558728933334351, '3) Embed each character as a scalar and concatenate the scalar embeddings (ICE)': 1.0882354974746704, 'The resulting embeddings can be used as inputs to any architecture, e.g., LSTM.': 1.0986119508743286, 'The paper applies these methods primarily to event detection tasks, such as hard drive failures and seizures in EEG data.': 1.0986087322235107, 'Empirical results largely suggest the a recurrent model combined with symbolization/embedding outperforms a comparable recurrent model applied to raw data.': 0.9198342561721802, 'Results are inconclusive as to which embedding layer works best.': 1.0121756792068481, 'Strengths:': 1.0697417259216309, 'The different embedding approaches, while simple, are designed to tackle a very interesting problem where the input consists of multivariate discrete sequences, which makes it different from standard language modeling and related domains.': 1.0986123085021973, 'The proposed approaches offer several different interesting perspectives on how to approach this problem.': 1.0985667705535889, 'The empirical results suggest that symbolizing the continuous input space can improve results for some problems.': 1.098171591758728, 'This is an interesting possibility as it enables the direct application of a variety of language modeling tools (e.g., embeddings).': 1.098596215248108, 'Weaknesses:': 1.098572015762329, 'The LSTMs (one layer each of 8, 16, and 15 cells, respectively) used in the three experiments sound *very* under capacity given the complexity of the tasks and the sizes of the data sets (tens to hundreds of thousands of sequences).': 1.0986119508743286, 'That might explain both the relatively small gap between the LSTMs and logistic regression *and* the improvement of the embedding-based LSTMs.': 1.0983155965805054, 'Hypothetically, if quantizing the inputs is really useful, the raw data LSTMs should be able to learn this transformation, but if they are under capacity, they might not be able to dos.': 0.4091757833957672, 'What is more, using the same architecture (# layers, # units, etc.) for very different kinds of inputs (raw, WdE, SCE, ICE, hand-engineered features) is poor methodology.': 1.0967144966125488, 'Obviously, hyperparameters should be tuned independently for each type of input.': 0.4196673035621643, 'The experiments omit obvious baselines, such as trying to directly learn an embedding of the continuous inputs.': 0.9643359184265137, 'The experimental results offer an incomplete, mixed conclusion.': 1.0986084938049316, 'First, no one embedding approach performs best across all tasks and metrics, and the authors offer no insights into why this might be.': 1.0985747575759888, 'Second, the current set of experiments are not sufficiently thorough to conclude that quantization and embedding is superior to working with the raw data.': 1.0972504615783691, 'The temporal weighting section appears out of place: it is unrelated to the core of the paper (quantizing and embedding continuous inputs), and there are no experiments to demonstrate its impact on performance.': 1.0968124866485596, ""The paper omits a large number of related works: anything by Eamonn Keogh's lab (e.g., Symbolic Aggregate approXimation or SAX), work on modifying loss functions for RNN classifiers (Dai and Le."": 0.3977503180503845, 'Semi-supervised sequence learning.': 1.0986123085021973, 'NIPS 2015; Lipton and Kale, et al.': 1.0986123085021973, 'Learning to Diagnose with LSTM Recurrent Neural Networks.': 1.0986123085021973, 'ICLR 2016), work on embedding non-traditional discrete sequential inputs (Choi, et al.': 1.0986123085021973, 'Multi-layer Representation Learning for Medical Concepts.': 1.0302413702011108, 'KDD 2016).': 0.4639725983142853, 'This is an interesting direction for research on time series modeling with neural nets, and the current work is a good first step.': 0.671350359916687, 'The authors need to perform more thorough experiments to test their hypotheses (i.e., that embedding helps performance).': 1.098314881324768, 'My intuition is that a continuous embedding layer + proper hyperparameter tuning will work just as well.': 1.0929431915283203, 'If quantization proves to be beneficial, then I encourage them to pursue some direction that eliminates the need for ad hoc quantization, perhaps some kind of differentiable clustering layer?': 0.40858033299446106, ""In absence of authors' responses, the rating is maintained."": 1.0986120700836182, 'This paper introduces an approach for learning predictive time series models that can handle heterogenous multivariate sequence.': 1.0986123085021973, 'The first step is in three possible ways to perform embedding of the d-dimensional sequences into d-character words, or a sum of d character embeddings, or a concatenation of d character embeddings.': 1.0985976457595825, 'The embedding layer is the first layer of a deep architecture such as LSTM.': 1.098597526550293, 'The models are then trained to perform event prediction at a fixed horizon, with temporal weighting, and applied to hard disk or heating system failures or seizures.': 1.0986123085021973, 'The approach is interesting and the results seem to outperform an LSTM baseline, but need additional clarification.': 1.0986121892929077, 'The experimental section on seizure prediction is very short and would need to be considerably extended, in an appendix.': 1.0985994338989258, 'What are the results obtained using LSTM vs. RNN?': 1.096872329711914, 'What is the state-of-the-art on that dataset?': 1.0986123085021973, 'Given that EEG data contain mostly frequential information, how is this properly handled in per-sample embeddings?': 1.0986123085021973, 'Please also extend your reference and previous work section to include PixelRNN as well as:': 1.0986123085021973, '* van den Oord, et al. (2016)': 0.16094475984573364, 'WaveNet: A Generative Model for Raw Audio': 0.4065905809402466, 'arXiv 1609.03499': 1.0958802700042725, '* Huang et al. (2013)': 1.0986123085021973, '""Learning deep structured semantic models for web search using clickthrough data""': 0.4490511417388916, 'CIKM': 1.0986123085021973, 'In the latter paper, the authors embedded 3-gram hashes of the input sequences (e.g., text), which is somewhat similar to a time-delay embedding of the input sequence.': 1.0984785556793213}"
228,https://openreview.net/forum?id=HyAbMKwxe,"{'The paper analyses the misclassification error of discriminators and highlights the fact that while uniform probability prior of the classes makes sense early in the optimization, the distribution deviates from this prior significantly as the parameters move away from the initial values.': 1.0979243516921997, 'Consequently, the optimized upper bound (log-loss) gets looser.': 1.0986123085021973, 'As a fix, an optimization procedure based on recomputing the bound is proposed.': 1.0986123085021973, 'The paper is well written.': 1.0986123085021973, 'While the main observation made in this paper is a well-known fact, it is presented in a clear and refreshing way that may make it useful to a wide audience at this venue.': 1.09861159324646, ""I would like to draw the author's attention to the close connections of this framework with curriculum learning."": 1.0986123085021973, 'More on this can be found in [1] (which is a relevant reference that should be cited).': 1.0986123085021973, 'A discussion on this could enrich the quality of the paper.': 1.0986123085021973, 'There is a large body of work on directly optimizing task losses[2][3] and the references therein.': 1.0986123085021973, 'These should also be discussed and related particularly to section 3 (optimizing the ROC curve).': 1.0986123085021973, '[1] Training Highly Multiclass Classifiers, Gupta et al. 2014.': 1.0986123085021973, '[2] Direct Loss Minimization for Structured Prediction, McAllester et al.': 1.0986123085021973, '[3] Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss, McAllester and Keshet.': 1.0986123085021973, 'Final comment:': 1.0986123085021973, 'I believe the material presented in this paper is of interest to a wide audience at ICLR.': 1.0986123085021973, 'The problem studied is interesting and the proposed approach is sound.': 1.0986123085021973, 'I recommend to accept the paper and increase my score (from 7 to 8).': 1.0986123085021973, 'The paper proposes new bounds on the misclassification error.': 1.0986123085021973, 'The bounds lead to training classifiers with an adaptive loss function, and the algorithm operates in successive steps: the parameters are trained by minimizing the log-loss weighted by the probability of the observed class as given by the parameters of the previous steps.': 1.098611831665039, 'The bound improves on standard log-likelihood when outliers/underfitting prevents the learning algorithm to properly optimize the true classification error.': 1.0986123085021973, 'Experiments are performed to confirm the therotical intuition and motivation.': 1.0962342023849487, 'They show different cases where the new algorithm leads to improved classification error because underfitting occurs when using standard log-loss, and other cases where the new bounds do not lead to any improvement because the log-loss is sufficient to fit the dataset.': 1.0986123085021973, 'The paper also discusses the relationship between the proposed idea and reinforcement learning, as well as with classifiers that have an ""uncertain"" label.': 1.0853549242019653, 'While the paper is easy to read and well-written overall, in a second read I found it difficult to fully understand because two problems are somewhat mixed together (here considering only binary classification for simplicity):': 1.0986123085021973, '(a) the optimization of the classification error of a *randomized* classifier, which predicts 1 with probability P(1|x, theta), and': 0.48231732845306396, '(b) the optimization of the deterministic classifier, which predicts sign(P(1|x, theta) - 0.5), in a way that is robust to outliers/underfitting.': 0.39549165964126587, 'The reason why I am confused is that ""The standard approach to supervised classification"", as is mentioned in the abstract, is to use deterministic classifiers at test time, and the log-loss (up to constants) is an upper bound on the classification error of the deterministic classifier.': 1.0978479385375977, 'However, the bounds discussed in the paper only concern the randomized classifier.': 0.6889210939407349, '=== question:': 1.0986123085021973, 'In the experiments, what kind of classifier is used?': 1.0966441631317139, 'The randomized one (as would the sentence in the first page suggest ""Assuming the class is chosen according to p(y|X, θ)""), or the more standard deterministic classifier argmax_y P(y|x, theta) ?': 1.0986123085021973, 'As far as I can see, there are two cases: either (i) the paper deals with learning randomized classifiers, in which case it should compare the performances with the deterministic counterparts that people use in practice, or (ii) the paper makes sense as soon as we accept that the optimization of criterion (a) is a good surrogate for (b).': 0.8482294678688049, 'In both cases,  I think the write-up should be made clearer (because in case (ii) the algorithm does not minimize an upper bound on the classification error, and in case (i) what is done does not correspond to what is usually done in binary classification).': 1.0986123085021973, '=== comments:': 1.0986123085021973, 'The section ""allowing uncertainty in the decision"" may be improved by adding some references, e.g. Bartlett & Wegkamp (2008)': 1.0986123085021973, '""Classification with a Reject Option using a Hinge Loss"" or Sayedi et al. (2010) ""Trading off Mistakes and Don’t Know Predictions"".': 1.0986123085021973, 'there seems to be a ""-"" sign missing in the P(1|x, theta) in L(theta, lambda) in Section 3.': 1.0986123085021973, 'The idea presented in the paper is interesting and original.': 1.0986123085021973, 'While I give a relatively low score for now, I am willing to increase this score if the clarifications are made.': 1.0986123085021973, 'Final comments:': 1.0986123085021973, 'I think the paper is clear enough in its current form, even though there should still be improvement in the justification of why and to what extent the error of the randomized classifier is a good surrogate for the error of the true classifier.': 1.0986123085021973, 'While the ""smoothed"" version of the 0/1 loss is an acceptable explanation in the standard classification setup, it is less clear in the section dealing with an additional ""uncertain"" label.': 1.0986123085021973, 'I increase my score from 5 to 6.': 1.0986123085021973, 'The paper proposes an alternative to conditional max.': 1.0986123085021973, 'log likelihood for training discriminative classifiers.': 1.0986123085021973, 'The argument is that the conditional log.': 1.0986123085021973, 'likelihood is an upper bound of the Bayes error which becomes lousy during training.': 1.0986123085021973, 'The paper then proposes better bounds computed and optimized in an iterative algorithm.': 1.0986123085021973, 'Extensions of this idea are developed for regularized losses and a weak form of policy learning.': 1.0986123085021973, 'Tests are performed on different datasets.': 1.0986123085021973, 'An interesting aspect of the contribution is to revisit a well-accepted methodology for training classifiers.': 1.0986123085021973, 'The idea looks fine and some of the results seem to validate it.': 1.0986123085021973, 'This is however still a preliminary work and one would like to see the ideas pushed further.': 1.0986123085021973, 'Globally, the paper lacks coherence and depth: the part on policy learning is not well connected to the rest of the paper and the link with RL is not motivated in the two examples (ROC optimization and uncertainties).': 1.0986121892929077, 'The experimental part needs a rewriting, e.g. I did not find a legend for identifying the different curves in the figures, which makes difficult to appreciate the results.': 1.0986123085021973}"
229,https://openreview.net/forum?id=HyAddcLge,"{'This paper was easy to read, the main idea was presented very clearly.': 0.7083396315574646, 'The main points of the paper (and my concerns are below) can be summarized as follows:': 1.0674326419830322, '1. synchronous algoriths suffer from some struggeling nodes, for which the algorithm has to wait. From my own experience, this has never happend for me on e.g. Amazon EC2 cloud, however, it happens on our own cluster at my university, if the cluster is shared and some users make some nodes very busy. So maybe if the nodes would be dedicated to just user\'s job, it wouldn\'t be such a big concer (I am not sure what kind of cluster was used to produce Figure 3 and 4). Also how many experiments have you run? In my own experience, most of the time I get the gradient on time from all nodes equality fast, but maybe just in less than 0.1% of iterations I observe that it took maybe twice as long for some node. Also the increasing shape of the curve is somehow implying some weird implementation of communication. Isn\'t it only because you are somehow serialize the communication? And it would be maybe much faster if a ""MPI_Reduce"" would be used (even if we wait for the slowest guy)?': 1.098611831665039, '2. asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. Moreover, those algorithms can be divergence it special care is not given to stale gradients. Also they have a nice guarantees for convex functions, but the non-convex DNN may cause pain.': 1.0986123085021973, '3.they propose to take gradient from the first ""N"" workers out of ""N+b""': 1.0986123085021973, 'workers available.': 1.0986123085021973, 'My concern here is that they focused only on the': 1.0986123085021973, 'workers, but what if the ""parameter server"" will became to slow?': 1.0986123085021973, 'What': 1.0986123085021973, 'if the parameter server would be the bottleneck?': 1.0977767705917358, 'How would you address': 1.097599744796753, 'this situation?': 1.0985418558120728, 'But still if the number of nodes (N) is not large, and': 1.046885371208191, 'the deep DNN is used, I can imagine that the communciation will not': 1.0980415344238281, 'take more than 30% of the run-time.': 1.0742087364196777, 'My largest concern is with the experiments.': 1.0377895832061768, 'Different batch size': 1.0986123085021973, 'implies that different learning rate should be chosen, right?': 1.0984433889389038, 'How did': 1.098569393157959, 'you tune the learning rates and other parameters for e.g. Figure 5 you': 1.0982955694198608, 'provide some formulas in (A2) but clearly this can bias your Figures,': 1.0985732078552246, 'right?': 1.0986123085021973, 'meaning, that if you tune ""\\gamma, \\beta"" for each N,': 0.8527576327323914, 'it could': 1.0986123085021973, 'be somehow more representative?': 1.0986117124557495, 'also it would be nicer if you run the': 1.0966256856918335, 'experiment many times and then report average, best and worst case': 0.41899609565734863, 'behaviour.': 1.09860098361969, 'because now it can be just coinsidence, right?': 0.601677656173706, 'This paper proposed a synchronous parallel SGD by employing several backup machines.': 1.0986123085021973, 'The parameter server does not have to wait for the return from all machines to perform the update on the model, which reduce the synchronization overhead.': 1.0986123085021973, 'It sounds like a reasonable and straightforward idea.': 1.098611831665039, 'My main concern is that this approach is only suitable for some very specific scenario, that is, most learners (except a small number of learners) are at the same pace to return the results.': 1.0986123085021973, 'If the efficiency of learners does not follow such distribution, I do not think that the proposed algorithm will work.': 1.0986123085021973, 'So I suggest two revisions:': 1.08635413646698, 'provide more experiments to show the performance with different efficiency distributions of learners.': 1.0985835790634155, 'assuming that all learners follow the same distribution of efficiency and show the expected idle time is minor by using the proposed algorithm.': 1.0986123085021973, 'The paper claim that, when supported by a number of backup workers, synchronized-SGD': 1.0986123085021973, 'actually works better than async-SGD.': 1.0986123085021973, 'The paper first analyze the problem of staled updates': 1.0986123085021973, 'in async-SGDs, and proposed the sync-SGD with backup workers.': 1.0986123085021973, 'In the experiments, the': 1.0986123085021973, 'authors shows the effectiveness of the proposed method in applications to Inception Net': 1.0986123085021973, 'and PixelCNN.': 1.0986123085021973, 'The idea is very simple, but in practice it can be quite useful in industry settings where': 1.0986123085021973, 'adding some backup workders is not a big problem in cost.': 1.0986123085021973, 'Nevertheless, I think the': 1.0986123085021973, 'proposed solution is quite straightforward to come up with when we assume that': 1.0986123085021973, 'each worker contains the full dataset and we have budge to add more workers.': 1.0986123085021973, 'So,': 1.0986123085021973, 'under this setting, it seems quite natural to have a better performance with the additional': 1.0986123085021973, 'backup workers that avoid the staggering worker problem.': 1.0986123085021973, ""And, with this assumtion I'm not"": 1.0986123085021973, 'sure if the proposed solution is solving difficult enough problem with novel enough idea.': 1.0986123085021973, 'In the experiments, for fair comparison, I think the Async-SGD should also have a mechanism': 1.0986123085021973, 'to cut off updates of too much staledness just as the proposed method ignores all the remaining': 1.0986123085021973, 'updates after having N updates.': 1.0986123085021973, 'For example, one can measure the average time spent to': 1.0986123085021973, 'obtain N updates in sync-SGD setting and use that time as the cut-off threashold in Async-SGD': 1.0986123085021973, 'so that Async-SGD does not perform so poorly.': 1.0986123085021973}"
230,https://openreview.net/forum?id=HyCRyS9gx,"{'This paper presents a meta-learning algorithm which learns to learn generative models from a small set of examples.': 1.0986108779907227, 'It’s similar in structure to the matching networks of Vinyals et al. (2016), and is trained in a meta-learning framework where the inputs correspond to datasets.': 1.0960999727249146, 'Results are shown on Omniglot in terms of log-likelihoods and in terms of generated samples.': 1.0985304117202759, 'The proposed idea seems reasonable, but I’m struggling to understand various aspects of the paper.': 1.0985901355743408, 'The exposition is hard to follow, partly because existing methods are described using terminology fairly different from that of the original authors.': 1.0986120700836182, 'Most importantly, I can’t tell which aspects are meant to be novel, since there are only a few sentences devoted to matching networks, even though this work builds closely upon them.': 1.098611831665039, '(I brought this up in my Reviewer Question, and the paper has not been revised to make this clearer.)': 1.0983262062072754, 'I’m also confused about the meta-learning setup.': 1.0986123085021973, 'One natural formulation for meta-learning of generative models would be that the inputs consist of small datasets X, and the task is to predict the distribution from which X was sampled.': 1.0986123085021973, 'But this would imply a uniform weighting of data points, which is different from the proposed method.': 1.0979433059692383, 'Based on 3.1, it seems like one additionally has some sort of query q, but it’s not clear what this represents.': 1.0986123085021973, 'In terms of experimental validation, there aren’t any comparisons against prior work.': 1.0984134674072266, 'This seems necessary, since several other methods have already been proposed which are similar in spirit.': 0.7816410064697266, 'This paper proposes an interesting idea for rapidly adapting generative models in the low data regime.': 1.0986123085021973, 'The idea is to use similar techniques that are used in one-shot learning, specifically ideas from matching networks.': 1.0986123085021973, 'To that end, the authors propose the generative matching networks model, which is effectively a variational auto-encoder that can be conditioned on an input dataset.': 1.0986123085021973, 'Given a query point, the model matches the query point to points in the conditioning set using an attention model in an embedding space (this is similar to matching networks).': 1.0986123085021973, 'The results on the Omniglot dataset show that this method is successfully able to rapidly adapt to new input distributions given few examples.': 1.0986123085021973, 'I think that the method is very interesting, however the major issue for me with this paper is a lack of clarity.': 1.0986113548278809, 'I outline more details below, but overall I found the paper somewhat difficult to follow.': 1.0986119508743286, 'There are a lot of details that I feel are scattered throughout, and I did not get a sense after reading this paper that I would be able to implement the method and replicate the results.': 1.0986123085021973, 'My suggestion is to consolidate the major implementation details into a single section, and be explicit about the functional form of the different embedding functions and their variants.': 1.0986123085021973, 'I was a bit disappointed to see that weak supervision in the form of labels had to be used.': 1.0986123085021973, 'How does the method perform in a completely unsupervised setting?': 1.098604679107666, 'This could be an interesting baseline.': 1.0963906049728394, 'There is a lack of definition of the different functions.': 1.0857863426208496, 'Some basic insight into the functional forms of f, g, \\phi, sim and R would be nice.': 1.0985945463180542, 'Otherwise it is very unclear to me what’s going on.': 1.0978516340255737, 'Section 3.2: “only state of the recurrent controller was used for matching”, my reading of this section (after several passes) is that the pseudo-input is used in the place of a regular input.': 1.0986123085021973, 'Is this correct?': 1.0986120700836182, 'Otherwise, this sentence/section needs more clarification.': 0.9395697116851807, 'I noticed upon further reading in section 4.2 that there are two versions of the model: one in which a pseudo input is used, and one in which a pseudo input is not used (the conditional version).': 0.6132422685623169, 'What is the difference in functional form between these?': 1.0986123085021973, 'That is, how do the formulas for the embeddings f and g change between these settings?': 1.0986123085021973, '“since the result was fully contrastive we did not apply any further binarization” what does it mean for a result to be fully contrastive?': 1.0943443775177002, 'For clarity, the figures and table refer to the number of shots, but this is never defined.': 1.0986123085021973, 'I assume this is T here.': 1.0986123085021973, 'This should be made consistent.': 1.0986123085021973, 'Figure 2: why is the value of T only 9 in this case?': 1.0986123085021973, 'What does it mean for it to be 0?': 1.0986123085021973, 'It is stated earlier that T should go up to 20 (I assume #shot corresponds to T).': 1.0986123085021973, 'It also looks like the results continue to improve with an increased number of steps, I would like to see the results for 5 and maybe 6 steps as well.': 1.0986123085021973, 'Presumably there will come a point where you get diminishing returns.': 1.0986123085021973, 'Table 1: is the VAE a fair baseline?': 1.0986123085021973, 'You mention that Ctest affects Pd() in the evaluation.': 1.0986123085021973, 'The fact that the VAE does not have an associated Ctest implies that the two models are being evaluated with a different metric.': 1.0986111164093018, 'Can the authors clarify this?': 1.0986123085021973, 'It’s important that the comparison is apples-to-apples.': 1.0986123085021973, 'MNIST is much more common than Omniglot for evaluating generative models.': 1.0986123085021973, 'Would it be possible to perform similar experiments on this dataset?': 1.0986123085021973, 'That way it can be compared with many more models.': 1.0986123085021973, 'Further, why are the negative log-likelihood values monotonically decreasing in the number of shots?': 1.0986123085021973, 'That is, is there ever a case where increasing the number of shots can hurt things?': 1.0986123085021973, 'What happens at T=30?': 1.0986123085021973, '40?': 1.0986123085021973, 'As a minor grammatical issue, the paper is missing determiners in several sentences.': 1.0986123085021973, 'At one point, the model is referred to as “she” instead of “it”.': 1.0986123085021973, '“On figure 3” should be changed to “in figure 3” in the experiments section.': 1.0986123085021973, 'The paper explores a VAE architecture and training procedure that allows to generate new samples of a concept based on several exemplars that are shown to the model.': 1.0960618257522583, 'The proposed architecture processes the set of exemplars with a recurrent neural network and aggregation procedure similar to the one used in Matching Networks.': 1.0986123085021973, 'The resulting ""summary"" is used to condition a generative model (a VAE) that produces new samples of the same kind as the exemplars shown.': 1.093445062637329, 'The proposed aggregation and conditioning procedure are better suited to sets of exemplars that come from several classes than simple averaging.': 1.0940511226654053, 'Perhaps surprisingly the model generalizes from generation conditioned on samples from 2 classes to generation conditioned on samples from 4 classes.': 1.0986123085021973, 'The experiments are conducted on the OMNIGLOT dataset and are quite convincing.': 1.0986123085021973, 'An explicit comparison to previous works is lacking, but this is explained in the appendices, and a comparison to architectures similar to previous work is presented.': 1.0986123085021973}"
231,https://openreview.net/forum?id=HyET6tYex,"{'The authors explore whether the halting time distributions for various algorithms in various settings exhibit ""universality"", i.e. after rescaling to zero mean and unit variance, the distribution does not depend on stopping parameter, dimensionality and ensemble.': 1.0984172821044922, 'The idea of the described universality is very interesting.': 1.097406268119812, 'However I see several shortcomings in the paper:': 1.0888277292251587, 'In order to be of practical relevance, the actual stopping time might be more relevant than the scaled one.': 1.0986119508743286, 'The discussion of exponential tailed halting time distributions is a good start, but I am not sure how often this might be actually helpful.': 1.0986123085021973, 'Still, the findings in the paper might be interesting from a theoretical point of view.': 1.0984238386154175, 'Especially for ICLR, I think it would have been more interesting to look into comparisons between stochastic gradient descent, momentum, ADAM etc on different deep learning architectures.': 1.0986074209213257, 'Over which of those parameters does universality hold?.': 1.0979928970336914, 'How can different initializations influence the halting time distribution?': 1.0986123085021973, 'I would expect a sensible initialization to cut of part of the right tail of the distribution.': 1.0981597900390625, 'Additionally, I found the paper quite hard to read.': 1.0986108779907227, 'Here are some clarity issues:': 1.0986117124557495, 'abstract: ""even when the input is changed drastically"": From the abstract I\'m not sure what ""input"" refers to, here': 0.8080160617828369, 'I. Introduction: ""where the stopping condition is, essentially, the time to find the minimum"": this doesn\'t seem to make sense, a condition is not a time.': 1.0986121892929077, 'I guess the authors wanted to say that the stopping condition is that the minimum has been reached?': 1.0985164642333984, 'I.1 the notions of dimension N, epsilon and ensemble E are introduced without any clarification what they are.': 1.0985605716705322, 'From the later parts of the paper I got some ideas and examples, but here it is very hard to understand what these parameters should be (just some examples would be already helpful)': 1.0986123085021973, 'I.3 ""We use x^\\ell for \\ell \\in Z=\\{1, \\dots, S\\} where Z is a random sample from of training samples"" This formulation doesn\'t make sense.': 1.098606824874878, 'Either Z is a random sample, or Z={1, ..., S}.': 0.9800680875778198, 'II.1 it took me a long time to find the meaning of M. As this parameter seems to be crucial for universality in this case, it would be very helpful to point out more explicitly what it refers to.': 1.0352269411087036, 'Summary': 1.0986123085021973, 'For several algorithms, previous research has shown that the halting time follows a two-parameter distribution (the so-called universal property investigated by the authors).': 0.8826379776000977, 'In this work, the authors extend the investigation to new algorithms (spin-glass, gradient descent in deep learning).': 1.0851695537567139, 'An algorithm is considered to satisfy the universality property when the centered/scaled halting time fluctuations (empirical distribution of halting times) depend on the algorithm but do not depend on the target accuracy epsilon, an intrinsic measure of dimension N, the probability distribution/random ensemble.': 0.8607789278030396, '(This is clear from Eq 1 where on the left the empirical halting time distribution depends on epsilon, N, A, E and on the right, the approximation only depends on the algorithm)': 0.46103259921073914, 'The authors argue that empirically, the universal property is observed when both algorithms (spin glass and deep learning) perform well and that it is not observed when they do not perform well.': 0.8943068385124207, 'A moment-based indicator is introduced to assess whether universality is observed.': 1.0417903661727905, 'Review': 1.0978273153305054, 'This paper presents several problems.': 0.699226975440979, 'page 2: “[…] for sufficiently large N and eps = eps(N)”': 0.6115740537643433, 'The dependence of epsilon on N is troubling.': 1.0814932584762573, 'page 3: “Universality is a measure of stability in an algorithm […] For example […] halting time for the power method […] has infinite expectation and hence this type of universality is *not* present.': 0.5723456144332886, 'One could use this to conclude that the power method is naive.': 0.849306583404541, 'Therefore the presence of universality is a desirable feature of a numerical method”': 1.0514088869094849, 'No.': 1.0985983610153198, 'An algorithm is naive if there are better ways to answer the problem.': 1.0974200963974, 'One could not conclude from a halting time with infinite expectation (e.g. solving a problem extremely quickly 99% of the time, and looping forever in 1% of cases) or infinite variance, that the algorithm is naive.': 1.098444938659668, 'Moreover the universal property is more restrictive than having a finite halting time expectation.': 1.0701602697372437, 'Even if in many specific cases, having a finite halting time expectation is a desirable property, showing that the presence of universality is desirable would require a demonstration that the other more restrictive aspects are also desirable.': 1.0986123085021973, 'Also, the paragraph only concerns one algorithm.': 1.0985033512115479, 'why would the conclusions generalise to all numerical methods ?': 0.26647987961769104, 'Even if the universality property is arguably desirable (i.e. event if the conclusion of this paragraph is assumed correct), the paragraph does not support the given conclusion.': 1.0986123085021973, 'Comparing Eq 1 and figures 2,3,4,5': 0.6777417659759521, 'From Eq 1, universality means that the centered/scaled halting time fluctuations (which depend on A, epsilon, N, E) can be approximated by a distribution that only depends on A (not on epsilon, N, E) but in the experiments only E varies (figures 2,3,4,5).': 1.0962507724761963, 'The validity of the approximation with varying epsilon or N is never tested': 1.0982822179794312, 'The ensembles/distributions parameter E (on which halting fluctuations should not depend) and the algorithm A (on which halting fluctuations are allowed to depend) are not well defined, especially w.r.t.': 1.087050437927246, 'the common use of the words.': 1.0986121892929077, 'In the optimisation setting we are told that the functional form of the landscape function is part of A (in answer to the question of a reviewer)': 1.0985406637191772, 'but what is part of the functional form ?': 0.6604682207107544, 'what about computations where the landscape has no known functional form (black box) ?': 1.0978221893310547, 'The conclusion claims that the paper “attempts to exhibit cases” where one can answer 5 questions in a robust and quantitative way.': 1.0921523571014404, 'Question 1: “What are the conditions on the ensembles and the model that lead to such universality ?”': 1.0986114740371704, 'The only quantitative way would be to use the moments based indicator': 0.40562257170677185, 'however there is only one example of universality not being observed which concerns only one algorithm (conjugate gradient) and one type of failure (when M = N).': 1.0922695398330688, 'This does not demonstrate robustness of the method.': 1.0986089706420898, 'Question 2: “What constitutes a good set of hyper parameters for a given algorithm ?”': 1.0986123085021973, 'The proposed way to choose would be to test whether universality is observed.': 1.097575068473816, 'If it is then the hyper parameters are good, if not the hyper parameters are bad.': 1.0983211994171143, 'The correspondance between bad hyper-parameters and observing no universality concerns only one algorithm and one type of failure.': 0.7234706282615662, 'Other algorithms may fail in the universal regime or perform well in the non universal regime.': 0.7394658923149109, 'The paper does not show how to answer this question in a robust way.': 1.0507947206497192, 'Question 3: ""How can we go beyond inspection when tuning a system ? ""': 1.0986123085021973, 'The question is too vague and general and there is probably no robust and quantitative way to answer it at all.': 0.931154727935791, 'Question 4: ""How can we infer if an algorithm is a good match to the system at hand ? ""': 1.0986123085021973, 'The paper fails to demonstrate convincingly that universality is either a good or robust way to approach the very few studied algorithms.': 1.0580867528915405, 'The suggested generalisation to all systems and algorithms is extremely far fetched.': 0.5789751410484314, 'Question 5: ""What is the connection between the universal regime and the structure of the landscape ?""': 1.0986084938049316, 'Same as before, the question is extremely vague and cannot be answered in a robust or quantitative way at all.': 0.8011151552200317, 'The fact that what corresponds to A and what corresponds to E is not clear does not help.': 0.7572460770606995, 'In the conclusion it is written that the paper validates the claim that universality is present in all or nearly all sensible computation.': 1.0978000164031982, 'It does not.': 0.46892866492271423, 'The paper does not properly test whether universality is present (only 1 parameter in 3 that should not vary is tested).': 1.0966315269470215, 'The paper does not properly test whether universality is lost when the computation is no longer sensible (only one failure case tested).': 1.0984481573104858, 'Finally the experiments do not apply to all or nearly all computations but only to very few  specific algorithms.': 1.0938388109207153, 'The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one.': 1.098611831665039, 'However, as an empirical study, this paper comes up somewhat short:': 1.0986123085021973, '1. Exactly one algorithm is shown for the deep learning example. It would have been more convincing to compare distributions with one or more algorithms.': 1.097653865814209, '2. The definition (1),  and much of the work of Section 2.1 seems to have already been covered in Deift (2014), Section 1.3. In that paper, a number of different algorithms for the solution of linear systems are considered, and then the concept of universality becomes more plausible. I do not see enough of such algorithmic comparisons in this paper (same problem setup, different algorithms).': 1.0891016721725464, '3.  It seems to me that what practitioners might care about in practice are both the mean and variance in running times; these quantities are buried in (1). So I question how useful the distribution itself might be for algorithm tuning.': 1.0501738786697388, 'At the least, many more empirical comparisons should be provided to convince me that the universality holds across a broad range of algorithms.': 1.098609447479248}"
232,https://openreview.net/forum?id=HyEeMu_xx,"{'This paper proposes an attention mechanism which is essentially a gating on every spatial feature.': 1.0986123085021973, 'Though they claim novelty through the attention being progressive, progressive attention has been done before [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections], and the element-wise multiplicative gates are very similar to convolutional LSTMs and Highway Nets.': 1.0986123085021973, 'There is a lack of novelty and no significant results.': 1.0986123085021973, 'Pros:': 1.0986123085021973, 'The idea of progressive attention on features is good, but has been done in [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections]': 1.0986123085021973, 'Good visualisations.': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'No progressive baselines were evaluated, e.g. STN and HAN at every layer acting on featuremaps.': 1.0986123085021973, 'Not clear how the query is fed into the localisation networks of baselines.': 1.0986123085021973, 'The difference in performance between author-made synthetic data and the Visual Genome datasets between baselines and PAN is very different.': 1.0986123085021973, 'Why is this?': 1.0986123085021973, 'There is no significant performance gain on any standard datasets.': 1.0986123085021973, 'No real novelty.': 1.0986123085021973, 'This paper presents a hierarchical attention model that uses multiple stacked layers of soft attention in a convnet.': 1.0986123085021973, 'The authors provide results on a synthetic dataset in addition to doing attribute prediction on the Visual Genome dataset.': 1.0986123085021973, 'Overall I think this is a well executed paper, with good experimental results and nice qualitative visualizations.': 1.0986123085021973, 'The main thing I believe it is missing would be experiments on a dataset like VQA which would help better place the significance of this work in context of other approaches.': 1.0986089706420898, 'An important missing citation is Graves 2013 which had an early version of the attention model.': 1.0986123085021973, 'Minor typo:': 1.0986123085021973, '""It confins possible attributes.."" ->': 1.0986123085021973, 'It confines..': 1.0986123085021973, '""ImageNet (Deng et al., 2009), is used, and three additional"" -> "".., are used,""': 1.0986123085021973, 'The paper presents an architecture to incrementally attend to image regions - at multiple layers of a deep CNN.': 1.0986123085021973, 'In contrast to most other models, the model does not apply a weighted average pooling in the earlier layers of the network but only in the last layer.': 1.0986117124557495, 'Instead, the features are reweighted in each layer with the predicted attention.': 1.0986123085021973, '1. Contribution of approach: The approach to use attention in this way is to my knowledge novel and interesting.': 1.0986123085021973, '2. Qualitative results:': 1.0986123085021973, '2.1. I like the large number of qualitative results; however, I would have wished the focus would have been less on the “number” dataset and more on the Visual Genome dataset.': 1.0986123085021973, '2.2. The qualitative results for the Genome dataset unfortunately does not provide the predicted attributes. It would be interesting to see e.g. the highest predicted attributes for a given query. So far the results only show the intermediate results.': 1.0986121892929077, '3. Qualitative results:': 1.0986123085021973, '3.1. The paper presents results on two datasets, one simulated dataset as well as Visual Genome. On both it shows moderate but significant improvements over related approaches.': 1.0986123085021973, '3.2. For the visual genome dataset, it would be interesting to include a quantitative evaluation how good the localization performance is of the attention approach.': 1.0986123085021973, '3.3. It would be interesting to get a more detailed understanding of the model by providing results for different CNN layers where the attention is applied.': 0.459553062915802, '4. It would be interesting to see results on more established tasks, e.g. VQA, where the model should similarly apply. In fact, the task on the numbers seems to be identical to the VQA task (input/output), so most/all state-of-the-art VQA approaches should be applicable.': 0.5717962980270386, 'Other (minor/discussion points)': 1.06143319606781, 'Something seems wrong in the last two columns in Figure 11: the query “7” is blue not green.': 1.0986123085021973, 'Either the query or the answer seem wrong.': 1.0920445919036865, 'Section 3: “In each layer, the each attended feature map” -> “In each layer, each attended feature map”': 1.0986123085021973, 'I think Appendix A would be clearer if it would be stated that is the attention mechanism used in SAN and which work it is based on.': 0.6660269498825073, 'Summary:': 1.0986121892929077, 'While the experimental evaluation could be improved with more detailed evaluation, comparisons, and qualitative results, the presented evaluation is sufficient to validate the approach.': 1.0986049175262451, 'The approach itself is novel and interesting to my knowledge and speaks for acceptance.': 1.0881308317184448}"
233,https://openreview.net/forum?id=HyFkG45gl,"{'The authors describe a system for solving physics word problems.': 1.0986121892929077, 'The system consists of two neural networks: a labeler and a classifier, followed by a numerical integrator.': 1.0986034870147705, 'On the dataset that the authors synthesize, the full system attains near full performance.': 1.0986123085021973, 'Outside of the pipeline, the authors also provide some network activation visualizations.': 1.0986095666885376, 'The paper is clear, and the data generation procedure/grammar is rich and interesting.': 1.0986123085021973, 'However, overall the system is not well motivated.': 0.5025973916053772, 'Why did they consider this particular problem domain, and what challenges did they specifically hope to address?': 0.9447072148323059, 'Is it the ability to label sequences using LSTM networks, or the ability to classify what is being asked for in the question?': 1.098592758178711, 'This has already been illustrated, for example, by work on POS tagging and by memory networks for the babi tasks.': 1.098610520362854, 'A couple of standard architectural modifications, i.e. bi-directionality and a content-based attention mechanism, were also not considered.': 1.0986123085021973, 'This paper build a language-based solver for simple physics problems (a free falling object under constant velocity).': 1.0986123085021973, 'Given a natural language query sampled from a fixed grammar, the system uses two LSTM models to extract key components, e.g., physical parameters and the type of questions being asked, which are then sent to a numerical integrator for the answer.': 1.0986095666885376, 'The overall performance in the test set is almost perfect (99.8%).': 1.0986114740371704, 'Overall I found this paper quite interesting to read (and it is well written).': 1.0986123085021973, 'However, it is not clear how hard the problem is and how much this approach could generalize over more realistic (and complicated) situations.': 1.0977779626846313, 'The dataset are a bit small and might not cover the query space.': 1.0986123085021973, 'It might be better to ask AMT workers to come up with more complicated queries/answers.': 1.0986123085021973, 'The physics itself is also quite easy.': 1.0985381603240967, 'What happens if we apply the same idea on billiards?': 1.097279667854309, 'In this case, even we have a perfect physics simulator, the question to be asked could be very deep and requires multi-hop reasoning.': 1.0914602279663086, 'Finally, given the same problem setting (physics solver), in my opinion, a more interesting direction is to study how DNN can take the place of numerical integrator and gives rough answers to the question (i.e., intuitive physics).': 1.0986109972000122, 'It is a bit disappointing to see that DNN is only used to extract the parameters while still a traditional approach is used for core reasoning part.': 1.0986088514328003, 'It would be more interesting to see the other way round.': 1.0531212091445923, 'The paper uses neural networks to answer falling body physics questions by 1.': 1.0984773635864258, 'Resolving the parameters of the problem, and 2. Figure out which quantity is in question, compute it using a numerical integrator and return it as an answer.': 1.044541835784912, 'Learning and inference are performed on artificially generated questions using a probabilistic grammar.': 1.0986123085021973, 'Overall, the paper is clearly written and seems to be novel in its approach.': 0.8105639219284058, 'The main problems I see with this work are:': 1.0986123085021973, ""1. The task is artificial, and it's not clear how hard it is. The authors provide no baseline nor do they compare it to any real world problem. Without some measure of difficulty it's hard to tell if a much simple approach will do better, or if the task even makes sense."": 0.9585285186767578, '2. The labler LSTM uses only 10 hidden units. This is remarkably small for language modeling problems, and makes one further wonder about the difficulty of the task. The authors provide no reasoning for this choice.': 0.8255864381790161}"
234,https://openreview.net/forum?id=HyGTuv9eg,"{'The paper introduces a variation to the CNN-based texture synthesis procedure of Gatys et al.': 1.0986119508743286, 'that matches correlations between spatially shifted feature responses in addition to the correlations between feature responses at the same position in the feature maps.': 0.43056541681289673, 'The paper claims that this': 0.4493139386177063, 'a) improves texture synthesis for textures with long-range regular structures, that are not preserved with the Gatys et al. method': 0.06367696076631546, 'b) improves performance on texture inpainting tasks compared to the Gatys et al. method': 0.9797636270523071, 'c) improves results in season transfer when combined with the style transfer method by Gatys et al.': 0.955816924571991, 'Furthermore the paper shows that': 1.0977330207824707, 'd) by matching correlations between spatially flipped feature maps, symmetry properties around the flipping axis can be preserved.': 0.757445216178894, 'I agree with claim a).': 1.0986123085021973, 'However, the generated textures still have some issues such as greyish regions so the problem is not solved.': 1.0986123085021973, 'Additionally, the procedure proposed is very costly which makes an already slow texture synthesis method substantially slower.': 1.0986123085021973, 'For example, in comparison, the concurrent work by Liu et al. (http://arxiv.org/abs/1605.01141) that tackles the same problem by adding constraints to the Fourier spectrum of the synthesised texture shows comparable or better results while being far more efficient.': 1.098610758781433, 'Also with b) the presented results constitute an improvement over the Gatys et al. method but again the results are not too exciting - one would not prefer this model to other inpainting algorithms.': 1.09861159324646, 'With c) I don’t see a clear advantage of the proposed method to the existing Gatys et al. algorithm.': 0.9316222667694092, 'Finally, d) is a neat idea and the initial results look interesting but they don’t go much further than that.': 1.0985435247421265, 'All in all I think it is decent work but neither its originality and technical complexity nor the quality of the results are convincing enough for acceptance.': 0.20286867022514343, 'That said, I could imagine this to be a nice contribution to the workshop track though.': 1.0986123085021973, 'The paper investigates a simple extension of Gatys et al.': 0.28006669878959656, 'CNN-based texture descriptors for image generation.': 1.0986123085021973, 'Similar to Gatys et al., the method uses as texture descriptor the empirical intra-channel correlation matrix of the CNN feature response at some layer of a deep network.': 0.9035380482673645, 'Differently from Gatys et al., longer range correlations are measured by introducing a shift between the correlated feature responses, which translates in a simple modification of the original architecture.': 1.0986123085021973, 'The idea is simple but has interesting effects on the generated textures and can be extended to transformations other than translation.': 1.0986117124557495, 'While longer range correlations could be accounted for by considering the response of deeper CNN features in the original method by Gatys et al., the authors show that modelling them explicitly using shallower features is more effective, which is reasonable.': 1.0986123085021973, 'An important limitation that this work shares with most of its peers is the lack of a principled quantitative evaluation protocol, such that judging the effectiveness of the approach remains almost entirely a qualitative affair.': 1.0986123085021973, 'While this should not be considered a significant drawback of the paper due to the objective difficulty of solving this open issue, nevertheless it is somewhat limiting that no principled evaluation method could be devised and implemented.': 1.0986123085021973, 'The authors suggest that, as future work, a possible evaluation method could be based on a classification task': 1.0986123085021973, 'this is a potentially interesting approach that merits some further investigation.': 1.0986123085021973, 'This paper proposes a modification of the parametric texture synthesis model of Gatys et al. to take into account long-range correlations of textures.': 1.0986123085021973, 'To this end the authors add the Gram matrices between spatially shifted feature vectors to the synthesis loss.': 1.0986123085021973, 'Some of the synthesised textures are visually superior to the original Gatys et al. method, in particular on textures with very structured long-range correlations (such as bricks).': 1.0986123085021973, 'The paper is well written, the method and intuitions are clearly exposed and the authors perform quite a wide range of synthesis experiments on different textures.': 1.0986123085021973, 'My only concern, which is true for all methods including Gatys et al., is the variability of the samples.': 1.0986123085021973, 'Clearly the global minimum of the proposed objective is the original image itself.': 1.0986123085021973, 'This issue is partially circumvented by performing inpainting experiments, by which the synthesised paths needs to stay coherent with the borders (as the authors did).': 1.0986123085021973, 'There are no additional insights into this problem in this paper, which would have been a plus.': 1.0986123085021973, 'All in all, this work is a simple and nice modification of Gatys at al. which is worth publishing but does not constitute a major breakthrough.': 1.0986123085021973}"
235,https://openreview.net/forum?id=HyM25Mqel,"{'This paper studies the off-policy learning of actor-critic with experience replay.': 1.0986123085021973, 'This is an important and challenging problem in order to improve the sample efficiency of the reinforcement learning algorithms.': 1.0986123085021973, 'The paper attacks the problem by introducing a new way to truncate importance weight, a modified trust region optimization, and by combining retrace method.': 1.0986123085021973, 'The combination of the above techniques performs well on Atari and MuJoCo in terms of improving sample efficiency.': 1.0986123085021973, 'My main comment is how does each of the technique contribute to the performance gain?': 1.0986123085021973, 'If some experiments could be carried out to evaluate the separate gains from these tricks, it would be helpful.': 1.0986123085021973, 'This paper introduces an actor-critic deep RL approach with experience replay, which combines truncated importance sampling and trust region policy optimization.': 1.0986123085021973, 'The paper also proposes a new method called stochastic duelling networks to estimate the critic for continuous action spaces.': 1.0986123085021973, 'The method is applied to Atari games and continuous control problems, where it yields performance comparable to state-of-the-art methods.': 1.0986123085021973, 'As mentioned in the beginning of the paper, the main contributions of this work lies in combining 1) truncated importance sampling with retrace, 2) trust region policy optimization, and 3) stochastic duelling networks.': 1.0986123085021973, 'These improvements work well and may be beneficial to future work in RL.': 1.0986120700836182, 'However, each improvement appears to be quite incremental.': 1.0986111164093018, 'Moreover, the ACER framework seems much more complex and fragile to implement compared to the standard deep q-learning with prioritized replay (which appears to perform just as well on Atari games).': 1.0986123085021973, 'So for the Atari domain, I would still put my money on prioritized replay due to its simplicity.': 1.0986123085021973, 'Thirdly, improving sample efficiency for deep RL is a laudable goal, but really this goal should be pursued in a problem setting where sample efficiency is important.': 0.5146774053573608, 'Unfortunately, the paper only evaluates sample efficiency in the Atari and continuous control tasks domain; two domains where sample efficiency is not important.': 1.0985636711120605, 'Thus, it is not clear that the proposed method ACER will generalize to problems where we really care about sample efficiency.': 1.0985926389694214, 'Some technical aspects which need clarifications:': 1.0986121892929077, 'For Retrace, I assume that you compute recursively  starting from the end of each trajectory?': 0.8863829970359802, 'Please comment on this.': 1.0986123085021973, ""It's not clear to me how to derive eq."": 0.4055292010307312, '(7).': 1.0986123085021973, 'Is an approximation (double tilde) sign missing?': 1.0979139804840088, 'In section 3.1 the paper argued that  gives a lower-variance estimate of the action-value function.': 1.0898454189300537, 'Then why not use it in eq.': 1.098606824874878, '(8) for the bias correction term?': 1.0986123085021973, 'The paper states that it uses a replay memory of 50000 frames, so that across threads it is comparable in size to previous work.': 1.0985885858535767, 'However, for each thread this is much smaller compared to earlier experiments on Atari games.': 1.0605353116989136, 'For example, one million experience replay transitions were used in the paper ""Prioritized Experience Replay"" by Schaul et al.': 1.0986121892929077, 'This may have a huge impact on performance of the models (both for ACER and for the competing models).': 1.0939942598342896, 'In order to properly assess the improvements of ACER over previous work, the authors need to also experiment with larger experience replay memories.': 1.0750513076782227, 'Other comments:': 1.0986123085021973, 'Please move Section 7 to the appendix.': 1.0986123085021973, '""Moreover, when using small values of lambda to reduce variance, occasional large importance weights can still cause instability"": I think what is meant is using *large* values of lambda.': 1.0982117652893066, 'Above eq.': 1.0986123085021973, '(6) mention that the squared error is used.': 1.0986123085021973, 'Missing a ""t"" subscript at the beginning of eq.': 1.0773799419403076, '(9)?': 1.0986123085021973, 'It was hard to understand the stochastic duelling networks.': 0.8227592706680298, 'Please rephrase this part.': 1.0986123085021973, 'Please clarify this sentence ""To compare different agents, we adopt as our metric the median of the human normalized score over all 57 games.""': 0.8874641060829163, 'Figure 2 (Bottom): Please add label to vertical axes.': 1.0986123085021973, 'The paper looks at several innovations for deep RL, and evaluates their effect on solving games in the Atari domain.': 1.0986123085021973, 'The paper reads a bit like a laundry list of the researcher’s latest tricks.': 1.0986123085021973, 'It is written clearly enough, but lacks a compelling message.': 1.0986123085021973, 'I expect the work will be interesting to people already implementing deep RL methods, but will probably not get much attention from the broader community.': 1.0986123085021973, 'The claims on p.1 suggest the approach is stable and sample efficience, and so I expected to see some theoretical analysis with respect to these properties.': 1.0986123085021973, 'But this is an empirical claim; it would help to clarify that in the abstract.': 1.0986123085021973, 'The proposed innovations are based on sound methods.': 1.0986123085021973, 'It is particularly nice to see the same approach working for both discrete and continuous domains.': 1.0986123085021973, 'The paper has reasonably complete empirical results.': 1.0986123085021973, 'It would be nice to see confidence intervals on more of the plots.': 1.0986123085021973, 'Also, the results don’t really tease apart the effect of each of the various innovations, so it’s harder to understand the impact of each piece and to really get intuition, for example about why ACER outperforms A3C.  Also, it wasn’t clear to me why you only get matching results on discrete tasks, but get state-of-the-art on continuous tasks.': 1.0942894220352173, 'The paper has good coverage of the related literature.': 1.0986123085021973, 'It is nice to see this work draw more attention to Retrace, including the theoretical characterization in Sec.7.': 1.0985997915267944}"
236,https://openreview.net/forum?id=HyNxRZ9xg,"{'A method for click prediction is presented.': 1.0986123085021973, 'Inputs are a categorical variables and output is the click-through-rate.': 1.0986123085021973, 'The categorical input data is embedded into a feature vector using a discriminative scheme that tries to predict whether a sample is fake or not.': 1.0986123085021973, 'The embedding vector is passed through a series of SUM/MULT gates and K-most important interactions are identified (K-max pooling).': 1.0986123085021973, 'This process is repeated multiple times (i.e. multiple layers) and the final feature is passed into a fully connected layer to output the click prediction rate.': 1.0986123085021973, 'Authors claim:': 1.0986123085021973, '(1) Use of gates and K-max pooling allow modeling of interactions that lead to state of art results.': 1.0986123085021973, '(2) It is not straightforward to apply ideas in papers like word2vec to obtain feature embeddings and consequently they use the idea of discriminating between fake and true samples for feature learning.': 1.0986123085021973, 'Theoretically convolutions can act as “sum” gates between pairs of input dimensions.': 1.0986123085021973, 'Authors make these interactions explicit (i.e. imposed structure) by using gates.': 1.0986123085021973, 'Now, the merit of the proposed method can be tested if a network using gates outperforms a network without gates.': 1.0986123085021973, 'This baseline is critically missing – i.e. Embedding Vector followed by a series of convolution/pooling layers.': 1.0911749601364136, 'Another related issue is that I am not sure if the number of parameters in the proposed model and the baseline models is similar or not.': 0.33160215616226196, 'For instance – what is the total number of parameters in the CCPM model v/s the proposed model?': 1.095974087715149, 'Overall, there is no new idea in the paper.': 1.0882796049118042, 'This by itself is not grounds for rejection if the paper outperforms established baselines.': 0.9270809888839722, 'However, such comparison is weak and I encourage authors to perform these comparisons.': 1.0663769245147705, 'The paper proposes a way to learn continuous features for input data which consists of multiple categorical data.': 1.0986123085021973, 'The idea is to embed each category in a learnable low dimensional continuous space, explicitly compute the pair-wise interaction among different categories in a given input sample (which is achieved by either taking a component-wise dot product or component-wise addition), perform k-max pooling to select a subset of the most informative interactions, and repeat the process some number of times, until you get the final feature vector of the given input.': 1.0986123085021973, 'This feature vector is then used as input to a classifier/regressor to accomplish the final task.': 1.0986123085021973, 'The embeddings of the categories are learnt in the usual way.': 1.0986123085021973, 'In the experiment section, the authors show on a synthetic dataset that their procedure is indeed able to select the relevant interactions in the data.': 1.0985742807388306, 'On one real world dataset (iPinYou) the model seems to outperform a couple of simple baselines.': 1.0986123085021973, ""My major concern with this paper is that their's nothing new in it."": 1.0986123085021973, 'The idea of embedding the categorical data having mixed categories has already been handled in the past literature, where essentially one learns a separate lookup table for each class of categories: an input is represented by concatenation of the embeddings from these lookup table, and a non-linear function (a deep network) is plugged on top to get the features of the input.': 1.0985920429229736, 'The only rather marginal contribution is the explicit modeling of the interactions among categories in equations 2/3/4/5.': 1.0986123085021973, ""Other than that there's nothing else in the paper."": 1.0986123085021973, 'Not only that, I feel that these interactions can (and should) automatically be learned by plugging in a deep convolutional network on top of the embeddings of the input.': 1.0986123085021973, ""So I'm not sure how useful the contribution is."": 1.0986123085021973, 'The experimental section is rather weak.': 1.0986123085021973, 'They authors test their method on a single real world data set against a couple of rather weak baselines.': 1.0986123085021973, 'I would have much preferred for them to evaluate against numerous models proposed in the literature which handle similar problems, including wsabie.': 1.098588228225708, 'While the authors argued in their response that wsabie was not suited for their problem, i strongly disagree with that claim.': 1.0986123085021973, 'While the original wsabie paper showed experiments using images as inputs, their training methodology can easily be extended to other types of data sets, including categorical data.': 1.0986123085021973, 'For instance, I conjecture that the model i proposed above (embed all the categorical inputs, concatenate the embeddings, plug a deep conv-net on top and train using some margin loss) will perform as well if not better than the hand coded interaction model proposed in this paper.': 1.0986123085021973, 'Of course I could be wrong, but it would be far more convincing if their model was tested against such baselines.': 1.0986123085021973, 'In this paper, the author proposed an approach for feature combination of two embeddings v1 and v2.': 1.0986123085021973, 'This is done by first computing the pairwise combinations of the elements of v1 and v2 (with complicated nonlinearity), and then pick the K-Max as the output vector.': 1.098610758781433, 'For triple (or higher-order) combinations, two (or more) consecutive pairwise combinations are performed to yield the final representations.': 1.0986123085021973, 'It seems that the approach is not directly related to categorical data, and can be applied to any embeddings (even if they are not one-hot).': 1.0986121892929077, 'So is there any motivation that brings about this particular approach?': 1.0986123085021973, 'What is the connection?': 1.0986123085021973, 'There are many papers with similar ideas.': 1.0986123085021973, 'CCPM (A convolutional click prediction model) that the authors have compared against, also proposes very similar network structure (conv + K-max + conv + K-max).': 1.0986123085021973, 'In the paper, the author does not mention their conceptual similarity and difference versus CCPM.': 1.0986123085021973, 'Compact Bilinear Pooling, https://arxiv.org/abs/1511.06062 has been proposed a year ago and yields state-of-the-art performances in Visual Question Answering https://arxiv.org/abs/1606.01847.': 1.097938060760498, 'So the author might need to compare against those methods.': 1.0986123085021973, 'I understand that the proposed approach incorporates more nonlinear operations (rather than bilinear) in pairwise combination, but it is not clear whether bilinear operations is sufficient to achieve the same level of performance, and whether complicated operations (e.g., Eqn. 4) are needed.': 1.0739670991897583, 'In the experiment, the performance seems to be not as impressive.': 1.0986123085021973, 'There is about 1%-2% difference in performance between the proposed approach and baselines (e.g., in Tbl. 2, and Tbl. 3).': 1.0986123085021973, 'Is that a big deal for click-rate prediction?': 1.0986123085021973, 'When comparing among LR, FFM, CCPM, FNN, and proposed approach, the number of parameters (i.e., model capacity) are not shown.': 1.0986100435256958, 'This could be unfair since the proposed model could have more parameters (note that the authors seems to misunderstand the questions).': 1.0986123085021973, 'Besides, claiming that previous approaches does not learn representations seems to be a bit restrictive, since typical deep models learn the representation implicitly (e.g., CCPM and FNN listed in the paper as baselines).': 1.0986123085021973}"
237,https://openreview.net/forum?id=HyQJ-mclg,"{'There is a great deal of ongoing interest in compressing neural network models.': 1.0985257625579834, 'One line of work has focused on using low-precision representations of the model weights, even down to 1 or 2 bits.': 1.0986123085021973, 'However, so far these approaches have been accompanied by a significant impact on accuracy.': 1.098551869392395, 'The paper proposes an iterative quantization scheme, in which the network weights are quantized in stages': 1.0986121892929077, 'the largest weights (in absolute value) are quantized and fixed, while unquantized weights can adapt to compensate for any resulting error.': 0.25897839665412903, 'The experimental results show this is extremely effective, yielding models with 4 bit or 3 bit weights with essentially no reduction in accuracy.': 1.0986121892929077, 'While at 2 bits the accuracy decreases slightly, the results are substantially better than those achieved with other quantization approaches.': 1.0986123085021973, 'Overall this paper is clear, the technique is as far as I am aware novel, the experiments are thorough and the results are very compelling, so I recommend acceptance.': 1.0985907316207886, 'The paper could use another second pass for writing style and grammar.': 1.0986123085021973, 'Also, the description of the pruning-inspired partitioning strategy could be clarified somewhat...': 1.0985795259475708, 'e.g., the chosen splitting ratio of 50% only seems to be referenced in a figure caption and not the main text.': 0.36569127440452576, 'The idea of this paper is reasonable - gradually go from original weights to compressed weights by compressing a part of them and fine-tuning the rest.': 1.0986123085021973, 'Everything seems fine, results look good, and my questions have been addressed.': 1.0985963344573975, 'To improve the paper:': 1.0187846422195435, '1) It would be good to incorporate some of the answers into the paper, mainly the results with pruning + this method as that can be compared fairly to Han et al. and outperforms it.': 0.4421953558921814, '2) It would be good to better explain the encoding method (my question 4) as it is not that clear from the paper (e.g. made me make a mistake in question 5 for the computation of n2).': 0.4879087805747986, 'The ""5 bits"" is misleading as in fact what is used is variable length encoding (which is on average close to 5 bits) where:': 1.0986123085021973, '0 is represented with 1 bit, e.g. 0': 1.0671049356460571, 'other values are represented with 5 bits, where the first bit is needed to distinguish from 0, and the remaining 4 bits represent the 16 different values for the powers of 2.': 0.3743692934513092, 'Nice idea but not complete, model size is not reduced by the large factors found in one of your references (Song 2016), where they go to 5 bits, but this is ontop of pruning which gives overall 49X reduction in model size of VGG (without loss of accuracy).': 1.0986123085021973, 'You may achieve similar reductions with inclusion of pruning (or better since you go to 4 bits with no loss) but we should see this in the paper, so at the moment it is difficult to compare': 1.0986123085021973}"
238,https://openreview.net/forum?id=HyQWFOVge,"{'I agree with the other two reviewers that it is an interesting topic to investigate the feature learned by DML.': 1.0986123085021973, 'For classification task though, I feel intuitively softmax should have advantages over distance metric learning method because the loss function is designed to assign the correct class for the given image.': 1.0986123085021973, 'All the experimental results show that the softmax features work better than Rippel et al DML method.': 1.0985740423202515, 'However, does it support the claim that softmax-based features work much better than DML learned features?': 1.0986119508743286, 'I have doubts on this claim.': 1.0821709632873535, 'Also the experiments are a little bit misleading.': 1.098069429397583, 'What is vanilla googleNet softmax finetuned results?': 0.9521222710609436, 'It seems it is not Rippel et al.': 0.6007852554321289, '(softmax prob) result.': 0.6101453900337219, 'I am wondering whether the improvement comes from a) using retrieval (nearest neighbor) for classification or b) adding a new layer on top of pool5 or c) L2 normalization of the features.': 1.0986123085021973, 'It is not clear to me at all.': 1.0549209117889404, 'It appears to me the comparison is not apple vs apple between the proposed method and Rippel et al.': 1.0986123085021973, 'It would be great if we know adding feature reduction or adding another layer on top of pool5 can improve finetued softmax result.': 1.0986123085021973, 'However, I am not sure what is the biggest contributing factor to the superior results.': 1.0986101627349854, 'Before getting more clarifications from the authors, I lean toward rejection.': 1.05937659740448, 'I have a huge, big picture concern about this paper and the papers it most closely addresses (MagnetLoss and Lifted Feature Structure Embedding).': 1.0986120700836182, ""I don't understand why Distance Metric Learning (DML) is being used for classification tasks (Stanford Cars 196, UCSD Birds 200, Oxford 102 flowers, Stanford Dogs, ImageNet attributes, etc)."": 1.0942798852920532, 'As far as I can tell, there is really only a single ""retrieval""-like benchmark being used here - the Stanford Online Products database.': 1.0986121892929077, 'All the other datasets are used in a ""classification-by-retrieval"" approach which seems contrived.': 1.098542332649231, 'While ostensibly evaluating ""retrieval"", the retrieval ground truth is totally defined by category membership so these are still classification tasks with many instances in each category.': 1.0986123085021973, 'With the Online Products dataset the correspondence between queries and correct results is much more fine grained so it makes sense to think of it as a retrieval task.': 1.0793970823287964, 'It seems obvious that if your task is classification, a network trained with a classification loss will be best.': 1.0986088514328003, 'Even when these datasets are used in a ""retrieval"" setting, the ground truth is still defined by category membership.': 1.0986123085021973, ""It's still a classification task."": 1.0986123085021973, ""I don't really see the point of using DML in these scenarios."": 1.0986123085021973, 'I guess prior work claims to outperform SoftMax in these settings so this paper is fighting back against this and I should be thankful for this paper.': 1.0985944271087646, ""But I think this paper's narrative is a bit off."": 1.0986123085021973, 'The narrative shouldn\'t be ""We can get good retrieval features from softmax networks with appropriate normalization"".': 1.0986028909683228, 'It should be ""It never made sense to train or evaluate these things as retrieval tasks.': 1.0986123085021973, 'Direct classification is better"".': 1.0986123085021973, 'For example, why are you taking the second to last layer or pool5 layer from these networks?': 1.098610758781433, ""Why aren't you taking the last layer?"": 1.0986123085021973, 'That should do well in these evaluations, right?': 0.4539353549480438, 'Table 1 and 2 do show that using softmax probabilities directly tends to be better than doing classification-by-retrieval (works better or the same as doing retrieval with an earlier layer of features, except on Oxford flowers).': 1.0980974435806274, 'GoogLeNet is quite deep and gets auxiliary supervision.': 0.5626317858695984, 'By the second-to-last layer of the network, the activations could look a lot like category membership already.': 1.0985928773880005, ""And category membership is all that's needed for the tasks in 4.2 and 4.3."": 1.098591923713684, ""I don't think my pre-review question was adequately addressed."": 1.0952534675598145, 'I was getting at this concern by pointing out numerous scenarios where distance metric learning makes sense because you have fine-grained associations between instances at training time, NOT categorical associations': 1.0984187126159668, 'e.g. this product photo corresponds to this photo of the object in a scene': 0.6118970513343811, '[Bell et al. 2015], this 3d model correspond to this sketch': 1.0646084547042847, '[Wang et al. 2015], this sketch corresponds to this photo': 1.0696587562561035, '[Sangkloy et al. 2016], this ground view corresponds to this aerial view': 0.4520687460899353, '[Lin et al., 2015].': 1.0985190868377686, 'DeepFace and follow-up works on LFW could also fit into this space because there are few training samples per class (few training samples per person identity).': 1.0155833959579468, 'You cite DeepFace and Bell et al. 2015': 1.0583055019378662, ""but you don't compare on those benchmarks."": 0.9325957298278809, 'I think those are exactly the tasks where DML makes sense.': 1.0986123085021973, 'Maybe the ""retrieval on classification datasets"" would be a reasonable benchmark if the test and train classes were completely different.': 1.0553016662597656, ""Then you could argue that softmax is learning a useful representation yet the last layer isn't directly useful since the categories change."": 0.4048365354537964, ""But that's not the case here, is it?"": 1.0818328857421875, ""With all of this said, I'm not sure whether I'm positive or negative about this paper."": 0.46034377813339233, ""I think you're onto something significant"": 1.0983577966690063, 'people have been using DML where it is not appropriate': 1.0986123085021973, 'but addressed it in the wrong way': 1.0986123085021973, 'by using softmax for ""classification by retrieval"".': 1.0986123085021973, ""But you don't need to do retrieval!"": 1.0986014604568481, 'Softmax is already telling you the class prediction!': 1.0986123085021973, 'Why go through the extra step of finding nearest neighbors with some intermediate feature?': 1.0717277526855469, 'AnonReviewer3 also raises some good points and you should be thankful that a reviewer is willing to dig so deep to help make your experiments sound!': 1.0971283912658691, ""I don't think his/her concerns are disqualifying for this paper, though, as long as it is fixed."": 1.0853439569473267, 'I look forward to hearing your response.': 1.0986095666885376, 'I want this paper to be published, but I think it needs to be tweaked.': 0.9293297529220581, 'There has been substantial recent interest in representation learning, and specifically, using distance metric learning (DML) to learn representations where semantic distance between inputs can be measured.': 1.098607063293457, 'This is a topic of particular relevance / interest to ICLR.': 1.0986123085021973, 'This paper poses a simple yet provocative question: can a standard SoftMax based approach learn features that match or even outperform recent state-of-the-art DML approaches?': 1.0986123085021973, 'Thorough experiments seem to indicate that this is indeed the case.': 1.0986123085021973, 'Comparisons are made to recent DML approaches including Magnet Loss (ICLR2016) and Lifted structure embedding (CVPR2016) and superior results are shown across a number of datasets / tasks for which the DML approaches were designed.': 1.0986123085021973, 'This main result is a bit surprising since SoftMax is a natural and trivial baseline, so it should have been properly evaluated in previous DML literature.': 1.0986123085021973, 'The authors argue that previous approaches did not fully/properly tune the softmax baselines, or that comparisons were not apples-to-apples.': 1.0986123085021973, 'Also, one change in the current paper is the addition of L2 normalization, which is well motivated and helps improve SoftMax feature distances.': 1.0986123085021973, 'Different dimensionality reduction approaches are also tested.': 1.0986123085021973, 'These changes are minor, but especially the L2 normalization proves to be a simple but effective improvement for SoftMax features.': 1.0986123085021973, 'A big issue is with how pre-training is performed (in Magnet Loss the softmax baselines were pretrained for less time on ImageNet).': 1.0986123085021973, 'The approach taken here is reasonable, but so is the approach in Magnet Loss (for different reasons).': 1.0968892574310303, 'Ultimately, both are fine.': 1.098365068435669, 'Unfortunately, due to use of different schemes, the results are not comparable.': 0.9936320781707764, 'Let me copy-paste what I wrote in an earlier comment:': 1.0986028909683228, 'My main concern about the paper is that the comparisons in Tables 1 and 2 and Figure 4 to Rippel et al. are not apples-to-apples.': 1.096266508102417, ""Basically, the papers shows that absolute results of using SoftMax w full pre-training (PT) on ImageNet is superior to any of the results in Rippel's paper (including both the Softmax and Magnet results)."": 1.088842511177063, 'But as the current results show, PT appears to be critical to obtaining such good numbers - The Rippel SoftMax numbers use only 3 PT stages and are dramatically worse than the full PT on ImageNet.': 1.0965334177017212, 'As it stands, I am not convinced that SoftMax is actually better than Magnet.': 1.0499706268310547, ""Here is the evidence we have (I'll use Stanford Dogs as an example, but any of the datasets have the same conclusion): (1) Softmax w 3 stages of PT: 26.6% (from Rippel paper) and 32.7% (from authors' reproduction) (2) Magnet w 3 stages of PT: 24.9% (3) Softmax w full PT: 18.3% (4) Magnet w full PT: not shown From this all I see is that PT is critical for getting absolute good results."": 1.0983551740646362, 'However, what about Magnet w full PT?': 1.0732688903808594, 'These results are not shown either here or in the original Rippel paper (I went back and looked).': 1.0752655267715454, 'As such, I do not think it is justifiable to claim superiority of Softmax to Magnet based on available evidence.': 1.0986123085021973, '(Note: I looked back carefully at Rippel\'s paper, and it appears that the authors use 3 PT stages as a form of ""warmup"".': 1.0986123085021973, 'There is a statement that using full PT would ""defeat the purpose of pursuing DML"".': 1.0986123085021973, ""I'm not sure if I agree w Rippel's statement since in the present paper there is clear evidence that full PT is hugely helpful, at least for softmax."": 1.0986123085021973, ""That being said, I did not see any evidence in the Rippel paper that PT is harmful or that DML wouldn't work with full PT.)"": 1.0986123085021973, ""The authors responded to my concern by claiming that “from Rippel's results, it is no doubt that Magnet@3epochPT > Magnet@FullPT"": 1.0986123085021973, 'and Magnet@3epoch > Softmax@3epochPT.”': 1.0986123085021973, 'However, I went back to Rippel’s paper, and simply the Magnet@FullPT experiment never appears.': 1.0986123085021973, 'I further went and contacted Oren Rippel himself, and he verified he never ran the Magnet@FullPT experiment.': 1.0986123085021973, 'I encourage the authors to contact Oren Rippel regarding this if they wish to verify (I have asked Oren Rippel to not reveal my identity).': 1.0986123085021973, '[Disclaimer: I am NOT Oren Rippel].': 1.0986123085021973, 'The authors mentioned that they are training Magnet@FullPT.': 1.098610758781433, 'If results were shown for Magnet@FullPT and also retrain the Magnet@3epochPT as a sanity check, that would help alleviate this concern.': 0.4474526643753052, 'Alternatively, the language in Section 4 and the Tables could be altered to make clear that the methods use different pretraining and hence are not comparable.': 0.40682387351989746, 'Overall, I am actually quite sympathetic to this work.': 1.0951091051101685, 'I think it could serve as an important sanity-check paper for the community and quite relevant to ICLR.': 0.5120563507080078, 'Having proper and strong SoftMax baselines should prove quite useful to the DML community and to this line of work.': 0.9811557531356812, 'However, currently I find the main results (table 1, table 2, figure 4, etc.) to be misleading.': 0.5350238084793091, 'If indeed it were the case that Magnet@3epochPT > Magnet@FullPT, then it would be fine.': 0.4829026162624359, 'However, at this point as far as I know no one has actually tried Magnet@FullPT.': 0.8572247624397278, 'And, given the general importance and effectiveness of pre-training, especially when transferring to small dataset, I would be hugely surprised if Magnet@FullPT was not superior by a large margin.': 0.9661515355110168, 'I think either having this experiment in place or altering the writing / presentation of the results would be critical to allow for publishing.': 1.0986123085021973}"
239,https://openreview.net/forum?id=HyTqHL5xg,"{'This is mainly a (well-written) toy application paper.': 1.0986123085021973, 'It explains SGVB can be applied to state-space models.': 0.5159511566162109, 'The main idea is to cast a state-space model as a deterministic temporal transformation, with innovation variables acting as latent variables.': 1.0986106395721436, 'The prior over the innovation variables is not a function of time.': 1.0986123085021973, 'Approximate inference is performed over these innovation variables, rather the states.': 0.9780703783035278, ""This is a solution to a fairly specific problem (e.g. it doesn't discuss how priors over the beta's can depend on the past), but an interesting application nonetheless."": 1.0986089706420898, 'The ideas could have been explained more compactly and more clearly; the paper dives into specifics fairly quickly, which seems a missed opportunity.': 1.0986119508743286, 'My compliments for the amount of detail put in the paper and appendix.': 1.0986121892929077, 'The experiments are on toy examples, but show promise.': 1.0979735851287842, 'Section 2.1: “In our notation, one would typically set beta_t = w_t, though other variants are possible” -> It’s probably better to clarify that if F_t and B_t and not in beta_t, they are not given a Bayesian treatment (but e.g. merely optimized).': 1.0986123085021973, 'Section 2.2 last paragraph: “A key contribution is […] forcing the latent space to fit the transition”.': 1.0986123085021973, 'This seems rather trivial to achieve.': 1.098589539527893, 'Eq 9: “This interpretation implies the factorization of the recognition model:..”': 1.0326757431030273, 'The factorization is not implied anywhere: i.e. you could in principle use q(beta|x) = q(w|x,v)q(v)': 1.0946546792984009, 'This paper presents a variational inference based method for learning nonlinear dynamical systems.': 1.0986123085021973, 'Unlike the deep Kalman filter, the proposed method learns a state space model, which forces the latent state to maintain all of the information relevant to predictions, rather than leaving it implicit in the observations.': 1.0986123085021973, 'Experiments show the proposed method is better able to learn meaningful representations of sequence data.': 1.0985358953475952, 'The proposed DVBF is well motivated, and for the most part the presentation is clear.': 1.0986043214797974, 'The experiments show interesting results on illustrative toy examples.': 1.0986123085021973, 'I think the contribution is interesting and potentially useful, so I’d recommend acceptance.': 1.0986123085021973, 'The SVAE method of Johnson et al. (2016) deserves more discussion than the two sentences devoted to it, since the method seems pretty closely related.': 1.0986123085021973, 'Like the DVBF, the SVAE imposes a Markovianity assumption, and it is able to handle similar kinds of problems.': 1.0986123085021973, 'From what I understand, the most important algorithmic difference is that the SVAE q network predicts potentials, whereas the DVBF q network predicts innovations.': 1.0986123085021973, 'What are the tradeoffs between the two?': 1.0986123085021973, 'Section 2.2 says they do the latter in the interest of solving control-related tasks, but I’m not clear why this follows.': 1.0986123085021973, 'Is there a reason SVAEs don’t meet all the desiderata mentioned at the end of the Introduction?': 1.0986123085021973, 'Since the SVAE code is publicly available, one could probably compare against it in the experiments.': 1.0986123085021973, 'I’m a bit confused about the role of uncertainty about v. In principle, one could estimate the transition parameters by maximum likelihood (i.e. fitting a point estimate of v), but this isn’t what’s done.': 1.0986096858978271, 'Instead, v is integrated out as part of the marginal likelihood, which I interpret as giving the flexibility to model different dynamics for different sequences.': 1.0986123085021973, 'But if this is the case, then shouldn’t the q distribution for v depend on the data, rather than being data-independent as in Eqn.': 1.0986123085021973, '(9)?': 1.0986123085021973, 'The paper proposes to use the very standard SVGB in a sequential setting like several previous works did.': 1.0986123085021973, 'However, they proposes to have a clear state space constraints similar to Linear Gaussian Models: Markovian latent space and conditional independence of observed variables given the latent variables.': 1.0986123085021973, 'However the model is in this case non-linear.': 1.0986123085021973, 'These assumptions are well motivated by the goal of having meaningful latent variables.': 1.0986123085021973, ""The experiments are interesting but I'm still not completely convinced by the regression results in Figure 3, namely that one could obtain the angle and velocity from the state but using a function more powerful than a linear function."": 1.0986123085021973, ""Also, why isn't the model from (Watter et al., 2015) not included ?"": 1.0986123085021973, ""After rereading I'm not sure I understand why the coordinates should be combined in a 3x3 checkerboard as said in Figure 5a."": 1.0986123085021973, 'Then paper is well motivated and the resulting model is novel enough, the bouncing ball experiment is not quite convincing, especially in prediction, as the problem is fully determined by its initial velocity and position.': 1.0986123085021973}"
240,https://openreview.net/forum?id=HyWDCXjgx,"{'The paper presents a large-scale visual search system for finding product images given a fashion item.': 1.0924402475357056, 'The exploration is interesting and the paper does a nice job of discussing the challenges of operating in this domain.': 1.0918612480163574, 'The proposed approach addresses several of the challenges.': 1.0986123085021973, 'However, there are several concerns.': 1.0986123085021973, '1) The main concern is that there are no comparisons or even mentions of the work done by Tamara Berg’s group on fashion recognition and fashion attributes, e.g.,': 0.4780845046043396, '“Automatic Attribute Discovery and Characterization from Noisy Web Data” ECCV 2010': 1.0983965396881104, '“Where to Buy It: Matching Street Clothing Photos in Online Shops” ICCV 2015,': 1.0985995531082153, '“Retrieving Similar Styles to Parse Clothing, TPAMI 2014,': 1.0986106395721436, 'etc': 1.0986123085021973, 'It is difficult to show the contribution and novelty of this work without discussing and comparing with this extensive prior art.': 1.098456859588623, '2) There are not enough details about the attribute dataset and the collection process.': 1.0986061096191406, 'What is the source of the images?': 1.0986123085021973, 'Are these clean product images or real-world images?': 1.0986123085021973, 'How is the annotation done?': 1.0986123085021973, 'What instructions are the annotators given?': 1.0986123085021973, 'What annotations are being collected?': 1.0986123085021973, 'I understand data statistics for example may be proprietary, but these kinds of qualitative details are important to understand the contributions of the paper.': 1.0985382795333862, 'How can others compare to this work?': 1.0986123085021973, '3) There are some missing baselines.': 1.0916944742202759, 'How do the results in Table 2 compare to simpler methods, e.g., the BM or CM methods described in the text?': 1.0985832214355469, 'While the paper presents an interesting exploration, all these concerns would need to be addressed before the paper can be ready for publication.': 1.0977317094802856, 'This paper introduces a pratical large-scale visual search system for a fashion site.': 1.0985970497131348, 'It uses RNN to recognize multi-label attributes and uses state-of-art faster RCNN to extract features inside those region-of-interest (ROI).': 1.0986123085021973, 'The technical contribution of this paper is not clear.': 0.9386546015739441, 'Most of the approaches used are standard state-of-art methods and there are not a lot of novelties in applying those methods.': 1.0986123085021973, 'For multi-label recognition task, there are other available methods, e.g. using binary models, changing cross-entropy loss function, etc.': 1.0986093282699585, ""There aren't any comparison between the RNN method and other simple baselines."": 0.5085294246673584, 'The order of the sequential RNN prediction is not clear either.': 0.8981385231018066, 'It seems that the attributes form a tree hierarchy and that is used as the order of sequence.': 1.0986121892929077, 'The paper is not well written either.': 1.0986123085021973, ""Most results are reported in the internal dataset and the authors won't release the dataset."": 1.0973327159881592, 'The manuscript is a bit scattered and hard to follow.': 1.0985901355743408, ""There is technical depth but the paper doesn't do a good job explaining what shortcoming the proposed methods are overcoming and what baselines they are outperforming."": 1.0986123085021973, 'The writing could be improved.': 0.49283474683761597, 'There are numerous grammatical errors.': 0.878251314163208, 'The experiments in 3.1 are interesting, but you need to be clearer about the relationship of your ResCeption method to the state-of-the-art.': 1.0986123085021973, 'The use of extensive footnotes on page 5 is a bit odd.': 1.0486955642700195, '""That is a competitive result"" is vague.': 1.0986123085021973, 'A footnote links to ""http://image-net.org/challenges/LSVRC/2015/results"" which doesn\'t seem to even show the same task you are evaluating.': 1.0986123085021973, 'ResCeption: ""The best validation error is reached at 23.37% and 6.17% at top-1 and top-5, respectively"".': 1.0986123085021973, 'Single model ResNet-152 gets 19.38 and 4.49, respectively.': 1.0971862077713013, 'Resnet-34 is 21.8 and 5.7, respectively.': 1.0983535051345825, 'VGGv5 is 24.4 and 7.1, respectively.': 1.0030800104141235, '[source: Deep Residual Learning for Image Recognition, He et al. 2015].': 1.023687720298767, 'I think it would be more honest for you to report results of competitors and say that your model is worse than ResNet and slightly better than VGG on ImageNet classification.': 1.0986123085021973, '3.5, retrieval on Holidays, is a bit too much of a diversion from the goal of this paper.': 1.0866990089416504, 'If this paper is more about the novel architecture and less about the particular fashion attribute task then the narrative needs to change accordingly.': 1.0985935926437378, 'Perhaps my biggest concern is that this paper is missing baselines (e.g. non recurrent models, attribute classification instead of detection) and comparisons to prior work by Berg et al.': 1.0986121892929077, '""Our policy restricts to reveal much more details about the internal dataset"" This is a significant issue.': 1.0986123085021973, 'The dataset used in this work cannot be shared?': 1.098542332649231, 'How are future works going to compare to your benchmark?': 0.9284234642982483}"
241,https://openreview.net/forum?id=HyWG0H5ge,"{'This paper develops a theoretical guarantee for the convergence of the training error.': 1.0973806381225586, 'The result is quite general that covers the training of a wide range of neural network models.': 1.098611831665039, 'The key idea of the paper is approximate the training loss by its linear approximation.': 1.0716556310653687, 'Since its linearity in the variables (thus convex), the authors plug in results that has been developed in the literature of online learning.': 1.0986123085021973, 'This paper has good novelty in using the Taylor approximation thus greatly simplifying the analysis of the behaviour of the model.': 1.0986119508743286, 'However, there are two problems about the main result of this paper, Theorem 2.': 1.09861159324646, '1. It is not clear if the Taylor optimum would converge or not.': 1.0986123085021973, 'As noticed by the authors, the upper bound is path dependent.': 1.0985565185546875, 'Appendix 3 tries to claim that this Taylor optimum indeed converges, but the proof is buggy.': 0.424083411693573, 'In the proof of Lemma 2, it is proved that the difference between two sequential Taylor optimum is approaching 0.': 1.0986123085021973, 'Note that this is actually weaker than being Cauchy sequence and insufficient to guarantee convergence.': 1.0986123085021973, '2. The lefthand side of Equation (3) (I will denote it by L3 in this review) is not equivalent to training error.': 0.7072004079818726, 'An upper bound on this average error is not sufficient to guarantee the convergence of the training error neither.': 1.0985538959503174, 'Take the gradient descent for example (thus each minibatch x_0^n is the whole training set), the convergence of the training error should be lim_{n -> \\infty} l(f_{w^n}(x_0^n), y^n).': 1.098608136177063, 'The convergence of L3 is necessary but not sufficient to imply the convergence of the training error.': 1.0984394550323486, 'Another concern about Theorem 2 (but it is minor compared to the two problems mentioned above) is that to achieve the O(1/\\sqrt{n}) rate, the algorithm has to pick a particular learning rate.': 1.0986123085021973, 'Larger or smaller learning rate (in the order of n) will lead to significantly worse regret.': 1.0914616584777832, 'But in the experiments of the paper, the learning rates are not picked according to the theorem.': 1.0892804861068726, 'Overall, this paper has a good motivation and good novelty.': 1.0622390508651733, 'It could be further developed into a good paper.': 0.4251419007778168, 'But due to the two problems and a buggy proof mentioned above, I think it is not ready for publish yet.': 1.0973505973815918, 'This paper adopts Taylor approximations of neural nets for separating convex and non-convex components of the optimization.': 1.0986123085021973, 'This enables them to bound the training error by the Taylor optimum and regret (theorem 2).': 1.0986123085021973, 'This is a nice theoretical result applicable to popular deep nets.': 1.0986123085021973, 'The empirical studies back up the theoretical claim.': 1.0985839366912842, 'It is interesting to derive such a bound and show it satisfies a regret bound along with empirical evidence on the CIFAR-10 for cross entropy loss and auto encoder for MSE loss.': 1.0985959768295288, 'At least empirically, by comparing the observed training loss and taylor loss, the better a particular optimizer performs (training loss statement, not a validation or observed test statement) the smaller the difference between these two.': 1.0986123085021973, 'Also shown is the regret loss is satisfied at different scales of the network, by layer, neuron and whole network.': 1.0986120700836182, 'The Taylor approximation can be used to investigate activation configurations of the network, and used to connect this to difficulty in optimizing  at kinks in the loss surface, along with an empirical study of exploration of activation surface of the SGD/Adam/RMSprop optimizers, the more exploration the better the resulting training loss.': 1.0986123085021973, 'Not that it impacts the paper but the weaker performance of the SGD could be related to the fixed learning rate, if we anneal this learning rate, which should improve performance, does this translate to more exploration and tightening between the actual loss and the Taylor loss?': 1.0986123085021973, 'It might be useful to use a cross validation set for some of the empirical studies, in the end we would like to say something about generalization of the resulting network': 1.098581075668335, 'Is there a reason the subscript on the Jacobian changes to a_l in the <Gl,V> definition?': 1.0986121892929077}"
242,https://openreview.net/forum?id=HyWWpw5ex,"{'The paper introduces a time dependent recommender system based on point processes parametrized by time dependent user and item latent representations.': 1.0985156297683716, 'The later are modeled as coupled – autoregressive processes – i.e. the representation of a user/item changes when he interacts with an item/user, and is a function of both the user and the item representations before time t.': 1.0985102653503418, 'This is called coevolution here and the autoregressive process is called recurrent NN.': 1.0985569953918457, 'The model may also incorporate heterogeneous inputs.': 1.0986123085021973, 'Experiments are performed on several datasets, and the model is compared with different baselines.': 1.0986123085021973, 'There are several contributions in the paper: 1) modeling recommendation via parametrized point processes where the parameter dynamics are modeled by latent user/item representations, 2) an optimization algorithm for maximizing the likelihood of this process, with different technical tricks that seem to break its intrinsic complexity, 3) evaluation experiments for time dependent recommendation.': 1.0986123085021973, 'The paper by the same authors (NIPS 2016) describes a similar model of continuous time coevolution, and a similar evaluation.': 1.0985283851623535, 'The difference lies in the details of the model: the point process model is not the same and of the latent factor dynamic model is slightly different, but the modeling approach and the arguments are exactly the same.': 1.0986075401306152, 'By the end, one does not know what makes this model perform better than the one proposed in NIPS, is it the choice for the process, the new parametrization?': 1.0986121892929077, 'Both are quite similar.': 1.0984612703323364, 'There is no justification on the choice of the specific form of the point process in the two papers.': 1.0985983610153198, 'Did the authors tried other forms as well?': 1.0984538793563843, 'The same remark applies for the form of the dynamical process: the non-linearity used for the modeling of the latent user/item vectors here is limited to a sigmoid function, which probably does not change much w.r.t.': 1.0986123085021973, 'a linear model, but there is no evidence of the role of this non linearity in the paper.': 1.0986123085021973, 'Note that there are some inconsistencies between the results in the two papers.': 1.0986123085021973, 'Concerning the evaluation, the authors introduce two criteria.': 1.0986123085021973, 'I did not get exactly how they evaluate the item recommendation: it is mentioned that at each time t, the model predicts the item the user will interact with.': 1.0986123085021973, 'Do you mean, the next item the user will interact with after time t?': 1.0986123085021973, 'For the time prediction, why is it a relevant metric for recommendation?': 1.0986123085021973, 'A comparison of the complexity, or execution time of the different methods would be helpful.': 1.0986123085021973, 'The complexity of your method is apparently proportional to #items*#users, what are the complexity limits of your methods.': 1.0986123085021973, 'Overall, the paper is quite nice and looks technically sound, albeit many details are missing.': 1.0986123085021973, 'On the other hand, I have a mixed feeling because of the similarity with NIPS paper.': 1.0986123085021973, 'The authors should have make a better work at convincing us that this is not a marginal extension of previous work by the authors.': 1.0986123085021973, 'I was not convinced either by the evaluation criteria and there is no evidence that the model can be used for large datasets.': 1.0986073017120361, 'This paper proposes a method to model time changing dynamics in collaborative filtering.': 1.003110647201538, 'Comments:': 1.0986123085021973, '1) The main idea of the paper is build upon similar to a previous work by the same group of author (Wang et.al KDD), the major difference appears to be change some of the latent factors to be RNN': 1.0147210359573364, '2)': 1.0986123085021973, 'The author describes a BPTT technique to train the model': 1.0161257982254028, '3)': 1.0986123085021973, 'The author introduced time prediction as a new metric to evaluate the effectiveness of time dependent model.': 1.066097617149353, 'However, this need to be condition on a given user-item pair.': 1.097713828086853, '4) It would be interesting to consider other metrics, for example': 1.0986123085021973, 'The switching time where a user changes his/her to another item': 1.0981411933898926, 'Jointly predict the next item and switching time.': 1.0986123085021973, 'In summary, this is a paper that improves over an existing work on time dynamics model in recommender system.': 0.8814341425895691, 'The time prediction metric is interesting and opens up interesting discussion on how we should evaluate recommender systems when time is involved (see also comments).': 1.0986121892929077, 'The paper seeks to predict user events (interactions with items at a particular point in time).': 1.0986123085021973, 'Roughly speaking the contributions are as follows:': 1.0986123085021973, ""(a) the paper models the co-evolutionary process of users' preferences toward items"": 1.0986123085021973, '(b) the paper is able to incorporate external sources of information, such as user and item features': 1.0985796451568604, '(c) the process proposed is generative, so is able to estimate specific time-points at which events occur': 1.082957148551941, '(d) the model is able to account for non-linearities in the above': 1.0986120700836182, 'Following the pre-review questions, I understand that it is the combination of (a) and (c) that is the most novel aspect of the paper.': 1.0986123085021973, 'A fully generative process which can be sampled is certainly nice (though of course, non-generative processes like regular old regression can estimate specific time points and such too, so not sure in practice how relevant this distinction is).': 1.0986123085021973, 'Other than that the above parts have all appeared in some combination in previous work, though the combination of parts here certainly passes the novelty bar.': 1.0986123085021973, ""I hadn't quite followed the issue mentioned in the pre-review discussion that the model requires multiple interactions per userXitem pair in order to fit the model (e.g. a user interacts with the same business multiple times)."": 1.0986123085021973, 'This is a slightly unusual setting compared to most temporal recommender systems work.': 1.0986123085021973, ""I question to some extent whether this problem setting isn't a bit restrictive."": 1.0986123085021973, 'That being said I take the point about why the authors had to subsample the Yelp data, but keeping only users with ""hundreds"" of events means that you\'re left with a very biased sample of the user base.': 1.0986123085021973, 'Other than the above issues, the paper is technically nice, and the experiments include strong baselines and reports good performance.': 1.0986123085021973}"
243,https://openreview.net/forum?id=HyY4Owjll,"{'This paper extends boosting to the task of learning generative models of data.': 1.0972023010253906, 'The strong learner is obtained as a geometric average of “weak learners”, which can themselves be normalized (e.g. VAE) or un-normalized (e.g. RBMs) generative models (genBGM), or a classifier trained to discriminate between the strong learner at iteration T-1 and the true data distribution (discBGM).': 0.6671993136405945, 'This latter method is closely related to Noise Contrastive Estimation, GANs, etc.': 0.9394528865814209, 'The approach benefits from strong theoretical guarantees, with strict conditions under which each boosting iteration is guaranteed to improve the log-likelihood.': 1.0130531787872314, 'The downside of the method appears to be the lack of normalization constant for the resulting strong learner and the use of heuristics to weight each weak learner (which seems to matter in practice, from Sec. 3.2).': 1.0971065759658813, 'The discriminative approach further suffers from an expensive training procedure: each round of boosting first requires generating a “training set” worth of samples from the previous strong learner, where samples are obtained via MCMC.': 1.098604679107666, 'The experimental section is clearly the weak point of the paper.': 1.0986123085021973, 'The method is evaluated on a synthetic dataset, and a single real-world dataset, MNIST: both for generation and as a feature extraction mechanism for classification.': 1.098604440689087, 'Of these, the synthetic experiments were the clearest in showcasing the method.': 1.0986123085021973, 'On MNIST, the baseline models are much too weak for the results to be convincing.': 1.0986123085021973, 'A modestly sized VAE can obtain 90 nats within hours on a single GPU, clearly an achievable goal.': 1.0986123085021973, 'Furthermore, despite arguments to the contrary, I firmly believe that mixing base learners is an academic exercise, if only because of the burden of implementing K different models & training algorithms.': 1.098361849784851, 'This section fails to answer a more fundamental question: is it better to train a large VAE by maximizing the elbow, or e.g. train 10 iterations of boosting, using VAEs 1/10th the size of the baseline model ?': 1.097543716430664, 'Experimental details are also lacking, especially with respect to the sampling procedure used to draw samples from the BGM.': 1.0599764585494995, 'The paper would also benefit from likelihood estimates obtained via AIS.': 1.0986123085021973, 'With regards to novelty and prior work, there is also a missing reference to “Self Supervised Boosting” by Welling et al': 1.0986123085021973, '[R1].': 1.0986123085021973, 'After a cursory read through, there seems to be strong similarities to the GenBGM approach which ought to be discussed.': 1.0986121892929077, 'Overall, I am on the fence.': 1.0986123085021973, 'The idea of boosting generative models is intriguing, seems well motivated and has potential for impact.': 1.0986123085021973, 'For this reason, and given the theoretical contributions, I am willing to overlook some of the issues highlighted above, and hope the authors can address some of them in time for the rebuttal.': 1.0803624391555786, '[R1] https://papers.nips.cc/paper/2275-self-supervised-boosting.pdf': 1.0986123085021973, 'PROS:': 1.0986123085021973, 'Novel and intriguing idea': 1.0986123085021973, 'Strong theoretical guarantees': 1.0986123085021973, 'CONS:': 1.0986123085021973, 'Resulting boosted model is un-normalized': 1.0986123085021973, 'Discriminator based boosting is expensive, due to sampling via MCMC': 1.0986123085021973, 'Weak experimental section': 1.0986123085021973, 'The authors propose two approaches to combine multiple weak generative models into a stronger one using principles from boosting.': 1.0886491537094116, 'The approach is simple and elegant and basically creates an unnormalized product of experts model, where the individual experts are trained greedily to optimize the overall joint model.': 1.0983562469482422, 'Unfortunately, this approach results in a joint model that has some undesirable properties: a unknown normalisation constant for the joint model and therefore an intractable log-likelihood on the test set; and it makes drawing exact samples from the joint model intractable.': 1.0986123085021973, 'These problems can unfortunately not be fixed by using different base learners, but are a direct result of the product of experts formulation of boosting.': 0.9173917770385742, 'The experiments on 2 dimensional toy data illustrate that the proposed procedure works in principle and that the boosting formulation produces better results than individual weak learners and better results than e.g. bagging.': 1.0986121892929077, 'But the experiments on MNIST are less convincing: Without an undisputable measure like e.g. log-likelihood it is hard to draw conclusions from the samples in Figure 2; and visually they look weak compared to even simple models like e.g. NADE.': 1.0986123085021973, 'I think the paper could be improved significantly by adding a quantitative analysis: investigating the effect of combining undirected (e.g. RBM), undirected (e.g. VAE) and autoregressive (e.g. NADE) models and by measuring the improvement over the number of base learners.': 1.098607063293457, 'But this would require a method to estimate the partition function Z or estimating some proxy.': 1.0986123085021973, 'The paper proposes two approaches to boosting generative models, both based on likelihood ratio estimates.': 1.0986123085021973, 'The approaches are evaluated on synthetic data, as well as on MNIST dataset for the tasks of generating samples and semi-supervised learning.': 1.0986123085021973, 'While the idea of boosting generative models and the proposed methods are interesting, the reviewer finds the experiments unconvincing for the following reasons.': 1.0986123085021973, ""1. The bagging baseline in section 3.1 seems to be just refitting a model to the same dataset, raising the probability to power alpha, and renormalizing. This makes it more peaked, but it's not clear why this is a good baseline. Please let me know if I misunderstood the procedure."": 0.968134880065918, '2. The sample generation experiment in section 3.2 uses a very slowly converging Markov chain, as can be seen in the similarity of plots c and f, d and g, e and h. It seems unlikely therefore that the resulting samples are from the stationary distribution. A qualitative evaluation using AIS seems to be necessary here.': 0.5900788903236389, '3. In the same section the choices for alphas seem quite arbitrary - what happens when a more obvious choice of alpha_i=1 for all i is made?': 1.0986121892929077, '4. It seems hard to infer anything from the semisupervised classification results reported: the baseline RBM seems to perform as well as the boosted models.': 1.0986121892929077, 'The work is mostly clearly written and (as far as the reviewer knows) original.': 1.0986123085021973}"
244,https://openreview.net/forum?id=Hyanrrqlg,"{'The paper proposed a very complex compression and reconstruction method (with additional parameters) for reducing the memory footprint of deep networks.': 1.0985759496688843, 'The authors show that this complex proposal is better than simple hashed net proposal.': 1.0986113548278809, 'One question: Are you also counting the extra parameters for reconstruction network for the memory comparison?': 1.0561248064041138, 'Otherwise, the experiments are unfair.': 1.0986123085021973, 'Since hashing and reconstruction cost will dominate the feed-forward and back-propagation updates, it is imperative to compare the two methods on running time.': 0.7372758388519287, 'For hashed net, this is quite simple, yet it created an additional bottleneck.': 1.0986123085021973, 'Please also show the impact on running time.': 1.0986123085021973, 'Small improvements for a big loss in computational cost may not be acceptable.': 1.0986123085021973, 'I am not convinced that this method will be lightweight.': 1.0986123085021973, 'If we are allowed complicated compression and reconstruction then we can use any off-shelf methods, but the cost will be huge': 1.0986123085021973, 'The paper presents a method to reduce the memory footprint of a neural network at some increase in the computation cost.': 1.0978630781173706, ""This paper is a generalization of HashedNets by Chen et al. (ICML'15) where parameters of a neural network are mapped into smaller memory arrays using some hash functions with possible collisions."": 1.0986123085021973, 'Instead of training the original parameters, given a hash function, the elements of the compressed memory arrays are trained using back-propagation.': 1.0985817909240723, 'In this paper, some new tricks are proposed including: (1) the compression space is shared among the layers of the neural network (2) multiple hash functions are used to reduce the effects of collisions (3) a small network is used to combine the elements retrieved from multiple hash tables into a single parameter.': 1.09861159324646, 'Fig 1 of the paper describes the gist of the approach vs. HashedNets.': 1.0985372066497803, 'On the positive side,': 0.5261837244033813, '+': 1.0986123085021973, 'The proposed ideas are novel and seem useful.': 1.0816411972045898, '+ Some theoretical justification is presented to describe why using multiple hash functions is a good idea.': 0.47918373346328735, '+ All of the experiments suggest that the proposed MFH approach outperforms HashedNets.': 1.0986123085021973, 'On the negative side,': 1.0986123085021973, 'The computation cost seems worse than HashedNets and is not discussed.': 1.0986123085021973, 'Immediate practical implication of the paper is not clear given that alternative pruning strategies perform better and should be faster at inference.': 1.0986123085021973, 'That said, I believe this paper benefits the deep learning community as it sheds light into ways to share parameters across layers of a neural network potentially leading to more interesting follow-ups.': 1.0986123085021973, 'I recommend accept, while asking the authors to address the comments below.': 1.0986123085021973, 'More comments:': 1.0986123085021973, 'Please discuss the computation cost for both HashedNets and MFH for both fully connected and convolutional layers.': 1.0986123085021973, 'Are the experiments only run once for each configuration?': 1.0986123085021973, 'Please run multiple times and report average / standard error.': 1.0986123085021973, 'For completeness, please add U1 results to Table 1.': 1.0986123085021973, 'In Table 1, U4-G3 is listed twice with two different numbers.': 1.0986123085021973, 'Some sentences are not grammatically correct.': 1.0986123085021973, 'Please improve the writing.': 1.0986123085021973, 'The paper describes an extension of the HasheNets work, with several novel twists.': 1.0986123085021973, 'Instead of using a single hash function, the proposed HFH approach uses multiple hash function to associate each ""virtual"" (to-be-synthesized) weight location to several components of an underlying parameter vector (shared across all layers).': 1.0983600616455078, 'These components are then passed through a small MLP to synthesize the final weight.': 1.0986123085021973, 'This is an interesting and novel idea, and the experiments demonstrate that it improves substantially over HashedNets.': 1.0986123085021973, 'However, HashedNets is not a particularly compelling technique for neural network model compression, especially when compared with more recent work on pruning- and quantization-based approaches.': 0.5309110879898071, 'The experiments in this paper demonstrate that the proposed approach yields worse accuracy at the same compression ratios as pruning-based approaches, while providing no runtime speedup benefits.': 1.0986123085021973, ""While the authors mention the technique is only 20% slower (which I am pleasantly surprised by), I don't understand why this technique should ever be used over competing approaches for the kinds of networks the authors present experimental results on."": 1.0986123085021973, 'The authors suggest that the technique could be combined with pruning based approaches... this may be true, but no experiments to this effect are provided.': 1.0986123085021973, ""The paper also suggests that ease of setting the compression ratio is a benefit of HFH, but I don't think that's a sufficient win to justify the numerous other downsides (in accuracy and speed)."": 1.0986123085021973, 'In response to a question, the authors point out that the technique works very well for compressing embeddings, and for this setting the technique does appear like a genuinely useful contribution, given the marginal overhead and substantial train-time benefits.': 1.0986123085021973, 'If the paper focused on this setting and showed experimental results on e.g. language modeling tasks or other scenarios with high-dimensional sparse/one-hot inputs require large embedding layers, I could enthusiastically recommend acceptance.': 1.0986123085021973, ""However for the CNN and MLP networks which are the main focus of the experiments, I don't think the technique is suitable, as much as I like the basic idea."": 1.0986121892929077}"
245,https://openreview.net/forum?id=HycUbvcge,"{'The authors propose a method that extends the non-linear two-view representation learning methods, and the linear multiview techniques, and combines information from multiple sources into a new non-linear representation learning techniques.': 1.0986123085021973, 'In general, the method is well described and seems to lead to benefits in different experiments of phonetic transcription of hashtag recommendation.': 1.0986123085021973, 'Even if the method is mostly a extension of classical tools (the scheme learns a (deep) network for each view essentially), the combination of the different sources of information seems to be effective for the studied datasets.': 0.4383327066898346, 'It would be interesting to add or discuss the following issues:': 1.0986123085021973, 'what is the complexity of the proposed method, esp.': 1.0951356887817383, 'the representation learning part?': 0.6681725382804871, 'would there by any alternative solution to combine the different networks/views?': 1.0454339981079102, 'That could make the proposed solution more novel.': 1.0986123085021973, 'the experimental settings, especially in the synthetic experiments, should be more detailed.': 0.46308523416519165, 'If possible, the datasets should be made available to encourage reproducibility.': 1.0419566631317139, 'the related work is far from complete unfortunately, especially from the perspective of the numerous multiview/multi-modal/multi-layer algorithms that have been proposed in the literature, in different applications domaines like image retrieval or classification, or bibliographic data for example (authors like A. Kumar, X. Dong, Ping-Yu Chen, M. Bronstein, and many others have proposed works in that direction in the last 5 years).': 1.0365197658538818, 'No need to compare to all these works obviously, but a more complete description of the related could help appreciating better the true benefits of DGCCA.': 1.0986086130142212, 'This paper proposes a deep extension of generalized CCA.': 1.0986119508743286, 'The main contribution of the paper is deriving the gradient update for the GCCA objective.': 1.0932211875915527, 'I disagree with the claim that “this is the first Multiview representation learning technique that combines the flexibility of nonlinear representation learning with the statistical power of incorporating information from many independent resources or views”.': 1.0986123085021973, '[R1] proposes a Multiview representation learning method which is both non-linear and capable of handling more than 2 views.': 0.5545156002044678, 'This is very much relevant to what authors are proposing.': 1.0986123085021973, 'The objective function proposed in [R1] maximizes the correlation between views and minimizes the self and cross reconstruction errors.': 1.0986123085021973, 'This is intuitively similar to nonlinear version of PCA+CCA for multiple views.': 1.0986077785491943, 'Comparing these 2 methods is crucial to prove the usefulness of DGCCA and the paper is incomplete without this comparison.': 1.0986123085021973, 'Authors should also change their strong claim.': 1.0986123085021973, 'Related work section is minimal.': 1.0986123085021973, 'There are significant advances in 2-view non-linear representation learning which are worth mentioning.': 1.0986123085021973, 'References:': 1.0986123085021973, '[R1] Janarthanan Rajendran, Mitesh M. Khapra, Sarath Chandar, Balaraman Ravindran: Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning.': 1.0986123085021973, 'HLT-NAACL 2016: 171-181': 1.0986123085021973, 'The proposed method is simple and elegant; it builds upon the huge success of gradient based optimization for deep non-linear function approximators and combines it with established (linear) many-view CCA methods.': 1.0986123085021973, 'A major contribution of this paper is the derivation of the gradients with respect to the non-linear encoding networks which project the different views into a common space.': 1.0986123085021973, 'The derivation seems correct.': 1.0986123085021973, 'In general this approach seems very interesting and I could imagine that it might be applicable to many other similarly structured problems.': 1.0986123085021973, 'The paper is well written; but it could be enhanced with an explicit description of the complete algorithm which also highlights how the joint embeddings G and U are updated.': 1.0986123085021973, 'I don’t have prior experience with CCA-style many-view techniques and it is therefore hard for me to judge the practical/empirical progress presented here.': 1.0986123085021973, 'But the experiments seem reasonable convincing; although generally only performed on small and medium sized datasets.': 1.0986123085021973, 'Detailed comments:': 1.0986123085021973, 'The colours or the sign of the x-axis in figure 3b seem to be flipped compared to figure 4.': 1.0986123085021973, 'It would be nice to additionally see a continuous (rainbow-coloured) version for Figures 2, 3 and 4 to better identify neighbouring datapoints; but more importantly: I’d like to see how the average reconstruction error between the individual network outputs and the learned representation develop during training.': 1.0986123085021973, 'Is the mismatch between different views on a validation/test-set a useful metric for cross validation?': 1.0986123085021973, 'In general, it seems the method is sensitive to regularization and hyperparameter selection  (because it has many more parameters compared to GCCA and different regularization parameters have been chosen for different views) and I wonder if there is a clear metric to optimize these.': 1.0986123085021973}"
246,https://openreview.net/forum?id=HyecJGP5ge,"{'The authors propose a simple modification of online dictionary learning: inspired by neurogenesis, they propose to add steps of atom addition, or atom deletion, in order to extent the online dictionary learning algorithm algorithm of Mairal et al.': 1.0986123085021973, 'Such extensions helps to adapt the dictionary to changing properties of the data.': 1.0985102653503418, 'The online adaptation is very interesting, even if it is quite simple.': 0.692086398601532, 'The overall algorithm is quite reasonable, but not always described in sufficient details: for example, the thresholds or conditions for neuronal birth or death are not supported by a strong analysis, even if the resulting algorithm seems to perform well on quite extensive experiments.': 1.0986123085021973, 'The overall idea is nevertheless interesting (even if not completely new), and the paper generally well written and pretty easy to follow.': 1.0985461473464966, 'The analysis is however quite minimal: it could have been interesting to study the evolving properties of the dictionary, to analyse its accuracy for following the changes in the data, etc.': 1.0986123085021973, 'Still: this is a nice work!': 1.0986123085021973, 'The paper is interesting, it relates findings from neurscience and biology to a method for sparse coding that is adaptive and able to automatically generate (or even delete) codes as new data is coming, from a nonstationary distribution.': 1.0986123085021973, 'I have a few points to make:': 1.0986120700836182, '1. the algorithm could be discussed more, to give a more solid view of the contribution. The technique is not novel in spirit. Codes are added when they are needed, and removed when they dont do much.': 1.0332614183425903, '2. Is there a way to relate the organization of the data to the behavior of this method? In this paper, buildings are shown first, and natural images (which are less structured, more difficult) later. Is this just a way to perform curriculum learning? What happens when data simply changes in structure, with no apparent movement from simple to more complex (e.g. from flowers, to birds, to fish, to leaves, to trees etc)': 0.8250024318695068, 'In a way, it makes sense to see an improvement when the training data has such a structure, by going from something artificial and simpler to a more complex, less structured domain.': 1.0986123085021973, 'The paper is interesting, the idea useful with some interesting insights.': 1.0986123085021973, 'I am not sure it is ready for publication yet.': 1.0986123085021973, ""I'd like to thank the authors for their detailed response and clarifications."": 1.0986123085021973, 'This work proposes new training scheme for online sparse dictionary learning.': 1.0917611122131348, 'The model assumes a non-stationary flow of the incoming data.': 1.0883697271347046, 'The goal (and the challenge) is to learn a model in an online manner in a way that is capable of  adjusting to the new incoming data without forgetting how to represent previously seen data.': 1.0980665683746338, 'The proposed approach deals with this problem by incorporating a mechanism for adding or deleting atoms in the dictionary.': 1.0904414653778076, 'This procedure is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus.': 1.0986114740371704, 'The paper has two main innovations over the baseline approach (Mairal et al): (i) “neuronal birth” which represents an adaptive way of increasing the number of atoms in the dictionary (ii) ""neuronal death"", which corresponds to removing “useless” dictionary atoms.': 1.0986117124557495, 'Neural death is implemented by including an group-sparsity regularization to the dictionary atoms themselves (the group corresponds to a column of the dictionary).': 1.0986123085021973, 'This promotes to shrink to zero atoms that are not very useful, keeping controlled the increase of the dictionary size.': 1.0986123085021973, 'I believe that the strong side of the paper is its connections with the adult neurogenesis phenomenon, which is, in my opinion a very nice feature.': 1.0986121892929077, 'The paper is very well written and easy to follow.': 1.0986123085021973, 'On the other hand, the overall technique is not very novel.': 1.0986123085021973, 'Although not exactly equivalent, similar ideas have been explored.': 1.0986123085021973, 'While the neural death is implemente elegantly with a sparsity-promoting regularization term, the neural birth is performed by relying on heuristics that measure how well the dictionary can represent new incoming data.': 1.0986123085021973, 'Which depending on the ""level"" of non-stationarity in the incoming data (or presence of outliers) could be difficult to set.': 1.0986123085021973, 'Still, having adaptive dictionary size is very interesting.': 1.0986123085021973, 'The authors could also cite some references in model selection literature.': 1.0986123085021973, 'In particular, some ideas such as MDL have been used for automatically selecting the dictionary size (I believe this work does not address the online setting, but still its a relevant reference to have).': 1.0986123085021973, 'For instance,': 1.0986123085021973, 'Ramirez, Ignacio, and Guillermo Sapiro.': 1.0986123085021973, '""An MDL framework for sparse coding and dictionary learning.""': 1.0986123085021973, 'IEEE Transactions on Signal Processing 60.6 (2012): 2913-2927.': 1.0986123085021973}"
247,https://openreview.net/forum?id=HyenWc5gx,"{'This paper proposes a method for transfer learning, i.e. leveraging a network trained on some original task A in learning a new task B, which not only improves performance on the new task B, but also tries to avoid degradation in performance on A.': 0.49343788623809814, 'The general idea is based on encouraging a model trained on A, while training on the new task B, to match fake targets produced by the model itself but when it is trained only on the original task A.': 0.9637057781219482, 'Experiments show that this method can help in improving the result on task B, and is better than other baselines, including standard fine-tuning.': 0.9171755313873291, 'General comments/questions:': 1.0969504117965698, 'As far as I can tell, there is no experimental result supporting the claim that your model still performs well on the original task.': 1.0986123085021973, 'All experiments show that you can improve on the new task only.': 1.0986123085021973, 'The introduction makes a strong statements about the distilling logical rule engine into a neural network, which I find a bit misleading.': 1.0986123085021973, 'The approach in the paper is not specific to transferring from logical rules (as stated in the Sec 2) and is simply relying on the rule engine to provide labels for unlabelled data.': 1.0986123085021973, 'One of the obvious baselines to compare with your approach is standard multi-task learning on both tasks A and B together.': 1.0986108779907227, 'That is, you train the model from scratch on both tasks simultaneously (which sharing parameters).': 1.0986123085021973, 'It is not clear this is the same as what is referred to in Sec. 8 as ""joint training"".': 1.0965121984481812, 'Can you please explain more clearly what you refer to as joint training?': 1.0985608100891113, ""Why can't we find the same baselines in both Table 2 and Table 3?"": 0.9875297546386719, 'For example Table 2 is missing ""joint training"", and Table 3 is missing GRU trained on the target task.': 1.0986117124557495, 'While the idea is presented as a general method for transfer learning, experiments are focused on one domain (sentiment analysis on SemEval task).': 1.0986123085021973, 'I think that either experiments should include applying the idea on at least one other different domain, or the writing of the paper should be modified to make the focus more specific to this domain/task.': 1.0986120700836182, 'Writing comments': 1.0414201021194458, 'The writing of the paper in general needs some improvement, but more specifically in the experiment section, where experiment setting and baselines should be explained more concisely.': 1.096868634223938, 'Ensemble methodology paragraph does not fit the flow of the paper.': 1.0879874229431152, 'I would rather explain it in the experiments section, rather than including it as part of your approach.': 1.0852388143539429, 'Table 1 seems like reporting cross-validation results, and I do not think is very informative to general reader.': 1.0983953475952148, 'This paper proposes a regularization technique for neural network training that relies on having multiple related tasks or datasets in a transfer learning setting.': 1.0986120700836182, 'The proposed technique is straightforward to describe and can also leverage external labeling systems perhaps based on logical rules.': 1.0986123085021973, 'The paper is clearly written and the experiments seem relatively thorough.': 1.0986123085021973, 'Overall this is a nice paper but does not fully address how robust the proposed technique is.': 1.039819598197937, 'For each experiment there seems to be a slightly different application of the proposed technique, or a lot of ensembling and cross validation.': 1.0856170654296875, 'I can’t figure out if this is because the proposed technique does not work well in general and thus required a lot of fiddling to get right in experiments, or if this is simply an artifact of ad-hoc experiments to try and get the best performance overall.': 1.0986106395721436, 'If more datasets or addressing this issue directly in discussion was able to show this the strengths and limitations of the proposed technique more clearly, this could be a great paper.': 1.0986008644104004, 'Overall the proposed method seems nice and possibly useful for other problems.': 1.0985844135284424, 'However in the details of logical rule distillation and various experiment settings it seems like there is a lot of running the model many times or selecting a particular way of reusing the models and data that makes me wonder how robust the technique is or whether it requires a lot of trying various approaches, ensembling, or picking the best model from cross validation to show real gains.': 1.0986123085021973, 'The authors could help by discussing this explicitly for all experiments in one place rather than listing the various choices / approaches in each experiment.': 1.0986123085021973, 'As an example, these sorts of phrases make me very unsure how reliable the method is in practice versus how much the authors had to engineer this regularizer to perform well:': 1.098611831665039, '“We noticed that equation 8 is actually prone to overfitting away from a good solution on the test set although it often finds a pretty good one early in training.': 1.0986123085021973, '“': 1.0986123085021973, 'The introduction section should first review the definitions of transfer learning vs multi-task learning to make the discussion more clear.': 1.0986123085021973, 'It also deems justification why “catastrophic forgetting” is actually a problem.': 1.0986123085021973, 'If the final target task is the only thing of interest then forgetting the source task is not an issue and the authors should motivate why forgetting matters in their setting.': 1.0984705686569214, 'This paper explores sequential transfer so it’s not obvious why forgetting the source task matters.': 1.0979971885681152, 'Section 7 introduces the logical rules engine in a fairly specific context.': 1.0986123085021973, 'Rather it would be good state more generally what this system entails to help people figure out how this method would apply to other problems.': 1.0986123085021973, 'This paper introduces a new method for transfer learning that avoids the catastrophic forgetting problem.': 1.098595380783081, 'It also describes an ensembling strategy for combining models that were learned using transfer learning from different sources.': 1.09861159324646, 'It puts all of this together in the context of recurrent neural networks for text analytics problems, to achieve new state-of-the-art results for a subtask of the SemEval 2016 competition.': 1.0986120700836182, 'As the paper acknowledges, 1.5% improvement over the state-of-the-art is somewhat disappointing considering that it uses an ensemble of 5 quite different networks.': 1.0986123085021973, 'These are interesting contributions, but due to the many pieces, unfortunately, the paper does not seem to have a clear focus.': 1.0986123085021973, ""From the title and abstract/conclusion I would've expected a focus on the transfer learning problem."": 1.0986123085021973, ""However, the description of the authors' approach is merely a page, and its evaluation is only another page."": 1.0986095666885376, 'In order to show that this idea is a new methodological advance,': 0.12480290234088898, ""it would've been good to show that it also works in at least one other application (e.g., just some multi-task supervised learning problem)."": 0.9137505292892456, 'Rather, the paper takes a quite domain-specific approach and discusses the pieces the authors used to obtain state-of-the-art performance for one problem.': 1.0986123085021973, 'That is OK, but I would\'ve rather expected that from a paper called something like ""Improved knowledge transfer and distillation for text analytics"".': 1.0986123085021973, 'If accepted, I encourage the authors to change the title to something along those lines.': 0.6632876992225647, ""The many pieces also made it hard for me to follow the authors' train of thought."": 0.42758476734161377, ""I'm sure the authors had a good reason for their section ordering, but I didn't see the red thread in it."": 0.9901267886161804, 'How about re-organizing the sections as follows to discuss one contribution at a time?': 1.0985684394836426, '1,2,4,3,8 including 6, put 9 into an appendix and point to it from here, 7, 5, 10.': 0.42567092180252075, 'That would first discuss the transfer learning piece (4, and experiments potentially in a subsection with previous sections 3,8,6), then discuss the distillation of logical rules (7), and then discuss ensembling and experiments for it (5 and 10).': 1.064008116722107, 'One clue that the current structure is suboptimal is that there are 11 sections...': 1.0984026193618774, ""I like the authors' idea for transfer learning without catastropic forgetting, and I must admit I would've rather liked to read a paper solely about that (studying where it works, and where it fails) than about the many other topics of the paper."": 1.0924561023712158, 'I weakly vote for acceptance since I like the ideas, but if the paper does not make it in, I would suggest that the authors consider splitting it into two papers, each of which could hopefully be more focused.': 0.9187742471694946}"
248,https://openreview.net/forum?id=HyoST_9xl,"{'Training highly non-convex deep neural networks is a very important practical problem, and this paper provides a great exploration of an interesting new idea for more effective training.': 1.0986123085021973, 'The empirical evaluation both in the paper itself and in the authors’ comments during discussion convincingly demonstrates that the method achieves consistent improvements in accuracy across multiple architectures, tasks and datasets.': 1.0986123085021973, 'The algorithm is very simple (alternating between training the full dense network and a sparse version of it), which is actually a positive since that means it may get adapted in practice by the research community.': 1.0986123085021973, 'The paper should be revised to incorporate the additional experiments and comments from the discussion, particularly the accuracy comparisons with the same number of epochs.': 1.0986123085021973, 'Summary:': 1.0986123085021973, 'The paper proposes a model training strategy to achieve higher accuracy.': 1.0986099243164062, 'The issue is train a too large model and you going to over-fit and your model will capture noise.': 1.0986123085021973, 'Prune models or make it too small then it will miss important connections and under-fit.': 1.0986123085021973, 'Thus, the proposed method involves various training steps: first they train a dense network, then prune it making it sparse then train a sparse network and finally they add connections back and train the model as dense again (DSD).': 1.0986087322235107, 'The DSD method is generic method that can be used in CNN/RNN/LSTM.': 1.098266839981079, 'The reasons why models have better accuracy after DSD are: escape of saddle point, sparsity makes model more robust to noise and symmetry break allowing richer representations.': 1.0986120700836182, 'Pro:': 1.0986123085021973, 'The main point that this paper wants to show is that a model has the capacity to achieve higher accuracy, because it was shown that it is possible to compress a model without losing accuracy.': 1.0986121892929077, 'And lossless compression means that there’s significant redundancy in the models that were trained using current training methods.': 1.0984196662902832, 'This is an important observation that large models can get better accuracies as better training schemes are used.': 1.0888882875442505, 'Cons & Questions:': 0.4179922342300415, 'The issue is that the accuracy is slightly increased (2 or 3%) for most models.': 1.098583459854126, 'And the question is what is the price paid for this improvement?': 0.9888007044792175, 'Resource and performance concerns arises because training a large model is computationally expensive (hours or even days using high performance GPUs).': 1.060044765472412, 'Second question, can I keep adding Dense, Sparse and Dense training iterations to get higher and higher accuracy improvement?': 0.4312717914581299, 'Are there limitations to this DSDSD… approach?': 1.0860509872436523, 'This paper presents a training strategy for deep networks.': 1.0986002683639526, 'First, the network is trained in a standard fashion.': 1.0986123085021973, 'Second, small magnitude weights are clamped to 0; the rest of the weights continue to be trained.': 1.0986123085021973, 'Finally, all the weights are again jointly trained.': 1.0986123085021973, 'Experiments on a variety of image, text, and speech datasets demonstrate the approach can obtain high-quality results.': 1.0986123085021973, 'The proposed idea is novel and interesting.': 1.0986123085021973, 'In a sense it is close to Dropout, though as noted in the paper the deterministic weight clamping method is different.': 1.0986123085021973, 'The main advantage of the proposed method is its simplicity.': 1.0986123085021973, 'Three hyper-parameters are needed: the number of weights to clamp to 0, and the numbers of epochs of training used in the first dense phase and the sparse phase.': 1.0986123085021973, 'Given these, it can be plugged in to training a range of networks, as shown in the experiments.': 1.0986123085021973, 'The concern I have is regarding the current empirical evaluation.': 1.0986123085021973, 'As noted in the question phase, it seems the baseline methods are not trained for as many epochs as the proposed method.': 1.0986123085021973, 'Standard tricks, such as dropping the learning rate upon ""convergence"" and continuing to learn, can be employed.': 1.0986123085021973, 'The response seems to indicate that these approaches can be effective.': 1.0986123085021973, 'I think a more thorough empirical analysis of performance over epochs, learning rates, etc. would strengthen the paper.': 1.0986123085021973, 'An exploration regarding the sparsity hyper-parameter would also be interesting.': 1.0986123085021973}"
249,https://openreview.net/forum?id=Hyq4yhile,"{'This paper presents an approach for skills transfer from one task to another in a control setting (trained by RL) by forcing the embeddings learned on two different tasks to be close (L2 penalty).': 0.6604408025741577, 'The experiments are conducted in MuJoCo, with a set of experiments being from the state of the joints/links (5.2/5.3) and a set of experiments on the pixels (5.4).': 0.5328424572944641, 'They exhibit transfer from arms with different number of links, and from a torque-driven arm to a tendon-driven arm.': 1.0895698070526123, 'One limitation of the paper is that the authors suppose that time alignment is trivial, because the tasks are all episodic and in the same domain.': 0.7521126866340637, 'Time alignment is one form of domain adaptation / transfer that is not dealt with in the paper, that could be dealt with through subsampling, dynamic time warping, or learning a matching function (e.g. neural network).': 1.0984182357788086, 'General remarks: The approach is compared to CCA, which is a relevant baseline.': 1.0984296798706055, 'However, as the paper is purely experimental, another baseline (worse than CCA) would be to just have the random projections for ""f"" and ""g"" (the embedding functions on the two domains), to check that the bad performance of the ""no transfer"" version of the model is due to over-specialisation of these embeddings.': 1.0983731746673584, 'I would also add (for information) that the problem of learning invariant feature spaces is also linked to metric learning (e.g. [Xing et al. 2002]).': 1.0819400548934937, 'More generally, no parallel is drawn with multi-task learning in ML.': 0.9763243198394775, 'In the case of knowledge transfer (4.1.1), it may make sense to anneal \\alpha.': 0.9548274874687195, 'The experiments feel a bit rushed.': 1.0986123085021973, 'In particular, the performance of the baseline being always 0 (no transfer at all) is uninformative, at least a much bigger sample budget should be tested.': 1.0986123085021973, 'Also, why does Figure 7.b contain no ""CCA"" nor ""direct mapping"" results?': 1.0986123085021973, 'Another concern that I have with the experiments: (if/how) did the author control for the fact that the embeddings were trained with more iterations in the case of doing transfer?': 1.0986123085021973, 'Overall, the study of transfer is most welcomed in RL.': 1.0986123085021973, 'The experiments in this paper are interesting enough for publication, but the paper could have been more thorough.': 1.0986123085021973, 'This paper explores transfer in reinforcement learning between agents that may be morphologically distinct.': 1.0986123085021973, 'The key idea is for the source and target agent to have learned a shared skill, and then to use this to construct abstract feature spaces to enable the transfer of a new unshared skill in the source agent to the target agent.': 0.43555301427841187, 'The paper is related to much other work on transfer that uses shared latent spaces, such as CCA and its variants, including manifold alignment and kernel CCA.': 1.0986123085021973, 'The paper reports on experiments using a simple physics simulator between robot arms consisting of three vs. four links.': 1.0986123085021973, 'For comparison, a simple CCA based approach is shown, although it would have been preferable to see comparisons for something more current and up to date, such as manifold alignment or kernel CCA.': 1.09837007522583, 'A three layer neural net is used to construct the latent feature spaces.': 1.0986123085021973, 'The problem of transfer in RL is extremely important, and receives less attention than it should.': 1.0986123085021973, 'This work uses an interesting hypothesis of trying to construct transfer based on shared skills between source and target agent.': 1.0986123085021973, 'This is a promising approach.': 1.0986123085021973, 'However, the comparisons to related approaches is not very up to date, and the domains are fairly simplistic.': 1.0986123085021973, 'There is little by way of theoretical development of the ideas using MDP theory.': 1.0986123085021973, 'The paper considers the problem of transferring skills between robots with different morphologies, in the context of agents that have to perform several tasks.': 1.0986123085021973, 'A core component of the proposed approach is to use a task-invariant future space, which can be shared between tasks & between agents.': 1.0986123085021973, 'Compared to previous work (Ammar et al. 2015), it seems the main contribution here is to “assume that good correspondences in episodic tasks can be extracted through time alignment” (Sec. 2).': 1.0986123085021973, 'This is an interesting hypothesis.': 1.0986123085021973, 'There is also similarity to work by Raimalwala et al (2016), but the authors argue their method is better equipped to handle non-linear dynamics.': 1.0986123085021973, 'These are two interesting hypotheses, however I don’t see that they have been verified in the presented empirical results.': 1.0986123085021973, 'In particular, the question of the pairing correspondence seems crucial.': 1.0986123085021973, 'What happens when the time alignment is not suitable.': 1.0986123085021973, 'Is it possible to use dynamic time warping (or similar method) to achieve reasonable results?': 1.0986123085021973, 'Robustness to misspecification of the pairing correspondence P seems a major concern.': 1.0986123085021973, 'In general, more comparison to other transfer methods, including those listed in Sec.2, would be very valuable.': 1.0986123085021973, 'The addition of Sec.5.1 is definitely a right step in this direction, but represents a small portion of the recent work on transfer learning.': 1.0986123085021973, 'I appreciate that other methods transfer other pieces of information (e.g. the policy), but still if the end goal is better performance, what is worth transferring (in addition to how to do the transfer) should be a reasonable question to explore.': 1.0986123085021973, 'Overall, the paper tackles an important problem, but this is a very active area of research, and further comparison to other methods would be worthwhile.': 1.0986123085021973, 'The method proposed of transferring the representation is well motivated, cleanly described, and conceptually sound.': 1.0986123085021973, 'The assumption that time alignment can be used for the state pairing seems problematic, and should be further validated.': 1.0986123085021973}"
250,https://openreview.net/forum?id=HysBZSqlx,"{'The paper presents a new environment, called Retro Learning Environment (RLE), for reinforcement learning.': 1.0986123085021973, 'The authors focus on Super Nintendo but claim that the interface supports many others (including ALE).': 1.0986123085021973, 'Benchmark results are given for standard algorithms in 5 new Super Nintendo games, and some results using a new ""rivalry metric"".': 1.0986123085021973, 'These environments (or, more generally, standardized evaluation methods like public data sets, competitions, etc.) have a long history of improving the quality of AI and machine learning research.': 1.0985885858535767, 'One example in the past few years was the Atari Learning Environment (ALE) which has now turned into a standard benchmark for comparison of algorithms and results.': 1.0986123085021973, 'In this sense, the RLE could be a worthy contribution to the field by encouraging new challenging domains for research.': 1.0986121892929077, 'That said, the main focus of this paper is presenting this new framework and showcasing the importance of new challenging domains.': 1.098524808883667, 'The results of experiments themselves are for existing algorithms.': 1.0986123085021973, 'There are some new results that show reward shaping and policy shaping (having a bias toward going right in Super Mario) help during learning.': 1.098611831665039, 'And, yes, domain knowledge helps, but this is obvious.': 1.0986123085021973, 'The rivalry training is an interesting idea, when training against a different opponent, the learner overfits to that opponent and forgets to play against the in-game AI; but then oddly, it gets evaluated on how well it does against the in-game AI!': 1.0986123085021973, 'Also the part of the paper that describes the scientific results (especially the rivalry training) is less polished, so this is disappointing.': 1.0986123085021973, ""In the end, I'm not very excited about this paper."": 0.6005516648292542, 'I was hoping for a more significant scientific contribution to accompany in this new environment.': 1.0986084938049316, ""It's not clear if this is necessary for publication, but also it's not clear that ICLR is the right venue for this work due to the contribution being mainly about the new code (for example, mloss.org could be a better 'venue', JMLR has an associated journal track for accompanying papers: http://www.jmlr.org/mloss/)"": 1.098611831665039, 'Post response:': 1.0679742097854614, 'Thank you for the clarifications.': 1.0725008249282837, 'Ultimately I have not changed my opinion on the paper.': 1.0985820293426514, ""Though I do think RLE could have a nice impact long-term, there is little new science in this paper, ad it's either too straight-forward (reward shaping, policy-shaping) or not quite developed enough (rivalry training)."": 1.098422646522522, 'This paper introduces a new reinforcement learning environment called « The Retro Learning Environment”, that interfaces with the open-source LibRetro API to offer access to various emulators and associated games (i.e. similar to the Atari 2600 Arcade Learning Environment, but more generic).': 1.0976098775863647, 'The first supported platform is the SNES, with 5 games (more consoles and games may be added later).': 1.098608136177063, 'Authors argue that SNES games pose more challenges than Atari’s (due to more complex graphics, AI and game mechanics).': 1.0986100435256958, 'Several DQN variants are evaluated in experiments, and it is also proposed to compare learning algorihms by letting them compete against each other in multiplayer games.': 0.41040199995040894, 'I like the idea of going toward more complex games than those found on Atari 2600, and having an environment where new consoles and games can easily be added sounds promising.': 0.5173991918563843, 'With OpenAI Universe and DeepMind Lab that just came out, though, I am not sure we really need another one right now.': 1.0986123085021973, 'Especially since using ROMs of emulated games we do not own is technically illegal: it looks like this did not cause too much trouble for Atari': 1.0986123085021973, 'but it might start raising eyebrows if the community moves to more advanced and recent games, especially some Nintendo still makes money from.': 1.0986123085021973, 'Besides the introduction of the environment, it is good to have DQN benchmarks on five games, but this does not add a lot of value.': 1.0591027736663818, 'The authors also mention as contribution ""A new benchmarking technique, allowing algorithms to compete against each other, rather than playing against the in-game AI"", but this seems a bit exaggerated to me: the idea of pitting AIs against each other has been at the core of many AI competitions for decades, so it is hardly something new.': 0.5210040807723999, 'The finding that reinforcement learning algorithms tend to specialize to their opponent is also not particular surprising.': 1.0986100435256958, 'Overall I believe this is an ok paper but I do not feel it brings enough to the table for a major conference.': 1.0986117124557495, ""This does not mean, however, that this new environment won't find a spot in the (now somewhat crowded) space of game-playing frameworks."": 1.0986119508743286, 'Other small comments:': 0.7227221131324768, 'There are lots of typos (way too many to mention them all)': 0.6000193357467651, 'It is said that Infinite Mario ""still serves as a benchmark platform"", however as far as I know it had to be shutdown due to Nintendo not being too happy about it': 1.098487377166748, '""RLE requires an emulator and a computer version of the console game (ROM file) upon initialization rather than a ROM file only.': 1.0986123085021973, 'The emulators are provided with RLE"" => how is that different from ALE that requires the emulator Stella which is also provided with ALE?': 1.0958658456802368, 'Why is there no DQN / DDDQN result on Super Mario?': 0.654163122177124, 'It is not clear if Figure 2 displays the F-Zero results using reward shaping or not': 0.4583994150161743, 'The Du et al reference seems incomplete': 0.9569141864776611, 'This paper presents a valuable new collection of video game benchmarks, in an extendable framework, and establishes initial baselines on a few of them.': 1.0986123085021973, 'Reward structures: for how many of the possible games have you implemented the means to extract scores and incremental reward structures?': 1.0986123085021973, 'From the github repo it looks like about 10': 0.8877332210540771, 'do you plan to add more, and when?': 1.0986123085021973, '“rivalry” training: this is one of the weaker components of the paper, and it should probably be emphasised less.': 1.0984458923339844, 'On this topic, there is a vast body of (uncited) multi-agent literature, it is a well-studied problem setup (more so than RL itself).': 1.0986123085021973, 'To avoid controversy, I would recommend not claiming any novel contribution on the topic (I don’t think that you really invented “a new method to train an agent by enabling it to train against several opponents” nor “a new benchmarking technique for agents evaluation, by enabling them to compete against each other, rather than playing against the in-game AI”).': 1.0986117124557495, 'Instead, just explain that you have established single-agent and multi-agent baselines for your new benchmark suite.': 1.0986119508743286, 'Your definition of Q-function (“predicts the score at the end of the game given the current state and selected action”) is incorrect.': 1.0986123085021973, 'It should read something like: it estimates the cumulative discounted reward that can be obtained from state s, starting with action a (and then following a certain policy).': 1.0986123085021973, 'Minor:': 1.0986077785491943, '* Eq (1): the Q-net inside the max() is the target network, with different parameters theta’': 1.0986123085021973, '* the Du et al. reference is missing the year': 1.0986123085021973, '* some of the other references should point at the corresponding published papers instead of the arxiv versions': 1.0986123085021973}"
251,https://openreview.net/forum?id=Hyvw0L9el,"{'This paper proposes an extension of PixelCNN method that can be conditioned on text and spatially-structured constraints (segmentation / keypoints).': 1.0986123085021973, 'It is similar to Reed 2016a except the extension is built on top of PixelCNN instead of GAN.': 1.0986120700836182, ""After reading the author's comment, I realized the argument is not that conditional PixelCNN is much better than conditional GAN."": 1.0986119508743286, 'I think the authors can add more discussions about pros and cons of each model in the paper.': 1.0986120700836182, 'I agree with the other reviewer that some analysis of training and generation time would be helpful.': 1.0986123085021973, 'I understand it takes O(N) instead of O(1) for PixelCNN method to do sampling, but is that the main reason that the experiments can only be conducted in low resolution (32 x 32)?': 0.8205596208572388, 'I also think since there are not quantitative comparisons, it makes more sense to show more visualizations than 3 examples.': 1.0986123085021973, 'Overall, the generated results look reasonably good and have enough diversity.': 1.0986121892929077, 'The color mistake is an issue where the author has provided some explanations in the comment.': 1.098597526550293, 'I would say the technical novelty is incremental since the extension is straightforward and similar to previous work.': 1.0986123085021973, 'I lean toward accepting because it is very relevant to ICLR community': 1.0986123085021973, 'and it provides a good opportunity for future investigation and comparison between different deep image synthesis methods.': 0.3293667733669281, 'This work focuses on conditional image synthesis in the autoregressive framework.': 1.0986123085021973, 'Based on PixelCNN, it trains models that condition on text as well as segmentation masks or keypoints.': 1.0517044067382812, 'Experiments show results for keypoint conditional synthesis on the CUB (birds) and MHP (human pose) dataset, and segmentation conditional synthesis on MS-COCO (objects).': 1.0986123085021973, 'This extension to keypoint/segment conditioning is the primary contribution over existing PixelCNN work.': 1.0986123085021973, 'Qualitative comparison is made to GAN approaches for synthesis.': 1.0848839282989502, 'Pros:': 1.0986123085021973, '(1) The paper demonstrates additional capabilities for image generation in the autoregressive framework, suggesting that it can keep pace with the latest capabilities of GANs.': 1.0986123085021973, '(2) Qualitative comparison in Figure 9 suggests that PixelCNN and GAN-based methods may make different kinds of mistakes, with PixelCNN being more robust against introducing artifacts.': 1.0986119508743286, '(3) Some effort is put forth to establish quantitative evaluation in terms of log-likelihoods (Table 1).': 1.0986123085021973, 'Cons:': 1.088233232498169, '(1) Comparison to other work is difficult and limited to qualitative results.': 1.0892928838729858, 'The qualitative results can still be somewhat difficult to interpret.': 1.0983994007110596, 'I believe supplementary material or an appendix with many additional examples could partially alleviate this problem.': 1.0986120700836182, '(2) The extension of PixelCNN to conditioning on additional data is fairly straightforward.': 0.9176108241081238, 'This is a solid engineering contribution, but not a surprising new concept.': 1.0986121892929077, '""First, it allows us to assess whether auto-regressive models are able to match the GAN results of Reed et al. (2016a).""': 1.0636991262435913, 'Does it, though?': 1.0985904932022095, 'Because the resolution is so bad.': 1.0986120700836182, ""And resolution limitations aren't addressed until the second-to-last paragraph of the paper."": 1.0985997915267944, 'And Figure 9 only shows 3 results (picked randomly?': 1.0020561218261719, 'Picked to be favorable to PixelCNN?).': 0.4548635482788086, 'That hardly seems conclusive.': 1.098610520362854, 'The segmentation masks and keypoints are pretty strong input constraints.': 1.0968154668807983, ""It's hard to tell how much coherent object and scene detail is emerging because the resolution is so low."": 1.0986123085021973, 'For example, the cows in figure 5 look like color blobs, basically.': 1.0986119508743286, 'Any color blob that follows an exact cow segmentation mask will look cow-like.': 1.0986123085021973, 'The amount of variation is impressive, though.': 0.40571820735931396, 'How can we assess how much the model is ""replaying"" training data?': 1.0986123085021973, 'Figure 8 tries to get at this, but I wonder how much each of the ""red birds"", for instance, is mostly copied from a particular training example.': 1.0986123085021973, ""I'm unsatisfied with the answers to the pre-review questions."": 1.0986121892929077, ""You didn't answer my questions."": 1.090881109237671, 'The paper would benefit from concrete numbers on training time / epochs and testing time.': 1.0986123085021973, ""You didn't say why you can't make high resolution comparisons."": 1.098610520362854, ""Yes, it's slower at test time."": 1.0982980728149414, 'Is it prohibitively slow?': 0.40726280212402344, 'Or is it slightly inconvenient?': 1.0986123085021973, ""There really aren't that many comparisons in the paper, anyway, so it if takes an hour to generate a result that doesn't seem prohibitive."": 1.0969226360321045, 'To be clear about my biases: I don\'t think PixelCNN is ""the right way"" to do deep image generation.': 1.0985546112060547, ""Texture synthesis methods used these causal neighborhoods with some success, but only because there wasn't a clear way to do the optimization more globally (Kwatra et al, Texture Optimization for Example-based Synthesis being one of the first alternatives)."": 1.0816315412521362, 'It seems simply incorrect to make hard decisions about what pixel values should be in one part of the image before synthesizing another part of the image (Another texture synthesis strategy to help fight back against this strict causality was coarse-to-fine synthesis.': 0.9888384342193604, 'And I do see some deep image synthesis methods exploring that).': 1.090039610862732, 'It seems much more correct to have a deeper network and let all output pixels be conditioned on all other pixels (this conditioning will implicitly emerge at intermediate parts of the network).': 0.6418155431747437, 'That said, I could be totally wrong, and the advantages stated in the paper could outweigh the disadvantages.': 0.0882038027048111, ""But this paper doesn't feel very honest about the disadvantages."": 0.7025637030601501, 'Overall, I think the results are somewhat tantalizing, especially the ability to generate diverse outputs.': 0.8605360984802246, 'But the resolution is extremely low, especially compared to the richness of the inputs.': 0.9094936847686768, 'The network gets a lot of hand holding from rich inputs (it does at least learn to obey them).': 0.8040200471878052, 'The deep image synthesis literature is moving very quickly.': 1.0972919464111328, 'The field needs to move on from ""proof of concept"" papers (the first to show a particular result is possible) to more thorough comparisons.': 0.9707318544387817, ""This paper has an opportunity to be a more in depth comparison, but it's not very deep in that regard."": 0.8190217018127441, ""There isn't really an apples to apples comparison between PixelCNN and GAN nor is there a conclusion statement about why that is impossible."": 0.36309704184532166, ""There isn't any large scale comparison, either qualitative or quantified by user studies, about the quality of the results."": 0.9531856775283813}"
252,https://openreview.net/forum?id=HyxQzBceg,"{'Update: raised the score, because I think the arguments about adversarial examples are compelling.': 1.0986123085021973, ""I think that the paper convincingly proves that this method acts as a decent regularizer, but I'm not convinced that it's a competitive regularizer."": 1.0986123085021973, ""For example, I don't believe that there is sufficient evidence that it gives a better regularizer than dropout/normalization/etc."": 1.0986123085021973, 'I also think that it will be much harder to tune than these other methods (discussed in my rebuttal reply).': 1.0986123085021973, 'Summary: If I understand correctly, this paper proposes to take the ""bottleneck"" term from variational autoencoders which pulls the latent variable towards a noise prior (like N(0,1)) and apply it in a supervised learning context where the reconstruction term log(p(x|z)) is replaced with the usual supervised cross-entropy objective.': 1.0986123085021973, 'The argument is that this is an effective regularizer and increases robustness to adversarial attacks.': 1.0986121892929077, 'Pros:': 1.0985686779022217, 'The presentation is quite good and the paper is easy to follow.': 1.0986015796661377, 'The idea is reasonable and the relationship to previous work is well described.': 1.0884318351745605, ""The robustness to adversarial examples experiment seems convincing, though I'm not an expert in this area."": 1.0986000299453735, 'Is there any way to compare to an external quantitative baseline on robustness to adversarial examples?': 1.0986123085021973, ""This would help a lot, since I'm not sure how the method here compares with other regularizers in terms of combatting adversarial examples."": 1.0968831777572632, 'For example, if one uses a very high dropout rate, does this confer a comparable robustness to adversarial examples (perhaps at the expense of accuracy)?': 1.0258945226669312, 'Cons:': 1.0584523677825928, ""MNIST accuracy results don't seem very strong, unless I'm missing something."": 1.0967719554901123, 'The Maxout paper from ICML 2013 listed many permutation invariant MNIST results with error rates below 1%.': 1.0982171297073364, ""So the 1.13% error rate listed here doesn't necessarily prove that the method is a competitive regularizer."": 1.0986123085021973, 'I also suspect that tuning this method to make it work well is harder than other regularizers like dropout.': 1.0986123085021973, 'There are many distinct architectural choices with this method, particularly in how many hidden layers come before and after z.': 0.45348840951919556, 'For example, the output could directly follow z, or there could be several layers between z and the output.': 0.5030525326728821, ""As far as I can tell the paper says that p(y | z) is a simple logistic regression (i.e. one weight matrix followed by softmax), but it's not obvious why this choice was made."": 1.0986123085021973, 'Did it work best empirically?': 1.0986123085021973, 'Other:': 1.0986123085021973, 'I wonder what would happen if you ""trained against"" the discovered adversarial examples while also using the method from this paper.': 1.0986123085021973, 'Would it learn to have a higher variance p(z | x) when presented with an adversarial example?': 1.0986123085021973, 'Summary:': 1.0986123085021973, 'The paper “Deep Variational Information Bottleneck” explores the optimization of neural networks for variational approximations of the information bottleneck (IB; Tishby et al., 1999).': 1.0986121892929077, 'On the example of MNIST, the authors show that this may be used for regularization or to improve robustness against adversarial attacks.': 1.0986123085021973, 'Review:': 1.0986088514328003, 'The IB is potentially very useful for important applications (regularization, adversarial robustness, and privacy are mentioned in the paper).': 1.0986123085021973, 'Combining the IB with recent advances in deep learning to make it more widely applicable is an excellent idea.': 1.0986123085021973, 'But given that the theoretical contribution is a fairly straight-forward application of well-known ideas, I would have liked to see a stronger experimental section.': 1.0986123085021973, 'Since the proposed approach allows us to scale IB, a better demonstration of this would have been on a larger problem than MNIST.': 1.0986123085021973, 'It is also not clear whether the proposed approach will still work well to regularize more interesting networks with many layers.': 1.0986123085021973, 'Why is dropout not included in the quantitative comparison of robustness to adversarial examples (Figure 4)?': 1.0980486869812012, 'How was the number of samples (12) chosen?': 1.0986123085021973, 'What are the error bars in Figure 1 (a)?': 1.0968886613845825, 'On page 7 the authors claim “the posterior covariance becomes larger” as beta “decreases” (increases?).': 1.0986123085021973, 'Is this really the case?': 1.09489905834198, 'It’s hard to judge based on Figure 1, since the figures are differently scaled.': 1.0983057022094727, 'It might be worth comparing to variational fair autoencoders (Louizos et al., 2016), which also try to learn representations minimizing the information shared with an aspect of the input.': 1.0976569652557373, 'The paper is well written and easy to follow.': 1.0986123085021973, 'Thank you for an interesting read.': 1.0986123085021973, 'I personally like the information bottleneck principle and am very happy to see its application to deep neural networks.': 1.0986123085021973, 'To my knowledge, this is the first paper that applies IB to train deep networks (the original papers only presented the concept), but see below for the note of independent work claim.': 1.0986123085021973, 'The derivation of the variational lowerbound is very clear, even for those who are not very familiar with variational inference.': 1.0985207557678223, 'Also the explanation of the IB principle is clear.': 1.0985944271087646, 'Experimental results seem to be very promising.': 1.0986123085021973, 'I found the presentation for the model a bit confusing.': 1.0986121892929077, 'In variational inference/information maximisation, p usually denotes the model and q represents the ""inference engine"".': 1.0985966920852661, 'This means the choice of inference method is independent to the modelling procedure.': 1.0986121892929077, 'However the presented VIB assumed p(x, y) as the **underlying data distribution** (and approximated by the empirical distribution), thus here the model is actually q(y|z)p(z|x).': 1.0986123085021973, 'Then the authors presented p(y|x) as the **predictive distribution** in page 8, paragraph 2 of section 4.2.3.': 1.0986123085021973, 'Predictive in what sense?': 1.0986123085021973, 'I guess you meant p(y|x)': 1.0986123085021973, '= \\int q(y|z) p(z|x) dz in this case, but this makes the two definitions contradict to each other!': 1.0986123085021973, 'The authors have made an interesting connection to variational auto-encoder and the warm-up training (by tuning beta).': 1.0986123085021973, 'However, even when the loss function formula is the same to the variational lowerbound used in VAE (in this case beta = 1), the underlying model is different!': 1.0986123085021973, 'For example, r(z) in VIB is the variational approximation to p(z) (which means r(z) is not a component in the model), while in VAE it is the prior distribution which is actually defined in the modelling procedure.': 1.0983961820602417, 'Similaly p(z|x) in VIB is included in the model, while in VAE that is the approximate posterior and can be independently chosen (e.g. you can use p(x|z) as a deep NN but p(z|x) as a deep NN or a Gaussian process).': 0.4313772916793823, 'In summary, I think the presentation for the modelling procedure is unclear.': 1.0964053869247437, 'I hope these point would be made clearer in revision since the current presentation makes me uncomfortable as a Bayesian person.': 1.0986027717590332, ""In the VAE part, it's better to clearly mention the difference between VIB and VAE, and provide some intuitions if the VIB interpretation is preferred."": 1.0930598974227905, 'Typos:': 1.0986123085021973, 'Eq. 9-11: did you mean q(y|z) instead of q(z|y)?': 1.0986123085021973, 'Fig 2 ""as beta becomes smaller"": did you mean ""larger""?': 1.0986123085021973, '**claim for independent work**': 1.0986123085021973, 'The authors claimed that the manuscript presented an independent work to Chalk et al. 2016 which is online since May 2016.': 1.0986123085021973, 'It seems to me that nowadays deep learning research is very competitve that many people publish the same idea at the same time.': 1.0986123085021973, ""So I would trust this claim and commend the authors' honesty, but in case this is not true, I would not recommend the manuscript for acceptance."": 1.0986123085021973}"
253,https://openreview.net/forum?id=S11KBYclx,"{'The paper addresses the problem of predicting learning curves.': 1.0986123085021973, 'The key difference from prior work is that (1) the authors learn a neural network that generalizes across hyperparameter settings and (2) the authors use a Bayesian neural network with SGHMC.': 1.0986123085021973, 'The authors demonstrate that the proposed approach is effective on extrapolating partially observed curves as well as predicting unobserved learning curves on various architectures (FC, CNN, LR and VAE).': 1.0986123085021973, ""This seems very promising for Bayesian optimization, I'd love to see an experiment that evaluates the relative advantage of this proposed method :)"": 1.0986121892929077, 'Have you thought about ways to handle learning rate decays?': 1.0986123085021973, 'Perhaps you could run the algorithm on a random subset of data and extrapolate from that?': 1.0986123085021973, 'I was thinking of other evaluation measures in addition to MSE and LL.': 1.0985040664672852, 'In practice, we care about the most promising run.': 1.0986123085021973, 'Would it make sense to evaluate how accurately each method identified the best run?': 1.0986123085021973, 'Minor comments:': 1.0986123085021973, 'Fonts are too small and almost illegible on my hard copy.': 1.0986120700836182, 'Please increase the font size for legends and axes in the figures.': 1.0985639095306396, 'Fig 6: not all figures seem to have six lines.': 0.6219633221626282, 'Are the lines overlapping in some cases?': 1.0986121892929077, 'This paper is about using Bayesian neural networks to model learning curves (that arise from training ML algorithms).': 1.09861159324646, 'The application is hyper-parameter optimization: if we can model the learning curve, we can terminate bad runs early and save time.': 1.0986123085021973, 'The paper builds on existing work that used parametric learning curves.': 1.0966390371322632, 'Here, the parameters of these learning curves form the last layer of a Bayesian neural network.': 1.0985904932022095, 'This seems like a totally sensible idea.': 1.0954850912094116, 'I think the main strength of this paper is that it addresses an actual need.': 1.0986123085021973, 'Based on my personal experience, there is high demand for a working system to do early termination in hyperparameter optimization.': 1.0986123085021973, ""What I'd like to know, which I wish I'd asked during pre-review questions, is whether the authors plan to release their code."": 1.0986123085021973, 'Do you?': 1.0986024141311646, ""I sincerely hope so, because I think the code would be a significant part of the paper's contribution, since the nature of the paper is more practical than conceptual."": 1.089446783065796, 'The experiments in the paper seem thorough but the results are a bit underwhelming.': 1.0934937000274658, ""I'm less interested in the part about whether the learning curves are actually modeled well, and more interested in the impact on hyperparameter optimization."": 1.09861159324646, 'I was hoping to see BIG speedups as a result of using this method, but I am left feeling unsure how big the speedup really is.': 1.0984792709350586, 'Instead of ""objective function vs. iterations"" I would be more interested in the inverse plot: number of iterations needed to get to a fixed objective function value.': 1.0985968112945557, ""Since what I'm really interested in is how much time I can save."": 0.9515365362167358, 'Ideally there would also be some mention of real time as sometimes these hyperparameter optimization methods are themselves so slow that they end up being unusable.': 1.098121166229248, 'Finally, one figure that I feel is missing is a histogram of termination times over different runs.': 1.0985932350158691, 'This would provide me with more intuition than all the other figures.': 0.41322001814842224, 'Because it would tell me, what fraction of runs are being terminated early.': 1.0986123085021973, 'And, how early?': 1.0352075099945068, 'Right now I have no sense of this, except that at least *some* runs are clearly being terminated early, since this is neccessary for the proposed method to outperform other methods.': 1.0986113548278809, 'Overall, I think this paper merits acceptance because it is a solid effort on an interesting problem.': 1.0812007188796997, 'The progress is fairly incremental but I can live with that.': 1.0986096858978271, 'This paper proposes a new Bayesian neural network architecture for predicting the values of learning curves during the training of machine learning models.': 1.0986123085021973, 'This is an exploratory paper, in that the ultimate goal is to use this method in a Bayesian optimization system, but for now the experiments are limited to assessing the quality of the predictions.': 1.0986086130142212, 'This builds on previous work in Domhan, 2015, however in this work the model incorporates information from all tested hyperparameter settings rather than just extrapolating from a single learning curve.': 1.0986120700836182, 'This paper also explores two MCMC methods for inference: SGLD and SGHMC, but I couldn’t tell if either of these were tested in Domhan, 2015 as well.': 1.098571538925171, 'The performance seems overall positive, particularly in the initial phase of each curve where there is very little information.': 1.0986121892929077, 'In this case, as expected, sharing knowledge across curves helps.': 1.0982381105422974, 'One regime which did not seem to be tested, but might be very informative, is when some curves in the training set have been mostly, or fully observed.': 1.0986121892929077, 'This might be a case where sharing information really helps.': 1.091193437576294, 'Something that concerns me about this approach is the timing.': 1.0904866456985474, 'The authors stated that to train the network takes about 20-60 seconds.': 1.0923340320587158, 'In the worst case, with 100 epochs, this results in a little over 1.5 hours spent training the Bayesian network.': 1.0986123085021973, 'This is a non-trivial fraction of the several hours it takes to train the model being tuned.': 1.0986123085021973, 'The Bayesian network makes many separate predictions, as shown in Figure 2.': 1.0985913276672363, 'It would be interesting to see how accurate some of these individual pieces are.': 1.0986042022705078, 'For example, did you bound the asymptotic value of the learning curve, since you mostly predicted accuracy?': 1.0986123085021973, 'If not, did the value tend to lie in [0,1]?': 1.0978665351867676, 'Below are some minor questions/comments.': 1.091204285621643, 'Figure 1 axes should read “validation accuracy”': 1.0985722541809082, 'Figure 6 can you describe LastSeenValue (although it seems self-explanatory, it’s good to be explicit) in the bottom left figure, and why isn’t it used anywhere else as a baseline?': 1.0982738733291626, 'Figure 7 and Table 1 are you predicting just the final value of the curves?': 1.0937926769256592, 'Or every value along each curve, conditioned on the previous values?': 1.086621642112732, 'Why do you only use 5 basis functions?': 1.0986123085021973, 'Does this sufficiently capture all of the flexibility of these learning curves?': 1.0134196281433105, 'Would more basis functions help or hurt?': 1.098610758781433}"
254,https://openreview.net/forum?id=S13wCE9xx,"{'The paper considers Grassmannian SGD to optimize the skip gram negative sampling (SGNS) objective for learning better word embeddings.': 1.0986123085021973, 'It is not clear why the proposed optimization approach has any advantage over the existing vanilla SGD-based approach - neither approach comes with theoretical guarantees - the empirical comparisons show marginal improvements.': 1.0986123085021973, 'Furthermore, the key idea here - that of projector splitting algorithm - has been applied on numerous occasions to machine learning problems - see references by Vandereycken on matrix completion and by Sepulchre on matrix factorization.': 1.0986123085021973, 'The computational cost of the two approaches is not carefully discussed.': 1.0986123085021973, 'For instance, how expensive is the SVD in (7)?': 1.0738707780838013, 'One can always perform an efficient low-rank update to the SVD - therefore, a rank one update requires O(nd) operations.': 1.0986121892929077, 'What is the computational cost of each iteration of the proposed approach?': 1.0920743942260742, 'Dear authors,': 1.0986123085021973, ""The authors' response clarified some of my confusion."": 1.0883156061172485, 'But I still have the following question:': 0.8505977988243103, 'The response said a first contribution is a different formulation: you divide the word embedding learning into two steps, step 1 looks for a low-rank X (by Riemannian optimization), step 2 factorizes X into two matrices (W, C).': 1.0981199741363525, 'You are claiming that your model outperforms previous approaches that directly optimizes over (W, C).': 1.0986123085021973, 'But since the end result (the factors) is the same, can the authors provide some intuition and justification why the proposed method works better?': 1.0979169607162476, 'As far as I can see, though parameterized differently, the first step of your method and previous methods (SGD) are both optimizing over low-rank matrices.': 1.0986123085021973, 'Admittedly, Riemannian optimization avoids the rotational degree of freedom (the invertible matrix S you are mentioning in sec 2.3), but I am not 100% certain at this point this is the source of your gain; learning curves of objectives would help to see if Riemannian optimization is indeed more effective.': 0.05363832414150238, 'Another detail I could not easily find is the following.': 1.0985651016235352, 'You said a disadvantage of other approaches is that their factors W and C do not directly reflect similarity.': 1.0982980728149414, 'Did you try to multiply the factors W and C from other optimizers and then factorize the product using the method in section 2.3, and use the new W for your downstream tasks?': 1.0986123085021973, 'I am not sure if this would cause much difference in the performance.': 1.0986123085021973, 'Overall, I think it is always interesting to apply advanced optimization techniques to machine learning problems.': 1.098507046699524, 'The current paper would be stronger from the machine learning perspective, if more thorough comparison and discussion (as mentioned above) are provided.': 1.098609447479248, 'On the other hand, my expertise is not in NLP and I leave it to other reviewers to decide the significance in experimental results.': 1.090165615081787, 'This paper presents a principled optimization method for SGNS (word2vec).': 1.098611831665039, 'While the proposed method is elegant from a theoretical perspective, I am not sure what the tangible benefits of this approach are.': 1.0972663164138794, 'For example, does using Riemannian optimization allow the model to converge faster than the alternatives?': 1.0986123085021973, 'The evaluation doesn\'t show a dramatic advantage to RO-SGNS; the 1% difference on the word similarity benchmarks is within the range of hyperparameter effects (see ""Improving Distributional Similarity with Lessons Learned from Word Embeddings"", (Levy et al., 2015)).': 1.0986096858978271, 'The theoretical connection to Riemannian optimization is nice though, and it might be useful for understanding related methods in the future.': 1.0986121892929077}"
255,https://openreview.net/forum?id=S19eAF9ee,"{'The paper proposes to combine graph convolution with RNNs to solve problems in which inputs are graphs.': 1.0986123085021973, 'The two key ideas are: (i) a graph convolutional layer is used to extract features which are then fed in an RNN, and (ii) matrix multiplications are replaced by graph convolution operations.': 1.0986123085021973, '(i) is applied to language modelling, yielding lower perplexity on Penn Treebank (PTB) compared with LSTM.': 1.0986123085021973, '(ii) outperformed LSTM + CNN on the moving-MNIST.': 1.0986123085021973, 'Both two models/ideas are actually trivial and in line with the current trend of combining different architectures.': 1.0986123085021973, 'For instance, the idea of replacing matrix multiplications by graph convolution is a small extension for Shi et al.': 1.0986123085021973, ""Regarding to the experiment on PTB (section 5.2), I'm skeptical about the way the experiment carried out."": 1.0986123085021973, 'The reason is that, instead of using the given development set to tune the models, the authors blindly used an available configuration which is for a different model.': 1.0986123085021973, 'Pros:': 1.0986123085021973, 'good experimental results': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'ideas are quite trivial': 1.0986123085021973, 'the experiment on PTB was carried out improperly': 1.0986123085021973, 'This paper investigates the  modeling of graph sequences .': 1.0986123085021973, 'Authors propose Graph Convolutional Recurrent Networks (GRCN)  that extends convLSTM  (Shi et al. 2015) for data having an unregular graph structure at each timestep.': 1.0986123085021973, 'They replace the 2D convolution with a graph convolutional operator from (Defferrad et al., 2016).': 1.0986123085021973, 'Authors propose two variations of the GRCN model.': 1.0986123085021973, 'In Model 1,  the graph convolution is only applied on the input data.': 1.0986123085021973, 'In Model 2, the graph convolution  is applied on both  input data and the previous hidden states.': 1.0986123085021973, 'They evaluate their approaches on two different tasks, video generation using the movingMNIST dataset and world-level language modelling using Penntreebank.': 1.0986123085021973, 'On movingMNIST authors show that their GRCN 2 improves upon convLSTM.': 1.0986123085021973, 'However, they evaluate only with one-layer convLSTM, while Shi et al. report better results with 3 layers (also not as good as  GRCN) .': 1.0986123085021973, 'It would be nice to evaluate GCRCN in that setting as well.': 1.0986123085021973, 'While the authors show an improvement of GRCN relatively to convLSTM, GRCN on this task seems relatively weak compared to recent works such as the Video Pixel Networks (Kalchbrenner et al., 2016).': 1.0986123085021973, 'It contradicts the claim that ""Model 2 has shown good performance in the case of video prediction"" in the conclusion.': 1.0986123085021973, 'For the Penntreebank experiments, author compares  their model 1 with FC-LSTM, with or without dropout.': 1.0986123085021973, 'However, the results in (Zaremba et al., 2014) still seems different than the one reported here.': 1.0986123085021973, 'In (Zaremba et al., 2014), they  reports a test perplexity of 78.4 for the large regularized LSTM in their table 1 which outperforms the score of the GRCN.': 1.0986123085021973, 'Also, following works such as variational dropout or zoneout have since improve upon Zaremba results.': 1.0986123085021973, 'Is there some differences in the experimental setting?': 1.0986123085021973, 'It would be nice to have results that are directly comparable to previous work.': 1.0986123085021973, 'Interesting model,': 1.0986123085021973, 'Overall, the proposed contribution is relatively incremental compared to (Shi et al. 2015) and (Defferrad et al., 2016).': 0.9786798357963562, 'Weak results of GRCN relatively to previous works in the experiments, that do not convince of the GRCN advantages.': 0.9045908451080322, 'The authors address the problem of modeling temporally-changing signal on a graph, where the signal at one node changes as a function of the inputs and the hidden states of its neighborhood, the size of which is a hyperparameter.': 1.0935395956039429, 'The approach follows closely that of Shi et al. 2015, but it is generalized to arbitrary graph structures rather than a fixed grid by using graph convolutions of Defferrard et al. 2016.': 1.097786784172058, 'This is not a strict generalization because the graph formulation treats all edges equally, while the conv kernels in Shi et al. have a built in directionality.': 1.0977007150650024, 'The authors show results on a moving MNIST and on the Penn Tree Bank Language Modeling task.': 1.097593069076538, 'The paper, model and experiments are decent but I have some concerns:': 0.7606167793273926, ""1. The proposed model is not exceptionally novel from a technical perspective. I usually don't mind if this is the case provided that the authors make up for the deficiency with thorough experimental evaluation, clear write up, and interesting insights into the pros/cons of the approach with respect to previous models. In this case I lean towards this not being the case."": 1.0982767343521118, ""2. The experiment results section is rather terse and light on interpretation. I'm not fully up to date on the latest of Penn Tree Bank language modeling results but I do know that it is a hotly contested and well-known dataset. I am surprised to see a comparison only to Zaremba et al 2014 where I would expect to see multiple other results."": 1.04198157787323, ""3. The writing is not very clear and the authors don't make sufficiently strong attempt to compare the models or provide insight or comparisons into why the proposed model works better. In particular, unless I'm mistaken the word probabilities are a function of the neighborhood in the graph. What is the width of this graph? For example, suppose I sample a word in one part of the graph, doesn't this information have to propagate to the other parts of the graph along the edges? Also, it's not clear to me how the model can achieve reasonable results on moving MNIST when it cannot distinguish the direction of the moving edges. The authors state this but do not provide satisfying insight into how this can work. How does a pixel know that it should turn on in the next frame? I wish the authors thought about this more and presented it more clearly."": 1.09767746925354, 'In summary, the paper has somewhat weak technical contribution, the experiments section is not very thorough, and the insights are sparse.': 1.0986123085021973}"
256,https://openreview.net/forum?id=S1AG8zYeg,"{'This paper extends the ""order matters"" idea in (Vinyals et al., 2015) from the sentence level to an interesting application on discourse level.': 1.0986119508743286, 'Experiments in this paper show the capacity of the proposed model on both order discrimination task and sentence ordering.': 1.0985459089279175, 'I think the problem is interesting and the results are promising.': 1.0980114936828613, 'However, there are some problems about technical details:': 1.0986123085021973, 'Why there are two components of LSTM hidden state (h_{enc}^{t-1},c_{ent}^{t-1}), what information is captured by each of these hidden states?': 1.0986111164093018, 'Refer to (Vinyals et al. 2015a)?': 1.0986117124557495, 'Some notations in this paper are confusing.': 1.0983132123947144, 'For example, what is the form of W in the feed-forward scoring function?': 1.0986121892929077, 'Does it have the same form as the W in the bilinear score function?': 1.0986123085021973, 'What is the connection between the encoder and decoder in the proposed model?': 1.0836423635482788, 'How to combine them together?': 1.098142385482788, 'I read something relevant from the caption of Figure 1, but it is still not clear to me.': 1.0986123085021973, 'This paper presents an empirical study on sentence ordering using RNN variants.': 1.0986106395721436, 'I’m not sure how strong novelty this paper brings in terms of technical contributions.': 1.0986123085021973, 'The proposed method closely follows the read, process, and write framework of Vinyals et al. (2015) that has been developed for set-to-sequence type tasks.': 1.0986123085021973, 'Sentence ordering is a good fit for set-to-sequence formulation, as the input sentences are rather a set than a sequence.': 1.0986123085021973, 'Thus, the main contribution of this work can be viewed as a new application of an existing model rather than a new model development.': 1.098611831665039, 'That said, the empirical evaluation of the paper is very solid and the authors have updated the supplementary material to strengthen the evaluation even further in response to the QAs.': 1.0986123085021973, 'The main contribution of this paper is to introduce a new neural network model for sentence ordering.': 1.0986123085021973, 'This model is presented as an encoder-decoder, and uses recent development for neural network (pointer networks and order invariant RNNs).': 1.0986123085021973, 'The first processing step of the model is to encode each sentence using a word level LSTM recurrent network.': 1.0986123085021973, 'An order invariant encoder (based on Vinyals et al. (2015)) is then used to obtain a representation of the set of sentences.': 1.0986123085021973, 'The input at each time step of this encoder is a ""bag-of-sentences"", over which attention probabilities are computed.': 1.0986123085021973, 'The last hidden representation of the encoder is then used to initialize the decoder.': 1.0986123085021973, 'This decoder is a pointer network, and is used to predict the order of the input sentences.': 1.0986123085021973, 'The model introduced in this paper is then compared to standard method for sentence ordering, as well as a model with the decoder only.': 1.0986123085021973, 'The encoder-decoder approach outperforms the other methods on the task of ordering a given set of sentences.': 1.0986123085021973, 'The authors also show that the model learns relatively good sentence representations (e.g. compared to Skip-thought vectors).': 1.0986123085021973, 'This paper is relatively well written, and easy to follow.': 1.0986123085021973, 'The model described in the paper does not really introduce anything new, but is a natural combination of existing techniques to solve the task of sentence ordering.': 1.0986123085021973, 'In particular, it uses an order-invariant encoder to get a representation of the set of sentences, and a pointer network to predict the ordering.': 1.0986123085021973, 'While I am not entirely convinced by the task of sentence ordering, this approach seems promising to learn sentence representations.': 1.0986123085021973, 'Overall, this is a pretty solid and well executed paper.': 1.0986123085021973, 'pros:': 1.0986123085021973, '- sound model for sentence ordering': 1.0986123085021973, '- strong experimental results': 1.0986123085021973, 'cons:': 1.0986123085021973, '- might be a bit incremental': 1.0986123085021973, '- usefulness of sentence ordering': 1.0986123085021973}"
257,https://openreview.net/forum?id=S1Bb3D5gg,"{'Attempts to use chatbots for every form of human-computer interaction has been a major trend in 2016, with claims that they could solve many forms of dialogs beyond simple chit-chat.': 1.0986099243164062, 'This paper represents a serious reality check.': 0.719165563583374, 'While it is mostly relevant for Dialog/Natural Language venues (to educate software engineer about the limitations of current chatbots), it can also be published at Machine Learning venues (to educate researchers about the need for more realistic validation of ML applied to dialogs), so I would consider this work of  high significance.': 1.0986121892929077, 'Two important conjectures are underlying this paper and likely to open to more research.': 1.0592900514602661, 'While they are not in writing, Antoine Bordes clearly stated them during a NIPS workshop presentation that covered this work.': 1.0985103845596313, 'Considering the metrics chosen in this paper:': 1.0986112356185913, '1)': 1.0985524654388428, 'The performance of end2end ML approaches is still insufficient for goal oriented dialogs.': 1.071498990058899, '2) When comparing algorithms, relative performance on synthetic data is a good predictor of performance on natural data.': 1.0963677167892456, 'This would be quite a departure from previous observations, but the authors made a strong effort to match the synthetic and natural conditions.': 1.0986095666885376, 'While its original algorithmic contribution consists in one rather simple addition to memory networks (match type), it is the first time these are deployed and tested on a goal-oriented dialog, and the experimental protocol is excellent.': 1.0986123085021973, 'The overall paper clarity is excellent and accessible to a readership beyond ML and dialog researchers.': 1.098607063293457, 'I was in particular impressed by how the short appendix on memory networks summarized them so well, followed by the tables that explained the influence of the number of hops.': 1.096120834350586, 'While this paper represents the state-of-the-art in the exploration of more rigorous metrics for dialog modeling, it also reminds us how brittle and somewhat arbitrary these remain.': 1.0941749811172485, 'Note this is more a recommendation for future research than  for revision.': 1.0986101627349854, 'First they use the per-response accuracy (basically the next utterance classification among a fixed list of responses).': 1.0986032485961914, 'Looking at table 3 clearly shows how absurd this can be in practice: all that matters is a correct API call and a reasonably short dialog, though this would only give us a 1/7 accuracy, as the 6 bot responses needed to reach the API call also have to be exact.': 1.085820198059082, 'Would the per-dialog accuracy, where all responses must be correct, be better?': 0.6171600818634033, 'Table 2 shows how sensitive it is to the experimental protocol.': 0.6659273505210876, 'I was initially puzzled that the accuracy for subtask T3 (0.0) was much lower that the accuracy for the full dialog T5 (19.7), until the authors pointed me to the tasks definitions (3.1.1) where T3 requires displaying 3 options while T5 only requires displaying one.': 0.6871240139007568, 'For the concierge data, what would happen if ‘correct’ meant being the best, not among the 5-best?': 0.43729233741760254, 'While I cannot fault the authors for using standard dialog metrics, and coming up with new ones that are actually too pessimistic, I can think of one way to represent dialogs that could result in more meaningful metrics in goal oriented dialogs.': 0.9121034145355225, 'Suppose I sell Virtual Assistants as a service, being paid upon successful completion of a dialog.': 1.0986123085021973, 'What is the metric that would maximize my revenue?': 1.0986123085021973, 'In this restaurant problem, the loss would probably be some weighted sum of the number of errors in the API call, the number of turns to reach that API call and the number of rejected options by the user.': 1.0986123085021973, 'However, such as loss cannot be measured on canned dialogs and would either require a real human user or an realistic simulator': 1.0986123085021973, 'Another issue closely related to representation learning that this paper fails to address or explain properly is what happens if the vocabulary used by the user does not match exactly the vocabulary in the knowledge base.': 1.0986123085021973, 'In particular, for the match type algorithm to code ‘Indian’ as ‘type of cuisine’, this word would have to occur exactly in the KB.': 1.0986123085021973, 'I can imagine situations where the KB uses some obfuscated terminology, and we would like ML to learn the associations rather than humans to hand-describe them.': 1.0986123085021973, 'SYNOPSIS:': 1.0986123085021973, 'This paper introduces a new dataset for evaluating end-to-end goal-oriented dialog systems.': 1.0986123085021973, 'All data is generated in the restaurant setting, where the goal is to find availability and eventually book a table based on parameters provided by the user to the bot as part of a dialog.': 1.0986123085021973, 'Data is generated by running a simulation using an underlying knowledge base to generate samples for the different parameters (cuisine, price range, etc), and then applying rule-based transformations to render natural language descriptions.': 1.0986123085021973, 'The objective is to rank a set of candidate responses for each next turn of the dialog, and evaluation is reported in terms of per-response accuracy and per-dialog accuracy.': 1.0986123085021973, 'The authors show that Memory Networks are able to improve over basic bag-of-words baselines.': 1.0986123085021973, 'THOUGHTS:': 1.0986123085021973, 'I want to thank the authors for an interesting contribution.': 1.0986123085021973, 'Having said that, I am skeptical about the utility of end-to-end trained systems in the narrow-domain setting.': 1.0986123085021973, 'In the open-domain setting, there is a strong argument to be made that hand-coding all states and responses would not scale, and hence end-to-end trained methods make a lot of sense.': 1.0986123085021973, 'However, in the narrow-domain setting, we usually know and understand the domain quite well, and the goal is to obtain high user satisfaction.': 1.0986123085021973, ""Doesn't it then make sense in these cases to use the domain knowledge to engineer the best system possible?"": 1.0986123085021973, ""Given that the domain is already restricted, I'm also a bit disappointed that the goal is to RANK instead of GENERATE responses, although I understand that this makes evaluation much easier."": 1.0986123085021973, ""I'm also unsure how these candidate responses would actually be obtained in practice?"": 1.0986123085021973, 'It seems that the models rank the set of all responses in train/val/test (last sentence before Sec 3.2).': 1.0986123085021973, 'Since a key argument for the end-to-end training approach is ease of scaling to new domains without having to manually re-engineer the system, where is this information obtained for a new domain in practice?': 1.0986123085021973, 'Generating responses would allow much better generalization to new domains, as opposed to simply ranking some list of hand-collected generic responses, and in my mind this is the weakest part of this work.': 0.9128718376159668, 'Finally, as data is generated using a simulation by expanding (cuisine, price, ...)': 1.0986123085021973, 'tuples using NL-generation rules, it necessarily constrains the variability in the training responses.': 1.0986123085021973, 'Of course, this is traded off with the ability to generate unlimited data using the simulator.': 1.0986123085021973, 'But I was unable to see the list of rules that was used.': 1.0986123085021973, 'It would be good to publish this as well.': 1.0986123085021973, 'Overall, despite my skepticism, I think it is an interesting contribution worthy of publication at the conference.': 1.0986123085021973, ""I've updated my score following the clarifications and new results."": 1.0986123085021973, 'This paper presents a new, public dataset and tasks for goal-oriented dialogue applications.': 1.0986123085021973, 'The dataset and tasks are constructed artificially using rule-based programs, in such a way that different aspects of dialogue system performance can be evaluated ranging from issuing API calls to displaying options, as well as full-fledged dialogue.': 1.0986120700836182, 'This is a welcome contribution to the dialogue literature, which will help facilitate future research into developing and understanding dialogue systems.': 1.0912812948226929, 'Still, there are pitfalls in taking this approach.': 1.0347901582717896, 'First, it is not clear how suitable Deep Learning models are for these tasks compared to traditional methods (rule-based systems or shallow models), since Deep Learning models are known to require many training examples and therefore performance difference between different neural networks may simply boil down to regularization techniques.': 1.0986120700836182, ""The tasks 1-5 are also completely deterministic, which means evaluating performance on these tasks won't measure the ability of the models to handle noisy and ambiguous interactions (e.g. inferring a distribution over user goals, or executing dialogue repair strategies), which is a very important aspect in dialogue applications."": 1.09861159324646, 'Overall, I still believe this is an interesting direction to explore.': 0.6574869155883789, 'As discussed in the comments below, the paper does not have any baseline model with word order information.': 0.88123619556427, 'I think this is a strong weakness of the paper, because it makes the neural networks appear unreasonably strong, yet simpler baselines could very likely be be competitive (or better) than the proposed neural networks.': 1.0986123085021973, ""To maintain a fair evaluation and correctly assess the power of representation learning for this task, I think it's important that the authors experiment with one additional non-neural network benchmark model which takes into account word order information."": 1.0985689163208008, 'This would more convincly demonstrate the utility of Deep Learning models for this task.': 1.0985450744628906, 'For example, the one could experiment with a logistic regression model which takes as input 1) word embeddings (similar to the Supervised Embeddings model), 2) bi-gram features, and 3) match-type features.': 0.5982895493507385, 'If such a baseline is included, I will increase my rating to 8.': 1.056376338005066, 'Final minor comment: in the conclusion, the paper states ""the existing work has no well defined measures of performances"".': 1.0303592681884766, 'This is not really true.': 1.0856375694274902, 'End-to-end trainable models for task-oriented dialogue have well-defined performance measures.': 1.0866161584854126, 'See, for example ""A Network-based End-to-End Trainable Task-oriented Dialogue System"" by Wen et al.': 0.7749457955360413, 'On the other hand, non-goal-oriented dialogue are generally harder to evaluate, but given human subjects these can also be evaluated.': 1.021553635597229, 'In fact, this is what Liu et al (2016) do for Twitter.': 0.9445167183876038, 'See also, ""Strategy and Policy Learning for Non-Task-Oriented Conversational Systems"" by Yu et al.': 1.0539164543151855, ""I've updated my score following the new results added in the paper."": 1.085603952407837}"
258,https://openreview.net/forum?id=S1Bm3T_lg,"{'This paper proposes a new learning model ""Compositional Kernel Machines (CKMs)"" that extends the classic kernel machines by constructing compositional kernel functions using sum-product networks.': 1.372111439704895, 'This paper considers the convnets as nicely learned nonlinear decision functions and resort their success in classification to their compositional nature.': 1.3672912120819092, 'This perspective motivates the design of compositional kernel functions and the sum-product implementation is indeed interesting.': 1.3862922191619873, ""I agree the composition is important for convnets, but it is not the whole story of convnets' success."": 1.2738277912139893, 'One essential difference between convnets and CKMs is that all the kernels in convnets are learned directly from data while CKMs still build on top of feature descriptors.': 1.3862943649291992, 'This, I believe, limits the representation power of CKMs.': 1.3862943649291992, 'A recent paper ""Deep Convolutional Networks are Hierarchical Kernel Machines"" by Anselmi, F. et al. seems to be interesting to the authors.': 1.3862943649291992, 'Experiments seem to be preliminary in this paper.': 1.3862943649291992, ""It's good to see promising results of CKMs on small NORB, but it is quite important to show competitive results on recent classification standard benchmarks, such as MNIST, CIFAR10/100 and even Imagenet, in order to establish a novel learning model."": 1.386197805404663, 'In NORB compositions, CKMs seem to be better than convnets at classifying images by their dominant objects.': 1.3724696636199951, 'I suspect it is because the use of sparse ORB features.': 0.8449670672416687, 'It will be great if this paper could show the accuracy of ORB features with matching kernel SVMs.': 1.2749727964401245, 'Some details about this experiment need further clarification, such as what are the high and low probabilities of sampling from each collections and how many images are generated.': 1.3862937688827515, 'In NORB Symmetries, CKMs show better performance than convnets with small data, but the convnets seem not converged yet.': 1.3108423948287964, 'Could it be possible to show results with larger dataset?': 1.386293649673462, 'The authors propose a method to efficiently augment an SVM variant with many virtual instances, and show promising preliminary results.': 1.3862943649291992, 'The paper was an interesting read, with thoughtful methodology, but has partially unsupported and potentially misleading claims.': 1.3862943649291992, 'Pros:': 1.3862943649291992, 'Thoughtful methodology with sensible design choices': 1.3862943649291992, 'Potentially useful for smaller (n < 10000) datasets with a lot of statistical structure': 1.3862943649291992, 'Nice connections with sum-product literature': 1.3862943649291992, 'Cons:': 1.3862943649291992, 'Claims about scalability are very unclear': 0.8442626595497131, 'Generally the paper does not succeed in telling a complete story about the properties and applicability of the proposed method.': 1.0586360692977905, 'Experiments are very preliminary': 1.3847321271896362, 'The scalability claims are particularly unclear.': 1.3862943649291992, 'The paper repeatedly mentions lack of scalability as a drawback for convnets, but it appears the proposed CKM is less scalable than a standard SVM, yet SVMs often handle much fewer training instances than deep neural networks.': 1.3862942457199097, 'It appears the scalability advantages are mostly for training sets with roughly fewer than 10,000 instances': 1.3862943649291992, ""and even if the method could scale to >> 10,000 training instances, it's unclear whether the predictive accuracy would be competitive with convnets in that domain."": 1.3388514518737793, 'Moreover, the idea of doing 10^6 operations simply for creating virtual instances on 10^4 training points and 100 test points is still somewhat daunting.': 1.3862943649291992, 'What if we had 10^6 training instances and 10^5 testing instances?': 1.386290192604065, 'Because scalability (in the number of training instances) is one of the biggest drawbacks of using SVMs (e.g. with Gaussian kernels) on modern datasets, the scalability claims in this paper need to be significantly expanded and clarified.': 1.3858455419540405, 'On a related note, the suggestion that convnets grow quadratically in computation with additional training instances in the introduction needs to be augmented with more detail, and is potentially misleading.': 1.3862943649291992, 'Convnets typically scale linearly with additional training data.': 1.3862943649291992, 'In general, the paper suffers greatly from a lack of clarity and issues of presentation.': 1.3862943649291992, 'As above, the full story is not presented, with critical details often missing.': 1.3862943649291992, 'Moreover, it would strengthen the paper to remove broad claims such as ""Just as support vector machines (SVMs) eclipsed multilayer perceptrons in the 1990s, CKMs could become a compelling alternative to convnets with reduced training time and sample complexity"", suggesting that CKMs could eclipse convolutional neural networks, and instead provide more helpful and precise information.': 1.3862909078598022, 'Convnets are multilayer perceptrons used in the 1990s (as well as now) and they are not eclipsed by SVMs': 1.3862887620925903, 'they have different relative advantages.': 0.9799764752388, 'And based on the information presented, broadly advertising scalability over convnets is misleading.': 1.2959645986557007, 'Can CKMs scale to datasets with millions of training and test instances?': 1.3862943649291992, 'It seems as if the scalability advantages are limited to smaller datasets, and asymptotic scalability could be much worse in general.': 1.3862943649291992, 'And even if CKMs could scale to such datasets would they have as good predictive accuracy as convnets on those applications?': 1.3862943649291992, 'Being specific and with full disclosure about the precise strengths and limitations of the work would greatly improve this paper.': 1.3862943649291992, 'CKMs may be more robust to adversarial examples than standard convnets, due to the virtual instances.': 0.9457630515098572, 'But there are many approaches to make deep nets more robust to adversarial examples.': 1.3852884769439697, 'It would be useful to consider and compare to these.': 0.8659772276878357, 'The ideas behind CKMs also are not inherently specific to kernel methods.': 1.205243706703186, 'Have you considered looking at using virtual instances in a similar way with deep networks?': 1.0436439514160156, 'A full exploration might be its own paper, but the idea is worth at least brief discussion in the text.': 1.2868565320968628, 'A big advantage of SVMs (with Gaussian kernels) over deep neural nets is that one can achieve quite good performance with very little human intervention (design choices).': 1.38094961643219, 'However, CKMs seem to require extensive intervention, in terms of architecture (as with a neural network), and in insuring that the virtual instances are created in a plausible manner for the particular application at hand.': 1.1405471563339233, ""It's very unclear in general how one would want to create sensible virtual instances and this topic deserves further consideration."": 0.8324303030967712, 'Moreover, unlike SVMs (with for example Gaussian or linear kernels) or standard convolutional networks, which are quite general models, CKMs as applied in this paper seem more like SVMs (or kernel methods) which have been highly tailored to a particular application': 1.109273910522461, 'in this case, the NORB dataset.': 1.3860411643981934, 'There is certainly nothing wrong with the tailored approach, but it would help to be clear and detailed about where the presented ideas can be applied out of the box, or how one would go about making the relevant design choices for a range of different problems.': 0.9494089484214783, 'And indeed, it would be good to avoid the potentially misleading suggestions early in the paper that the proposed method is a general alternative to convnets.': 1.1416531801223755, 'The experiments give some insights into the advantages of the proposed approach, but are very limited.': 1.3796234130859375, 'To get a sense of the properties': 1.3862943649291992, 'the strengths and limitations': 1.386266827583313, 'of the proposed method, one needs a greater range of datasets with a much larger range of training and test sizes.': 1.3844741582870483, 'The comparisons are also quite limited: why not an SVM with a Gaussian kernel?': 1.105968952178955, 'What about an SVM using convnet features from the dataset at hand (light blue curve in figure 3)': 0.9071860313415527, 'it should do at least as well as the light blue curve.': 1.3862942457199097, 'There are also other works that could be considered which combine some of the advantages of kernel methods with deep networks.': 1.3862943649291992, 'Also the claim that the approach helps with the curse of dimensionality is sensible but not particularly explored.': 1.3862943649291992, 'It also seems the curse of dimensionality could affect the scalability of creating a useful set of virtual instances.': 1.3862943649291992, ""And it's unclear how CKM would work without any ORB features."": 1.3862943649291992, ""Even if the method can (be adapted to) scale to n >> 10000, it's unclear whether it will be more useful than convnets in that domain."": 1.386278748512268, 'Indeed, in the experiments here, convnets essentially match CKMs in performance after 12,000 examples, and would probably perform better than CKMs on larger datasets.': 1.3852866888046265, ""We can only speculate because the experiments don't consider larger problems."": 1.3862943649291992, 'The methodology largely takes inspiration from sum product networks, but its application in the context of a kernel approach is reasonably original, and worthy of exploration.': 1.3862943649291992, ""It's reasonable to expect the approach to be significant, but its significance is not demonstrated."": 1.3862943649291992, 'The quality is high in the sense that the methods and insights are thoughtful, but suffers from broad claims and a lack of full and precise detail.': 1.384346604347229, 'In short: I like the paper, but it needs more specific details, and a full disclosure of where the method should be most applicable, and its precise advantages and limitations.': 1.3850539922714233, 'Code would be helpful for reproducibility.': 1.3862942457199097, 'Thank you for an interesting read.': 1.3862943649291992, 'The ideas presented have a good basis of being true, but the experiments are rather too simple.': 1.385503888130188, 'It would be interesting to see more empirical evidence.': 1.3862943649291992, 'Pros': 1.3862943649291992, 'The approach seems to decrease the training time, which is of prime importance in deep learning.': 1.3862942457199097, 'Although, that comes at a price of slightly more complex model.': 1.3862943649291992, 'There is a grounded theory for sum-product functions which is basis for the compositional architecture described in the paper.': 1.3862943649291992, 'Theoretically, any semiring and kernel could be used for the model which decreases need for handcrafting the structure of the model, which is a big problem in existing convolutional neural networks.': 1.2347145080566406, 'Cons': 1.3862943649291992, 'The experiments are on very simple dataset NORB.': 1.3862943649291992, ""Although, it is great to understand a model's dynamics on a simpler dataset, some analysis on complex datasets are important to act as empirical evidence."": 1.356838583946228, 'The compositional kernel approach is compared to convolutional neural networks, hence it is only fair to compare said results on large datasets such as Imagenet.': 1.3843141794204712, 'Minor': 1.3862943649291992, 'Section 3.4 claims that CKMs model symmetries of objects.': 1.3842443227767944, 'It felt that ample justification was not provided for this claim': 1.3862943649291992, 'This paper proposes a new learning framework called ""compositional kernel machines"" (CKM).': 1.3862937688827515, 'It combines two ideas: kernel methods and sum-product network (SPN).': 0.7120293974876404, 'CKM first defines leaf kernels on elements of the query and training examples, then it defines kernel recursively (similar to sum-product network).': 1.3862636089324951, 'This paper has shown that the evaluation CKM can be done efficiently using the same tricks in SPN.': 1.3862935304641724, 'Positive: I think the idea in this paper is interesting.': 1.3862943649291992, 'Instance-based learning methods (such as SVM with kernels) have been successful in the past, but have been replaced by deep learning methods (e.g. convnet) in the past few years.': 1.3862943649291992, 'This paper investigate an unexplored area of how to combine the ideas from kernel methods and deep networks (SPN in this case).': 1.3862943649291992, 'Negative: Although the idea of this paper is interesting, this paper is clearly very preliminary.': 1.2491203546524048, 'In its current form, I simply do not see any advantage of the proposed framework over convnet.': 1.3861784934997559, 'I will elaborate below.': 1.3862769603729248, '1) One of the most important claims of this paper is that CKM is faster to learn than convnet.': 1.3678436279296875, 'I am not clear why that is the case.': 1.3862916231155396, 'Both CKM and convnet use gradient descent during learning, why would CKM be faster?': 1.3308565616607666, 'Also during inference, the running time of convnet only depends on its network structure.': 1.36681067943573, 'But for CKM, in addition to the network structure, it also depends on the size of training set.': 1.3335455656051636, 'From this perspective, it does not seem CKM is very scalable when the training size is big.': 1.3168644905090332, 'That is probably why this paper has to use all kinds of specialized data structures and tricks (even on a fairly simple dataset like NORB)': 1.386191725730896, '2) I am having a hard time understanding what the leaf kernel is capturing.': 1.383996844291687, 'For example, if the ""elements"" correspond to raw pixel intensities, a leaf kernel essentially compares the intensity value of a pixel in the query image with that in a training image.': 1.386292815208435, ""But in this case, wouldn't you end up comparing a lot of background pixels across these two images (which does not help with recognition)?"": 1.3862862586975098, 'I think it probably helps to explain Sec 3.1 a bit better.': 1.3621718883514404, 'In its current form, this part is very dense and hard to understand.': 1.3061834573745728, '3) It is also not entirely clear to me how you would design the architecture of the sum-product function.': 1.3310546875, 'The example is Sec 3.1 seems to be fairly arbitrary.': 1.3862719535827637, '4)': 1.3862943649291992, 'The experiment section is probably the weakest part.': 0.90577632188797, ""NORB is a very small and toy-ish dataset by today's standard."": 1.3862943649291992, 'Even on this small dataset, the proposed method is only slighly better than SVM (it is not clear whether ""SVM"" in Table 2 is linear SVM or kernel SVM.': 1.2292766571044922, 'If it is linear SVM, I suspect the performance of ""SVM"" will be even higher when you use kernel SVM), and far worse than convnet.': 0.697746992111206, 'The proposed method only shows improvement over convnet on synthetic datasets (NORB compositions, NORM symmetries)': 1.3693982362747192, 'Overall, I think this paper has some interesting ideas.': 1.2184710502624512, 'But in its current form, it is a bit too preliminary and more work is needed to show its advantage.': 1.3855012655258179, 'Having said that, I acknowledge that in the machine learning history, many important ideas seem pre-mature when they were first proposed, and it took time for these ideas to develop.': 1.3352044820785522}"
259,https://openreview.net/forum?id=S1HEBe_Jl,"{'The submission proposes to modify the typical GAN architecture slightly to include ""encrypt"" (Alice) and ""decrypt"" (Bob) modules as well as a module trying to decrypt the signal without a key (Eve).': 1.0986123085021973, 'Through repeated transmission of signals, the adversarial game is intended to converge to a system in which Alice and Bob can communicate securely (or at least a designated part of the signal should be secure), while a sophisticated Eve cannot break their code.': 1.0986123085021973, 'Examples are given on toy data:': 1.0986121892929077, '""As a proof-of-concept, we implemented Alice, Bob, and Eve networks that take N-bit random plain-text and key values, and produce N-entry floating-point ciphertexts, for N = 16, 32, and 64.': 0.2907402515411377, 'Both plaintext and key values are uniformly distributed.""': 1.0982905626296997, 'The idea considered here is cute.': 1.0981308221817017, 'If some, but not necessarily all of the signal is meant to be secure, the modules can learn to encrypt and decrypt a signal, while an adversary is simultaneously learned that tries to break the encryption.': 1.0986063480377197, 'In this way, some of the data can remain unencrypted, while the portion that is e.g. correlated with the encrypted signal will have to be encrypted in order for Eve to not be able to predict the encrypted part.': 1.098527193069458, 'While this is a nice thought experiment, there are significant barriers to this submission having a practical impact:': 1.0985649824142456, '1) GANs, and from the convergence figures also the objective considered here, are quite unstable to optimize.': 0.7206950783729553, 'The only guarantees of privacy are for an Eve that is converged to a very strong adversary (stronger than a dedicated attack over time).': 1.0985982418060303, 'I do not see how one can have any sort of reliable guarantee of the safety of the data transmission from the proposed approach, at least the paper does not outline such a guarantee.': 1.0986067056655884, '2) Public key encryption systems are readily available, computationally feasible, and successfully applied almost anywhere.': 0.556771993637085, 'The toy examples given in the paper do not at all convince me that this is solving a real-world problem at this point.': 1.09860098361969, 'Perhaps a good example will come up in the near future, and this work will be shown to be justified, but until such an example is shown, the approach is more of an interesting thought experiment.': 1.098610758781433, 'The paper deals with an interesting application of adversarial training to encryption.': 1.0986123085021973, 'It considers the standard scenario of Alice, Eve and Bob, where A and B aim to exchange messages conditioned on a shared key, while Eve should be unable to encrypt the message.': 1.0654138326644897, 'Experiments are performed in a simple symmetric 16 bit encryption task, and an application on privacy.': 1.0986123085021973, 'The concepts, ideas and previous literature are quite nicely and carefully presented.': 1.0986123085021973, 'The only major concern I have - and I apologize to the authors for not raising this earlier - are the experiments in section 3.': 1.0986123085021973, ""In particular, I don't quite get the scenario."": 1.0986123085021973, 'The reasoning here seems to be as follows: given information < A, B, C, D >, I want to give the public the value of D (e.g. movies watched) without releasing information about C (e.g. gender).': 1.0986123085021973, 'In this scenario, Eve would need to be able to reconstruct D as good as possible without gaining information about C.': 1.0986123085021973, 'What is described in section 3, however, is that D and D-public are both reconstructed by Bob, but why would Bob reconstruct the latter (he is not public, in particular because he is allowed to reconstruct C, which is not tested here)?': 1.0986123085021973, 'Also, Eve only tries to estimate C, thus rendering the scenario not different in any way to the scenario considered in section 2.': 1.0986058712005615, 'I have two more minor concerns:': 1.0986123085021973, '1) As raised in the pre-review, Eve should actually be stronger then Alice and Bob in order to be able to compensate for the missing key.': 0.7652806043624878, 'The authors noted they have been doing these experiments and are going to add the results.': 0.978558361530304, '2) In any natural encryption case I would expect the length of the key to be much shorter then the length of the message.': 1.0985987186431885, 'This, however, could potentially make the scenario much easier for Eve (although I doubt any of the results will change if the key is long enough).': 0.9924426078796387, 'I like the creative application of adversarial training to a completely different domain, and I believe it could be the starting point of a very interesting direction in cryptographic systems or in privacy applications (although it is unclear whether the weak guarantees of neural network based approaches can ever be overcome).': 1.0803576707839966, 'At the same time the application in the privacy setting leaves me quite confused, and the symmetric encryption example is not particularly strong either.': 0.6641013622283936, ""I'd appreciate if the authors could address the major concern I raised above, and I will be quite happy to raise the score in case this confusion can be resolved."": 0.8911512494087219, 'This paper proposed to use GAN for encrypted communications.': 0.9242554903030396, 'In section 2, the authors proposed a 3 part neural network trained to encode and decode data.': 1.0984241962432861, 'This model does not have any practical value except paving the way for describing the next model in section 3: it is strictly worse than any provable cryptography system.': 1.0986123085021973, 'In section 3, the authors designed a task where they want to hide part of the data, which has correlated fields, while publishing the rest.': 1.0986087322235107, ""However, I'm having trouble thinking of an application where this system is better than simply decorrelating the data and encrypting the fields one wants to hide with a provable cryptography system while publishing the rest in plain text."": 1.0986123085021973}"
260,https://openreview.net/forum?id=S1HcOI5le,"{'This paper proposes a k-shot learning framework that can be used on existing pre-trained networks by grouping filters that produce similar activations.': 1.0978950262069702, 'The grouped filters are learned together to address overfitting when only few training samples are available.': 1.09834885597229, ""The idea of the paper is interesting there are some encouraging results, but the current version doesn't seem ready for publication:"": 1.0985156297683716, 'Performance:': 1.093323826789856, 'The method should be compared with other state-of-the-art k-shot learning methods (e.g., Matching Networks by Vinyals et al., 2016).': 0.9568761587142944, ""It's not clear how this method compares against them."": 1.0985395908355713, 'Missing explanation:': 1.0986123085021973, 'Experimental setting for k-shot learning should be more detailed.': 1.0986121892929077, 'Measure:': 1.0986123085021973, 'Accuracy difference does not look like a good idea for comparing the baseline method and the proposed one.': 1.098555564880371, 'Just raw accuracies would be fine.': 1.0645461082458496, 'Many grammatical errors and inappropriate formatting of citations, such as:': 0.9848472476005554, 'M. et al. (2011)': 1.0973668098449707, 'ImageNet (Alex et al. (2012))': 0.7658926248550415, 'Judy et al. (2013): this reference appears three times in the reference section.': 1.0488824844360352, 'This paper proposes a regularization technique for k-shot learning based on orthogonal grouping of units in a neural network.': 1.0986119508743286, 'The units within a group are forced to be maximally similar, at the same time the units from different groups are encouraged to be orthogonal.': 1.0984952449798584, 'While I like the motivation of the approach, the empirical analysis provided in the paper doesn’t look particularly convincing.': 1.0986021757125854, 'My main concerns are the following:': 1.0985711812973022, '1. The method is sensitive to the values of alpha and beta and a poor choice of those hyperparameters can lead to a quite drastic drop in performance comparing the minor gains one gets when alpha and beta are set properly.': 1.0895962715148926, ""2. It seems strange that the best performance is obtained when the group's size ratio is 0.5. From the figures in the paper, it follows that usually, one has more “orthogonal” groups in a filter bank. I have an impression that the empirical evidence doesn’t align well with the motivation of the proposed approach."": 0.10976205766201019, '3. The paper contains a significant amount of typos and incorrectly formatted references. There are also several places in the manuscript that I found hard to understand due to unusual phrasing.': 0.2585288882255554, 'I would like to thank the authors for answering/addressing my pre-review questions.': 1.0986119508743286, 'I would be grateful if the authors could provide more clarifications of the following:': 1.0885246992111206, '1. Question 2: I’m not sure if modifying \\theta_{map} alone would result in any learning at all. Do I understand correctly that \\theta_{map} is only used to define groups? If so, then I don’t see how the proposed method can be used in the purely unsupervised regime.': 0.3764759302139282, '2. Question 3: I was not referring to the fixed clustering based on the filter of the pre-trained network. One can perform that clustering at every step of the k-shot learning process. I’m not sure I understand why the authors visualize grouping of _filters_ while in the actual algorithm they group _activations_.': 0.17967760562896729, 'Overall, the paper is quite interesting but needs a stronger empirical justification of the approach as well as a better presentation of the material.': 1.0986087322235107, 'The authors of this work propose a learnable approach to reducing the dimensionality of learned filters in deep neural networks.': 1.0986123085021973, 'This is an interesting approach, but the presented work looks a bit raw.': 1.0986047983169556, '1. There are many typos in this manuscript.': 0.8895531892776489, ""2. The experimental results are rather weak and don't show much improvement in accuracy. Instead the authors could position this work as a compression mechanism and would have to compare to low rank approximation of filters for DNNs. Yet this is not done."": 1.0986123085021973, '3. Aside from compression, OMG can be viewed as a form of regularization to reduce the unnecessary capacity of the network to improve generalization. Again, this is not addressed in enough detail.': 1.0986123085021973, ""4. If the authors care to compare their approach to other 1-shot learning methods, then they would have to evaluate their approach with siamese and triplet learning networks. This isn't done."": 1.0986123085021973}"
261,https://openreview.net/forum?id=S1J0E-71l,"{'Summary:': 1.0977983474731445, 'This paper proposes to use surprisal-driven feedback for training recurrent neural networks where they feedback the next-step prediction error of the network as an input to the network.': 1.0980088710784912, 'Authors have shown a result on language modeling tasks.': 1.0985621213912964, 'Contributions:': 1.098603367805481, 'The introduction of surprisal-driven feedback, which is just the feedback from the errors of the model from the previous time-steps.': 0.2665116786956787, 'Questions:': 1.094185709953308, 'A point which is not fully clear from the paper is whether if you have used the ground-truth labels on the test set for the surprisal feedback part of the model?': 1.0986123085021973, 'I assume that authors do that since they claim that they use the misprediction error as additional input.': 1.0399192571640015, 'Criticisms:': 1.0986123085021973, 'The paper is really badly written, authors should rethink the organization of the paper.': 1.0980850458145142, 'Most of the equations presented in the paper, about BPTT are not necessary for the main-text and could be moved to Appendix.': 1.0166869163513184, 'The justification is not convincing enough.': 1.0986123085021973, 'Experimental results are lacking, only results on a single dataset are provided.': 1.0779322385787964, 'Although the authors claim that they got SOTA on enwiki8, there are other papers such as the HyperNetworks that got better results (1.34) than the result they achieve.': 1.0176277160644531, 'This claim is wrong.': 1.0970299243927002, 'The model requires the ground-truth labels for the test-set, however, this assumption really limits the application of this technique to a very limited set of applications(more or less rules out most conditional language modeling tasks).': 1.0986123085021973, 'High-level Review:': 1.0985692739486694, 'Pros:': 1.0930993556976318, '- A simple modification of the model that seems to improve the results and it is an interesting modification.': 0.34574615955352783, 'Cons:': 1.0986123085021973, '- The authors need to use test-set labels.': 0.39455127716064453, '- Writing of the paper is bad.': 0.59816575050354, '- The authors assume that they have access to the ground-truth labels during the test-set.': 0.7509127259254456, '- Experimental results are lacking': 1.0961986780166626, 'This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function in order to enhance the modelling power of a dynamic system such as RNNs.': 1.0986121892929077, 'This paper makes an  erroneous assumption: test label information is not given in most of the real world applications, except few applications.': 1.0986123085021973, 'This means that the language modelling task, which is the only experiment of this paper, may not be the right task to test this approach.': 1.0986073017120361, 'Also, comparing against the models that do not use test error signal at inference time is unfair.': 1.0986123085021973, 'We cannot just say that the test label information is being observed, this only holds in online-prediction problems.': 1.0986123085021973, 'The experiment is only conducted on one dataset, reporting state-of-the-art result, but unfortunately this is not true.': 1.0986099243164062, 'There are already more than four papers reporting better numbers than the one reported in this task, however the author did not cite them.': 1.0986123085021973, 'I understand that this paper came before the other papers, but the manuscript should be updated before the final decision.': 0.4988645613193512, 'The model size is still missing and without this information, it is hard to judge the contribution of the proposed trick.': 0.9880673885345459, 'This paper proposes to leverage ""surprisal"" as top-down signal in RNN.': 1.0834736824035645, 'More specifically author uses the error corresponding to the previous prediction as an extra input at the current timestep in a LSTM.': 1.0855159759521484, 'The general idea of suprising-driven feedback is interesting for online prediction task.': 1.0984928607940674, 'It is a simple enough idea that seems to bring some significant improvements.': 1.096152663230896, 'However, the paper in its current form has some important flaws.': 1.0986123085021973, 'Overall, the paper writing could be improved.': 1.0986123085021973, 'In particular, section 2.4 and 2.5 is composed mostly by the equations of the forward and backward propagation of feedback RNN and feedback LSTM.': 0.8050918579101562, 'However, author provides no analysis along with those equations.': 1.098610520362854, 'It is therefore not clear what insight the author tries to express in those sections.': 1.0132988691329956, 'In addition, feedback RNN is not evaluated in the experimental section, so it is not clear why feedback RNN is described.': 0.3088212013244629, 'The experimental evaluation is limited.': 1.0986123085021973, 'Only one dataset enwik8 is explored.': 1.0971976518630981, 'I think it is necessary to try the idea on different datasets to see if feedback LSTM sees some consistent improvements.': 1.0957834720611572, 'Also, author claims state-of-art on enwik8, but hypernetwork, already cited in the paper, achieves better results (1.34 BPC, table 4 in the hypernetworks paper).': 1.0986123085021973, 'Author only compares to methods that do not use last prediction error as extra signal.': 1.0986117124557495, 'I would argue that a comparison with dynamic evaluation would be more fair.': 1.0986123085021973, 'Feedback LSTM uses prediction error as extra input in the forward prop, while dynamic evaluation  backprop it through the network and change the weight accordingly.': 1.0986089706420898, 'Also they don\'t propagate the prediction error in the same way, they both leverage ""extra"" supervised information through the prediction errors.': 1.0972273349761963, 'In summary:': 0.7423417568206787, 'Interesting idea': 1.0986123085021973, 'Seems to improve performances': 0.4126276671886444, 'Paper writing': 1.0986123085021973, 'Weak evaluation (only one dataset)': 1.0265378952026367, 'Compare only with approaches that does not use the last-timestep error signal': 1.0903105735778809}"
262,https://openreview.net/forum?id=S1JG13oee,"{'In this paper, the authors extend the f-GAN by using Bregman divergences for density ratio matching.': 0.882843017578125, 'The argument against f-GAN (which is a generalization of the regular GAN) is that the actual objective optimized by the generator during training is different from the theoretically motivated objective due to gradient issues with the theoretically motivated objective.': 1.0984222888946533, 'In b-GANS, the discriminator is a density ratio estimator (r(x) = p(x) / q(x)), and the generator tries to minimize the f-divergence between p and q by writing p(x) = r(x)q(x).': 1.0984090566635132, 'My main problem with this paper is that it is unclear why any of this is useful.': 1.0951262712478638, 'The connection to density estimation is interesting, but any derived conclusions between the two seem questionable.': 1.0978832244873047, 'For example, in previous density estimation literature, the Pearson divergence is more stable.': 1.0094488859176636, 'The authors claim that the same holds for GANS and try to show this in their experiments.': 1.0981495380401611, 'Unfortunately, the experiments section is very confusing with unilluminating figures.': 1.0411807298660278, 'Looking at the graph of density ratios is not particularly illuminating.': 1.0973786115646362, 'They claim that for the Pearson divergence and modified KL-divergence, ""the learning did not stop"" by looking at the graph of density ratios.': 1.0985963344573975, 'This is completely hand-wavey and no further evidence is given to back this claim.': 1.097570538520813, 'Also, why was the normal GAN objective not tried in light of this analysis?': 1.0946786403656006, 'Furthermore, it seems that despite criticizing normal GANs for using a heuristic objective for the generator, multiple heuristics objectives and tricks are used to make b-GAN work.': 0.42315009236335754, 'I think this paper would be much improved if it was rewritten in a clear fashion.': 1.0347018241882324, 'As it stands, it is difficult to understand the motivation or intuition behind this work.': 1.0986123085021973, 'This paper proposes b-GAN, which trains a discriminator by estimating density ratio that minimizes Bregman divergence.': 1.0986123085021973, 'The authors also discuss how b-GANs relate to f-GAN and the original GAN work, providing a unifying view through the lens of density ratio estimation.': 1.098611831665039, 'Note that the unifying view applies only to GAN variants which optimize density ratios.': 1.0986123085021973, 'In general, GANs which use MMD in the discriminator step do not fit in the b-GAN framework except for special choices of the kernel.': 1.0986123085021973, 'I was a bit confused about the dual relationship between f-GAN and b-GAN.': 1.0986123085021973, 'Are the conditions on the function f the same in both cases?': 1.0986123085021973, ""If so, what's the difference between f-GAN and b-GAN (other than the fact that the former has been derived using f-divergence and the latter has been derived using Bregman divergence)?"": 1.0986123085021973, 'One of the original claims was that b-GANs optimize f-divergence directly as opposed to f-GAN and GAN.': 1.0986123085021973, ""However, in practice, the authors optimize an approximation to the f-divergence; the quality of the approximation is not quantified anywhere, so b-GAN doesn't seem more principled than f-GAN and GAN."": 1.0969493389129639, 'The experiments left me a bit confused and were not very illuminating on the choice of f.': 1.0986123085021973, 'Overall, I liked the connections to the density ratio estimation literature.': 1.0986123085021973, 'The appendix seems like a scattered collection right now.': 1.0986123085021973, 'Some re-writing of the text would significantly improve this paper.': 1.0986123085021973, 'This submission introduces a formulation of Generative Adversarial Networks (GANs) under the lens of density ratio estimation, when using Bregman divergences.': 1.0986123085021973, 'Even thought GANs already perform density estimation, the motivation of using Bregman divergences is to obtain an objective function with stronger gradients.': 1.0986123085021973, 'I have three concerns with this submission.': 1.0986123085021973, 'First, the exposition of the paper must be significantly improved.': 1.0986123085021973, 'The current version of the manuscript is at some points unreadable, and does a poor job at motivating, describing, and justifying the contributions.': 1.0986123085021973, 'Second, the authors scatter a variety of alternatives and heuristics throughout the description of the proposed b-GAN.': 1.0986123085021973, 'This introduces a great amount of complexity when it comes to understanding, implementing, and using b-GAN.': 1.0986123085021973, 'Further work is necessary to rule out (in a principled manner!)': 1.0986123085021973, 'many of the proposed variants of the algorithm.': 1.0986123085021973, 'Third, it is next to impossible to interpret the experimental results, in particular Figures 2, 3, 4.': 1.0986123085021973, 'The authors claim that these figures show that ""learning does not stop"", but such behavior can also be attributed to the typical chaotic dynamics of GANs.': 1.0986123085021973, 'Even after reading Appendix A, I am left unconvinced on whether the proposed approach provides with any practical advantage (even no comparison is offered to other GAN approaches with similar architectures).': 1.0986123085021973, 'Overall, I believe this submission calls for significant improvements before being considered for publication.': 1.0986123085021973}"
263,https://openreview.net/forum?id=S1Jhfftgx,"{'This paper attempted to solve an interesting problem': 1.0986123085021973, 'incorporating hard constraints in seq2seq model.': 1.0986123085021973, 'The main idea is to modify the weight of the neural network in order to find a feasible solution.': 1.0986123085021973, 'Overall, the idea presented in the paper is interesting, and it tries to solve an important problem.': 1.0986123085021973, 'However, it seems to me the paper is not ready to publish yet.': 1.098556399345398, 'Comments:': 1.0971759557724, 'The first section of the paper is clear and well-motivated.': 1.0986123085021973, 'The authors should report test running time.': 1.0986123085021973, 'The proposed approach changes the weight matrix.': 1.097769021987915, 'As a result, it needs to reevaluate the values of hidden states and perform the greedy search for each iteration of optimizing Eq (7).': 1.0986106395721436, 'This is actually pretty expensive in comparison to running the beam search or other inference methods.': 1.0986123085021973, ""Therefore, I'm not convinced that the proposed approach is a right direction for solving this problem (In table, 1, the authors mention that they run 100 steps of SGD)."": 1.0982288122177124, 'If I understand correctly, Eq (7) is a noncontinuous function w.r.t W_\\lambda and the simple SGD algorithm will not be able to find its minimum.': 1.0985580682754517, 'For dependency parsing, there are standard splits of PTB.': 1.098370909690857, 'I would suggest the authors follow the same splits of train, dev, and test in order to compare with existing results.': 1.0983279943466187, 'Minor comments: several sentences are misleading and should be rewritten carefully.': 1.0983797311782837, 'Beginning of Section 3: ""A major advantage of neural network is that once trained, inference is extremely efficient.""': 1.0272395610809326, 'This sentence is not generally right,  and I guess the authors mean if using greedy search as inference method, the inference is efficient.': 1.0986120700836182, 'The description in the end of section 2 is awkward.': 1.0906217098236084, 'To me,  feed-forward and RNN  are general families that cover many specific types of neural networks, and the training procedures are not necessarily to aim to optimize Eq. (2).': 1.0986055135726929, 'Therefore, the description here might not be true.': 0.5938209891319275, ""In fact, I don't think there is a need to bring up feed-forward networks here; instead, the authors should provide more details the connection between RNN and Eq (2) here."": 1.0953725576400757, 'The second paragraph of section 3 is related to [1], where it shows the search space of the inference can be represented as an imperative program.': 1.0986119508743286, '[1] Credit assignment compiler for joint prediction, NIPS 2016': 0.336510568857193, 'This paper proposes a way of enforcing constraints (or penalizing violations of those constraints) on outputs in structured prediction problems, while keeping inference unconstrained.': 1.0986123085021973, 'The idea is to tweak the neural network parameters to make those output constraints hold.': 1.0986123085021973, 'The underlying model is that of structured prediction energy networks (SPENs), recently proposed by Belanger et al.': 1.098611831665039, ""Overall, I didn't find the approach very convincing and the paper has a few problems regarding the empirical evaluation."": 1.0986123085021973, ""There's also some imprecisions throughout."": 1.0022647380828857, 'The proposed approach (secs 6 and 7) looks more like a ""little hack"" to try to make it vaguely similar to Lagrangian relaxation methods than something that is theoretically well motivated.': 1.0986123085021973, 'Before eq. 6: ""an exponential number of dual variables""': 1.0986123085021973, 'why exponential?': 1.0985618829727173, ""it's not one dual variable per output."": 1.098611831665039, 'From the clarification questions:': 1.0922397375106812, 'The accuracy reported in Table 1 needs to be explained.': 0.6962471008300781, 'for the parsing experiments it would be good to report the usual F1 metric of parseval, and to compare with state of the art systems.': 1.0986101627349854, 'should use the standard training/dev/test splits of the Penn Treebank.': 1.0986123085021973, 'The reported conversion rate in Table 1 does not tell us how many violations are left by the unconstrained decoder to start with.': 1.0986123085021973, 'It would be good to know what happens in highly structured problems where these violations are frequent, since these are the problems where the proposed approach could be more beneficial.': 1.0986123085021973, 'Minor comments/typos:': 1.0986123085021973, 'sec.1: ""there are"" -> there is?': 0.7838326692581177, 'sec 1: ""We find that out method is able to completely satisfy constraints on 81% of the outputs.""': 1.0967366695404053, '-> at this point, without specifying the problem, the model, and the constraints, this means very little.': 1.0986123085021973, 'How many constrains does the unconstrained method satisfies?': 1.0865062475204468, 'sec 2 (last paragraph): ""For RNNs, each output depends on hidden states that are functions of previous output values""': 0.7190837860107422, ""this is not very accurate, as it doesn't hold for general RNNs, but only for those (e.g. RNN decoders in language modeling) where the outputs are fed back to the input in the next time frame."": 1.0134366750717163, 'sec 3: ""A major advantage of neural networks is that once trained, inference is extremely efficient.""': 0.9119663834571838, 'advantage over what?': 1.0986123085021973, 'also, this is not necessarily true, depends on the network and on its size.': 1.097420334815979, 'sec 3: ""our goal is take advantage"" -> to take advantage': 0.9853811860084534, 'last paragraph of sec 6: ""the larger model affords us"" -> offers?': 1.0985246896743774, 'This paper proposes a dual-decomposition-inspired technique for enforcing constraints in neural network prediction systems.': 1.0984280109405518, ""Many things don't quite make sense to me:"": 1.098595380783081, ""1. Most seq2seq models (such as those used for parsing) have substantially better performance when coupled with beam search than greedy search, and exact search is infeasible. This is because these models are trained to condition on discrete values of past outputs in each timestamp, and hence the problem of finding the highest-scoring total sequence of outputs is not solvable efficiently. It's unclear what kind of model this paper is using which allows for greedy decoding, and how well it compares to the state-of-the-art, specially when constraint-aware beam search is used. This comparison is specially interesting because both constrained beam search and this dual-decomposition-like approach require multiple computations of the model's score."": 0.4719878137111664, ""2. It's unclear (to me at least) how to differentiate the constraint term g() in the objective function in the general case (though the particular example used here is understandable)"": 0.8302485346794128, '3. The paper claims that ""Lagrangian relaxation methods for NLP have multipliers for each output variable that can be combined with linear models [...] . Since our non-linear functions and global constraints do not afford us the same ability"" but it is possible to add linear terms to the outputs of neural networks, possibly avoiding rerunning all the expensive inference terms.': 0.48422735929489136, 'Moreover, the justification for the particular method is hand-wavy at best, with inconvenient terms from equations ignored or changed at will.': 1.0882924795150757, 'At this point it might be better to omit the attempted theoretical explanation and just present this method as a heuristic which is likely to achieve the desired result.': 1.0986078977584839, 'This, plus the concerns around lack of clear comparisons with baselines on benchmark problems lead me to recommend rejection.': 1.0986123085021973, 'Further explanation of how this compares with beam search, how this relates to the state-of-the-art, and a better explanation for how to come up with differentiable constraint sets, are probably required for acceptance.': 1.0978443622589111}"
264,https://openreview.net/forum?id=S1LVSrcge,"{'TLDR:': 1.0986123085021973, 'The authors present Variable Computation in Recurrent Neural Networks (VCRNN).': 1.0986123085021973, 'VCRNN is similar in nature to Adaptive Computation Time (Graves et al., 2016).': 0.5291179418563843, 'Imagine a vanilla RNN, at each timestep only a subset (i.e., ""variable computation"") of the state is updated.': 1.0986123085021973, 'Experimental results are not convincing, there is limited comparison to other cited work and basic LSTM baseline.': 1.0985941886901855, '=== Gating Mechanism ===': 1.0986123085021973, 'At each timestep, VCRNN generates a m_t vector which can be seen as a gating mechanism.': 1.0986123085021973, 'Based off this m_t vector, a D-first (D-first as in literally the first D RNN states) subset of the vanilla RNN state is gated to be updated or not.': 1.0983346700668335, 'Extra hyperparams epsilon and \\bar{m} are needed': 1.0967365503311157, 'authors did not give us a value or explain how this was selected or how sensitive and critical these hyperparms are.': 0.8950496912002563, 'This mechanism while novel, feels a bit clunky and awkward.': 0.8642320036888123, 'It does not feel well principled that only the D-first states get updated, rather than a generalized solution where any subset of the state can be updated.': 1.0986045598983765, 'A short section in the text comparing to the soft-gating mechanisms of GRUs/LSTMs/Multiplicative RNNs (Wu et al., 2016) would be nice as well.': 1.0986030101776123, '=== Variable Computation ===': 1.0986120700836182, 'One of the arguments made is that their VCRNN model can save computation versus vanilla RNNs.': 0.9306312799453735, 'While this may be technically true, in practice this is probably not the case.': 0.9889808297157288, 'The size of the RNNs they compare to do not saturate any modern GPU cores.': 1.0026179552078247, 'In theory computation might be saved, but in practice there will probably be no difference in wallclock time.': 1.095072627067566, 'The authors also did not report any wallclock numbers, which makes this argument hard to sell.': 0.9555121660232544, '=== Evaluation ===': 1.0985697507858276, 'This reviewer wished there was more citations to other work for comparison and a stronger baseline (than just a vanilla RNN).': 1.0800594091415405, 'First, LSTMs are very simple and quite standard nowadays': 0.5518642067909241, 'there is a lack of comparison to any basic stacked LSTM architecture in all the experiments.': 1.0954804420471191, 'The PTB BPC numbers are quite discouraging as well (compared to state-of-the-art).': 0.8106054067611694, 'The VCRNN does not beat the basic vanilla RNN baseline.': 0.4161808490753174, 'The authors also only cite/compare to a basic RNN architecture, however there has been many contributions since a basic RNN architecture that performs vastly better.': 1.0816493034362793, 'Please see Chung et al., 2016 Table 1.': 1.096898078918457, 'Chung et al., 2016 also experimented w/ PTB BPC and they cite and compare to a large number of other (important) contributions.': 1.0937669277191162, 'One cool experiment the authors did is graph the per-character computation of VCRNN (i.e., see Figure 2).': 0.850203812122345, 'It shows after a space/word boundary, we use more computation!': 0.6164495944976807, 'Cool!': 1.0846705436706543, 'However, this makes me wonder what a GRU/LSTM does as well?': 0.7428445816040039, 'What is the magnitude of the of the change in the state vector after a space in GRU/LSTM': 1.0986123085021973, 'I suspect them to do something similar.': 1.0986123085021973, '=== Minor ===': 1.0986123085021973, '* Please add Equations numbers to the paper, hard to refer to in a review and discussion!': 1.0986123085021973, 'References': 1.0986123085021973, 'Chung et al., ""Hierarchical Multiscale Recurrent Neural Networks,"" in 2016.': 1.0986123085021973, 'Graves et al., ""Adaptive Computation Time for Recurrent Neural Networks,"" in 2016.': 1.0986123085021973, 'Wu et al., ""On Multiplicative Integration with Recurrent Neural Networks,"" in 2016.': 1.0980273485183716, 'This is high novelty work, and an enjoyable read.': 1.0877922773361206, 'My concerns about the paper more or less mirror my pre-review questions.': 1.098606824874878, 'I certainly agree that the learned variable computation mechanism is obviously doing something interesting.': 1.0986123085021973, 'The empirical results really need to be grounded with respect to the state of the art, and LSTMs are still an elephant in the room.': 1.0945310592651367, '(Note that I do not consider beating LSTMs, GRUs, or any method in particular as a prerequisite for acceptance, but the comparison nevertheless should be made.)': 0.6003172397613525, 'In pre-review responses the authors brought up that LSTMs perform more computation per timestep than Elman networks, and while that is true, this is an axis along which they can be compared, this factor controlled for (at least in expectation, by varying the number of LSTM cells), etc.': 1.0986121892929077, 'A brief discussion of the proposed gating mechanism in light of the currently popular ones would strengthen the presentation.': 1.0986123085021973, '2017/1/20:': 1.0986123085021973, ""In light of my concerns being addressed I'm modifying my review to a 7, with the understanding that the manuscript will be amended to include the new comparisons posted as a comment."": 1.0986123085021973, 'This paper describes a simple but clever method for allowing variable amounts of computation at each time step in RNNs.': 1.0986123085021973, 'The new architecture seems to outperform vanilla RNNs on various sequence modelling tasks.': 1.0986123085021973, 'Visualizations of the assignment of computational resources over time support the hypothesis that the model is able to learn to assign more computations whenever longer longer term dependencies need to be taken into account.': 1.0986123085021973, 'The proposed model is evaluated on a multitude of tasks and its ability to outperform similar architectures seems consistent.': 1.0986123085021973, 'Some of the tasks allow for an interesting analysis of the amount of computation the model requests at each time step.': 1.0986123085021973, 'It’s very interesting to see how the model seems to use more resources at the start of each word or ASCII character.': 1.098611831665039, 'I also like the investigation of the effect of imposing a pattern of computational budget assignment which uses prior knowledge about the task.': 1.0986123085021973, 'The superior performance of the architecture is impressive but I’m not yet convinced that the baseline models had an equal number of hyperparameters to tune.': 1.0986123085021973, 'I’ll come back to this point in the next paragraph because it’s mainly a clarity issue.': 1.0986123085021973, 'The abstract claims that the model is computationally more efficient than regular RNNs.': 1.0615991353988647, 'There are no wall time measurements supporting this claim.': 1.0986015796661377, 'While the model is theoretically able to save computations, the points made by the paper are clearly more conceptual and about the ability of the model to choose how to allocate its resources.': 1.0986123085021973, 'This makes the paper interesting enough by itself but the claims of computational gains are misleading without actual results to back them up.': 1.0916221141815186, 'I also find it unfortunate that it’s not clear from the text how the hyperparameter \\bar{m} was chosen.': 1.098517656326294, 'Whether it was chosen randomly or set using a hyperparameter search on held-out data influences the fairness of a comparison with RNNs which did not have a similar type of hyperparameter for controlling regularization like for example dropout or weight noise (even if regularization of RNNs is a bit tricky).': 1.0986121892929077, 'I don’t consider this a very serious flaw because I’m impressed enough by the fact that the new architecture achieves roughly similar performance while learning to allocate resources but I do think that details of this type are too important to be absent from the text.': 0.8603213429450989, 'Even if the superior performance is due to this extra regularization controlling parameter it can actually be seen as a useful part of the architecture but it would be nice to know how sensitive the model is to its precise value.': 1.0986123085021973, 'To my knowledge, the proposed architecture is novel.': 1.0986123085021973, 'The way the amount of computation is determined is unlike other methods for variable computation I have seen and quite inventive.': 1.0986123085021973, 'Originality is one of this paper’s strongest points.': 1.0986123085021973, 'It’s currently hard to predict whether this method for variable computation will be used a lot in practice given that this also depends on how feasible it is to obtain actual computational gains at the hardware level.': 1.0986121892929077, 'That said, the architecture may turn out to be useful for learning long-term dependencies.': 1.0986123085021973, 'I also think that the interpretability of the value m_t is a nice property of the method and that it’s visualizations are very interesting.': 1.0986120700836182, 'It might shed some more light into what makes certain tasks difficult for RNNs.': 1.0986123085021973, 'Pros:': 1.0986123085021973, 'Original clever idea.': 1.0986123085021973, 'Nice interesting visualizations.': 1.0986123085021973, 'Interesting experiments.': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'Some experimental details are not clear.': 1.0931307077407837, 'I’m not convinced of the strength of the baseline.': 1.0986037254333496, 'The paper shouldn’t claim actual computational savings without reporting wall-clock times.': 1.0986099243164062, 'Edit:': 1.098611831665039, ""I'm very positively impressed by the way the authors ended up addressing the biggest concerns I had about the paper and raised my score."": 1.0910239219665527, 'Adding an LSTM baseline and results with a GRU version of the model significantly improves the empirical quality of the paper.': 1.0897459983825684, 'On top of that, the authors addressed my question about some experimental detail I found important and promised to change the wording of the paper to remove confusion about whether the computational savings are conceptual or in actual wall time.': 1.0986123085021973, ""I think it's fine that they are conceptual only as long as this is clear from the paper and abstract."": 1.0979429483413696, 'I want to make clear to the AC that since the changes to the paper are currently still promises, my new score should be assumed to apply to an updated version of the paper in which the aforementioned concerns have indeed been addressed.': 1.0963134765625, ""Since I didn't know that the difference with the SOTA for some of these tasks was so large, I had to lower my score again after learning about this."": 1.0986109972000122, ""I still think it's a good paper but with these results I cannot say that it stands out."": 1.0221728086471558}"
265,https://openreview.net/forum?id=S1OufnIlx,"{'The paper is well motivated and well written.': 1.0986123085021973, 'The setting of the experiments is to investigate a particular case.': 1.0986123085021973, 'While the results of experiments are interesting, such investigation is not likely to systematically improve our understanding of the adversarial example phenomenon.': 1.0986123085021973, 'Overall, the contribution of the paper seems incremental.': 1.0986123085021973, 'Pros:': 1.0986123085021973, '1. This paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples. This method could be useful when the number of classes in the dataset is huge.': 1.0986123085021973, '2. Some observations of the experiments are interesting. For example, overall photo transformation does not affect much the accuracy on clean image, but could destroy some adversarial methods.': 1.0985994338989258, 'Cons:': 1.0699288845062256, '1. As noticed by the authors, some similar works exist in the literature. According to the authors, what differs this work from other existing works is that this paper tend to fool NN by making very small perturbations of the input. But based on the experiments and the demonstration (the real pictures), it is arguable that the perturbations in the experiments are still small.': 1.098604679107666, '2. Some hypotheses proposed in the paper based on one-shot experiments seems too rushy.': 1.0985795259475708, '3. As mentioned above, the results of this paper seems not really improving the understanding of the adversarial example phenomenon.': 1.0986074209213257, 'In some sense, the Sharif et al. work ""scooped"" this paper, but as the authors indicate, the spirit of the work remains somewhat different.': 1.0986123085021973, ""Sharif's approach was constrained in an interesting way (usable surface area limited to front portion of glasses frames) and also a bit gimmicky (focused on fooling a small scale face ID system to select among a set of celebrities)."": 1.0986120700836182, 'The present work is less sensational and more methodical in its study of physical manifestations of adversarial patterns for standard benchmark objects.': 1.0984910726547241, 'I think the paper is at least a little above the bar since it poses an interesting question and carries out an informative empirical study.': 1.0986123085021973, 'Description.': 1.098604679107666, 'The paper investigates whether adversarial examples survive different geometric and photometric image transformations,': 0.6452397108078003, 'including a complex transformation where the image is printed on the paper and captured again by a cell-phone camera.': 1.0985268354415894, 'The paper considers three different methods to generate adversarial examples — images with added small amount of noise that changes the output of a classification neural network.': 1.0660549402236938, 'In the quantitative experiments the paper assumes available access to the neural network and its parameters.': 1.0986123085021973, 'Qualitative results are shown for a set-up where the network used to generate adversarial images is different from the test network.': 1.0986123085021973, 'Strong  points.': 1.0986123085021973, 'adversarial examples are an interesting phenomenon that is worth detailed investigation.': 1.0986123085021973, 'the paper is well written and presented.': 1.0986123085021973, 'Results showing (and quantifying) that adversarial examples can survive a complex image transformation such as printing and re-capturing are interesting.': 1.0986123085021973, 'Experiments are well done and solid.': 1.0986123085021973, 'Weak points:': 1.0986123085021973, 'Probably the main negative point is the amount of novelty and contribution.': 1.0986123085021973, 'The paper essentially presents a set of experiments evaluating whether adversarial examples survive different image transformations.': 1.0986123085021973, 'Apart from that there is no other main contribution / novelty.': 1.0986123085021973, 'While the experiments are solid and well-done, this seems borderline.': 1.0986123085021973, 'Detailed evaluation.': 1.0986123085021973, 'Originality:': 1.0986123085021973, 'the main contribution of this work is the experimental evaluation showing (and quantifying) how adversarial examples behave under various image transformations.': 1.0986123085021973, 'Quality:': 1.0986123085021973, 'The shown experiments are solid and well done.': 1.0986123085021973, 'Clarity:': 1.0986123085021973, 'The paper is well written and clear.': 1.0986123085021973, 'Significance:': 1.0986123085021973, 'The findings and shown experiments are interesting, but I not sure if the scale and amount of contribution is significant enough for the main conference track.': 1.0986123085021973, 'Overall:': 1.0986123085021973, 'Experimental paper.': 1.0986123085021973, 'Well written.': 1.0986123085021973, 'Solid experiments.': 1.0986123085021973, 'Not sure if contribution is significant enough.': 1.0986123085021973}"
266,https://openreview.net/forum?id=S1QefL5ge,"{'The authors present an online learning method for learning the structure of sum-product networks.': 1.0986123085021973, 'The algorithm assumes Gaussian coordinate-wise marginal distributions, and learns both parameters and structure online.': 1.0986123085021973, 'The parameters are updated by a recursive procedure that reweights nodes in the network that most contribute to the likelihood of the current data point.': 1.0986123085021973, 'The structure learning is done by either merging independent product Gaussian nodes into multivariate leaf nodes, or creating a mixture over the two nodes when the multivariate would be too large.': 1.0986123085021973, 'The fact that the dataset is scaled to some larger datasets (in terms of the number of datapoints) is promising, although the number of variables is still quite small.': 1.0986123085021973, 'Current benchmarks for tractable continuous density modeling with neural networks include the NICE and Real-NVP families of models, which can be scaled to both large numbers of datapoints and variables.': 1.0986123085021973, 'Intractable methods like GAN, GenMMN, VAE have the same property.': 1.0986123085021973, 'The main issue with this work for the ICLR audience is the use of mainly a set of SPN-specific datasets that are not used in the deep learning generative modeling literature.': 1.0986123085021973, 'The use of GenMMN as a baseline is also not a good choice to bridge the gap to the neural community, as its Parzen-window based likelihood evaluation is not really meaningful.': 1.0986123085021973, 'Better ways to evaluate the likelihood through annealed importance sampling are discussed in ""On the Quantitative Analysis of Decoder-Based Generative Models"" by Wu et al.': 1.0986123085021973, 'I would recommend the use of a simple VAE type model to get a lower bound on the likelihood, or something like Real-NVP.': 1.0986123085021973, 'Most neural network density models are scalable to large numbers of observations as well as instances, and it is not clear that this method scales well ""horizontally"" like this.': 1.0986123085021973, 'Evaluating the feasibility of modeling something like MNIST would be interesting.': 1.0986123085021973, 'SPNs have the strength that not only marginal but also various type of conditional queries are tractable, but performance on this is not evaluated or compared.': 1.0986123085021973, 'One interesting application could be in imputation of unknown pixels or color channels in images, for which there is not currently a high-performing tractable model.': 1.0986123085021973, 'Despite the disconnect from other ICLR generative modeling literature, the algorithm here seems simple and intuitive and convincingly works better than the previous state of the art for online SPN structure learning.': 1.0986123085021973, 'I think VAE is a much better baseline for continuous data than GenMMN when attempting to compare to neural network approaches.': 1.0986123085021973, 'Further, the sum-product network could actually be combined with such deep latent variable models as an observation model or posterior, which could be a very powerful combination.': 1.0986123085021973, 'I would like it if these SPN models were better known by the ICLR probabilistic modeling community, but I do not know if this paper does enough to make them relevant.': 1.0986123085021973, 'As with the other reviewers, I am not an expert on SPNs.': 1.0986123085021973, 'However, this seems to be a simple and effective algorithm for online structure induction, and the scalability aspect is something that is important in much recent work in the learning of representations.': 1.0986123085021973, 'I think it is good enough for publication, although I would prefer to see many of the above additions to more clearly bridge the gap with other literature in deep generative modeling.': 1.0986123085021973, '# Summary': 1.0986123085021973, 'This paper proposes an algorithm to learn the structure of continuous SPNs in a single pass through the data,': 1.0986123085021973, 'basically by ""growing"" the SPN when two variables are correlated.': 1.0986123085021973, '## NOTE': 1.0986123085021973, 'I am not an expert on SPNs, and can not really judge how impressive the presented results are due to lack of familiarity with the datsets.': 1.0986099243164062, '# Pro': 1.0986123085021973, 'This looks like possibly impactful work, proposing a simple and elegant algorithm for learning SPN structure single-pass, rather than just using random structure which has been done in other work in the online settings.': 1.0908030271530151, '# Con': 1.0986123085021973, 'The paper is heavily updated between submission deadline and submission of reviews.': 1.0986123085021973, 'The paper reads like a rush job, sloppily written - at least the first version.': 1.0986123085021973, 'Comparison to literature is severely lacking; eg ""several automated structure learning techniques have been proposed"" followed by 6 citations but no discussion of any of them, which one is most related, which ideas carry over from the offline setting to this online setting, etc.': 1.0986123085021973, 'Also since this work presents both joint structure & *parameter* learning, comparison to the online parameter learning papers (3 cited) would be appreciated, specifically since these prior approaches seem to be more principled with Bayesian Moment Matching in Jaini 2016 for example.': 1.0986123085021973, 'I do not know enough about SPNs and the datasets to properly judge how strong the results are, but they seem to be a bit underwhelming on the large datasets wrt Random': 1.0944840908050537, '# Remaining questions after the paper updates': 1.0986123085021973, 'Table 3: Random structure as baseline ok, but how were the parameters here learned?': 1.0986119508743286, 'Your simple running average or with more advanced methods?': 1.0986123085021973, 'Table 1: you are presenting *positive* average log-likelihood values?': 1.0986123085021973, 'This should be an average of log(p<=1) < 0 values?': 1.09861159324646, 'What am I missing here?': 1.0986095666885376, 'I recommend reject mostly because this paper should have been finished and polished at submission time, not at review deadline time.': 1.0986123085021973, 'The authors contribute an algorithm for building sum-product networks (SPNs) from data, assuming a Gaussian distribution for all dimensions of the observed data.': 1.0986123085021973, 'Due to the restricted structure of the SPN architecture, building a valid architecture that is tailored to a specific dataset is not an obvious exercise, and so structure-learning algorithms are employed.': 1.0873510837554932, 'For Gaussian distributed observations, the authors state that the previous state of the art is to chose a random SPN that satisfies the completeness and decomposibility constraints that SPNs must observe, and to then learn the parameters (as done in Jaini 2016).': 1.0985099077224731, 'In the contributed manuscript, the algorithm begins with a completely factorized model, and then by passing through the data, builds up more structure, while updating appropriate node statistics to maintain the validity of the SPN.': 1.0985695123672485, 'The above Jaini reference figures heavily into the reading of the paper because it is (to my limited knowledge) the previous work SOTA on SPNs applied to Gaussian distributed data, and also because the authors of the current manuscript compare performance to datasets studied in Jaini et al.': 1.0981329679489136, 'I personally was unfamiliar with most of these datasets, and so have no basis to judge loglikelihoods, given a particular model, as being either good or poor.': 1.0985759496688843, 'Nevertheless, the current manuscript reports results on these datasets that better (5 / 7) than other methods, such as SPNS (constructed randomly), Stacked Restricted Boltzmann Machines or Generative Moment Matching networks.': 1.098497748374939, 'Overall:': 1.0874881744384766, 'First let me say, I am not really qualified to make a decision on the acceptance or rejection of this manuscript (yet I am forced to make just such a choice) because I am not an expert in SPNs.': 1.052743673324585, 'I was also unfamiliar with the datasets, so I had no intuitive understanding of the algorithms performance, even when viewed as a black-box.': 1.0003163814544678, 'The algorithm is presented without theoretical inspiration or justification.': 1.0986123085021973, 'These latter are by no means a bad thing, but it again gives me little hold onto when evaluating the manuscript.': 1.0942704677581787, 'The manuscript is clearly written, and to my limited knowledge novel, and their algorithm does a good job (5/7) on selected datasets.': 0.967355489730835, ""My overall impression is that there isn't very much work here (e.g., much of the text is similar to Jaini, and most of the other experiments are repeated verbatim from Jaini), but again I may be missing something (this manuscript DOES mostly Jaini)."": 0.6907755732536316, 'I say this mostly because I am unfamiliar with the datasets.': 1.0986114740371704, 'Hopefully my reviewing peers will have enough background to know if the results are impressive or not, and my review should be weighted minimally.': 0.8027439713478088, 'Smallish Problems': 1.0986123085021973, 'I wanted to see nonuniform covariances in the data of the the toy task (Fig 3) for each gaussian component.': 0.9824825525283813, 'The SPN construction method has two obvious hyper parameters, it is important to see how those parameters affect the graph structure.': 1.0331004858016968, '(I submitted this as a pre-review question, to which the authors responded that they would look into this.)': 0.5993760228157043}"
267,https://openreview.net/forum?id=S1RP6GLle,"{'The paper presents a new framework to solve the SR problem - amortized MAP inference and adopts a pre-learned affine projection layer to ensure the output is consistent with LR.': 1.098611831665039, 'Also, it proposes three different methods to solve the problem of minimizing cross-entropy.': 1.0972542762756348, 'Generally, it is a great paper.': 1.0983575582504272, 'However, I still have several comments:': 1.0985702276229858, '1) The proposed amortized MAP inference is novel and different from the previous SR methods.': 1.0986123085021973, 'Combined with GAN, this framework can obtain plausible and good results.': 1.0984652042388916, 'Compared with another GAN-based SR methods - Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, question may arise as to what this new formulation adds to the latest state-of-the-art.': 1.077685832977295, '2) Using an affine projection architecture as a constraint, the model do not need any corresponding {HR, LR} image pairs for training.': 1.098578691482544, 'However, when training the affine projection layer, we still need the {HR, LR} image pairs.': 1.0986123085021973, 'Does it mean that we merely transfer this training procedure to the training of affine projection?': 1.0986123085021973, '3) The paper presents many results of the framework, including the results of natural images from ImageNet.': 0.4118405282497406, 'Can the author also provide the results of Set5, Set14 or BSD100, which are conventional test dataset for SR, so that we can perform a fair comparison with previous work.': 1.0986121892929077, '4) I see that the size of the results of nature images presented in this paper are limited to 128*128.': 0.5599172115325928, 'Can this framework perform well on images with larger size?': 1.0986123085021973, 'Because SR will encounter input with arbitrary size.': 1.0986026525497437, '5) A normal GAN will have a noise term as a latent space, so that it can be better illustrated as learning a distribution.': 0.5440288782119751, 'Do the author try the noise vector?': 1.0986123085021973, 'Overall, this paper provides a new framework for SR with solid theoretical analysis.': 1.0628702640533447, 'The idea is novel and the author explore many methods.': 1.0985901355743408, 'Though there still exist questions like the necessity and more experiments are needed.': 1.0980559587478638, 'I think this work will will provide good inspiration to the community.': 0.44884300231933594, 'Sincere apologies for the late review.': 1.097930908203125, 'This paper argues to approach Super-Resolution as amortised MAP estimation.': 1.0964241027832031, 'A projection step to keep consistent HR-LR dependencies is proposed and experimentally verified to obtain better results throughout.': 1.096709132194519, 'Further three different methods to solve the resulting cross-entropy problem in Eq.9 are proposed and tested.': 1.0986123085021973, 'Summary: Very good paper, very well written and presented. Experimental results are sufficient, the paper presents well chosen toy examples and real world applications. From my understanding the contributions for the field of super-resolutions are novel (3.2,3.3,3.4), parts that are specific for the training of GANs may have appeared in different variants elsewhere (see also discussion). I believe that this paper will be relevant to future work on super-resolution, the finding that GAN based model training yields most visually appealing results suggests further work in this domain.': 1.0986123085021973, 'Manuscript should be proof-read once more, there were some very few typos that may be worth fixing.': 1.0986120700836182, 'The paper presents an amortised MAP estimation method for SR problems.': 1.0986024141311646, 'By learning a neural network which learns to project to an affine subspace of SR solutions which are consistent with the LR method the method enables finding propoer solutions with by using a variety of methods: GANs, noise assisted and density assisted optimisation.': 1.0986123085021973, 'Results are nicely demonstrated on several datasets.': 1.0986123085021973, 'I like the paper all in all, though I feel the writing can be polished by quite a bit and presentation should be made clearer.': 1.0986123085021973, 'It was hard to follow at times and considering the subject matter is quite complicated making it clearer would help.': 1.0986123085021973, 'Also, I would love to see some more analysis of the resulting the networks - what kind of features to they learn?': 1.0986123085021973}"
268,https://openreview.net/forum?id=S1TER2oll,"{'Authors propose a mechanism for selecting the design of filters in convolutional layers.': 1.0986123085021973, 'The basic idea is that convolution should be applied to input feature dimensions that are highly correlated in order to detect rare events.': 1.0986123085021973, 'For example, adjacent pixels in images are correlated and edges are rare events of interest to be detected.': 1.0986123085021973, 'Authors argue that square filters are therefore appropriate in images.': 1.0986123085021973, 'However, in data such as bird songs high correlations might exist between non-adjacent harmonics and a convolution filter should take a weighted summation over these input feature dimensions.': 1.0986123085021973, 'Such an operation can thus be thought of computing data-dependent dilated convolutions.': 1.0986123085021973, 'Paper theoretically motivates this choice using the idea of Gaussian complexity of the learner (i.e. a CNN in this case).': 1.098348617553711, 'The main idea being that choosing convolution filters that sum over correlated features results in lower Gaussian complexity and thus the learner has higher ability to generalize.': 1.0986123085021973, 'While I am no expert in theoretical analysis of learning algorithms – there are parts of proof that look sound, but there are parts that are rather hand wavy (for eg, extension to networks using max-pooling from average pooling).': 1.0986123085021973, 'Also, the theory is not directly applicable to choosing filters when number of layers are more than 1.': 1.0986123085021973, 'I am willing to overlook the paucity in rigor in some parts of the theoretical arguments because the empirical evidence looks convincing.': 1.0986123085021973, 'The method of choosing the filter shape can be briefly summarized as:': 1.098347544670105, '(a) The covariance matrix of the input features is computed.': 1.0886507034301758, '(b) Using the covariance matrix, feature dimensions with highest correlations are determined by solving equation (7).': 1.098167896270752, 'A hard limit on maximum number of filter dimensions is imposed (typically ~ 10-15).': 1.098570466041565, 'This leads to choice of a single design for all filters in the layer.': 1.0979841947555542, '(c) Authors extend the framework to work with multiple layers in the following way: A subset of feature dimensions cannot account for all variance in the inputs and there is some residual variance.': 1.081488013267517, 'The filter design of the next layer attempts to minimize this residual variance.': 1.0954358577728271, 'This process is repeated iteratively by solving eq (8) to obtain filter designs for all the layers.': 1.098423957824707, 'Ideally for determining filter designs of different layers – one should have computed the covariance statistics of outputs of the previous layer.': 1.0950995683670044, 'However this assumes that filters of the previous layer are already known and this is not computationally feasible to implement.': 0.8615305423736572, 'Authors instead use the method described in (c).': 1.0983924865722656, 'A question which comes to my mind is – a single feature design is chosen for each layer.': 1.0986123085021973, 'Have the authors considered using the process in (c) to chose different filter designs for different filters in the same layer as opposed to using the same filter design for all the filters?': 1.041204810142517, 'Regarding baselines:': 0.4064280390739441, 'B1.': 1.098588466644287, 'It would be great to see a comparison with randomly chosen filter designs.': 1.0973896980285645, 'Two comparisons could be made – (1a)': 1.0985774993896484, 'A single random design is chosen for each layer.': 1.098610281944275, '(1b)': 1.0985143184661865, 'The design of each filter is chosen randomly (i.e. allowing for different designs of filter within each layer).': 1.0943658351898193, 'B2.': 1.0986123085021973, 'Since the theory is not really applicable to CNNs with more that one layer – I wonder how much of the benefit is obtained by choosing the filter design just in a single layer v/s all the layers.': 1.0986114740371704, 'A good comparison would be when filter design of the first layer are chosen using the described method and filters in higher layers are chosen to be square.': 1.0986123085021973, 'B3.': 1.0986123085021973, 'Authors mention the use L1 regularization in the baselines.': 1.0986123085021973, 'Was the L1 penalty cross-validated?': 1.0986123085021973, 'If so, then upto what range?': 1.0986123085021973, 'Somethings which are unclear:': 1.0986123085021973, '“exclude data that represent obvious noise”': 1.0986123085021973, 'DFMax mentioned in the supplementary materials': 1.0986123085021973, 'Overall I think this is very interesting idea for filter design.': 1.0986123085021973, 'The authors have done a fair set of experiments but I would really like to see results of B1, B2 and the answer to question in B3.': 1.0986123085021973, 'I have currently set my rating to a weak reject, but I am happy to raise my ratings to – “Good paper, accept” if the authors provide results of experiments and answers to questions in my comments above.': 1.0986123085021973, 'This work proposes a way how to learn filter shapes for CNNs in an unsupervised manner for multiple tasks by solving a lasso problem.': 1.0986123085021973, 'Even though this method does not seem to be applicable for image classification CNNs (as image data generally do not have bias towards anisotropic structures), it gives an empirical methodology to design filter shapes for tasks with different input data structure.': 1.0132582187652588, 'Authors show that this method is applicable for spectrogram classification and gene sequence classification.': 1.0986123085021973, 'The paper is well written and and is of interest to the community as it presents a unsupervised method applicable to problems with less training data.': 1.0986123085021973, 'Authors compare the performance of the proposed method against reasonable baselines (i.e. handcrafted filter sizes) and based on the evaluation it seems to improve the results and help to avoid over-fitting (probably due to reduced filter size and thus number of parameters).': 1.0986123085021973, 'In this way it is an interesting combination of unsupervised methods for a supervised training.': 1.0986123085021973, 'Unfortunately, I am not able to validate correctness of the theoretical justification.': 0.5718973875045776, 'As a side note:': 0.9912684559822083, '* It would be useful to give some reference showing that using spectrogram for sound classification is a reasonable choice': 1.0986123085021973, 'The paper proposes a method for optimising the shape of filters in convolutional neural network layers, i.e. the structure of their receptive fields.': 1.0986121892929077, 'CNNs for images almost invariably feature small square filters (e.g. 3x3, 5x5, ...) and this paper provides an algorithm to optimise this aspect of the model architecture (which is often treated as fixed) based on data.': 1.0986123085021973, 'It is argued that this is especially useful for data modalities where the assumption of locality breaks down, as in e.g. spectrograms, where correlations between harmonics are often relevant to the task at hand, but they are not local in frequency space.': 1.0986123085021973, ""Improved performance is demonstrated on two tasks that are fairly non-standard, but I think that is fine given that the proposed approach probably isn't useful for the vast majority of popular benchmark datasets (e.g. MNIST, CIFAR-10), where the locality assumption holds and a square filter shape is probably close to optimal anyway."": 1.0986123085021973, 'Fig.': 1.090299367904663, '1 is a nice demonstration of this.': 0.41873055696487427, ""The paper spends quite a bit of space on a theoretical argument for the proposed method based on Gaussian complexity, which is interesting but maybe doesn't warrant quite so much detail."": 1.0986121892929077, 'In contrast, section 3.3 (about how to deal with pooling) is quite handwavy in comparison.': 1.0986123085021973, 'This is probably fine but the level of detail in the preceding sections makes it a bit suspicious.': 1.0986123085021973, ""I'm also not 100% convinced that the theoretical argument is particularly relevant, because it seems to rely on some assumptions that are clearly untrue for practical CNNs, such as 1-norm weight constraints and the fact that it is probably okay to swap out the L1 norm for the L2 norm."": 1.0985904932022095, 'I would also like to see a bit more discussion about Fig. 4, especially about the fact that some of the filter shapes end up having many fewer nonzeros than the algorithm enforces (e.g. 3 nonzeros for layers 6 and 7, whereas the maximum is 13).': 1.0985146760940552, ""Of course this is a perfectly valid outcome as the algorithm doesn't force the solution to have an exact number of nonzeros, but surely the authors will agree that it is a bit surprising/unintuitive?"": 0.44402259588241577, 'The same figure also features an interesting phase transition between layers 1-4 and 5-8, with the former 4 layers having very similar, almost circular/square filter shapes, and the later having very different, spread out shapes.': 1.0986053943634033, 'Some comments about why this happens would be welcome.': 1.098608136177063, 'Regarding my question about computational performance, I still think that this warrants some discussion in the paper as well.': 1.0947414636611938, 'For many new techniques, whether they end up being adopted mainly depends on the ratio between the amount of work that goes into implementing them, and the benefit they provide.': 1.092768669128418, ""I'm not convinced that the proposed approach is very practical."": 1.0761399269104004, 'My intuition is that creating efficient implementations of various non-square convolutions for each new problem might end up not being worth the effort, but I could be wrong here.': 0.7778823971748352, 'Minor comments:': 1.0986123085021973, 'please have the manuscript proofread for spelling and grammar.': 1.0986123085021973, 'there is a bit of repetition in sections 2 and 3, e.g. the last paragraphs of sections 2.1 and 2.2 basically say the same thing, it would be good to consolidate this.': 0.11197055131196976, 'a few things mentioned in the paper that were unclear to me (""syllables"", ""exclude data that represent obvious noise"", choice of ""max nonzero elements"" parameter) have already been adequately addressed by the authors in their response to my questions, but it would be good to include these answers in the manuscript as well.': 1.0423017740249634, 'the comparison in Fig. 5 with L1 regularisation on the filter weights does not seem entirely fair, since the resulting shape would have to be encompassed in a 5x5 window whereas Fig.': 1.0667625665664673, '4 shows that the filter shapes found by the algorithm often extend beyond that.': 1.0927951335906982, 'I appreciate that training nets with very large square filters is problematic in many ways, but the claim ""L1 regularization cannot achieve the same effect as filter shaping"" is not really convincingly backed up by this experiment.': 1.0984270572662354}"
269,https://openreview.net/forum?id=S1VaB4cex,"{""Looking through the comment section here, I agree to a large degree with the author's standpoint on many issues discussed."": 1.3862943649291992, 'Points (1) through (4) in the authors comment below are, in my opinion, a good summary of the contributions of the paper.': 1.3862943649291992, ""While I don't think those contributions are groundbreaking, I believe they are significant enough to merit acceptance."": 1.3862943649291992, 'The reason I am commenting here is because, having looked at several comment sections for this ICLR, I am seeing a general trend that reviews have a strong focus on performance, i.e. reviews tend to be very short and judge papers, to a large degree, on whether they are a few percentage points better or worse than the reported baseline.': 1.3862943649291992, 'E.g. see the comments ""the experimental evaluation is not convincing, e.g. no improvement on SVHN"" or ""the effect of drop-path seems to vanish with data augmentation"" below.': 1.3862943649291992, 'I believe that papers should be judged more on their scientific contributions (see points (1), (2) and (4) below), especially when those papers themselves state that their focus is on those scientific contributions, not on amazing performance.': 1.3862943649291992, 'Further, I believe the trend to focus excessively on performance is problematic for a number of reasons:': 1.3862943649291992, '- The Deep Learning community has focused very heavily on a few datasets (MNIST, ImageNet, CIFAR-10, CIFAR-100, SVHN). This means that at any time, a large chunk of the deep learning literature is battling for 5 SOTA titles. Hence, expecting any new model to attain one of those titles is a very high bar.': 1.3862943649291992, '- It is an arbitrary standard. Say the SOTA on ImageNet improves by 2% a year. Then a paper that outperforms by 1% in 2014 would underperform by 1% in 2015. By the performance standard, the same paper with the same ideas and the same scientific merit would have declined drastically in value over that one year. Is that really true?': 1.3032342195510864, '- How does one even draw a ""fair comparison"" on these standard datasets at this point? The bag of tricks for neural networks includes: drop-out, l2, l1, ensembling, various forms of data augmentation, various forms of normalization and initialization, various non-linearities, various learning rate schedules, various forms of pooling, label smoothing, gradient clipping etc. etc. There are a gazillion ways to eke out fractions of percentage points of performance. And - every single paper has a unique combination of tricks that they use for their model, even though the tricks themselves are unrelated to the model. Hence, the only truly fair comparison would be to compare against every reference model with the exact trick combination that the paper presenting the reference model used, which would take an exorbitant amount of time. What\'s worse, many papers do not even report all of the tricks they used. One would have to get the authors code and reverse engineer the model, not to mention slight differences introduced by using e.g. TensorFlow vs. Torch vs. Caffe. In this light, the request from one of the reviewers to have a baseline ""against which the improvements can be clearly demonstrated by making isolated changes"" seems unrealistic to me.': 0.5664646029472351, '- The ML community should not make excessive fine-tuning of models mandatory for publication. By requiring models to beat SOTA, we force each author to fine-tune their model ad nauseum, which leads to an arms race. To get a publications, authors would spend ever more time fine-tuning their models. This can not only lead to ""training on the test set"", but also wastes the time of researcher that could be better spent exploring new ideas.': 1.235637903213501, '- It gives too much power to bad research. In science, there is always a certain background rate of ""bad"" results published: either the numbers are outright fake or the experimental protocol was invalid, e.g. someone used the test set as a validation set or someone did an exorbitant number of random reruns and only published the best single result. What\'s worse, these ""bad"" results are far more likely to hold the SOTA title at any given time than a ""good"" result. By requiring new publications to beat SOTA, we give too much power to bad results.': 0.2634037137031555, ""- It punishes authors for reporting many or strong baselines. In this paper, authors were careful to report many recent results. Table 1 is thorough. And now they are criticized for not beating all of those baselines. I have a feeling that if the authors of this paper had been more selective about which baselines they report, i.e. those that they can beat, they would have received higher scores on the paper. I have written an in-depth review for another paper at this conference that used, in my opinion, very weak baselines and ended up getting high reviewer marks. I don't think that was a coincidence."": 0.30786141753196716, 'The same arguments apply, though I think to a lesser degree, to judging models excessively on how many parameters they have or their runtime.': 1.3862011432647705, 'However, I agree with reviewers that more information about how models compare in terms of those metrics would enhance this paper.': 1.1906386613845825, 'I would like to see a discussion of that in the final version.': 1.3862943649291992, 'In general, I think this paper would benefit from an appendix with more details on model and training procedure.': 1.3790565729141235, 'I also agree with reviewers that 80 layers, which is the deepest that authors can go while improving test error (Table 3), is not ultra-deep.': 1.3858133554458618, 'Hence putting ""ultra-deep"" in the paper title seems exaggerated and I would recommend scaling back the language.': 1.3862873315811157, ""However, I don't think being ultra-deep (~1000 layers) is necessary, because as Veit et al showed, networks that appear ultra deep might not be ultra deep in practice."": 1.3460441827774048, 'Training an 80-layer net that functions at test time without residual connections seems to be enough of an achievement.': 1.3862943649291992, 'In summary, I think if a paper makes scientific contribution (see points (1), (2) and (4) below) independent of performance, then competitive performance should be enough for publication, instead of requiring SOTA.': 1.3862943649291992, 'I believe this paper achieves that mark.': 1.3862943649291992, 'This paper presents a strategy for building deep neural networks via rules for expansion and merging of sub-networks.': 1.3862943649291992, 'pros:': 1.3862943649291992, 'the idea is novel': 1.3862943649291992, 'the approach is described clearly': 1.3862943649291992, 'cons:': 1.3862943649291992, 'the experimental evaluation is not convincing, e.g. no improvement on SVHN': 1.3862943649291992, 'number of parameters should be mentioned for all models for fair comparison': 0.8099182844161987, 'the effect of drop-path seems to vanish with data augmentation': 1.3862943649291992, 'This paper proposes a new architecture that does not explicitly use residuals but constructs an architecture that is composed of networks with fractal structure by using expand and join operations.': 1.044765830039978, ""Using the fractal architecture,  authors argue and try to demonstrate that the large nominal network depth with many short paths is the key for 'training 'ultra-deep” networks while residuals are incidental."": 0.36118465662002563, ""The main bottleneck of this paper is that number of parameters needed for the FractalNet is significantly higher than the baselines which makes it hard to scale to ''ultra-deep” networks."": 0.437581330537796, 'Authors replied that Wide ResNets also require many parameters but this is not the case for ResNet and other ResNet variants.': 1.3092190027236938, 'ResNet and ResNet with Stochastic depth scales to depth of 110 with 1.7M parameters and to depth of 1202 with 10.2M parameters which is much less than the number of parameters for depths of 20 and 40 in Table 1(Huang et al, 2016a).': 0.6142016053199768, 'It is not clear whether FractalNet can perform better than these depths with a reasonable computation.': 1.1956928968429565, 'Authors report less parameters for 40 layers but this scaling trick is not validated for other depths including depth 20 in Table 1.': 0.9008474946022034, 'On the other hand, the number of parameters for 40 layers with scaling trick is clearly still large compared to most of the baselines.': 0.4970223903656006, 'Unsatisfactory comparison to these baselines makes the claims of authors unconvincing.': 1.3734488487243652, 'Authors also claim that drop-path to provide improvement compared to layer dropping procedure in Huang et al, 2016b however the results show that the empirical gain of this specific regularization disappears when well-known data augmentation techniques applied.': 1.376671552658081, 'Therefore the empirical effectiveness of drop-path is not convincing too.': 1.3712880611419678, 'DenseNets (Huang et al, 2016a) should be also included in the comparison since it outperforms most of the state of art Res Nets on both CIFAR10 and ImageNet and more importantly outperforms the proposed FractalNet significantly and it requires significantly less computation.': 0.918470025062561, 'Table 1 has Res-Net variants as baselines however Table 2 has only ResNet.': 0.6980298757553101, 'Therefore ImageNet comparison only shows that one can run FractalNet on ImageNet and can perform comparably well to ResNet which is not a satisfactory result given the improvements of other baselines over ResNet.': 0.9567967653274536, 'In addition, there is no improvement in SVHN dataset results and this is not discussed in the empirical analysis.': 1.3371937274932861, 'Also, authors give a list of some improvements over Inception (Szegedy et al., 2015) but again these intuitive claims about effectiveness of these changes are not supported with any empirical analysis.': 1.3861185312271118, 'Although the paper attempts to explore many interesting intuitive directions using the proposed architecture, the empirical results are not support the given claims and the large number of parameters makes the model restrictive in practice hence the contribution does not seem to be significant.': 0.745468258857727, 'Pros:': 1.3862943649291992, 'Provides an interesting architecture compared to ResNet and its variants and investigates the differences to residual networks which can stimulate some other promising analysis': 1.0918508768081665, '-    Number of parameters are very large compared to baselines that can have even much higher depths with smaller number of parameters': 0.8090662360191345, 'The claims are intuitive but not supported well with empirical evidence': 1.3862872123718262, 'Path regularization does not yield improvement when the data augmentation is used': 1.386284589767456, '-     The empirical results do not show whether the method is promising for “ultra-deep” networks': 1.2525490522384644, 'This paper proposes a design principle for computation blocks in convolutional networks based on repeated application of expand and join operations resulting in a fractal-like structure.': 1.386293649673462, 'This paper is primarily about experimental evaluation, since the objective is to show that a residual formulation is not necessary to obtain good performance, at least on some tasks.': 1.386292576789856, 'However, in my opinion the evaluations in the paper are not convincing.': 1.1656640768051147, 'The primary issue is lack of a proper baseline, against which the improvements can be clearly demonstrated by making isolated changes.': 1.3862943649291992, 'I understand that for this paper such a baseline is hard to construct, since it is about a novel architecture principle.': 1.3862943649291992, 'This is why more effort should be put into this, so that core insights from this paper can be useful even after better performing architectures are discovered.': 1.3862943649291992, 'The number of parameters and amount of computation should be used to indicate how fair the comparisons are between architectures.': 0.7003518342971802, 'Some detailed comments:': 1.3862943649291992, 'In Table 1 comparisons to Resnets, the resnets from He et al. 2016b and Wide Resnets should be compared to FractalNet (in lieu of a proper baseline).': 1.3759363889694214, 'The first outperforms FractalNet on CIFAR-100 while the second outperforms it on both.': 1.384842038154602, 'The authors compare to other results without augmentation, but did not perform additional experiments without augmentation for these architectures.': 1.3862943649291992, 'The 40 layer Fractal Net should not be compared to other models unless the parameter reduction tricks are utilized for the other models as well.': 1.3858927488327026, 'A proper comparison to Inception networks should also be performed for these networks.': 1.1177302598953247, ""My guess is that the reason behind a seemingly 'ad-hoc' design of Inception modules is to reduce the computational footprint of the model (which is not a central motivation of fractal nets)."": 1.3862942457199097, 'Since this model is directly related to the Inception module due to use of shorter and longer paths without shortcuts, one can easily simplify the Inception design to build a strong baseline e.g. by converting the concatenation operation to a mean operation among equally sized convolution outputs.': 1.3862930536270142, 'As an aside, note that Inception networks have already shown that residual networks are not necessary to obtain the best performance': 1.3862754106521606, '[1].': 1.3862943649291992, 'It should be noted that Residual/Highway architectures do have a type of anytime property, as shown by lesioning experiments in Srivastava et al and Viet et al.': 1.3264176845550537, 'The architecture specific drop-path regularization is interesting, but is used along with other regularizers such as dropout, batch norm and weight decay and its benefit on its own is not clear.': 1.3862943649291992, ""Overall, it's not clear to me that the experiments clearly demonstrate the utility of the proposed architecture."": 1.3862943649291992, '[1] Szegedy, Christian, Sergey Ioffe, and Vincent Vanhoucke.': 1.3862943649291992, '""Inception-v4, inception-resnet and the impact of residual connections on learning."" arXiv preprint arXiv:1602.07261 (2016).': 1.3862943649291992}"
270,https://openreview.net/forum?id=S1X7nhsxl,"{'This paper is well written, and well presented.': 1.0970752239227295, 'This method is using denoise autoencoder to learn an implicit probability distribution helps reduce training difficulty, which is neat.': 1.0986123085021973, 'In my view, joint training with an auto-encoder is providing extra auxiliary gradient information to improve generator.': 1.0986123085021973, 'Providing auxiliary information may be a methodology to improve GAN.': 0.6997318863868713, 'Extra comment:': 1.0986123085021973, 'Please add more discussion with EBGAN in next version.': 1.0951472520828247, 'This paper is about using denoising autoencoders to improve performance in GANs.': 1.098532795906067, 'In particular, the features as determined by the discriminator, of images generated by the generator, are fed into a denoising AE and we try to have these be reconstructed well.': 1.0986123085021973, 'I think it\'s an interesting idea to use this ""extra information""': 1.098609447479248, 'namely the feature representations learned by the discriminator.': 1.0910857915878296, 'It seems very much in the spirit of ICLR!': 1.0985437631607056, ""My main concern, though, is that I'm not wholly convinced on the nature of the improvement."": 1.0986123085021973, 'This method achieves higher inception scores than other methods in some cases, but I have a hard time interpreting these scores and thus a hard time getting excited by the results.': 1.0986121892929077, 'In particular, the authors have not convinced me that the benefits outweigh the required additional sophistication both conceptually and implementation-wise (speaking of which, will code be released?).': 1.0984525680541992, ""One thing I'd be curious to know is, how hard is it to get this thing to actually work?"": 1.0986120700836182, 'Also, I view GANs as a means to an end': 1.0930252075195312, ""while I'm not particularly excited about generating realistic images (especially in 32x32), I'm very excited about the future potential of GAN-based systems."": 0.6033160090446472, 'So it would have been nice to see these improvements in inception score translate into improvements in a more useful task.': 1.0985937118530273, ""But this criticism could probably apply to many GAN papers and so perhaps isn't fair here."": 1.0986123085021973, 'I do think the idea of exploiting ""extra information"" (like discriminator features) is interesting both inside and outside the context of this paper.': 1.0986123085021973, 'The authors present a way to complement the Gerative Adversarial Network traning procedure with an additional term based on denoising autoencoders.': 1.0986114740371704, 'The use of denoising autoencoders is motivated by the observation that they implicitly capture the distribution of the data they were trained on.': 1.0986123085021973, ""While sampling methods based denoising autoencoders alone don't amount to very interesting generative models (at least no-one could demonstrate otherwise), this paper shows that it can be combined successfully with generative adversarial networks."": 1.0986123085021973, 'My overall assessment of this paper is that it is well written, well reasoned, and presents a good idea motivated from first principles.': 1.0986123085021973, 'I feel that the idea presented here is not revolutionary or a very radical departure from what has been done before, I would have liked a slightly more structured experiments section which focusses on and provides insights into the relative merits of different choices one could make (see pre-review questions for details), rather than focussing just on demonstrating that a chosen variant works.': 1.0922359228134155, ""In addition to this general review, I have already posted specific questions and criticism in the pre-review questions - thanks for the authors' responses."": 1.0986121892929077, 'Based on those responses the area I am most uncomfortable about is whether the (Alain & Bengio, 2014) intuition about the denoising autoencoders is valid if it all happens in a nonlinear featurespace.': 1.0986123085021973, ""If the denoiser function's behaviour ends up depending on the Jacobian of the nonlinear transformation Phi, another question is whether this dependence is exploitable by the optimization scheme."": 1.0986049175262451}"
271,https://openreview.net/forum?id=S1Y0td9ee,"{'the paper proposed a method mainly for graph classification.': 1.0986123085021973, 'The proposal is to decompose graphs objects into hierarchies of small graphs followed by generating vector embeddings and aggregation using deep networks.': 1.0986045598983765, 'The approach is reasonable and intuitive however, experiments do not show superiority of their approach.': 1.0986117124557495, 'The proposed method outperforms Yanardag et al. 2015 and Niepert et al., 2016 on social networks graphs but are quite inferior to Niepert et al., 2016 on bio-informatics datasets.': 1.098578929901123, 'the authors did not report acccuracy for Yanardag et al. 2015 which on similar bio-ddatasets for example NCI1 is 80%, significantly better than achieved by the proposed method.': 1.0986123085021973, 'The authors claim that their method is tailored for social networks graph more is not supported by good arguments?': 1.0986099243164062, 'what models of graphs is this method more suitable?': 1.0986123085021973, 'The paper contributes to recent work investigating how neural networks can be used on graph-structured data.': 1.0986104011535645, 'As far as I can tell, the proposed approach is the following:': 1.0986123085021973, '1. Construct a hierarchical set of ""objects"" within the graph. Each object consists of multiple ""parts"" from the set of objects in the level below. There are potentially different ways a part can be part of an object (the different \\pi labels), which I would maybe call ""membership types"". In the experiments, the objects at the bottom level are vertices. At the next level they are radius 0 (just a vertex?) and radius 1 neighborhoods around each vertex, and the membership types here are either ""root"", or ""element"" (depending on whether a vertex is the center of the neighborhood or a neighbor). At the top level there is one object consisting of all of these neighborhoods, with membership types of ""radius 0 neighborhood"" (isn\'t this still just a vertex?) or ""radius 1 neighborhood"".': 1.0986063480377197, ""2. Every object has a representation. Each vertex's representation is a one-hot encoding of its degree. To construct an object's representation at the next level, the following scheme is employed:"": 0.5200772881507874, 'a. For each object, sum the representation of all of its parts having the same membership type.': 1.0986123085021973, 'b. Concatenate the sums obtained from different membership types.': 1.0986038446426392, 'c. Pass this vector through a multi-layer neural net.': 1.0986114740371704, ""I've provided this summary mainly because the description in the paper itself is somewhat hard to follow, and relevant details are scattered throughout the text, so I'd like to verify that my understanding is correct."": 1.0986123085021973, ""Some additional questions I have that weren't clear from the text: how many layers and hidden units were used?"": 1.0986123085021973, 'What are the dimensionalities of the representations used at each layer?': 1.0986123085021973, 'How is final classification performed?': 1.0986123085021973, 'What is the motivation for the chosen ""ego-graph"" representation?': 1.0986123085021973, 'The proposed approach is interesting and novel, the compression technique appears effective, and the results seem compelling.': 1.0986123085021973, 'However, the clarity and structure of the writing is quite poor.': 1.0986123085021973, 'It took me a while to figure out what was going on': 1.0986123085021973, 'the initial description is provided without any illustrative examples, and it required jumping around the paper to figure for example how the \\pi labels are actually used.': 1.0986123085021973, ""Important details around network architecture aren't provided, and very little in the way of motivation is given for many of the choices made."": 1.0986123085021973, 'Were other choices of decomposition/object-part structures investigated, given the generality of the shift-aggregate-extract formulation?': 1.0986123085021973, 'What motivated the choice of ""ego-graphs""?': 1.0986123085021973, 'Why one-hot degrees for the initial attributes?': 1.0986123085021973, 'Overall, I think the paper contains a useful contribution on a technical level, but the presentation needs to be significantly cleaned up before I can recommend acceptance.': 1.0986123085021973, 'Some of the key details in this paper are very poorly explained or not even explained at all.': 1.0986123085021973, ""The model sounds interesting and there may be something good here, but it should not be published in it's current form."": 1.0986123085021973, 'Specific comments:': 1.0986123085021973, 'The description of the R_l,pi convolutions in Section 2.1 was unclear.': 1.0986123085021973, ""Specifically, I wasn't confident that I understood what the labels pi represented."": 1.0986123085021973, 'The description of the SAEN structure in section 2.2 was worded poorly.': 1.0986123085021973, ""My understanding, based on Equation 1, is that the 'shift' operation is simply a summation of the representations of the member objects, and that the 'aggregate' operation simply concatenates the representations from multiple relations."": 1.0986123085021973, ""In the 'shift' step, it seems more appropriate to average over the object's member's representations h_j, rather than sum over them."": 1.0986123085021973, 'The compression technique presented in Section 2.3 requires that multiple objects at a level have the same representation.': 1.0986123085021973, 'Why would this ever occur, given that the representations are real valued and high-dimensional?': 1.0986123085021973, 'The text is unintelligible: ""two objects are equivalent if they are made by same sets of parts for all the pi-parameterizations of the R_l,pi decomposition relation.""': 1.0986123085021973, ""The 'ego graph patterns' in Figure 1 and 'Ego Graph  Neural Network' used in the experiments are never explained in the text, and no references are given."": 1.0986123085021973, 'Because of this, I cannot comment on the quality of the experiments.': 1.0986123085021973}"
272,https://openreview.net/forum?id=S1_pAu9xl,"{'The paper shows a different approach to a ternary quantization of weights.': 1.6094374656677246, 'Strengths:': 1.4548903703689575, '1. The paper shows performance improvements over existing solutions': 1.370314121246338, '2. The idea of learning the quantization instead of using pre-defined human-made algorithm is nice and very much in the spirit of modern machine learning.': 1.1744353771209717, 'Weaknesses:': 1.6094379425048828, '1. The paper is very incremental.': 1.602718710899353, '2. The paper is addressed to a very narrow audience. The paper very clearly assumes that the reader is familiar with the previous work on the ternary quantization. It is ""what is new in the topic"" update, not really a standalone paper. The description of the main algorithm is very concise, to say the least, and is probably clear to those who read some of the previous work on this narrow subject, but is unsuitable for a broader deep learning audience.': 1.2449281215667725, '3. There is no convincing motivation for the work. What is presented is an engineering gimmick, that would be cool and valuable if it really is used in production, but is that really needed for anything? Are there any practical applications that require this refinement? I do not find the motivation ""it is related to mobile, therefore it is cool"" sufficient.': 1.395562767982483, 'This paper is a small step further in a niche research, as long as the authors do not provide a sufficient practical motivation for pursuing this particular topic with the next step on a long list of small refinements, I do not think it belongs in ICLR with a broad and diversified audience.': 1.6094379425048828, 'Also - the code was not released is my understanding.': 1.6094379425048828, 'This paper presents new way for compressing CNN weights.': 1.6094379425048828, 'In particular this paper uses a new neural network quantization method that compresses network weights to ternary values.': 1.6094379425048828, 'The group has recently published multiple paper on this topic, and this one offers possibly the lowest returns I have seen.': 1.6094379425048828, 'Only a fraction of percentage in ImageNet.': 1.6094379425048828, 'Results on AlexNet are of very little interest now, given the group already showed this kind of older style-network can be compressed by large amounts.': 1.6094379425048828, 'I also would have liked to see this group release code for the compression, and also report data on the amount of effort required to compress: flops, time, number of passes, required original dataset, etc.': 1.6094379425048828, 'This data is important to decide if a compression is worth the effort.': 1.6094379425048828, 'This work presents a novel ternary weight quantization approach which quantizes weights to either 0 or one of two layer specific learned values.': 1.6094379425048828, 'Unlike past work, these quantized values are separate and learned stochastically alongside all other network parameters.': 1.6094379425048828, 'This approach achieves impressive quantization results while retaining or surpassing corresponding full-precision networks on CIFAR10 and ImageNet.': 1.6094379425048828, 'Overall well written and algorithm is presented clearly.': 1.6094352006912231, 'Approach appears to work well in the experiments, resulting in good compression without loss (and sometimes gain!) of performance.': 1.6091742515563965, 'I enjoyed the analysis of sparsity (and how it changes) over the course of training, though it is uncertain if any useful conclusion can be drawn from it.': 1.6050201654434204, 'Some points:': 1.6094379425048828, 'The energy analysis in Table 3 assumes dense activations due to the unpredictability of sparse activations.': 1.6094378232955933, 'Can the authors provide average activation sparsity for each network to help verify this assumption.': 1.6094379425048828, 'Even if the assumption does not hold, relatively close values for average activation between the networks would make the comparison more convincing.': 1.6094379425048828, 'In section 5.1.1, the authors suggest having a fixed t (threshold parameter set at 0.05) for all layers allows for varying sparsity (owed to the relative magnitude of different layer weights with respect to the maximum).': 1.6094379425048828, 'In Section 5.1.2 paragraph 2, this is further developed by suggesting additional sparsity can be achieved by allowing each layer a different values of t. How are these values set?': 1.6094379425048828, 'Does this multiple threshold style network appear in any of the tables or figures?': 1.6094378232955933, 'Can it be added?': 1.6094379425048828, 'The authors claim ""ii) Quantized weights play the role of ""learning rate multipliers"" during back propagation.""': 1.427050232887268, 'as a benefit of using trained quantization factors.': 1.3050235509872437, 'Why is this a benefit?': 1.6094270944595337, 'Figure and table captions are not very descriptive.': 1.125260591506958, 'Preliminary Rating:': 1.6094379425048828, 'I think this is an interesting paper with convincing results but is somewhat lacking in novelty.': 1.0895735025405884, 'Minor notes:': 1.3873854875564575, 'Table 3 lists FLOPS rather than Energy for the full precision model.': 1.5718532800674438, 'Why?': 1.5992071628570557, ""Section 5 'speeding up'"": 1.606160283088684, '5.1.1 figure reference error last line': 1.602076530456543, 'This paper presents a ternary quantization method for convolutional neural networks.': 1.4912083148956299, 'All weights are represented by ternary values multiplied by two scaling coefficients.': 1.6094379425048828, 'Both ternary weights and the scaling coefficients are updated using back-propagation.': 1.6094378232955933, 'This is useful for CNN model compression.': 1.3019399642944336, 'Experiments on AlexNet show that the proposed method is superior Ternary-Weight-Networks (TWN) and DoReFa-Net.': 0.9305590391159058, 'This work has the following strengths and weaknesses.': 1.6041110754013062, '(1).': 1.6094379425048828, 'Good results are shown on CIFAR-10 dataset.': 1.2829561233520508, '(2).': 1.0636619329452515, 'Massive energy saving of the ternary weights comparing to 32-bit weights.': 1.4404875040054321, '(3).': 1.6094379425048828, 'It is well written, and easy to understand.': 0.9078967571258545, 'It seems that this work is an incremental improvement on the existing works.': 0.9284076690673828, 'The main difference from Binary-Weight-Networks (BWN) proposed in XNOR-net[1] is using ternary weights instead of binary weights, while ternary weights have been used by many previous works.': 1.6093966960906982, 'Both BWN and the proposed method in this paper learn the scaling factors during training.': 1.6094324588775635, 'Comparing to Ternary-Weight-Networks (TWN), the main difference is that two independent quantization factors are used for positive and negative weights, while TWN utilizes the same scaling factor for all weights.': 1.6093066930770874, 'In the experiment, the authors did not process the first conv layer and the last fully-connected layer.': 1.6094378232955933, 'The results of processing all layers should be given for fair comparison.': 1.2810280323028564, 'In the experiment, comparison with BWN is not reported.': 1.5268223285675049, 'In [1], the top-1 and top-5 error of AlexNet on ImageNet of BWN is 43.2% and 20.6%, which is comparable with the method proposed in this paper (42.5% and 20.3%).': 1.6093796491622925, 'However, the BWN only uses binary weights and all layers are binarized including the first conv layer and the last fully-connected layer.': 1.5236393213272095, '(4).': 1.119627594947815, 'For the baseline method of full precision alexnet (with BN), the reported accuracy seems to be too low (44.1% top-1 error).': 1.4935332536697388, 'Commonly, using batch normalization can boost the accuracy, while the reported accuracy are much lower than alexnet without batch normalization.': 1.5878593921661377, 'On the other hand, the error rates of pre-trained model of alexnet (with BN) reported by the official MatConvNet': 0.9955145120620728, '[2] are 41.8% and 19.2%.': 1.5820960998535156, '(5).': 1.6094379425048828, 'The proposed method should be evaluated on the original AlexNet, whose accuracy is publicly available for almost all deep learning frameworks like caffe.': 0.9210386276245117, 'Moreover, more experiments should be added on ImageNet, like VGG-S, VGG-16, GoogleNet or ResNet.': 0.9341801404953003, '(6).': 1.6094379425048828, 'In previous methods such as XNOR-net and TWN, most of the 32-bit multiply operation can be replaced by addition by using binary or ternary weights.': 1.6094379425048828, 'However, the proposed method in this paper utilizes independent scaling factors for positive and negative weights.': 1.6094379425048828, 'Thus it seems that the multiply operation can not be replaced by addition.': 0.9248456954956055, 'References:': 1.6094379425048828, '[1] Mohammad R, Vicente O, Joseph R, Ali F. XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks.': 0.9353336095809937, 'ECCV 2016': 1.6094379425048828, '[2] http://www.vlfeat.org/matconvnet/pretrained/#imagenet-ilsvrc-classification': 1.5465365648269653, 'This paper studies in depth the idea of quantizing down convolutional layers to 3 bits, with a different positive and negative per-layer scale.': 1.6094379425048828, 'It goes on to provide an exhaustive analysis of performance (essentially no loss) on real benchmarks (this paper is remarkably MNIST-free).': 1.6094379425048828, ""The relevance of this paper is that it likely provides a lower bound on quantization approaches that don't sacrifice any performance, and hence can plausibly become the approach of choice for resource-constrained inference, and might suggest new hardware designs to take advantage of the proposed structure."": 1.6094046831130981, 'Furthermore, the paper provides power measurements, which is really the main metric that anyone working seriously in that space cares about.': 1.6094377040863037, ""(Nit: I don't see measurements for the full-precision baseline)."": 1.1557286977767944, 'I would have loved to see a SOTA result on ImageNet and a result on a strong LSTM baseline to be fully convinced.': 1.6094379425048828, 'I would have also liked to see discussion of the wall time to result using this training procedure.': 1.6094379425048828}"
273,https://openreview.net/forum?id=S1c2cvqee,"{'This paper introduces a reinforcement learning framework for designing a neural network architecture.': 1.058290958404541, 'For each time-step, the agent picks a new layer type with corresponding layer parameters (e.g., #filters).': 1.0986123085021973, 'In order to reduce the size of state-action space, they used a small set of design choices.': 1.0986123085021973, 'Strengths:': 1.0986123085021973, 'A novel approach for automatic design of neural network architectures.': 1.0986123085021973, 'Shows quite promising results on several datasets (MNIST, CIFAR-10).': 1.0977851152420044, 'Weakness:': 1.0986123085021973, 'Limited architecture design choices due to many prior assumptions (e.g., a set of possible number of convolution filters, at most 2 fully-connected layers, maximum depth, hard-coded dropout, etc.)': 1.0986123085021973, 'The method is demonstrated in tabular Q-learning setting, but it is unclear whether the proposed method would work in a large state-action space.': 1.0986123085021973, 'Overall, this is an interesting and novel approach for neural network architecture design, and it seems to be worth publication despite some weaknesses.': 1.0986123085021973, 'The paper looks solid and the idea is natural.': 1.0986123085021973, 'Results seem promising as well.': 1.0986101627349854, 'I am mostly concerned about the computational cost of the method.': 1.0984042882919312, '8-10 days on 10 GPUs for relatively tiny datasets is quite prohibitive for most applications I would ever encounter.': 0.9048183560371399, 'I think the main question is how this approach scales to larger images and also when applied to more exotic and possibly tiny datasets.': 1.0986123085021973, 'Can you run an experiment on Caltech-101 for instance?': 1.0980831384658813, 'I would be very curious to see if your approach is suitable for the low-data regime and areas where we all do not know right away how a suitable architecture looks like.': 0.9247733354568481, 'For Cifar-10/100, MNIST and SVHN, everyone knows very well what a reasonable model initialization looks like.': 1.0986123085021973, 'If you show proof that you can discover a competitive architecture for something like Caltech-101, I would recommend the paper for publication.': 1.0986117124557495, 'Minor:': 1.0986123085021973, 'ResNets should be mentioned in Table': 1.0986123085021973, 'Authors learn deep architectures on a few small vision problems using Q-learning and obtain solid results, SOTA results when limiting to certain types of layers and competitive against everything else.': 1.0986123085021973, 'It would be good to know how well this performs when allowing more complex structures.': 1.0986123085021973, 'Paper would be much more convincing on a real-size task such as ImageNet.': 1.0986123085021973}"
274,https://openreview.net/forum?id=S1dIzvclg,"{'The authors of the paper set out to answer the question whether chaotic behaviour is a necessary ingredient for RNNs to perform well on some tasks.': 1.0985910892486572, ""For that question's sake, they propose an architecture which is designed to not have chaos."": 1.0975536108016968, 'The subsequent experiments validate the claim that chaos is not necessary.': 1.0986123085021973, 'This paper is refreshing.': 1.0986123085021973, 'Instead of proposing another incremental improvement, the authors start out with a clear hypothesis and test it.': 1.0986123085021973, 'This might set the base for future design principles of RNNs.': 1.0986123085021973, 'The only downside is that the experiments are only conducted on tasks which are known to be not that demanding from a dynamical systems perspective; it would have been nice if the authors had traversed the set of data sets more to find data where chaos is actually necessary.': 1.0986123085021973, 'I think the authors provide an interesting direction for understanding and maybe constructing recurrent models that are easier to interpret.': 1.0986123085021973, 'Is not clear where such direction will lead': 1.0986123085021973, 'but I think it could be an interesting starting point for future work, one that worth exploring.': 0.42724549770355225, 'This paper poses an interesting idea: removing chaotic behavior or RNNs.': 1.0986123085021973, 'While many other papers on new RNN architecture usually focus too much on the performance improvement and leave the analysis part on their success as a black-box, this paper does a good job on presenting why its method may work well.': 1.0984795093536377, 'Although, the paper shows lots of comparison between the chaotic systems (GRUs & LSTMs) and the stable system (proposed CFN model), the reviewer is not fully convinced by the main claim of this paper, the nuance that chaotic behaviour makes dynamic system to have rich representation power but makes the system too unstable.': 1.0983591079711914, 'In the paper, the LSTM shows a very sensitive behaviour, even when a very small amount of noise is added to the input.': 1.0649189949035645, 'However, it still performs surprisingly well with this chaotic behaviour.': 1.0986121892929077, 'Measuring the model complexity is a very difficult task, therefore, many papers manage to use either same number of hidden units or choose approximately close model sizes.': 1.0986123085021973, 'In this paper, the experiments were carried by using the same amount of parameters for both the LSTM and CFN.': 1.0986121892929077, 'However, I think the CFN may have much more simpler computational graph.': 1.0986123085021973, 'Taking the idea of this work, can we develop a stable dynamic system, but which does not only have one attractor?': 0.8714606761932373, 'It is also interesting to see that the layers of CFNs are updated in different timescales in a sense that the decaying speed decreases when the layer gets higher.': 1.0986114740371704, 'Could you provide more statistics on this?': 1.0986099243164062, 'For example, what is the average relaxation time of the whole hidden units at each layer?': 1.0986121892929077, 'Batch normalization and layer normalization can be helpful to make the training of RNNs become more stable.': 1.087235689163208, 'How would the behaviour of batch normalized or perhaps layer normalized LSTM look like?': 1.0977803468704224, 'Also, it is often not trivial to make batch normalization or layer normalization to work on a new architecture.': 0.8144103288650513, 'I think it may be useful to compare batch normalized or layer normalized versions of the LSTM and CFN.': 1.098610520362854, 'The quality of the work is good, explanation is clear enough along with nice analyses and proofs.': 1.0974085330963135, 'Overall, the performance is not any better than LSTMs, but it is still interesting when thinking of simplicity of this model.': 0.4769052565097809, 'I am a bit concerned if this model might not work that well in more harder task, e.g., translation.': 0.4042530655860901, 'Figure 4 of this paper is very interesting, where the proposed architecture shows that the hidden units at the second layer tends to keep its information longer than the first layer ones.': 1.0985803604125977}"
275,https://openreview.net/forum?id=S1di0sfgl,"{'This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network.': 1.0986123085021973, 'The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks.': 1.0986123085021973, 'The paper is well written.': 1.0986123085021973, 'Question)': 1.0986123085021973, 'Can you extend it to bidirectional RNN?': 1.0986123085021973, ""The paper proposes a modified RNN architecture with multiple layers, where higher layers are only passed lower layer states if a FLUSH operation is predicted, consisting of passing up the state and reseting the lower layer's state."": 1.0986123085021973, 'In order to select one of three operations at each time step, the authors propose using the straight-through estimator with a slope-annealing trick during training.': 1.0986123085021973, 'Empirical results and visualizations illustrate that the modified architecture performs well at boundary detection.': 1.0986123085021973, 'Pros:': 1.0986123085021973, 'Paper is well-motivated, exceptionally well-composed': 1.0986123085021973, 'Provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation': 1.0986123085021973, 'The annealing trick with the straight-through estimator also seems potentially useful for other tasks containing discrete variables, and the trade-off in the flush operation is innovative.': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'In a couple cases the paper does not fully deliver.': 1.0986123085021973, 'Empirical results on computational savings are not given, and hierarchy beyond a single level (where the data contains separators such as spaces and pen up/down) does not seem to be demonstrated.': 1.0986123085021973, ""It's unclear whether better downstream performance is due to use of hierarchical information or due to the architecture changes acting as regularization, something which could hopefully be addressed."": 1.0986123085021973, 'This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word).': 1.0986123085021973, 'Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).': 1.0986123085021973, 'Their model is organized as a set of layers that aim at capturing the information form different “level of abstraction”.': 1.0986123085021973, 'The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c).': 1.0986123085021973, 'A key feature of their model is that c is a discrete variable, allowing potentially fast inference time.': 1.0986123085021973, 'However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012.': 1.0986123085021973, 'The experiment section is thorough and their model obtain competitive performance on several challenging tasks.': 1.0986123085021973, 'The qualitative results show also that their model can capture natural boundaries.': 1.0986123085021973, 'Overall this paper presents a strong and novel model with promising experimental results.': 1.0985698699951172, 'On a minor note, I have few remarks/complaints about the writing and the related work:': 1.0986106395721436, 'In the introduction:': 1.0986123085021973, '“One of the key principles of learning in deep neural networks as well as in the human brain” : please provide evidence for the “human brain” part of this claim.': 1.098608136177063, '“For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances” I believe you re missing Mikolov et al. 2010 in the references.': 1.0986121892929077, '“in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data”: missing reference to Lin et al., 1996': 1.0986123085021973, 'in the related work:': 1.0986120700836182, '“A more recent model, the clockwork RNN (CW-RNN) (Koutník et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)” : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.': 1.0982780456542969, 'While the above models focus on online prediction problems, where a prediction needs to be made…”: I believe there is a lot of missing references, in particular to Socher’s work or older recursive networks.': 1.0986123085021973, '“The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)”: this is not the first work using gradient clipping.': 1.0986123085021973, 'I believe it was introduced in Mikolov et al., 2010.': 1.0986123085021973, 'Missing references:': 1.0986123085021973, '“Recurrent neural network based language model.”': 1.0986123085021973, ', Mikolov et al. 2010': 1.0986123085021973, '“Learning long-term dependencies in NARX recurrent neural networks”, Lin et al. 1996': 1.0986123085021973, '“Sequence labelling in structured domains with hierarchical recurrent neural networks“, Fernandez et al. 2007': 1.0986123085021973, '“Learning sequential tasks by incrementally adding  higher  orders”, Ring, 1993': 1.0986123085021973}"
276,https://openreview.net/forum?id=S1j4RqYxg,"{'The authors present here a new algorithm for the effective calculation of polynomial features on Sparse Matrices.': 1.0986123085021973, 'The key idea is to use a proper mapping between matrices and their polynomial versions, in order to derive an effective CSR expansion algorithm.': 1.0986119508743286, 'The authors analyse the time complexity in a convincing way with experiments.': 1.0986121892929077, 'Overall, the algorithm is definitely interesting, quite simple and nice, with many possible applications.': 1.0795818567276, 'The paper is however very superficial in terms of experiments, or applications of the proposed scheme.': 1.0986119508743286, 'Most importantly, the fit with the main scope of ICLR is far from obvious with this work, that should probably re-submitted to better targets.': 1.0986123085021973, 'This paper proposes an algorithm for polynomial feature expansion on CSR matrices, which reduces the time complexity of the standard method by a factor d^k where d is the density of the sparse matrix.': 1.0986104011535645, 'The main contribution of this work is not significant enough.': 0.3889731764793396, 'The experiments are incomplete and not convincing.': 1.068672776222229, 'The background of the problem is not sufficiently introduced.': 1.0977206230163574, 'There are only two references in the introduction part (overall only three papers are cited), which are from decades ago.': 1.0944266319274902, 'Many more relevant papers should be cited from the recent literature.': 1.0985829830169678, 'The experiment part is very weak.': 0.38524481654167175, 'This paper claims that the time complexity of their algorithm is O(d^k D^k), which is an improvement over standard method O(D^k) by a factor d^k.': 1.0986027717590332, 'But in the experiments, when d=1, there is still a large gap (~14s vs. ~90s) between the proposed method and the standard one.': 1.098610520362854, 'The authors explain this as ""likely a language implementation"", which is not convincing.': 1.0957403182983398, 'To fairly compare the two methods, of course you need to implement both in the same programming language and run experiments in the same environment.': 1.0986067056655884, 'For higher degree feature expansion, there is no empirical experiments to show the advantage of the proposed method.': 1.0986074209213257, 'Some minor problems are listed below.': 1.0986123085021973, '1) In Section 2, the notation ""p_i:p_i+1"" is not clearly defined.': 0.325980007648468, '2) In Section 3.1, typo: ""efter"" - ""after""': 1.0118603706359863, '3) All the algorithms in this paper are not titled.': 0.956269383430481, 'The input and output is not clearly listed.': 1.0943137407302856, '4)': 1.0986123085021973, 'In Figure 1, the meaning of the colored area is not described.': 0.9938359260559082, 'Is it standard deviation or some quantile of the running time?': 0.27755850553512573, 'How many runs of each algorithm are used to generate the ribbons?': 1.0856502056121826, 'Many details of the experimental settings are missing.': 1.0919511318206787, 'The paper is beyond my expertise.': 1.0986123085021973, 'I cannot give any solid review comments regarding the techniques that are better than an educated guess.': 1.0986123085021973, 'However, it seems to me that the topic is not very relevant to the focus of ICLR.': 1.0984386205673218, 'Also the quality of writing requires improvement, especially literature review and experiment analysis.': 1.0986123085021973}"
277,https://openreview.net/forum?id=S1jE5L5gl,"{'The authors of the paper present a novel distribution for discrete variables called the ""concrete distribution"".': 1.0986119508743286, 'The distribution can be seen as a continuous relaxation for a distribution over discrete random variables.': 1.0971462726593018, 'The main motivation for introduction of the concrete distribution is the possibility to compute the gradient of discrete stochastic nodes in Stochastic Computational Graphs.': 1.0976781845092773, 'I think the paper is well written and sound, definitely of interest for the conference program.': 0.4401284158229828, 'As to the experimental part, the authors have results which support some kind of consistent superior performance for VIMCO for linear models and for concrete relaxations for non-linear models.': 1.0986096858978271, 'Any explanation for that?': 1.0985597372055054, 'Is this confirmed over different models and maybe datasets?': 1.0986121892929077, 'Similarly, it looks like VIMCO outperforms (in Figure 4) Concrete for large m, on the test NLL.': 1.0967968702316284, 'I would encourage to try with other values of m to see if this dependence on large m is confirmed or not.': 1.0986123085021973, ""I believe the paper should be accepted to the conference, however please consider that I'm not an expert in this field."": 1.0969111919403076, 'Some minor observations/comments/issues:': 1.0986096858978271, 'Section 2.1: there is a repetition ""be be"" in the first paragraph.': 1.0986123085021973, 'Section 2.4: I would add a reference for the ""multi-sample variational objective""': 1.0986123085021973, 'Section 3.1, just before Section 3.2: ""the Gumbel is a crucial 1"".': 1.0986096858978271, 'Why 1 and not ""one""?': 0.468730628490448, 'Section 3.3, last paragraph: ""Thus, in addition to relaxing the sampling pass of a SCG the log..."" I would add a comma after ""SCG"".': 1.084857702255249, 'More in general, the second part of the paragraph is very dense and not easy to ""absorb"".': 1.0958460569381714, ""I don't think it's an issue with the presentation: the concepts themselves are just dense."": 1.0965044498443604, 'However, maybe the authors could find a way to make the paragraph easier to assimilate for a less experienced reader.': 0.5509451627731323, 'Section 5.1, second paragraph: ""All our models are neural networks with layers of n-ary discrete stochastic nodes with log_2(n)-dimensional states on the corners of the hypercube {-1,1}^log_2(n).': 1.0986123085021973, 'The distribution of the nodes are parametrized by n real values log alpha_k"".': 0.39343005418777466, 'It is not clear to me, where does the log_2(n) come from.': 0.8436974287033081, 'Similarly for the {-1,1}.': 0.5365629196166992, 'Section 5.2: After ""this distribution.""': 1.0986123085021973, 'and ""We will"" there is an extra space.': 1.096313238143921, ""If a compare the last formula in Section 5.3 with Eq. 8, I don't see exactly why the former is a special case of the latter."": 1.0957273244857788, 'Is it because q(Z^i | x) is always one?': 0.3359702527523041, 'The authors describe the concrete distribution, a continuous approximation to': 1.0986121892929077, 'discrete distributions parameterized by a vector of continuous positive numbers': 1.0983939170837402, 'proportional to the probability of each discrete result.': 0.654904305934906, 'The concrete': 1.0986123085021973, 'distribution is obtained by using the softmax function to approximate the': 1.094670295715332, 'argmax operator.': 1.0986089706420898, 'The paper is clearly written, original and significant.': 1.098611831665039, 'The experiments clearly illustrate the advantages of the proposed method.': 0.6552240252494812, 'Some minor questions:': 1.0986123085021973, '""for the general n-ary case the Gumbel is a crucial 1 and the Gumbel-Max trick cannot be generalized': 0.9924476742744446, 'for other additive noise distributions""': 0.41382214426994324, 'What do you mean by this?': 1.098455786705017, 'Can you be more specific?': 0.8697542548179626, 'What is the temperature values used to obtain Table 1 and the table in Figure 4.': 1.0986123085021973, 'Thank you for an interesting read.': 1.0986121892929077, 'I think this paper has proposed a very useful method, which significantly simplifies the implementation of gradients for discrete random variables.': 1.0986123085021973, 'Using this trick quite a lot of discrete variable-based methods will be significantly easier to implement, e.g. a GAN-style generator for text (see the recent arxiv preprint arXiv:1611.04051).': 1.0986121892929077, ""I've got one suggestion to make the paper even better, but maybe the authors want to leave it to future work."": 1.0986084938049316, 'I think compared to lots of variance reduction techniques such as NVIL and VIMCO, this relaxation trick has smaller variance (from empirical observation of the reparameterisation trick), but in the price of introducing biases.': 1.0986121892929077, 'It would be fantastic if the authors can discuss the bias-variance trade-off, either in theoretical or experimental way.': 1.0986123085021973, 'My bet will be that here the variance dominates the stochastic estimation error of the gradient estimation, but it would be great if the authors can confirm this.': 1.0986086130142212, '**to area chair: concurrent paper by Jang et al. 2016**': 1.0986121892929077, ""It seems there's a concurrent submission by Jang et al."": 1.0986123085021973, ""I havent' read that paper in detail, but maybe the conference should accept or reject both?"": 1.0985008478164673}"
278,https://openreview.net/forum?id=S1jmAotxg,"{'This paper presents an approach which modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality.': 1.0986123085021973, 'This is achieved by using an inherently infinite prior, the stick-breaking process.': 1.0984199047088623, 'This is coupled with inference tailored to this model, specifically the Kumaraswamy distribution as an approximate variational posterior.': 1.0986123085021973, 'The resulting model is named the SB-VAE which also has a semi-supervised extension, in similar vein to the original VAE paper.': 0.5343219041824341, 'There\'s a lot of interest in VAEs these days; many lines of work seek to achieve automatic ""black-box"" inference in these models.': 1.0986123085021973, ""For example, the authors themselves mention parallel work by Blei's lab (also others) towards this direction."": 1.098611831665039, ""However, there's a lot of merit in investigating more bespoke solutions to new models, which is what the authors are doing in this paper."": 0.42337289452552795, ""Indeed, a (useful) side-effect of providing efficient inference for the SB-VAE is drawing attention to the use of the Kumaraswamy distribution which hasn't been popular in ML."": 1.0985310077667236, 'Although the paper is in general well structured, I found it confusing at parts.': 1.0868595838546753, 'I think the major source of confusion comes from the fact that the model specification and model inference are discussed in a somehow mixed manner.': 1.0986123085021973, 'The pre-review questions clarified most parts.': 1.0986123085021973, 'I have two main concerns regarding the methodology and motivation of this paper.': 0.48054787516593933, 'Firstly, conditioning the model directly on the stick-breaking weights seems a little odd.': 1.098610520362854, 'I initially thought that there was some mixture probabilistic model involved, but this is not the case.': 0.6724808216094971, ""To be fair, the authors discuss about this issue (which became clearer to me after the pre-review questions), and explain that they're investigating the apparently challenging problem of using a base distribution G_0."": 1.0986119508743286, 'The question is whether their relaxation is still useful.': 1.094831109046936, 'From the experiments it seems that the method is at least competitive, so the answer is yes.': 1.0452286005020142, 'Hopefully an extension will come in the future, as the authors mention.': 1.0986123085021973, 'The second concern is about the motivation of this method.': 1.0986121892929077, 'It seems that the paper fails to clearly explain in a convincing way why it is beneficial to reformulate the VAE as a SB-VAE.': 1.058521032333374, 'I understand that the non-parametric property induced by the prior might result in better capacity control, however I feel that this advantage (and potentially others which are still unclear to me) is not sufficiently explained and demonstrated.': 0.6543048024177551, 'Perhaps some comparison with a dropout approach or a more thorough discussion related to dropout would make this clearer.': 1.0773683786392212, 'Overall, I found this to be an interesting paper, it would be a good fit for ICLR.': 0.6630613207817078, 'The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process.': 1.0986087322235107, 'The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality.': 1.0986106395721436, 'To demonstrate the merit of their approach, the authors test this model on MNIST and SVHN in an unsupervised and semi-supervised fashion.': 1.0948524475097656, 'After reading the paper in more detail, I find that the claim that the dimensionality of the latent variable is stochastic does not seem quite correct: all latent variables are ""used"" (which actually enable backpropagation)': 1.0986123085021973, 'but the latent variables are parametrized differently (into ) and the decoding process is altered as to give the impression of sparsity.': 1.0986123085021973, 'The way all these latent variables are used does not involve any marginalization but is very similar to the common soft-gating mechanism already used in LSTM or attentional model.': 1.0986123085021973, 'With respect to the Figure 5b showing the decoder input weights: component collapsing probably does not have the same effect as Gaussian prior.': 1.0986123085021973, 'is positive therefore having a very small average value might mean that its value is close to zero most of the time, not requiring any update on the weight.': 1.0986123085021973, 'For the standard Gaussian prior, component collapsing means having a very noisy input with no signal involved, which forces the decoder to shut down this channel, i.e. have small incoming weights from this collapsed variable.': 1.09466552734375, ""Adding a histogram of the latent variables in addition to that might help decide if the associated weights are relatively large because they are actually used or if it's because the inputs are zero anyway."": 1.0985989570617676, 'The semi-supervised results are better than a weaker version of the model used in (Kingma et al., 2014), but as to have a fairer comparison, the results should be compared with the M1+M2 model in that paper, even if that requires also using two VAEs.': 1.0986076593399048, 'Summary: This is the first work to investigate stick-breaking priors, and corresponding inference methods, for use in VAEs. The background material is explained clearly, as well as the explanation of the priors and posteriors and their DNCP forms. The paper is really well written.': 1.0986123085021973, 'In experiments, they find that stick-breaking priors does not generally improve upon spherically Gaussian priors in the completely unsupervised setting, when measured w.r.t.': 1.0986123085021973, 'log-likelihood.': 1.0986123085021973, ""The fact that they do report this 'negative' result suggests good scientific taste."": 1.0986120700836182, 'In a semi-supervised setting, the results are better.': 1.0896426439285278, 'Comments:': 0.8487273454666138, 'sec 2.1: There is plenty of previous work with non-Gaussian p(z): DRAW, the generative ResNet paper in the IAF paper, Ladder VAEs, etc.': 0.42056068778038025, ""sec 2.2: two comma's"": 1.0986123085021973, 'text flow eq 6: please refer to appendix with the closed-form KL divergence': 0.8408339023590088, '""The v\'s are sampled via"" => ""In the posterior, the v\'s are sampled via"".': 0.54598468542099, ""It's not clear you're talking about the posterior here, instead of the prior."": 1.098407506942749, 'The last paragraph of section 4 is great.': 1.0986123085021973, 'Sec 7.1: ""Density estimation"" =>': 1.0986123085021973, ""Technically you're also doing mass estimation."": 1.0986123085021973, 'Sec 7.1: 100 IS samples is a bit on the low side.': 1.0986123085021973, 'Figure 3(f).': 1.0986123085021973, 'Interesting that k-NN works so well on raw pixels.': 1.0986123085021973}"
279,https://openreview.net/forum?id=S1oWlN9ll,"{'This paper proposed a proximal (quasi-) Newton’s method to learn binary DNN.': 1.098611831665039, 'The main contribution is to combine pre-conditioning with binarization in a proximal framework.': 1.0986053943634033, 'It is interesting to have a proximal Newton’s method to interpret the different DNN binarization schemes.': 1.0986121892929077, 'This gives a new interpretation of existing approaches.': 1.0037513971328735, 'However, the theoretical analysis is not very convincing or useful.': 1.0985504388809204, 'The formulated optimization problem (3)-(4) is essentially a mixed integer programming.': 1.0936611890792847, 'Even though the paper treats the integer part as a constraint and address it in proximal operators, the constraint set is still discrete and there is no guarantee that the proximal Newton algorithm could converge under practically useful conditions.': 1.0838130712509155, 'In practice it is hard to verify the assumption [d_t^t]_k > \\beta in Theorem 3.1.': 1.0986119508743286, 'This relation could be hard to hold in DNN as the loss surface could be extremely complicated.': 1.0986123085021973, 'The paper presents a second-order method for training a neural networks while ensuring at the same time that weights (and activations) are binary.': 1.0986123085021973, 'Through binarization, the method aims to achieve model compression for subsequent deployment on low-memory systems.': 1.0986123085021973, 'The method is abbreviated BPN for ""binarization using proximal Newton algorithm"".': 0.19783417880535126, 'The method incorporates the supervised loss function directly in the binarization procedure, which is an important and desirable property.': 1.0986065864562988, '(Authors mention that existing weight binarization methods ignore the effect of binarization to the loss.)': 0.368614137172699, 'The method is clearly described and related analytically to the previously proposed weight binarization methods.': 1.0957242250442505, 'The experiments are extensive with multiple datasets and architectures, and demonstrate the generally higher performance of the proposed approach.': 1.0986123085021973, 'A minor issue with the feed-forward network experiments is that only test errors are reported.': 1.0986123085021973, 'Such information does not really give evidence for the higher optimization performance.': 1.0986123085021973, '(see also comment ""RE: AnonReviewer3\'s questions"" stating that all baselines achieve near perfect training accuracy.)': 1.0849641561508179, 'Making the optimization problem harder (e.g. by including an explicit regularizer into the training objective, or by using a data extension scheme), and monitoring the training objective instead of the test error could be a more direct way of demonstrating superior optimization performance.': 1.0986123085021973, 'The superiority of BPN is however becoming more clearly apparent in the subsequent LSTM experiments.': 1.0331875085830688, 'Taking into account the loss in the binarization step through a proximal Newton algorithm is a nice idea.': 1.0986123085021973, 'This is at least one approach to bringing in the missing loss in the binarization step, which has recently gone from a two step process of train and binarize to a single step simultaneous train/compress.': 1.0986123085021973, 'Performance on a few small tasks show the benefit.': 1.0976824760437012, 'It would be nice to see some results on substantial networks and tasks which really need compression on embedded systems (a point made in the introduction).': 1.0986123085021973, 'Is it necessary to discuss exploding/vanishing gradients when the RNN experiments are carried out by an LSTM, and handled by the cell error carousel?': 1.0986121892929077, 'We see the desire to tie into proposition 2, but not clear that the degradation we see in the binary connect is related.': 1.0986123085021973, 'Adam is used in the LSTM optimization, was gradient clipping really needed, or is the degradation of binary connect simply related to capacity?': 1.098611831665039, 'For proposition 3.1, theorem 3.1 and proposition 3.2 put the pointers to proofs in appendix.': 1.0986121892929077}"
280,https://openreview.net/forum?id=S1vyujVye,"{'This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks.': 1.0986123085021973, 'In particular, the feature representations of the patches from the same image are encouraged to be closer than the those from different images.': 1.0986123085021973, 'The distance ratios of positive training pairs are optimized.': 1.0986123085021973, 'The proposed method are empirically shown to be effective as an initialization method for supervised training.': 1.0986123085021973, 'Strengths:': 1.0986123085021973, 'The training objective is reasonable.': 1.0986123085021973, 'In particular, high-level features show translation invariance.': 1.0986123085021973, 'The proposed methods are effective for initializing neural networks for supervised training on several datasets.': 1.0986123085021973, 'Weaknesses:': 1.0986123085021973, 'The methods are technically similar to the “exemplar network” (Dosovitskiy 2015).': 1.0986123085021973, 'Cropping patches from a single image can be taken as a type of data augmentation, which is comparable to the data augmentation of positive sample (the exemplar) in (Dosovitskiy 2015).': 0.9944109916687012, 'The paper is experimentally misleading.': 1.0986123085021973, 'The results reported in this paper are based on fine-tuning the whole network with supervision.': 1.0986003875732422, 'However, in Table 2, the results of exemplar convnets (Dosovitskiy 2015) is from unsupervised feature learning (the network is not finetuned with labeled samples, and only a classifier is trained upon the features).': 1.0986123085021973, 'Therefore, the comparison is not fair.': 1.0914472341537476, 'I suspect that exemplar convnets (Dosovitskiy 2015) would achieve similar improvements from fine-tuning; so, without such comparisons (head-to-head comparison with and without fine-tuning based on the same architecture except for the loss), the experimental results are not fully convincing.': 1.0986123085021973, 'Regarding the comparison to “What-where” autoencoder (Zhao et al, 2015), it will be interesting to compare against it in large-scale settings, as shown by Zhang et al, ICML 2016 (Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-Scale Image Classification).': 1.0478150844573975, 'Training an AlexNet is not very time-consuming with latest (e.g., TITAN-X level) GPUs.': 1.0835334062576294, 'The proposed method seems useful only for natural images where different patches from the same image can be similar to each other.': 0.5899432301521301, 'The proposed self supervised loss is formulated using a Siamese architecture and encourages patches from the same image to lie closer in feature space than a contrasting patch taken from a different, random image.': 1.0986062288284302, 'The loss is very similar in spirit to that of Doersch et al. ICCV 2015 and Isola et al. ICLR 2016 workshop.': 1.0986065864562988, 'It seems that the proposed loss is actually a simplified version of Doersch et al.': 1.0986099243164062, 'ICCV 2015 in that it does not make use of the spatial offset, a freely available self supervised signal in natural images.': 1.0986123085021973, 'Intuitively, it seems that the self-supervised problem posed by this method is strictly simpler, and therefore less powerful, than that of the aforementioned work.': 1.0986123085021973, 'I would like to see more discussion on the comparison of these two approaches.': 1.0986123085021973, 'Nevertheless the proposed method seems to be effective in achieving good empirical results using this simple loss.': 1.0986123085021973, 'Though more implementation details should be provided, such as the effect of patch size, overlap between sampled patches, and any other important measures taken to avoid trivial solutions.': 1.0986123085021973, 'This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting (though likely applicable to fully-connected nets as well).': 0.8054695129394531, 'The method is that of ‘spatial constrasting’, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score for patches from diferent images.': 0.4767511487007141, 'The method is simple enough that I am surprised that no-one has tried this before (at least according to the previous work in the submission).': 0.4413078725337982, 'Here are some comments:': 1.0986123085021973, 'The usage of P(f_i^1 | f_i^2) in Section 4.1 is a bit odd.': 1.0986123085021973, 'May be worth defining mathematically what kind of probability the authors are talking about, or just taking that part out (“probability” can be replaced with another word).': 0.6009846329689026, 'I would like to know more about how the method is using the “batch statistics” (end of Section 4.2) by sampling from it, unless the authors simply mean that the just sample from all the possible triples in their batch.': 1.0968431234359741, 'Shouldn’t the number of patches sampled in Algorithm 1 be a hyper-parameter rather than just be 1?': 1.0986123085021973, 'Have the authors tried any other value?': 1.0986123085021973, 'I think there are some missing details in the paper, like the patch size or whether the authors have played with it at all (I think this is an important hyper-parameter).': 0.9245151877403259, 'The STL results are quite impressive, but CIFAR-10 maybe not so much.': 1.0986123085021973, 'For CIFAR I’d expect that one can try to pre-train on, say, Imagenet + CIFAR to build a better representation.': 1.098610281944275, 'Have the authors considered this?': 1.0986123085021973, 'All in all, this is an interesting piece of work with some obvious applications, and it seems relatively straightforward to implemenent and try.': 1.0986106395721436, 'I think I would’ve liked more understanding of what the spatial contrasting actually learns, more empirical studies on the effects of various parameter choices (e.g., patch size) and more attempts at beating the state of the art (e.g. CIFAR).': 1.040854811668396}"
281,https://openreview.net/forum?id=S1xh5sYgx,"{'Strengths': 1.0986123085021973, '\uf06e': 1.0986123085021973, 'An interesting proposal for a smaller CNN architecture designed for embedded CNN applications.': 1.0986109972000122, 'Balanced exploration of CNN macroarchitecture and microarchitecture with fire modules.': 0.7059991955757141, 'x50 less memory usage than AlexNet, keeping similar accuracy': 0.41334787011146545, 'strong experimental results': 1.0986123085021973, 'Weaknesses': 1.0986123085021973, 'Would be nice to test Sqeezenet on multiple tasks': 1.0986095666885376, 'lack of insights and rigorous analysis into what factors are responsible for the success of SqueezeNet.': 1.0985429286956787, 'For example, how are ResNet and GoogleNet connected to the current architecture?': 1.0004594326019287, 'Another old paper (Analysis of correlation structure for a neural predictive model with application to speech recognition, Neural Networks, 1994) also showed that the “by-pass” architecture by mixing linear and nonlinear prediction terms improves long term dependency in NN based on rigorous perturbation analysis.': 1.0986123085021973, 'Can the current work be placed more rigorously on theoretical analysis?': 1.0986123085021973, 'Summary: The paper presents a smaller CNN architecture called SqueezeNet for embedded deployment. The paper explores CNN macroarchitecture and microarchitecture to develop SqueezeNet, which is composed of fire modules.': 1.0986114740371704, 'Pros:': 1.0986123085021973, 'Achieves x50 less memory usage than AlexNet while keeping similar accuracy.': 1.0874825716018677, 'Cons & Questions:': 1.098601222038269, 'Complex by-pass has less accuracy than simple by-pass.': 1.0985479354858398, 'And simple by-pass is like ResNet bottlenecks and complex by-pass is like inception modules in GoogLeNet.': 0.7769447565078735, 'Can we say that these two valiants of SqueezeNet are adaptation of concepts seen in GoogLeNet and ResNet?': 1.0986121892929077, 'If so, then shouldn’t be there a SqueezeNet like model that achieves similar accuracy compared with GoogLeNet and ResNet?': 1.0972936153411865, 'The Squeezenet paper came out in Feb 2016, and I read it with interest.': 1.0986109972000122, 'It has a series of completely reasonable engineering suggestions for how to save parameter memory for CNNs for object recognition (imagenet).': 1.0986123085021973, 'The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet.': 1.0986123085021973, '(Looks like ~500x if combined with Han, 2015).': 1.0894246101379395, 'So, very nice results, definitely worth publishing.': 1.0867156982421875, 'Since the arxiv paper came out, people have noticed and worked to extend the paper.': 1.032503366470337, 'This is already evidence that this paper will have impact': 1.0986123085021973, 'and deserves to have a permanent published home.': 1.0983737707138062, 'On the negative side, the architecture was only tested on ImageNet': 1.0986106395721436, 'unclear whether the ideas transfer to other tasks (e.g., audio or text recognition).': 1.094574213027954, 'And, as with many other architecture-tweaking papers, there is no real mathematical or theoretical support for the ideas: they are just sensible and empirically work.': 1.0986123085021973, 'Oh the whole, I think the paper deserves to appear at ICLR, being in the mainline of work on deep learning architectures.': 1.098384141921997}"
282,https://openreview.net/forum?id=SJ-uGHcee,"{'This paper presents iterative PoWER, an off-policy variation on PoWER, a policy gradient algorithm in the reward-weighted family.': 1.0986123085021973, ""I'm not familiar enough with this type lower bound scheme to comment on it."": 1.0986123085021973, 'It looks like the end result is less conservative step sizes in policy parameter space.': 1.0986123085021973, 'All expectation-based algorithms (and their KL-regularized cousins a-la TRPO) take smallish steps, and this might be a sensible way to accelerate them.': 1.0986123085021973, 'The description of the experiments in Section VI is insufficient for reproducibility.': 1.0986100435256958, 'Is ""The cart moved right"" supposed to be ""a positive force is applied to the cart""?': 1.0985605716705322, 'How is negative force applied?': 1.0986123085021973, 'What is the representation of the state?': 1.098598599433899, 'What is the distribution of initial states?': 1.0986123085021973, 'A linear policy is insufficient for swing up and balance of a cart-pole.': 1.0986123085021973, 'Are you only doing balancing?': 1.0986123085021973, 'What is the noise magnitude of the policy?': 1.0986123085021973, 'How was it chosen?': 1.0986123085021973, 'How long were the episodes?': 1.0986123085021973, 'The footnote at the bottom of page 8 threw me off.': 1.0986123085021973, ""If you're using Newton's method, where is the discussion of gradients and Hessians?"": 1.073453664779663, 'I thought the argmax_theta operator was a stand-in for an EM-style step, which I how I read Eq (8) in the Kober paper.': 1.0986078977584839, 'https://papers.nips.cc/paper/3545-policy-search-for-motor-primitives-in-robotics.pdf': 0.9836371541023254, 'I might be missing something basic here.': 1.0985854864120483, 'The control variates thing seems cool.': 1.0200012922286987, 'I only read up on it now': 1.0966424942016602, ""and I don't think I've seen it before in the RL literature."": 1.0163664817810059, 'Seems like a powerful tool.': 1.0984293222427368, 'Section 6.2 has too much business jargon, I could barely read it.': 0.9976866245269775, 'The paper considers the problem of reinforcement learning where the number of policy updates is required to be low.': 0.856341540813446, 'The problem is well motivated and the author provides an interesting modification to the PoWER algorithm, along with variational bounds on the value function (lemmas 3,4) which are interesting in themselves.': 1.0858533382415771, 'They also provide numerical results on the cartpole problem and a problem in online advertising with real data.': 0.43404996395111084, 'Overall this is a strong, well-written paper.': 0.4983837306499481, 'My main reservation is whether it is completely appropriate for ICLR, since the log-concavity assumption the model relies on appear to restrict to simpler models where representations will be not in fact be learned.': 1.0155645608901978, 'Other comments:': 1.0986123085021973, 'There is a general lack of baselines in the numerical experiment section.': 0.6331559419631958, 'I acknowledge this is somewhat of an unusual setting, but even a simple, well-justified baseline would have been welcome.': 0.8960221409797668, 'Since cartpole is a relatively simple problem and the advertising dataset is presumably private, perhaps a way to generate a synthetic advertising dataset would have been interesting.': 1.0986123085021973, 'I was confused by the control variates as constant scalars - are they meant to be constant baselines?': 1.0986123085021973, 'And if so, they appear to be treated as hyperparameters -  why are they not learned or estimated?': 1.0986123085021973, 'There is an interesting section on constrained optimization, but as it is, feels a bit disconnected from the rest of the paper.': 1.0986123085021973, 'It appears applicable to the problem of online advertising, but is not mentioned in the corresponding experimental section.': 1.0986123085021973, 'Also might be worth adding a citation to the literature of constrained MDPs which develops similar lagrangian ideas.': 1.0986123085021973, 'The paper presents an interesting modification to PoWER algorithm that is well motivated.': 1.0844348669052124, 'The main limitation of this paper is the lack of comparison with other methods and on richer problems.': 1.0985918045043945, ""The experiments haven't given confidence to show its claimed benefits, generality and scalability over prior methods."": 1.0960896015167236, 'Giving this confidence doesn’t necessarily require running your method on all large-scale domains or doing exhaustic hyper-parameter search, but for example it could go beyond current domains.': 1.0986123085021973, 'Cartpole only optimizes 5 parameters.': 1.081599235534668, 'Ad targeting task lacks comparison with alternative methods.': 1.0809837579727173, 'Since this method is built on PoWER and closely connected with RWR, it is likely there are limits to performance which may become apparent when the method is tried on other domains and with other benchmark methods, e.g. even standard ones like importance sampling-based off-policy learning is known to suffer in high-dimensional or continuous action space; limits of RWR/PoWER-like methods based on their connection with entropy-regularized RL.': 1.0986008644104004, 'https://arxiv.org/abs/1604.06778 may be a valuable starting point for comparison, e.g. it compares RWR with policy gradient methods, and it has open-sourced codes.': 1.0812292098999023}"
283,https://openreview.net/forum?id=SJ25-B5eg,"{'This paper proposes to use an SSNT model of p(x|y) to allow for a noisy channel model of conditional generation that (still) allows for incremental generation of y.': 1.0986123085021973, 'The authors also propose an approximate search strategy for decoding, and do an extensive empirical evaluation.': 1.0986123085021973, 'PROs: This paper is generally well written, and the SSNT model is quite interesting and its application here well motivated.': 1.0985984802246094, 'Furthermore, the empirical evaluation is very well done, and the authors obtain good results.': 1.0986121892929077, 'CONs: One might be concerned about whether the additional training and decoding complexity is warranted.': 1.0986123085021973, 'For instance, one might plausibly obtain the benefits of the proposed approach by reranking (full) outputs from a standard seq2seq model with a score combining p(y|x), p(x|y), and p(y).': 1.0986123085021973, ""(It's worth noting that Li et al."": 1.0986123085021973, '(NAACL 2016) do something similar for conversation modeling).': 1.0892537832260132, 'At the same time, being able to rerank during search may be helpful, and so it might be nice to see some experiments addressing this.': 1.0986123085021973, 'Other Comments:': 1.0986123085021973, '- Given that the main thrust of the paper is to provide a model for p(x|y), the paper might be slightly clearer if Section 2 were presented from the perspective of modeling p(x|y) instead of switching back to p(y|x) as in the original Yu et al. paper.': 1.0980644226074219, '- It initially seems strange to suggest a noisy-channel model as a way of addressing the ""explaining away"" problem, since now you have an explicit, uncalibrated p(y) term. However, since seq2seq models appear to naturally do a lot of target-side language modeling, incorporating an explicit p(x|y) term seems quite clever.': 1.0841920375823975, 'The paper proposes an online variant of segment to segment transducers, which allows to circumvent the necessity of observing whole sentence, before making target predictions.': 1.090461254119873, 'Authors mostly build on their previous work, allowing additionally to leverage independent priors on the target hypotheses, like the language grammar or sentence length.': 0.26877593994140625, 'Strong points:': 1.0986123085021973, 'well written, interesting idea of combining various sources of information in a Bayesian framework for seq2seq models': 1.0986123085021973, 'Handling something in an online manner typically makes things more difficult, and this is what the authors are trying to do here - which is definitely of interest to the community': 1.0986123085021973, 'strong experimental section, with some strong results (though not complete: see weak points)': 1.0986123085021973, 'Weak points:': 1.0986123085021973, 'Authors do not improve on computational complexity (w.r.t Tillmann proposal), hence the algorithms may be found difficult to apply in scenarios where inputs may be long (this already takes into account a rather constrained model of alignment latent variables)': 1.0986123085021973, 'What about the baseline where you only combine direct, LM and bias contributions (no channel)?': 1.0986123085021973, 'Was there any (non-obvious) algorithmic constraint why - this has not been included?': 1.0986123085021973, 'Some other (minor) comments:': 1.0986123085021973, 'Related to the first weak point: can you elaborate more on how the clue of your work is conceptually different from the work of Tillmann et al. (1997) (except, of course, the fact you use connectionist discriminative models to derive particular conditional probabilities).': 1.0986123085021973, 'How sensitive is the model to different choices of hyper-parameters in eq (3).': 1.0986123085021973, 'Do you naively search through the search space of those, or do something more clever?': 1.0986123085021973, 'Some more comments on details of the auxiliary direct model would be definitely of interest.': 1.0986123085021973, 'How crucial is the correct choice of the pruning variables (K1 and K2)?': 1.0986123085021973, 'Sec. 2: makes no Markovian assumptions -> no first-order Markovian assumption?': 1.0986123085021973, 'Typos:': 1.0986123085021973, 'Table 1: chanel -> channel (one before last row)': 1.0986123085021973, 'Apologies for late review.': 1.0986123085021973, ""This paper proposes the neural noisy channel model, P(x|y), where (x, y) is a input-to-out sequence pair,  based on the authors' previous work on segment to segment neural transduction (SSNT) model."": 1.0986123085021973, 'For the noisy channel model, the key difference from sequence-to-sequence is that the complete sequence y is not observed beforehand.': 1.0986123085021973, 'SSNT handles this problem elegantly by performing incremental alignment and prediction.': 0.7089675068855286, 'However, this paper does not present anything that is particular novel on top of the SSNT.': 0.42362168431282043, 'The SSNT model is still applicable by reverting the input and output sequences.': 1.0983306169509888, 'The authors said that an unidirectional LSTM has to be used as an encoder instead of the bidirectional LSTM, but I think the difference is minor.': 1.0947680473327637, 'The decoding algorithm presented in the appendix is relatively new.': 1.098554015159607, 'The experimental study is very comprehensive and strong, however, there is one important baseline number that is missing for all the experiments.': 1.098544716835022, 'Can you give the number that uses direct + LM + bias, and if you can give direct + bias number would be even better.': 1.0986084938049316, 'Although using a LM for the direct model does not make a lot of sense mathematically, however, it works pretty well in practice, and the LM can rescore and smooth your predictions, see': 1.0986119508743286, 'Deep Speech 2: End-to-End Speech Recognition in English and Mandarin': 1.0986121892929077, 'from Baidu for example.': 1.0986123085021973, 'I think the LM may be also the key to explain why noisy channel is much better than direct model in Table 3.': 0.9368481040000916, 'A couple minor questions are': 1.0986123085021973, '1. it is not very clear to me is your direct model in the experiments SSNT or sequence-to-sequence model?': 1.0986123085021973, '2. O(|x|^2*|y|) training complexity is OK, but it would be great to further cut down the computational cost, as it is still very expensive for long input sequences, for example, for paragraph or document level modeling, or speech sequences.': 1.0986123085021973, 'The paper is well written, and overall, it is still an interesting paper, as the channel model is always of great interest to the general public.': 1.0986123085021973}"
284,https://openreview.net/forum?id=SJ3rcZcxl,"{'**Edit: Based on the discussion below, my main problem (#2) was not correct.': 1.380953311920166, 'I have changed my overall rating from a 3 to a 7**': 1.3833707571029663, 'This paper makes a fascinating observation: one can introduce an action-dependent baseline (control variate) into REINFORCE, which introduces bias, and then include a correction term to remove the bias.': 1.3840968608856201, 'The variance of the correction term is low relative to the REINFORCE update and the action-dependent baseline, and so this results in benefits.': 1.2937120199203491, 'However, the paper is poorly executed.': 1.386274814605713, 'Below I list my concerns.': 1.3862943649291992, '1. The paper tries to distinguish between ""policy gradient"" methods and ""actor critic"" methods by defining them in a non-standard way. Specifically, when this paper says ""policy gradient"" it means REINFORCE. Historically, the two have meant different things: some policy gradient algorithms are actor-critics (e.g., Degris et al\'s INAC algorithm) while others are not (e.g. REINFORCE).': 0.3916529417037964, '2. The proposed Q-Prop algorithm includes many interesting design choices that make in unclear what the real source of improved performance is. Is the improved performance due to the use of the action-dependent control variate? Would the same setup but using a state-value baseline still perform just as well? Are the performance benefits due to the use of an off-policy advantage estimation algorithm, GAE(lambda)? Or, would performance have been similar with an on-policy advantage estimation algorithm? What about if a different off-policy advantage estimation algorithm was used, like Retrace(lambda), GTD2, ETD, or WIS-LSTD? Or, is the improved performance due to the use of a replay buffer?': 0.8458235263824463, 'Comparisons are not performed between variants of Q-Prop that show the importances of these different components.': 0.8303756713867188, 'Rather the authors opt to show better performance on a benchmark task.': 0.710350513458252, 'I find this to be non-scientific, and more of a paper showing a feat of engineering (by combining many different ideas) than it is a research paper that studies the details of which parts of Q-Prop make it work well.': 1.3862943649291992, 'For example, after reading this paper, it is not clear whether having the action-dependent baseline (or using the first order Taylor approximation for the baseline) is beneficial or not - it could be that the strong performance comes from GAE(lambda) or the use of a replay buffer.': 1.3307291269302368, 'At the very least I would have expected comparisons to Q-Prop using a state-value baseline (which would then be a variant of REINFORCE using off-policy data and a replay buffer, and which would show whether the action-dependent baseline is important).': 1.3862228393554688, '3. There is a fair amount of discussion about unbiased policy gradient algorithms, which is not accurate. Most policy gradient algorithms are biased, and making them unbiased tends to hurt performance. This is discussed in the paper ""Bias in Natural Actor-Critic Algorithms"", which applies to non-natural algorithms as well. Also, I suspect that the use of GAE(lambda) results in the exact sort of bias discussed in that paper, even when lambda=1. As a result, Q-Prop may act more like an average reward method than expected. This should be discussed.': 1.324293851852417, '4. The proposed algorithm can be applied to deep architectures, just as most linear-time policy gradient algorithms can. However, it does not have to be applied to deep architectures. The emphasis on ""deep"" therefore seems to detract from the core ideas of the paper.': 1.0235939025878906, ""5. The paper repeatedly says that importance sampling based methods result in high variance. This ignores weighted importance sampling methods that have very low variance. A good example of this is Mahmood et al's WIS-LSTD algorithm. WIS-LSTD has high computational complexity, so it would only be compared to on non-deep RL problems, of which there are plenty. Alternatively, algorithms like Retrace(lambda) have quite low variance since the likelihood ratios are never bigger than one. Others might argue that ETD algorithms are currently the most effective. The simple dismissal of these algorithms because the original importance sampling estimator proposed in 2000 has high variance is not sufficient."": 0.6463640332221985, '6. The paper does not compare to natural actor-critic algorithms. Once the weights, w, have been computed, REINFORCE uses samples of states from the normalized discounted state distribution and samples of the corresponding returns to estimate the policy gradient. One of the main reasons Q-Prop should work better than REINFORCE is that it includes a control variate that reduces the variance of the policy gradient update after w has been computed. Now, compare this to natural policy gradient algorithms. Once the weights, w, have been computed (admittedly, using compatible features for the advantage estimation but any features for the state-value estimation) the resulting update is = w. That is, is has zero variance and does not require additional sampling. It is as though a perfect control variate was used. Furthermore, natural gradient algorithms can be applied to deep architectures. Degris et al\'s INAC algorithm is linear time. Desjardin et al\'s ""natural neural networks"" paper also discusses efficient implementations of natural gradients for neural networks. Dabney\'s Natural Temporal Difference algorithms have linear time variants that fit this paper\'s description of actor-critic algorithms.': 0.6317936182022095, 'To summarize, given the weights w, REINFORCE has high variance, and Q-Prop claims to reduce the variance of REINFORCE.': 1.3408253192901611, 'However, natural policy gradient methods have zero variance given the weights w.': 1.3862923383712769, 'So, what is the benefit of Q-Prop over natural gradient algorithms using off-policy value function estimation methods to estimate Q (or A)?': 0.9045827984809875, 'That is, why should we expect Q-Prop to perform better than NAC-LSTD using GAE(lambda) with experience replay in place of LSTD?': 1.3862943649291992, '7. Equation (2) is false. The right side is proportional to the left side, not equal to it. There is a (1-gamma) term missing. There are also other typos throughout (e.g., Q and A sometimes are missing their action arguments).': 1.3862943649291992, 'Although I have listed my concerns, I would like to re-iterate that I do find the idea of an action-dependent baseline fascinating.': 1.3862943649291992, 'My problem with this paper is with its execution, not with the novelty, impact, or quality of the core idea.': 1.3862943649291992, 'The paper proposes using a first-order Taylor expansion as a control variate in policy gradient-style methods.': 1.3862943649291992, 'Empirical results in dynamical control tasks suggest that this algorithm reduces the sample complexity, while the theoretical results presented suggest the algorithm is unbiased but of lower variance.': 1.3862943649291992, 'The use of control variates is very important and the present paper is an interesting approach in this direction.': 1.3862943649291992, 'I am not fully convinced of the approach, because it is one of many possible, and the theoretical analysis relies on an approximation of the variance rather than exact calculations, which makes it less compelling.': 1.3862943649291992, 'However, this paper is a step in the right direction so it is worth accepting.': 1.3862943649291992, 'In the experiments, a few things need to be discussed further:': 1.3862943649291992, 'What is the running time of the proposed approach?': 1.3862943649291992, 'The computation of the extra terms required looks like it could be expensive.': 1.3862943649291992, 'running time comparison in addition to sample comparison should be included': 1.3847748041152954, 'The sensitivity to parameter settings of the proposed algorithm needs to be illustrated in separate graphs, since this is one of the main claims in the paper': 1.3819282054901123, 'It would be nice to have a toy example included in which one can actually compute exact values and plot learning curves to compare more directly bias and variance.': 1.3862943649291992, 'It would especially be nice to do this with a task that includes rare states, which is the case in which variance of other methods (eg importance sampling) really becomes significant.': 1.3862943649291992, 'This paper presents  a model-free policy gradient approach for reinforcement learning that combines on-policy updates with an off-policy critic.': 1.3797320127487183, 'The hope is to learn continuous control in a sample-efficient fashion.': 0.8571613430976868, 'The approach is validated on a number of low-dimensional continuous control tasks in a simulated environment.': 0.6352862119674683, 'The paper is very well written, easy to follow, and provides an adequate context with which to appreciate the contributions it brings.': 1.2546359300613403, 'Although this reviewer is not an expert in this literature, the proposed approach appears novel.': 0.862048327922821, 'The Q-Prop estimator appears to be a general and useful method for policy learning, and the experimental validations provide adequate support for the claims of improved sample efficiency.': 1.383028507232666, 'The detailed derivations given in the Supplementary Materials are very useful.': 0.7354892492294312, 'I like the paper and I don’t have much to comment on.': 1.3302654027938843, 'Perhaps a discussion of the following aspects would add to the depth:': 0.7288541197776794, '1) comparison of the methods at a given computational cost, instead of by the number of episodes seen.': 0.7026530504226685, '2) discussion of the limitations of the technique: are there situations where convergence is difficult': 0.700533926486969, 'Possible typo: in equation (4), should we read  instead of  ?': 0.9319059252738953, 'If not, then what is Q() without subscript w?': 1.2774196863174438, 'This paper proposed a new policy gradient method that uses the Taylor expansion of a critic as the control variate to reduce the variance in gradient estimation.': 1.3663698434829712, 'The key idea is that the critic can be learned in an off-policy manner so that it is more sample efficient.': 1.2564276456832886, 'Although the algorithm structure is similar to actor-critic, the critic information is “truncated” in a proper manner to reduce the variance in policy gradient.': 1.2741373777389526, 'The proposed methods are evaluated on OpenAI Gym’s MuJoCo domains.': 1.30269193649292, 'Q-Prop is shown to produce more stable performance compared to DDPG and has higher sample efficiency than TRPO.': 1.3853542804718018, 'The stability of off-policy TD learning for the critic is not guaranteed.': 1.3861230611801147, 'Do the authors observe instability of it in the experiment?': 1.0762101411819458, 'As the authors stated in the paper, the critic does not need to approximate the actual value function very well as long as it is correlated with \\hat{A}.': 1.3862937688827515, 'In the two adaptive Q-Prop schemes, the authors apply some tricks (conservative and aggressive adaptation) to control the possible unreliable estimate of the critic.': 1.3860260248184204, 'This could be another evidence that the off-policy critic is not reliable.': 1.2936733961105347, 'The authors may need to comment more on this point.': 1.3831580877304077, 'Especially, it will be useful if the authors could show/justify that by such a design Q-Prop is robust against unreliable critic estimate.': 1.3862943649291992, 'The authors seem to indicate that the advantage of Q-Prop over DDPG is in its insensitivity to hyperparameters.': 1.3729383945465088, 'In Figure 3(a), the authors show that DDPG is sensitive to hyperparameters.': 1.3839259147644043, 'However, the sensitivity of Q-Prop to the same hyperparameter is not shown.': 1.3831509351730347, 'Experiments in the paper show that Q-Prop has advantage over TRPO in sample complexity.': 1.3862943649291992, 'However, not much experiments are shown to justify the advantage of Q-Prop over DDPG.': 1.3862943649291992, 'This is important because Table 1 shows that TR-c-Q-Prop needs significantly more samples than DDPG on Hopper, HalfCheetah and Swimmer.': 1.3862943649291992, 'Any comment on that?': 1.3862943649291992}"
285,https://openreview.net/forum?id=SJ6yPD5xg,"{'This paper proposes a way of adding unsupervised auxiliary tasks to a deep RL agent like A3C. Authors propose a bunch of auxiliary control tasks and auxiliary reward tasks and evaluate the agent in Labyrinth and Atari.': 1.0986123085021973, 'Proposed UNREAL agent performs significantly better than A3C and also learns faster.': 1.0986123085021973, 'This is definitely a good contribution to the conference.': 1.0986123085021973, 'However, this is not a surprising result since adding additional auxiliary tasks that are relevant to the goal should always help in better and faster feature shaping.': 1.0986123085021973, 'This paper is a proof of concept for this idea.': 1.0986123085021973, 'The paper is well written and easy to follow by any reader with deep RL expertise.': 1.0986123085021973, 'Can authors comment about the computational resources needed to train the UNREAL agent?': 1.0986123085021973, 'The overall architecture is quite complicated.': 1.0986123085021973, 'Are the authors willing to release the source code for their model?': 1.0986123085021973, 'After rebuttal:': 1.0986123085021973, 'No change in the review.': 1.0986123085021973, ""This paper is about improving feature learning in deep reinforcement learning, by augmenting the main policy's optimization problem with terms corresponding to (domain-independent) auxiliary tasks."": 1.0986123085021973, 'These tasks are about control (learning other policies that attempt to maximally modify the state space, i.e. here the pixels), immediate reward prediction, and value function replay.': 1.0980268716812134, 'Except for the latter, these auxiliary tasks are only used to help shape the features (by sharing the CNN+LSTM feature extraction network).': 1.0986123085021973, 'Experiments show the benefits of this approach on Atari and Labyrinth problems, with in particular much better data efficiency than A3C.': 1.0986123085021973, 'The paper is well written, ideas are sound, and results pretty convincing, so to me this is a clear acceptance.': 1.0986123085021973, 'At high level I only have few things to say, none being of major concern:': 1.0986123085021973, 'I believe you should say something about the extra computational cost of optimizing these auxiliary tasks.': 1.0986123085021973, 'How much do you lose in terms of training speed?': 1.0986123085021973, 'Which are the most costly components?': 1.0986123085021973, 'If possible, please try to make it clearer in the abstract / intro that the agent is learning different policies for each task.': 1.0986123085021973, 'When I read in the abstract that the agent ""also maximises many other pseudo-reward functions simultaneously by reinforcement learning"", my first understanding was that it learned a single policy to optimize all rewards together, and I realized my mistake only when reaching eq. 1.': 0.45584923028945923, 'The ""feature control"" idea is not validated empirically (the preliminary experiment in Fig.': 1.0986123085021973, '5 is far from convincing as it only seems to help slightly initially).': 1.0986123085021973, 'I like that idea but I am worried by the fact the task is changing during learning, since the extracted features are being modified.': 1.0986123085021973, 'There might be stability / convergence issues at play here.': 1.0986123085021973, 'Since as you mentioned, ""the performance of our agents is still steadily improving"", why not keep them going to see how far they go?': 1.098583698272705, '(at least the best ones)': 1.0986123085021973, ""Why aren't the auxiliary tasks weight parameters (the lambda_*) hyperparameters to optimize?"": 1.0986123085021973, 'Were there any experiments to validate that using 1 was a good choice?': 0.5496370792388916, 'Please mention the fact that auxiliary tasks are not trained with ""true"" Q-Learning since they are trained off-policy with more than one step of empirical rewards (as discussed in the OpenReview comments)': 1.0840190649032593, 'Minor stuff:': 1.0977203845977783, '""Policy gradient algorithms adjust the policy to maximise the expected reward, L_pi = -..."" => that\'s actually a loss to be minimized': 0.730821967124939, 'In eq. 1 lambda_c should be within the sum': 1.0985772609710693, 'Just below eq. 1 r_t^(c) should be r_t+k^(c)': 1.0911798477172852, 'Figure 2 does not seem to be referenced in the text, also Figure 1(d) should be referenced in 3.3': 0.9984322190284729, '""the features discovered in this manner is shared"" => are shared': 1.098352313041687, 'The text around eq. 2 refers to the loss L_PC': 1.09626042842865, 'but that term is not defined and is not (explicitly) in eq. 2': 0.8152164816856384, 'Please explain what ""Clip"" means for dueling networks in the legend of Figure 3': 1.0986123085021973, 'I would have liked to see more ablated versions on Atari, to see in particular if the same patterns of individual contribution as on Labyrinth were observed': 1.065481185913086, 'In the legend of Figure 3 the % mentioned are for Labyrinth, which is not clear from the text.': 0.5616361498832703, 'In 4.1.2: ""Figure 3 (right) shows...""': 1.0986123085021973, '=> it is actually the top left plot of the figure.': 1.076194167137146, 'Also later ""This is shown in Figure 3 Top"" should be Figure 3 Top Right.': 0.4721919000148773, '""Figure 5 shows the learning curves for the top 5 hyperparameter settings on three Labyrinth navigation levels"" => I think it is referring to the left and middle plots of the figure, so only on two levels (the text above might also need fixing)': 1.0963716506958008, 'In 4.2: ""The left side shows the average performance curves of the top 5 agents for all three methods the right half shows...""': 1.0657563209533691, '=> missing a comma or something after ""methods""': 1.0986123085021973, 'Appendix: ""Further details are included in the supplementary materials.""': 0.9180176258087158, '=> where are they?': 1.0986123085021973, 'What is the value of lambda_PC?': 1.0986123085021973, '(=1 I guess?)': 1.0985928773880005, '[Edit] I know some of my questions were already answered in Comments, no need to re-answer them': 0.42737939953804016, 'This work proposes to train RL agents to also perform auxiliary tasks, positing that doing so will help models learn stronger features.': 1.0986121892929077, 'They propose two pseudo-control tasks, control the change in pixel intensity, and control the activation of latent features.': 1.0986123085021973, 'They also propose a supervised regression task, predict immediate reward following a sequence of events.': 1.0986123085021973, 'The latter is learned offline via a skewed sampling of an experience replay buffer in order to balance seeing reward or not to 1/2 chance.': 1.0986123085021973, 'Such agents perform significantly well on discrete-action-continuous-space RL tasks, and reach baseline performance in 10x less iterations.': 1.0984792709350586, 'This work contrasts with traditional ""passive"" unsupervised or model-based learning.': 1.0984817743301392, 'Instead of forcing the model to learn a potentially useless representation of the input, or to learn the possibly impossible (due to partial observability) task-modelling objective, learning to control local and internal features of the environment complements learning the optimal control policy.': 1.0984479188919067, 'To me the approach is novel and proposes a very interesting alternative to unsupervised learning that takes advantage of the ""possibility"" of control that an agent has over the environment.': 1.0985082387924194, 'The proposed tasks are explained at a rather high level, which is convenient to understand intuition, but I think some lower level of detail might be useful.': 1.0986123085021973, 'For example L_PC should be explicitly mentioned, before reaching the appendix.': 1.0986123085021973, 'Otherwise this work is clear and easily understandable by readers familiar with Deep RL.': 1.0986123085021973, 'The methodology is sound, on one hand hand the distribution of best hyperparameters might be different for A3C and UNREAL, but also measuring top-3 ensures that, presuming that the both best hyperparameters for A3C and for UNREAL are within the explored intervals, the per-method best hyperparameters are found.': 1.0925853252410889, 'I think one weakness of the paper (or rather, considering the number of things that can fit in a paper, crucially needed future work) is that there is very little experimental analysis of the effect of the auxiliary tasks appart from their (very strong) effect on performance.': 1.098602533340454, 'In the same vein, pixel/feature control seems to have the most impact, in Labyrinth just A3C+PC beats anything else (except UNREAL), I think it would have been worth looking at this, either in isolation or in more depth, measuring more than just performance on RL tasks.': 1.0488497018814087}"
286,https://openreview.net/forum?id=SJ8BZTjeg,"{'The papers investigates the task of unsupervised learning with deep features via k-means clustering.': 1.0986123085021973, 'The entire pipeline can be decomposed into two steps: (1) unsupervised feature learning based on GAN framework and (2) k-means clustering using learned deep network features.': 1.0986123085021973, 'Following the GAN framework and its extension InfoGAN, the first step is to train a pair of discriminator network and generator network from scratch using min-max objective.': 1.0986123085021973, 'Then, it applies k-means clustering on the top layer features from discriminator network.': 1.0986123085021973, 'For evaluation, the proposed unsupervised feature learning approach is compared against traditional hand-crafted features such as HOG and supervised method on three benchmark datasets.': 1.0986123085021973, 'Normalized Mutual Information (NMI) and Adjusted RAND Index (ARI) have been used as the evaluation metrics for experimental comparison.': 0.9790683388710022, 'Although the proposed method may be potentially useful in practice (if refined further), I find the method lacks novelty, and the experimental results are not significant enough.': 1.098611831665039, 'This paper proposed an unsupervised learning method based on running kmeans on the features learned by a discriminator network in a generative adversarial network setup.': 1.0985108613967896, 'Unsupervised learning methods with GANs is certainly a relevant topic but this paper does not propose anything particularly novel as far as I can tell.': 0.5583086013793945, 'More importantly, the evaluation methods in this paper are extremely lacking.': 1.0975650548934937, 'The authors omit classification results on CIFAR and STL-10 and instead the only quantitative evaluation plot the performance of the clustering algorithm on the features.': 1.0243650674819946, 'Not only are classification results not shown, no comparisons are made to the wealth of related work.': 0.4508836567401886, 'I list just a few highly related techniques below.': 1.0982176065444946, 'Finally, it appear the authors have not train their GANs correctly as the samples in Fig.2 appear to be from a model that has collapsed during training.': 1.0974067449569702, 'In summary, the ideas in this paper are potentially interesting but this paper should not be accepted in its current form due to lack of experimental results and comparisons.': 0.6141765117645264, '(non-exhaustive) list of related work on unsupervised learning (with and without GANs):': 0.5306148529052734, '[1] Springenberg.': 1.0562163591384888, 'Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks, ICLR 2016 (https://arxiv.org/abs/1511.06390)': 0.40549057722091675, '[2] Salimans et al.': 1.057358980178833, 'Improved Techniques for Training GANs.': 0.7030768990516663, 'NIPS 2016 (https://arxiv.org/abs/1606.03498)': 1.0957443714141846, '[3] Dosovitskiy et al.': 1.0968343019485474, 'Discriminative unsupervised feature learning with convolutional neural networks, NIPS 2014 (https://arxiv.org/abs/1406.6909)': 1.098136305809021, 'The paper proposes an approach to unsupervised learning based on generative adversarial networks (GANs) and clustering.': 0.8050171136856079, 'The general topic of unsupervised learning is important, and the proposed approach makes some sense, but experimental evaluation is very weak and does not allow to judge if the proposed method is competitive with existing alternatives.': 1.0741264820098877, 'Therefore the paper cannot be published in its current form.': 1.0986123085021973, 'More detailed remarks (many of these are copies of my pre-review questions the authors have not responded to):': 1.0986106395721436, '1) Realted work overview looks incomplete.': 0.7763171792030334, 'There has been work on combining clustering with deep learning, for example [1] or [2] look very related.': 1.0651881694793701, 'A long list of potentially related papers can be found here: https://amundtveit.com/2016/12/02/deep-learning-for-clustering/ .': 1.0957194566726685, 'From the GAN side, for example [3] looks related.': 1.092721700668335, 'I would like the authors to comment on relation of their approach to existing work, if possible compare with existing approaches, and if not possible - explain why.': 1.068425178527832, '[1] Xie et al., ""Unsupervised Deep Embedding for Clustering Analysis"", ICML 2016 http://jmlr.org/proceedings/papers/v48/xieb16.pdf': 0.9677891135215759, '[2] Yang et al., ""Joint Unsupervised Learning of Deep Representations and Image Clusters"", CVPR 2016 http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Joint_Unsupervised_Learning_CVPR_2016_paper.pdf': 0.6953124403953552, '[3] J.T. Springenberg, ""Unsupervised and semi-supervised learning with categorical generative adversarial networks"", ICLR 2016, https://arxiv.org/pdf/1511.06390v2.pdf': 0.9732961058616638, '2)': 1.098611831665039, 'The authors do not report classification accuracies, which makes it very difficult to compare their results with existing work.': 1.096155047416687, 'Classification accuracies should be reported.': 1.0986123085021973, 'They may not be a perfect measure of feature quality, but reporting them in addition to ARI and NMI would not hurt.': 0.07773871719837189, '3)': 1.0985041856765747, 'The authors have not compared their approach to existing unsupervised feature learning approaches, for example feature learning with k-means (Coates and Ng 2011), sparse coding methods such as Hierarchical Matching Pursuit (Bo et al., 2012 and 2013), Exemplar-CNN (Dosovitskiy et al. 2014)': 1.0526297092437744, '4) Looks like in Figure 2 every ""class"" consists essentially of a single image and its slight variations?': 1.0867114067077637, ""Doesn't this mean GAN training failed?"": 0.4812149703502655, 'Do all your GANs produce samples of this quality?': 1.0985100269317627, '5) Why do you not show results with visual features on STL-10?': 1.0967596769332886, '6) Supervisedly learned filters in Figure 3 looks unusual to me, they are normally not that smooth.': 1.0686582326889038, 'Have you optimized the hyperparameters?': 1.0986067056655884, 'What is the resulting accuracy?': 1.0986113548278809}"
287,https://openreview.net/forum?id=SJAr0QFxe,"{'I think the write-up can be improved.': 1.0986123085021973, 'The results of the paper also might be somewhat misleading.': 1.0983587503433228, 'The behavior for when weights are 0 is not revealing of how the model works in general.': 1.0986123085021973, 'I think the work also underestimates the effect of the nonlinearities on the learning dynamics of the model.': 1.0986123085021973, 'This paper studies the optimization issue of linear ResNet, and shows mathematically that for 2-shortcuts and zero initialization, the Hessian has condition number independent of depth.': 1.0986123085021973, 'I skimmed through the proof but have not checked them carefully.': 0.6866525411605835, 'This result is a nice observation for training deep linear networks.': 1.0978753566741943, 'But I do not think the paper has fully resolved the linear vs nonlinear issue.': 1.0986123085021973, 'Some question:': 1.0986123085021973, '1. Though the revision has added some results using ReLU units, it seems it is only added to the mid positions of the network (sec 5.3), is this how it is typically done in ResNet? Moreover, ReLU is not differentiable at zero point, which does not satisfy the condition you had in Theorem 1. Why not use differentiable activations like sigmoid or tanh?': 0.9749093055725098, '2. From equation (22) in the appendix, it seems for nonlinear activations, the condition number depends on the derivative \\sigma^\\prime at 0. Therefore, if we use tanh which has derivative 1 at zero, the condition number is the same for linear and tanh activations. But this probably is not enough to explain the bit difference in performance or optimization for linear and nonlinear networks, or how the situations evolve after learning the 0 point.': 0.5970691442489624, '3. As for the success of ResNet (or convnets in general) in computer vision, I believe there are more types of nonlinearity such as pooling? Can the result here generalizes to pooling as well?': 0.39207974076271057, 'Minor:': 1.0986123085021973, 'sec 1 last paragraph, low approximation error typically means more powerful model class and better training error, but not necessarily better test error': 1.0986123085021973, 'sec 4.1 what do you mean by ""zero initialization with small random perturbations""?': 1.0986123085021973, 'why not exactly zero initialization, how large is the random perturbation?': 1.0986123085021973, 'ResNet and other architectures that use shortcuts have shown empirical success in several domains and therefore, studying the optimization for such architectures is very valuable.': 1.0986123085021973, 'This paper is an attempt to address some of the properties of networks that use shortcuts.': 1.0986123085021973, 'Some of the experiments in the paper are interesting.': 1.0986123085021973, 'However, there are two main issues with the current paper:': 1.0986123085021973, '1- linear vs non-linear: I think studying linear networks is valuable but we should be careful not to extend the results to networks with non-linear activations without enough evidence.': 1.0986123085021973, 'This is especially true for Hessian as the Hessian of non-linear networks have very large condition number (see the ICLR submission ""Singularity of Hessian in Deep Learning"") even in cases where the optimization is not challenging.': 1.0986123085021973, ""Therefore, I don't agree with the claims in the paper on non-linear networks."": 1.0986123085021973, 'Moreover, one plot on MNIST is not enough to claim that non-linear networks behave similar to linear networks.': 1.0986123085021973, '2- Hessian at zero initial point: The explanation of why we should be interested in Hessain at zero initial point is not acceptable.': 1.0986123085021973, 'The zero initial point is not interesting because it is a very particular point that cannot tell us about the Hessian during optimization.': 1.0986123085021973}"
288,https://openreview.net/forum?id=SJBr9Mcxl,"{'This paper makes three main methodological contributions:': 1.0986123085021973, '- definition of Neural Feature (NF) as the pixel average of the top N images that highly activation a neuron': 1.0986123085021973, '- ranking of neurons based on color selectivity': 1.0986086130142212, '- ranking of neurons based on class selectivity': 1.0986123085021973, 'The main weaknesses of the paper are that none of the methodological contributions are very significant, and no singularly significant result arises from the application of the methods.': 1.0986086130142212, 'However, the main strengths of the paper are its assortment of moderately-sized interesting conclusions about the basic behavior of neural nets.': 1.0986123085021973, 'For example, a few are:': 1.0986123085021973, '- “Indexing on class selectivity neurons we found highly class selective neurons like digital-clock at conv2, cardoon at conv3 and ladybug at conv5, much before the fully connected layers.” As far as I know, this had not been previously reported.': 0.2687109410762787, '- Color selective neurons are found even in higher layers. (25% color selectivity in conv5)': 1.0963307619094849, '- “our main color axis emerge (black-white, blue-yellow, orange-cyan and cyan- magenta). Curiously, these two observations correlate with evidences in the human visual system (Shapley & Hawken (2011)).” Great observation!': 0.5602384805679321, 'Overall, I’d recommend the paper be accepted, because although it’s difficult to predict at this time, there’s a fair chance that one of the “smaller conclusions” would turn out to be important in hindsight a few years hence.': 1.0986117124557495, 'Other small comments:': 1.0986123085021973, '- The cite for “Learning to generate chairs…” is wrong (first two authors combined resulting in a confusing cite)': 1.0986123085021973, '- What exactly is the Color Selectivity Index computing? The Opponent Color Space isn’t well defined and it wasn’t previously familiar to me. Intuitively it seems to be selecting for units that respond to a constant color, but the highest color selectivity NF in Fig 5 i for a unit with two colors, not one. Finally, the very last unit (lowest color selectivity) is almost the same edge pattern, but with white -> black instead of blue -> orange. Why are these considered to be so drastically different? This should probably be more clearly described.': 0.953071117401123, '- For the sake of argument, imagine a mushroom sensitive neuron in conv5 that fires highly for mushrooms of *any* color but not for anything else. If the dataset contains only red-capped mushrooms, would the color selectivity index for this neuron be high or low? If it is high, it’s somewhat misleading because the unit itself actually isn’t color selective; the dataset just happens only to have red mushrooms in it. (It’s a subtle point but worth considering and probably discussing in the paper)': 1.033126950263977, 'The authors analyze trained neural networks by quantifying the selectivity of individual neurons in the network for a variety of specific features, including color and category.': 1.0986087322235107, 'Pros:': 1.0986123085021973, '*': 1.0986123085021973, 'The paper is clearly written and has good figures.': 1.0986123085021973, '* I think they executed their specific stated goal reasonably well technically.': 1.0986123085021973, 'E.g. the various indexes they use seem well-chosen for their purposes.': 1.0986123085021973, 'Cons:': 1.0986123085021973, '* I must admit that I am biased against the whole enterprise of this paper.': 1.0986123085021973, 'I do not think it is well-motivated or provides any useful insight whatever.': 1.0986123085021973, 'What I view their having done is produced, and then summarized anecdotally, a catalog of piecemeal facts about a neural network without any larger reason to think these particular facts are important.': 1.0986123085021973, 'In a way, I feel like this paper suffers from the same problem that plagues a typical line of research in neurophysiology, in which a catalog of selectivity distributions of various neurons for various properties is produced': 1.0986123085021973, 'full stop.': 1.0986123085021973, 'As if that were in and of itself important or useful information.': 1.0986123085021973, 'I do not feel that either the original neural version of that project, or this current model-based virtual electrophysiology, is that useful.': 1.0986123085021973, 'Why should we care about the distribution of color selectivities?': 1.0986123085021973, 'Why does knowing distribution as such constitute ""understanding""?': 1.0986121892929077, ""To my mind it doesn't, at least not directly."": 1.096494197845459, ""Here's what they could have done to make a more useful investigation:"": 1.0985883474349976, '(a) From a neuroscience point of view, they could have compared the properties that they measure in models to the same properties as measured in neurons the real brain.': 0.9975255131721497, 'If they could show that some models are better matches on these properties to the actual neural data than others, that would be a really interesting result.': 1.098373293876648, 'That is is to say, the two isolated catalogs of selectivities (from model neurons and real neurons)  alone seem pretty pointless.': 1.087653636932373, 'But if the correspondence between the two catalogs was made': 1.0986037254333496, 'both in terms of where the model neurons and the real neurons were similar, and (especially importantly) where they were different': 1.0681642293930054, 'that would be the beginning of nontrivial understanding.': 1.0986123085021973, 'Such results would also complement a growing body of literature that attempts to link CNNs to visual brain areas.': 0.41193515062332153, 'Finding good neural data is challenging, but whatever the result, the comparison would be interesting.': 0.5630896091461182, 'and/or': 1.0986013412475586, '(b) From an artificial intelligence point of view, they could have shown that their metrics are *prescriptive* constraints.': 0.9737269878387451, 'That is, suppose they had shown that the specific color and class selectivity indices that they compute, when imposed as a loss-function criterion on an untrained neural network, cause the network to develop useful filters and achieve significantly above-chance performance on the original task the networks were trained on.': 1.0151679515838623, 'This would be a really great result, because it would not only give us a priori reason to care about the specific property metrics they chose, but it would also help contribute to efforts to find unsupervised (or semi-supervised) learning procedures, since the metrics they compute can be estimated from comparatively small numbers of stimuli and/or high-level semantic labels.': 0.4636756181716919, 'To put this in perspective, imagine that they had actually tested the above hypothesis and found it to be false:  that is, that their metrics, when used as loss function constraints, do not improve performance noticeably above chance performance.': 0.5928453803062439, 'What would we then make of this whole investigation?': 0.4732232093811035, ""It would then be reasonable to think that the measured properties were essentially epiphenomenal and didn't contribute at all to the power of neural networks in solving perceptual tasks."": 1.0045757293701172, '(The same could be said about neurophysiology experiments doing the same thing.)': 0.880498468875885, '[': 1.0986123085021973, ""> NB: I've actually tried things just like this myself over the years, and have found exactly this disappointing result."": 0.8024914264678955, 'Specifically,  I\'ve found a number of high-level generic statistical property of DNNs that seem like they might potentially ""interesting"", e.g. because they apparently correlate with complexity or appear to illustrate difference between low, intermediate and high layers of DNNs.': 0.9326270222663879, 'Every single one of these, when imposed as optimization constraints, has basically lead nowhere on the challenging tasks (like ImageNet) that cause the DNNs to be interesting in the first place.': 1.0480393171310425, 'Basically, there is to my mind no evidence at this point that highly-summarized generic statistical distributions of selectivities, like those illustrated here, place any interesting constraints on filter weights at all.': 0.931442379951477, ""Of course, I haven't tried the specific properties the authors highlight in these papers, so maybe there's something important there.]"": 1.0592504739761353, ""I know that both of these asks are pretty hard, but I just don't know what else to say"": 0.8061853051185608, 'this work otherwise seems like a step backwards for what the community ought to be spending its time on.': 1.0986123085021973, 'This paper attempts to understand and visualize what deep nets are representing as one ascends from low levels to high levels of the network.': 1.0986123085021973, 'As has been shown previously, lower levels are more local image feature based, whereas higher levels correspond to abstract properties such as object identity.': 1.0986123085021973, 'In semantic space, we find higher level nodes to be more semantically selective, whereas low level nodes are more diffuse.': 1.0986123085021973, 'This seems like a good attempt to tease apart deep net representations.': 1.0986123085021973, 'Perhaps the most important finding is that color figures prominently into all levels of the network, and that performance on gray scale images is significantly diminished.': 1.0986123085021973, 'The new NF measure proposed here is sensible, but still based on the images shown to the network.': 1.0986123085021973, 'What one really wants to know is what function these nodes are computing - i.e., out of the space of *all* possible images, which most activate a unit?': 1.0986123085021973, 'Of course this is a difficult problem, but it would be nice to see us getting closer to understanding the answer.': 1.0986123085021973, 'The color analysis here I think brings us a bit closer.': 1.0986123085021973, 'The semantic analysis is nice': 1.0986123085021973, ""but I'm not sure what new insight we gain from this."": 1.0986123085021973}"
289,https://openreview.net/forum?id=SJCscQcge,"{'Paper summary:': 1.0984965562820435, 'This work proposes a new algorithm to generate k-adversarial images by modifying a small fraction of the image pixels and without requiring access to the classification network weight.': 0.9368805289268494, 'Review summary:': 1.0985032320022583, 'The topic of adversarial images generation is of both practical and theoretical interest.': 0.4849923253059387, 'This work proposes a new approach to the problem, however the paper suffers from multiple issues.': 1.0844700336456299, 'It is too verbose (spending long time on experiments of limited interest); disorganized (detailed description of the main algorithm in sections 4 and 5, yet a key piece is added in the experimental section 6); and more importantly the resulting experiments are of limited interest to the reader, and the main conclusions are left unclear.': 1.0985405445098877, 'This looks like an interesting line of work that has yet to materialize in a good document, it would need significant re-writing to be in good shape for ICLR.': 1.046463966369629, 'Pros:': 1.087027907371521, '* Interesting topic': 1.0986123085021973, '* Black-box setup is most relevant': 1.0986123085021973, '* Multiple experiments': 1.0986121892929077, '* Shows that with flipping only 1~5% of pixels, adversarial images can be created': 1.098610281944275, 'Cons:': 1.0986123085021973, '* Too long, yet key details are not well addressed': 1.0648118257522583, '* Some of the experiments are of little interest': 0.5006422996520996, '* Main experiments lack key measures or additional baselines': 1.0980983972549438, '* Limited technical novelty': 1.0986123085021973, 'Quality: the method description and experimental setup leave to be desired.': 1.0986123085021973, 'Clarity: the text is verbose, somewhat formal, and mostly clear; but could be improved by being more concise.': 1.0432037115097046, 'Originality: I am not aware of another work doing this exact same type of experiments.': 1.080277919769287, 'However the approach and results are not very surprising.': 1.0172806978225708, 'Significance: the work is incremental, the issues in the experiments limit potential impact of this paper.': 0.9583883285522461, 'Specific comments:': 1.0986123085021973, '* I would suggest to start by making the paper 30%~40% shorter.': 1.0986123085021973, 'Reducing the text length, will force to make the argumentation and descriptions more direct, and select only the important experiments.': 1.0986123085021973, '* Section 4 seems flawed.': 1.0986123085021973, 'If the modified single pixel can have values far outside of the [LB, UB] range; then this test sample is clearly outside of the training distribution; and thus it is not surprising that the classifier misbehaves (this would be true for most classifiers, e.g. decision forests or non-linear SVMs).': 1.058693528175354, 'These results would be interesting only if the modified pixel is clamped to the range [LB, UB].': 0.793014645576477, '*': 1.0986121892929077, '[LB, UB] is never specified, is it ?': 1.0828222036361694, 'How does p = 100, compares to [LB, UB] ?': 1.098406434059143, 'To be of any use, p should be reported in proportion to [LB, UB]': 0.41884300112724304, 'The modification is done after normalization, is this realistic ?': 1.0986123085021973, '* Alg 2, why not clamping to [LB, UB] ?': 1.0986123085021973, '* Section 6, “implementing algorithm LocSearchAdv”, the text is unclear on how p is adjusted; new variables are added.': 1.0986123085021973, 'This is confusion.': 1.0986123085021973, '* Section 6, what happens if p is _not_ adjusted ?': 1.0986123085021973, 'What happens if a simple greedy random search is used (e.g. try 100 times a set of 5 random pixels with value 255) ?': 1.0986123085021973, '* Section 6, PTB is computed over all pixels ?': 1.0986123085021973, 'including the ones not modified ?': 1.0986123085021973, 'why is that ?': 1.0986123085021973, 'Thus LocSearchAdv PTB value is not directly comparable to FGSM, since it intermingles with #PTBPixels (e.g. “in many cases far less average perturbation” claim).': 1.0986123085021973, '* Section 6, there is no discussion on the average number of model evaluations.': 1.0986123085021973, 'This would be equivalent to the number of requests made to a system that one would try to fool.': 1.0986123085021973, 'This number is important to claim the “effectiveness” of such black box attacks.': 1.0986123085021973, 'Right now the text only mentions the upper bound of 750 network evaluations.': 1.0986123085021973, '* How does the number of network evaluations changes when adjusting or not adjusting p during the optimization ?': 1.0986123085021973, '* Top-k is claimed as a main point of the paper, yet only one experiment is provided.': 1.0986123085021973, 'Please develop more, or tune-down the claims.': 1.0986123085021973, '* Why is FGSM not effective for batch normalized networks ?': 1.0986123085021973, 'Has this been reported before ?': 1.0986123085021973, 'Are there other already published techniques that are effective for this scenario ?': 1.0986123085021973, 'Comparing to more methods would be interesting.': 1.0986123085021973, 'If there is little to note from section 4 results, what should be concluded from section 6 ?': 1.0986123085021973, 'That is possible to obtain good results by modifying only few pixels ?': 1.0986123085021973, 'What about selecting the “top N” largest modified pixels from FGSM ?': 1.0986123085021973, 'Would these be enough ?': 1.0986123085021973, 'Please develop more the baselines, and the specific conclusions of interest.': 1.0986123085021973, 'Minor comments:': 1.0986123085021973, '* The is an abuse of footnotes, most of them should be inserted in the main text.': 1.0986123085021973, '* I would suggest to repeat twice or thrice the meaning of the main variables used (e.g. p, r, LB, UB)': 1.0986123085021973, '* Table 1,2,3 should be figures': 1.0986123085021973, '* Last line of first paragraph of section 6 is uninformative.': 1.0986123085021973, 'Very tiny -> small': 1.0986123085021973, 'The paper presents a method for generating adversarial input images for a convolutional neural network given only black box access (ability to obtain outputs for chosen inputs, but no access to the network parameters).': 1.0986123085021973, 'However, the notion of adversarial example is somewhat weakened in this setting: it is k-misclassification (ensuring the true label is not a top-k output), instead of misclassification to any desired target label.': 1.0986123085021973, 'A similar black-box setting is examined in Papernot et al. (2016c).': 1.0986123085021973, 'There, black-box access is used to train a substitute for the network, which is then attacked.': 1.0978680849075317, 'Here, black-box access in instead exploited via local search.': 0.4494091272354126, 'The input is perturbed, the resulting change in output scores is examined, and perturbations that push the scores towards k-misclassification are kept.': 1.0986123085021973, 'A major concern with regard to novelty is that this greedy local search procedure is analogous to gradient descent; a numeric approximation (observe change in output for corresponding change in input) is used instead of backpropagation, since one does not have access to the network parameters.': 1.0986123085021973, 'As such, the greedy local search algorithm itself, to which the paper devotes a large amount of discussion, is not surprising and the paper is fairly incremental in terms of technical novelty.': 1.0986123085021973, 'The authors propose a method to generate adversarial examples w/o relying on knowledge of the network architecture or network gradients.': 1.0986123085021973, 'The idea has some merit, however, as mentioned by one of the reviewers, the field has been studied widely, including black box setups.': 1.0985996723175049, 'My main concern is that the first set of experiments allows images that are not in image space.': 1.0986123085021973, 'The authors acknowledge this fact on page 7 in the first paragraph.': 1.0986123085021973, 'In my opinion, this renders these experiments completely meaningless.': 1.0986123085021973, 'At the very least, the outcome is not surprising to me at all.': 1.0986123085021973, 'The greedy search procedure remedies this issue.': 1.0986123085021973, 'The description of the proposed method is somewhat convoluted.': 1.0986123085021973, 'AFAICT, first a candidate set of pixels is generated by using PERT.': 1.0986123085021973, 'Then the pixels are perturbed using CYCLIC.': 1.0986123085021973, 'It is not clear why this approach results in good/minimal perturbations as the candidate pixels are found using a large ""p"" that can result in images outside the image space.': 1.0986123085021973, 'The choice of this method does not seem to be motivated by the authors.': 1.0986123085021973, 'In conclusion, while the authors to an interesting investigation and propose a method to generate adversarial images from a black-box network, the overall approach and conclusions seem relatively straight forward.': 1.0986123085021973, 'The paper is verbosely written and I feel like the findings could be summarized much more succinctly.': 1.0986123085021973}"
290,https://openreview.net/forum?id=SJDaqqveg,"{'This paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing.': 1.081531286239624, 'In particular, experiments are shown in a synthetic denoising task as well as in machine translation.': 1.0986123085021973, 'I like the idea of the paper, however, the experimental evaluation is not convincing.': 1.0981498956680298, 'Why is the LL numbers in Ranzato et al. 2015 and your paper so different?': 1.0986111164093018, 'Is the metric different?': 1.0986123085021973, 'is it the scheduler?': 1.0986123085021973, 'are the parameters different?': 1.0986123085021973, 'If one extrapolates the numbers, it seems that MIXER will be much better than the proposed approach.': 1.0984922647476196, ""I'd like to see a head-to-head comparison, either by reproducing the same setting or by running the mixer baseline."": 1.0986123085021973, 'The authors should also compare their results to the state-of-the-art.': 1.0986109972000122, 'How good is their machine translation system?': 1.0986123085021973, 'Only comparing to a single baseline and without reproducing the numbers is not sufficient.': 1.0986120700836182, ""While the idea makes sense, the authors needed to use many heuristics to make the model to work, e.g., using a delayed actor, update \\phi' with interpolation, penalize the variance, reducing the value of rare actions, etc."": 1.0986123085021973, 'Furthermore, there is no in depth analysis of how much performance each of these heuristics brings.': 1.0980843305587769, 'It seems that the authors need more work to make the model work without so many heuristics.': 1.0457998514175415, 'The authors also mentioned several optimization difficulties (some of which are non-intuitive),': 0.9018296599388123, '1) why does the critic assign very high value to actions with very low probability according to the actor?': 1.097765326499939, '2) why is a lower square error on Q resulting in much worst performance?': 1.0646847486495972, 'The paper will benefit from a serious re-write.': 0.9609318375587463, 'The technical part is not clearly written.': 1.0986123085021973, 'The manuscript also assumes that the reader knows algorithms such as REINFORCE.': 1.0982754230499268, 'I strongly suggest to include a brief description in the text.': 1.0985987186431885, 'This will help the reader understand how to use the critic within this framework.': 0.938352108001709, 'Also the experimental section will benefit from dividing it by experiment.': 0.4627564549446106, 'Right now is cumbersome to look at the details of each experiment as things are mixed up in the text.': 1.0985355377197266, 'The paper criticizes the REINFORCE algorithm a lot, particularly for its high variance, however the best results in the real setting are achieved with this algorithm (+ the critic).': 1.0972157716751099, 'How do you explain this?': 0.5582041144371033, 'The text is also not consistent with what the results show.': 1.0986123085021973, 'The discussion claims that using the critic on REINFORCE reduces the gap with the actor critic.': 1.0986123085021973, 'However, it is better than the proposed approach.': 1.0986123085021973, ""I'll revise my score if the authors address my questions."": 1.0986123085021973, 'In summary, an interesting idea, however many heuristics are used and the experimental evaluation is not sufficient.': 1.0986123085021973, 'This paper introduces an actor-critic approach for sequence prediction, and shows experiments on spelling correction and machine translation.': 1.0986123085021973, 'While previous works e.g. Ranzato et al. 2015 have used an RL-based approach such as REINFORCE for sequence prediction, the main contribution of this work is the use of actor-critic as a novel approach for how to determine the target of network predictions, given the setting that the network should be trained to generate correctly given outputs already produced by the model and not ground-truth reference outputs.': 1.0986123085021973, 'Specifically, the actor is the main prediction network and the critic is trained to output the value of specific tokens.': 1.0983015298843384, 'The motivations for the approach are well-presented, and while a somewhat natural extension, it is still novel and justified.': 1.0986123085021973, 'There are a number of details that are necessary for successful training, that are discussed well.': 1.0012224912643433, 'While the full Actor-Critic model does not show strong improvements over REINFORCE with critic, the critic-based models still outperform other baselines.': 1.098608136177063, 'It would be nice to include more discussion of the bias-variance tradeoff and future advantages of Actor-Critic (from the pre-review question response) in the paper.': 1.0979273319244385, 'The paper is solid and deserves acceptance': 0.826504647731781, 'The paper presents a nice application of actor-critic method for conditional sequence prediction.': 1.0986049175262451, 'The critic is trained conditional to target sequence output, while the actor is conditional on input sequence.': 1.0986121892929077, 'The paper presents a number of interesting design decisions in order to tackle non-standard RL problem with actor-critic (conditional sequence generation with sequence-level reward function, large action space, reward at final step) and shows encouraging results for applying RL in sequence prediction.': 1.0986123085021973, 'The interaction of actor and critic is an interesting aspect of this paper.': 1.097076177597046, 'Each has different pieces of information (input sequence, target output sequence), and effectively the actor gets target label information only through greedy optimization of the critic.': 1.0986080169677734, 'Letting the critic having access to information only available at train time is interesting and may be applicable to other applications that tie RL with supervised learning.': 1.0986123085021973, 'Pre-review discussion on Q-learning vs actor-critic has been good, and indeed I agree that making the critic having access to structured output label may be quite useful.': 1.0986123085021973, 'The pros include reasonable improvement over prior attempts at using RL to fine-tune sequence models.': 1.0986123085021973, 'One possible con is that the actor-critic is likely more unstable than simpler prior methods, thus requiring a number of tricks to alleviate, and it would be nice to see discussion on stability and hyper-parameter sensitivity.': 1.0986123085021973, 'Another possible con is that this is an application paper, but it explores a non-traditional approach in a widely applicable field.': 1.0986123085021973}"
291,https://openreview.net/forum?id=SJGCiw5gl,"{'Authors propose a strategy for pruning weights with the eventual goal of reducing GFLOP computations.': 1.0986123085021973, 'The pruning strategy is well motivated using the taylor expansion of the neural network function with respect to the feature activations.': 1.0986123085021973, 'The obtained strategy removes feature maps that have both a small activation and a small gradient (eqn 7).': 1.0986117124557495, '(A) Ideally the gradient of the output with respect to the activation functions should be 0 at the optimal, but as a result of stochastic gradient evaluations this would practically never be zero.': 1.0986123085021973, 'Small variance in the gradient across mini-batches indicates that irrespective of input data the specific network parameter is unlikely to change - intuitively these are parameters that are closer to convergence.': 1.0335028171539307, 'Parameters/weights that are close to convergence and also result in a small activation are intuitively good candidates for pruning.': 1.0986123085021973, 'This is essentially what eqn 7 conveys and is likely to be reason why just removing weights that result in small activations is not as good of a pruning strategy (as shown by results in the paper).': 0.40789008140563965, 'There are two kind of differences in weights that are removed by activation v/s taylor expansion:': 1.0986123085021973, '1. Weights with high-activations but very low gradients will be removed by taylor expansion, but not by activation alone.': 0.08039136976003647, '2. Weights with low-activation but high gradients will be removed by activation criterion, but not by taylor expansion.': 1.0454388856887817, 'It will be interesting to analyze which of (1) or (2) contribute more to the differences in weights that are removed by the taylor expansion v/s activation criterion.': 1.0986121892929077, ""Intuitively it seems that weight that satisfy (1) are important because they are converged and contribute significantly to network's activation."": 0.5057222843170166, 'It is possible that a modified criterion - eqn (7) + \\lambda feature activation, (where \\lambda needs to be found by cross-validation) may lead to even better results at the cost of more parameter tuning.': 1.0986121892929077, '(B)': 1.0986123085021973, 'Another interesting comparison is with the with the optimal damage framework - where the first order gradients are assumed to be zero and pruning is performed using the second-order information (also discussed by authors in the appendix).': 1.0985054969787598, 'Critically, only the diagonal of the Hessian is computed.': 1.0979766845703125, 'There is no comparison with optimal damage as authors claim it is memory and computation inefficient.': 1.0984771251678467, 'Back of envelope calculations suggest that this would result only in 50% increase in memory and computation during pruning, but no loss in efficiency during testing.': 1.0926138162612915, ""Therefore from a standpoint of deployment, I don't think this missing comparison is justified."": 1.0827759504318237, '(C) The eventual goal of the authors is to reduce GFLOPs.': 1.0986123085021973, 'Some recent papers have proposed using lower precision computation for this.': 1.0969189405441284, 'A comparison in GFLOPs with lower precision v/s pruning would be a great.': 0.6538316607475281, 'While both these approaches are complementary and it is expected that combining both of them can lead to superior performance than either of the two - it is unclear when we are operating in the low-precision regime how much pruning can be performed.': 1.011466383934021, 'Any analysis on this tradeoff would be great (but not necessary).': 1.0972095727920532, '(D)': 1.0986123085021973, 'On finetuning, authors report results of AlexNet and VGG on two different datasets - Flowers and Birds respectively.': 1.0500961542129517, 'Why is this the case?': 1.0986123085021973, 'It would be great to see the results of both the networks on both the datasets.': 0.5292400121688843, '(E) Authors report there is only a small drop in performance after pruning.': 1.0986123085021973, 'Suppose the network was originally trained with N iterations, and then M finetuning iterations were performed during pruning.': 1.0900193452835083, 'This means that pruned networks were trained for N + M iterations.': 1.093496561050415, 'The correct comparison in accuracies would be if we the original network was also trained for N + M iterations.': 0.8284791111946106, 'In figure 4, does the performance at 100% parameters reports accuracy after N+M iterations or after N iterations?': 1.0986123085021973, 'Overall I think the paper is technically and empirically sound, it proposes a new strategy for pruning:': 1.0986123085021973, '(1) Based on taylor expansion': 1.0986123085021973, '(2) Feature normalization to reduce parameter tuning efforts.': 1.0986123085021973, '(3) Iterative finetuning.': 1.0986123085021973, 'However, I would like to see some comparisons mentioned in my comments above.': 1.0986123085021973, 'If those comparisons are made I would change my ratings to an accept.': 1.0986123085021973, 'This paper presents a novel way of pruning filters from convolutional neural networks with a strong theoretical justification.': 1.0986121892929077, 'The proposed methods is derived from the first order Taylor expansion of the loss change while pruning a particular unit.': 1.09004807472229, 'This leads to simple weighting of the unit activation with its gradient w.r.t.': 1.0986123085021973, 'loss function and performs better than simply using the activation magnitude as the heuristic for pruning.': 1.0984869003295898, 'This intuitively makes sense, as we would like to remove not only the filters with low activation, but also filters where the incorrect activation value would not have small influence on the target loss.': 1.0983246564865112, 'Authors thoroughly investigate multiple baselines, including an oracle which sets an upper bound on the target performance even though it is computationally expensive.': 0.8120563626289368, 'The devised method seems to be quite elegant and authors show that it generalizes well on multiple tasks and is computationally more than feasible as it is easy to combine with traditional fine tuning procedure.': 1.0985082387924194, 'Also, the work clearly shows the trade-offs of increased speed and decreased performance, which is useful for practical applications.': 1.0986123085021973, 'It would be also useful to compare against different baselines, e.g. [1].': 1.0986123085021973, 'However this method seems to be more useful as it does not involve training of a new network (and thus is probably much faster).': 1.0986123085021973, 'Suggestion - maybe it can be extended in the future towards also removing only parts of the filters(e.g.': 1.0986123085021973, 'for the 3D convolution)?': 1.0986123085021973, 'This may be more complicated as it would need to change the implementation of convolution operator, but can lead to further speedup.': 1.0986123085021973, '[1] https://arxiv.org/abs/1503.02531': 1.0986123085021973, 'Authors propose a neural pruning technique starting from trained models using an approximation of change in the cost function and outperform other criteria.': 1.0986123085021973, 'Authors obtain solid speedups while maintaining reasonable accuracy thanks to finetuning after pruning.': 1.0986123085021973, 'Comparisons to existing methods is weak as GFLOPS graphs only show a couple simple baselines and no prior work baselines.': 1.0986123085021973, 'I would be more convinced of the superiority of the approach with such comparison.': 1.0986123085021973}"
292,https://openreview.net/forum?id=SJGPL9Dex,"{'This paper performs theoretical analysis to understand how sparse coding could be accelerated by neural networks.': 1.0986069440841675, 'The neural networks are generated by unfolding the ISTA/FISTA iterations.': 1.0528541803359985, 'Based on the results, the authors proposed a reparametrization approach for the neural network architecture to enforce the factorization property and recovered the original gain of LISTA, which justified the theoretical analysis.': 1.0986121892929077, 'My comments are listed below.': 1.098589539527893, 'It is not clear about the purpose of Section 2.3.2.': 1.098605990409851, 'Adapting the factorization to the input distribution based on (15) would be time consuming because the overhead of solving (15) may not save the total time.': 0.19631469249725342, 'In fact, the approach does not use (15) but back propagation to learn the factorization parameters.': 1.0986123085021973, 'Minor comments:': 1.0978312492370605, 'E(z_k) in (3) and (4) are not defined.': 1.097747564315796, 'E_x in (19) is not defined.': 1.077502727508545, 'Forward referencing (“Equation (20) defines…”) in the paragraph above Theorem 2.2. needs to be corrected.': 0.16544102132320404, 'This work presents an analysis of LISTA, which originally proposes to accelerate sparse coding algorithms with some prior on the structure of the problem.': 1.0986123085021973, 'The authors here propose a solid analysis of the acceleration performance of LISTA, using a specific matrix factorisation of the dictionary.': 1.0986123085021973, 'The analysis is well structured, and provides interesting insights.': 1.0986087322235107, 'It would have been good to tie more closely these insights to specific properties of data or input distributions.': 1.0986123085021973, 'The learned dictionary results in Section 3.3 are not very clear: is the dictionary learned with a sort of alternating minimisation strategy that would include LISTA as sparse coding step?': 1.0986123085021973, 'Or is it only the sparse coding that is studied, with a dictionary that has been learned a priori?': 1.0986117124557495, 'Overall, the paper does not propose a new algorithm and representation, but provides key insights on a well-known and interesting acceleration method on sparse coding.': 1.098611831665039, 'This is quite a nice work.': 1.0986121892929077, ""The title seems however a bit confusing as 'neural sparse coding' is actually rather 'LISTA', or 'neural network acceleration of sparse coding' - basically, it is not immediate to understand what 'neural sparse coding' means..."": 1.0986123085021973, 'This paper proposes a method for neural sparse coding inspired by LISTA (Gregor and LeCun 2010).': 1.0985709428787231, 'A theoretical analysis is presented that attempts to explain the non-asymptotic acceleration property of LISTA (via Theorem 2.2.': 1.0986123085021973, 'and Corollary 2.3).': 0.5000172257423401, 'FacNet is a specialization of LISTA, sharing the same network architecture but with additional constraints on the parameters.': 0.91340172290802, 'In numerical experiments, LISTA outperforms FacNet, up to some optimization errors.': 1.0977773666381836, 'It is not clear what is the advantage of using FacNet instead of LISTA.': 0.8719861507415771, 'Overall, the paper lacks clarity in several parts.': 1.0986123085021973, 'It would be good to state beforehand what the main contribution is.': 1.04666006565094, 'As stated in the clarification question/answer below, this paper would benefit from a more clear explanation about the connection of FacNet with LISTA.': 1.0982985496520996, 'Minor comments/typos:': 0.9209475517272949, 'p. 6: ""memory taps"" -> tapes?': 1.0986123085021973, 'sec 3.2: ""a gap appears has the number of iterations increases"" -> as?': 1.0986123085021973, 'sec. 4: ""numerical experiments of 3"" -> of sec 3': 1.0986123085021973}"
293,https://openreview.net/forum?id=SJIMPr9eg,"{""This paper proposes a boosting based ensemble procedure for residual networks by adopting the Deep Incremental Boosting method that was used for CNN's(Mosca & Magoulas, 2016a)."": 1.0986123085021973, 'At each step t, a new block of layers are added to the network at a position p_t and the weights of all layers are copied to the current network to speed up training.': 1.0986123085021973, 'The method is not sufficiently novel since the steps of Deep Incremental Boosting are slightly adopted.': 0.7914183735847473, 'Instead of adding a layer to the end of the network, this version adds a block of layers to a position p_t (starts at a selected position p_0) and merges layer accordingly hence slightly adopts DIB.': 1.0986123085021973, 'The empirical analysis does not use any data-augmentation.': 1.0965596437454224, 'It is not clear whether the improvements (if there is) of the ensemble disappear after data-augmentation.': 1.0986123085021973, 'Also, one of the main baselines, DIB has no-skip connections therefore this can negatively affect the fair comparison.': 1.0435190200805664, 'The authors argue that they did not involve state of art Res Nets since their analysis focuses on the ensemble approach, however any potential improvement of the ensemble can be compensated with an inherent feature of Res Net variant.': 1.0986119508743286, 'The boosting procedure can be computationally restrictive in case of ImageNet training and Res Net variants may perform much better in that case too.': 0.8077863454818726, 'Therefore the baselines should include the state of art Res Nets and Dense Convolutional networks hence current results are preliminary.': 0.04075636342167854, 'In addition, it is not clear how sensitive the boosting to the selection of injection point.': 0.16209514439105988, 'This paper adopts DIB to Res Nets and provides some empirical analysis however the contribution is not sufficiently novel and the empirical results are not satisfactory for demonstrating that the method is significant.': 1.0853655338287354, 'Pros': 1.0986123085021973, 'provides some preliminary results for boosting of Res Nets': 1.0986121892929077, 'Cons': 1.0986123085021973, 'not sufficiently novel: an incremental approach': 1.0986096858978271, 'empirical analysis is not satisfactory': 0.4332958161830902, 'The authors mention that they are not aiming to have SOTA results.': 1.0986123085021973, 'However, that an ensemble of resnets has lower performance than some of single network results, indicates that further experimentation preferably on larger datasets is necessary.': 1.0986123085021973, 'The literature review could at least mention some existing works such as wide resnets https://arxiv.org/abs/1605.07146 or the ones that use knowledge distillation for ensemble of networks for comparison on cifar.': 1.0986121892929077, 'While the manuscript is well-written and the idea is novel, it needs to be extended with experiments.': 1.0986123085021973, 'The paper under consideration proposes a set of procedures for incrementally expanding a residual network by adding layers via a boosting criterion.': 1.0986123085021973, 'The main barrier to publication is the weak empirical validation.': 0.8975619077682495, 'The tasks considered are quite small scale in 2016 (and MNIST with a convolutional net is basically an uninteresting test by this point).': 1.0986123085021973, ""The paper doesn't compare to the literature, and CIFAR-10 results fail to improve upon rather simple, single-network published baselines (Springenberg et al, 2015 for example, obtains 92% without data augmentation) and I'm pretty sure there's a simple ResNet result somewhere that outshines these too."": 1.0986071825027466, ""The CIFAR100 results are a little bit interesting as they are better than I'm used to seeing (I haven't done a recent literature crawl), and this is unsurprising"": 1.098564624786377, ""you'd expect ensembles to do well when there's a dearth of labeled training data, and here there are only a few hundred per label."": 1.0658469200134277, ""But then it's typical on both CIFAR10 and CIFAR100 to use simple data augmentation schemes which aren't employed here, and these and other forms of regularization are a simpler alternative to a complicated iterative augmentation scheme like this."": 1.0985549688339233, ""It'd be easier to sell this method either as an option for scarce labeled datasets where data augmentation is non-trivial (but then for most image-related applications, random crops and reflections are easy and valid), but that would necessitate different benchmarks, and comparison against simpler methods like said data augmentation, dropout (especially, due to the ensemble interpretation), and so on."": 1.0986123085021973}"
294,https://openreview.net/forum?id=SJJKxrsgl,"{'This paper presents a succinct argument that the principle of optimizing receptive field location and size in a simulated eye that can make saccades with respect to a classification error of images of data whose labels depend on variable-size and variable-location subimages, explains the existence of a foveal area in e.g. the primate retina.': 1.0985252857208252, 'The argument could be improved by using more-realistic image data and drawing more direct correspondence with the number, receptive field sizes and eccentricities of retinal cells in e.g. the macaque, but the authors would then face the challenge of identifying a loss function that is both biologically plausible and supportive of their claim.': 1.0959346294403076, 'The argument could also be improved by commenting on the timescales involved.': 1.0986123085021973, 'Presumably the density of the foveal center depends on the number of of saccades allowed by the inference process, as well as the size of the target sub-images, and also has an impact on the overall classification accuracy.': 1.0441726446151733, 'Why does the classification error rate of dataset 2 remain stubbornly at 24%?': 1.0985908508300781, 'This seems so high that the model may not be working the way we’d like it to.': 1.0961742401123047, 'It seems that the overall argument of the paper pre-supposes that the model can be trained to be a good classifier.': 1.098589539527893, 'If there are other training strategies or other models that work better and differently, then it raises the question of why do our eyes and visual cortex not work more like *those ones* if evolutionary pressures are applying the same pressure as our training objective.': 1.0986014604568481, 'Why does the model with zooming powers out-do the translation-only model on dataset 1 (where all target images are the same size) and tie the translation-only model dataset 2 (where the target images have different sizes, for which the zooming model should be tailor-made?).': 1.0979206562042236, 'Between this strange tie and the high classification rate on Dataset 2, I wonder if maybe one or both models isn’t being trained to its potential, which would undermine the overall claim.': 1.09858238697052, 'Comparing this model to other attention models (e.g. spatial transformer networks, DRAW) would be irrelevant to what I take to be the main point of the paper, but it would address the potential concerns above that training just didn’t go very well, or there was some problem with the model parameterization that could be easily fixed.': 0.7623229622840881, 'The paper presented an extension to the current visual attention model that learns a deformable sampling lattice.': 1.098611831665039, 'Comparing to the fixed sampling lattice from previous works, the proposed method shows different sampling strategy can emerge depending on the visual classification tasks.': 1.0986123085021973, 'The authors empirically demonstrated the learnt sampling lattice outperforms the fixed strategies.': 1.098609209060669, 'More interestingly, when the attention mechanism is constrained  to be translation only, the proposed model learns a sampling lattice resembles the retina found in the primate retina.': 1.0986123085021973, 'Pros:': 1.0965439081192017, '+': 1.0986123085021973, 'The paper is generally well organized and written': 1.0932964086532593, 'The qualitative analysis in the experimental section is very comprehensive.': 1.0986043214797974, 'Cons:': 1.0986123085021973, 'The paper could benefit substantially from additional experiments on different datasets.': 0.9177568554878235, 'It is not clear from the tables the proposed learnt sampling  lattice offer any computation benefit when comparing to  a fixed sampling strategy with zooming capability, e.g. the one used in DRAW model.': 1.0986123085021973, 'Overall, I really like the paper.': 1.0986123085021973, 'I think the experimental section can be improved by additional experiments and more quantitative analysis with other baselines.': 1.0986123085021973, 'Because the current revision of the paper only shows experiments on digit dataset with black background, it is hard to generalize the finding or even to verify the claims in the paper, e.g.  linear relationship': 1.0986123085021973, 'between eccentricity and sampling interval leads to the primate retina, from the results on a single dataset.': 1.0986123085021973, 'This paper proposed a neural attention model which has a learnable and differentiable sampling lattice.': 1.0986123085021973, 'The work is well motivated as few previous work focus on learning the sampling lattice but with a fixed lattice.': 1.0986123085021973, 'This work is quite similar to Spatial Transformer Networks (Jaderberg 2015), but the sampling lattice is learned by the model.': 1.0986123085021973, ""The experiments showed that the model can learn a meaning lattice to the visual search task where the sampling lattice looks similar to human being's."": 1.0986123085021973, 'The main concern of the paper is that experiments are not sufficient.': 1.0986123085021973, 'The paper only reports the results on a modified clustered MNIST dataset.': 1.0986123085021973, 'It would be more interesting if the authors could conduct  the model on real datasets, such as Toronto Face dataset, CUB bird dataset and SVHN.': 1.0986123085021973, 'For example, for the Face dataset, it would be nice if the model can learn to attend different parts of the face for expression recognition, or attend different part of birds for fine-grained classification.': 1.0986123085021973, 'Since the authors replied in the pre-review question that the model can learn meaningful lattice on MSCOCO dataset, I think it would be better to add that results into the paper.': 1.0986123085021973, 'Another drawback of the model is that the paper only compare with different variants of itselves.': 1.0986123085021973, 'I suggest that this paper should compare with  Spatial Transformer Networks, DRAW, etc., on the same dataset to show the advantage of the learned sampling lattice.': 1.0986123085021973}"
295,https://openreview.net/forum?id=SJJN38cge,"{'This work proposes to use basic probability assignment to improve deep transfer learning.': 1.0986123085021973, 'A particular re-weighting scheme inspired by Dempster-Shaffer and exploiting the confusion matrix of the source task is introduced.': 1.0986123085021973, 'The authors also suggest learning the convolutional filters separately to break non-convexity.': 1.0986123085021973, 'The main problem with this paper is the writing.': 1.0986123085021973, 'There are many typos, and the presentation is not clear.': 1.0986123085021973, ""For example, the way the training set for weak classifiers are constructed remains unclear to me despite the author's previous answer."": 1.0986123085021973, 'I do not buy the explanation about the use of both training and validation sets to compute BPA.': 1.0986123085021973, 'Also, I am not convinced non-convexity is a problem here and the author does not provide an ablation study to validate the necessity of separately learning the filters.': 1.0986123085021973, 'One last question is CIFAR has three channels and MNIST only one: How it this handled when pairing the datasets in the second set of experiments?': 1.0986123085021973, 'Overall, I believe the proposed idea of reweighing is interesting, but the work can be globally improved/clarified.': 1.0986123085021973, 'I suggest a reject.': 1.0986123085021973, 'Update: I thank the author for his comments!': 1.0986123085021973, ""At this point, the paper is still not suitable for publication, so I'm leaving the rating untouched."": 1.0986123085021973, 'This paper proposes a transfer learning method addressing optimization complexity and class imbalance.': 1.0986123085021973, 'My main concerns are the following:': 1.0986123085021973, '1. The paper is quite hard to read due to typos, unusual phrasing and loose use of terminology like “distributed”, “transfer learning” (meaning “fine-tuning”), “softmax” (meaning “fully-connected”), “deep learning” (meaning “base neural network”),  etc. I’m still not sure I got all the details of the actual algorithm right.': 1.0986123085021973, '2. The captions to the figures and tables are not very informative – one has to jump back and forth through the paper to understand what the numbers/images mean.': 1.0986123085021973, '3. From what I understand, the authors use “conventional transfer learning” to refer to fine-tuning of the fully-connected layers only (I’m judging by Figure 1). In this case, it’s essential to compare the proposed method with regimes when some of the convolutional layers are also updated. This comparison is not present in the paper.': 1.0986123085021973, 'Comments on the pre-review questions:': 1.0986123085021973, '1. Question 1: If the paper only considers the case |C|==|L|, it would be better to reduce the notation clutter.': 0.9918113946914673, '2. Question 2: It is still not clear what the authors mean by distributed transfer learning. Figure 1 is supposed to highlight the difference from the conventional approach (fine-tuning of the fully-connected layers; by the way, I don’t think, Softmax is a conventional term for fully-connected layers). From the diagram, it follows that the base CNN has the same number of convolutional filters at every layer and, in order to obtain a distributed ensemble, we need to connect (for some reason) filters with the same indices. This does not make a lot of sense to me but I’m probably misinterpreting the figure. Could the authors revise the diagram to make it clearer?': 1.0980933904647827, 'Overall, I think the paper needs significant refinement in order improve the clarity of presentation and thus cannot be accepted as it is now.': 1.0316760540008545, 'This paper proposed to use the BPA criterion for classifier ensembles.': 1.0986123085021973, 'My major concern with the paper is that it attempts to mix quite a few concepts together, and as a result, some of the simple notions becomes a bit hard to understand.': 1.0986089706420898, 'For example:': 1.0986123085021973, '(1) ""Distributed"" in this paper basically means classifier ensembles, and has nothing to do with the distributed training or distributed computation mechanism.': 1.0986123085021973, 'Granted, one can train these individual classifiers in a distributed fashion but this is not the point of the paper.': 1.0986123085021973, '(2) The paper uses ""Transfer learning"" in its narrow sense: it basically means fine-tuning the last layer of a pre-trained classifier.': 1.0986123085021973, 'Aside from the concept mixture of the paper, other comments I have about the paper are:': 1.0986123085021973, '(1) I am not sure how BPA address class inbalance better than simple re-weighting.': 1.0986123085021973, 'Essentially, the BPA criteria is putting equal weights on different classes, regardless of the number of training data points each class has.': 1.0986123085021973, 'This is a very easy thing to address in conventional training: adding a class-specific weight term to each data point with the value being the inverse of the number of data points will do.': 1.0986123085021973, '(2) Algorithm 2 is not presented correctly as it implies that test data is used during training, which is not correct: only training and validation dataset should be used.': 1.0986123085021973, 'I find the paper\'s use of ""train/validation"" and ""test"" quite confusing: why ""train/validation"" is always presented together?': 1.0986123085021973, 'How to properly distinguish between them?': 1.0986123085021973, '(3) If I understand correctly, the paper is proposing to compute the BPA in a batch fashion, i.e. BPA can only be computed when running the model over the full train/validation dataset.': 1.0986123085021973, 'This contradicts with the stochastic gradient descent that are usually used in deep net training - how does BPA deal with that?': 1.0986123085021973, 'I believe that an experimental report on the computation cost and timing is missing.': 1.0986123085021973, 'In general, I find the paper not presented in its clearest form and a number of key definitions ambiguous.': 1.0986123085021973}"
296,https://openreview.net/forum?id=SJMGPrcle,"{'This paper shows that a deep RL approach augmented with auxiliary tasks improves performance on navigation in complex environments.': 1.0986123085021973, 'Specifically, A3C is used for the RL problem, and the agent is simultaneously trained on an unsupervised depth prediction task and a self-supervised loop closure classification task.': 1.0986007452011108, 'While the use of auxiliary tasks to improve training of models including RL agents is not new, the main contribution here is the use of tasks that encourage learning an intrinsic representation of space and movement that enables significant improvements on maze navigation tasks.': 1.0986123085021973, 'The paper is well written, experiments are convincing, and the value of the auxiliary tasks for the problem are clear.': 1.0986123085021973, 'However, the contribution is relatively incremental given previous work on RL for navigation and on auxiliary tasks.': 1.098608374595642, 'The work could become of greater interest provided broader analysis and insights on either optimal combinations of tasks for visual navigation (e.g. the value of other visual / geometry-based tasks), or on auxiliary tasks with RL in general.': 1.0986123085021973, 'As it is, it is a useful demonstration of the benefit of geometry-based auxiliary tasks for navigation, but of relatively narrow interest.': 1.0986123085021973, 'I do like the demonstration that including learning of auxiliary tasks does not interfere with the RL tasks but even helps.': 1.0986123085021973, 'This is also not so surprising with deep networks.': 1.0986123085021973, 'The deep structure of the model allows the model to learn first a good representation of the world on which it can base its solutions for specific goals.': 1.0986123085021973, 'While even early representations do of course depend on the task performance itself, it is clear that there are common first stages in sensory representations like the need for edge detection etc.': 1.0986123085021973, 'Thus, training by additional tasks will at least increase the effective training size.': 1.0986123085021973, 'It is of course unclear how to adjust for this to make a fair comparison, but the paper could have included some more insights such as the change in representation with and without auxiliary training.': 1.0986123085021973, 'I still strongly disagree with the implied definition of supervised or even self-supervised learning.': 1.0985521078109741, 'The definition of unsupervised is learning without external labels.': 1.0986123085021973, 'It does not matter if this comes from a human or for example from an expensive machine that is used to train a network so that a task can be solved later without this expensive machine.': 1.0986039638519287, 'I would call EM a self-supervised method where labels are predicted from the model itself and used to bootstrap parameter learning.': 1.098476767539978, 'In this case you are using externally supplied labels, which is clearly a supervised learning task!': 1.0986123085021973, 'This relatively novel work proposes to augment current RL models by adding self-supervised tasks encouraging better internal representations.': 1.0986123085021973, 'The proposed tasks are depth prediction and loop closure detection.': 1.0770379304885864, 'While these tasks assume a 3D environment as well some position information, such priors are well suited to a large variety of tasks pertaining to navigation and robotics.': 1.0986123085021973, 'Extensive experiments suggest to incorporating such auxiliary tasks increase performance and to a large extent learning speed.': 1.0986123085021973, 'Additional analysis of value functions and internal representations suggest that some structure is being discovered by the model, which would not be without the auxiliary tasks.': 1.0986123085021973, 'While specific to 3D-environment tasks, this work provides additional proof that using input data in addition to sparse external reward signals helps to boost learning speed as well as learning better internal representation.': 1.0986123085021973, 'It is original, clearly presented, and strongly supported by empirical evidence.': 1.0547664165496826, 'One small downside of the experimental method (or maybe just the results shown) is that by picking top-5 runs, it is hard to judge whether such a model is better suited to the particular hyperparameter range that was chosen, or is simply more robust to these hyperparameter settings.': 1.0986123085021973, 'Maybe an analysis of performance as a function of hyperparameters would help confirm the superiority of the approach to the baselines.': 1.0857785940170288, 'My own suspicion is that adding auxiliary tasks would make the model robust to bad hyperparameters.': 1.0986101627349854, 'Another downside is that the authors dismiss navigation literature as ""not RL"".': 1.054378628730774, 'I sympathize with the limit on the number of things that can fit in a paper, but some experimental comparison with such literature may have proven insightful, if just in measuring the quality of the learned representations.': 1.0986123085021973}"
297,https://openreview.net/forum?id=SJNDWNOlg,"{'The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval.': 1.0986123085021973, 'The authors focus on testing various architectural choices, but do not propose or compare to end-to-end learning frameworks.': 1.0986123085021973, 'Technically, the contribution is clear, particularly with the promised clarifications on how multiple scales are handled in the representation.': 1.0986123085021973, 'However, I am still not entirely clear whether there would be a difference in the multi-scale settting for full and cropped queries.': 1.0986123085021973, 'While the paper focuses on comparing different baseline architectures for CNN-based image retrieval, several recent papers have proposed to learn end-to-end representations specific for this task, with very good result (see for instance the recent work by Gordo et al.': 1.0986114740371704, '""End-to-end Learning of Deep Visual Representations for Image Retrieval"").': 1.0986123085021973, 'The authors clarify that their work is orthogonal to papers such as Gordo et al.': 1.0986067056655884, 'as they assess instead the performance of networks pre-trained from image classification.': 1.0920358896255493, 'In fact, they also indicate that image retrieval is more difficult than image classification': 1.0986121892929077, 'this is because it is performed by using features originally trained for classification.': 1.0980466604232788, 'I can partially accept this argument.': 1.0986123085021973, 'However, given the results in recent papers, it is clear than end-to-end training is far superior in practice and it is not clear the analysis developed by the authors in this work would transfer or be useful for that case as well.': 1.0986123085021973, 'Authors investigate how to use pretrained CNNs for retrieval and perform an extensive evaluation of the influence of various parameters.': 1.0985759496688843, 'For detailed comments on everything see the questions I posted earlier.': 0.762805700302124, 'The summary is here:': 1.0986123085021973, ""I don't think we learn much from this paper: we already knew that we should use the last conv layer, we knew we should use PCA with whitening, we knew we should use original size images (authors say Tolias didn't do this as they resized the images, but they did this exactly for the same reason as authors didn't evaluate on Holidays - the images are too big."": 1.0531543493270874, 'So they basically used ""as large as possible"" image sizes, which is what this paper effectively suggests as well), etc.': 1.0986123085021973, 'This paper essentially concatenates methods that people have already used, and performs some more parameter tweaking to achieve the state-of-the-art (while the tweaking is actually performed on the test set of some of the tests).': 1.0986123085021973, ""The setting of the state-of-the-art results is quite misleading as it doesn't really come from the good choice of parameters, but mainly due to the usage of the deeper VGG-19 network."": 1.0986123085021973, ""Furthermore, I don't think it's sufficient to just try one network and claim these are the best practices for using CNNs for instance retrieval - what about ResNet, what about Inception, I don't know how to apply any of these conclusions for those networks, and would these conclusions even hold for them."": 1.0986123085021973, ""Furthermore the parameter tweaking was done on Oxford, I really can't tell what conclusions would we get if we tuned on UKB for example."": 1.0986123085021973, 'So a more appropriate paper title would be ""What are the best parameter values for VGG-19 on Oxford/Paris benchmarks?"" - I don\'t think this is sufficiently novel nor interesting for the community.': 1.0986123085021973, 'This paper explores different strategies for instance-level image retrieval with deep CNNs.': 1.0986123085021973, 'The approach consists of extracting features from a network pre-trained for image classification (e.g. VGG), and post-process them for image retrieval.': 1.0986053943634033, 'In other words, the network is off-the-shelf and solely acts as a feature extractor.': 1.0986123085021973, 'The post-processing strategies are borrowed from traditional retrieval pipelines relying on hand-crafted features (e.g. SIFT + Fisher Vectors), denoted by the authors as ""traditional wisdom"".': 0.8962962627410889, 'Specifically, the authors examine where to extract features in the network (i.e. features are neurons activations of a convolution layer), which type of feature aggregation and normalization performs best, whether resizing images helps, whether combining multiple scales helps, and so on.': 1.0967077016830444, 'While this type of experimental study is reasonable and well motivated, it suffers from a huge problem.': 1.0986123085021973, 'Namely it ""ignores"" 2 major recent works that are in direct contradictions with many claims of the paper ([a] ""End-to-end Learning of Deep Visual Representations for Image Retrieval"" by  Gordo et al.': 1.083879828453064, 'and [b] ""CNN Image Retrieval Learns from BoW:': 1.0986123085021973, 'Unsupervised Fine-Tuning with Hard Examples"" by Radenović et al., both ECCV\'16 papers).': 1.0986123085021973, 'These works have shown that training for retrieval can be achieved with a siamese architectures and have demonstrated outstanding performance.': 1.0986123085021973, 'As a result, many claims and findings of the paper are either outdated, questionable or just wrong.': 1.0986123085021973, 'Here are some of the misleading claims:': 1.0986123085021973, '- ""Features aggregated from these feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in recent years.""': 1.0986123085021973, 'Until [a] (not cited), the state-of-the-art was still largely dominated by sparse invariant features based methods (see last Table in [a]).': 1.0986123085021973, '- ""the proposed method [...] outperforms the state-of-the-art methods on four typical datasets""': 1.0986123085021973, 'That is not true, for the same reasons than above, and also because the state-of-the-art is now dominated by [a] and [b].': 1.0983381271362305, '- ""Also in situations where a large numbers of training samples are not available, instance retrieval using unsupervised method is still preferable and may be the only option."".': 0.9513781070709229, 'This is a questionable opinion.': 1.0984400510787964, 'The method exposed in ""End-to-end Learning of Deep Visual Representations for Image Retrieval"" by Gordo et al. outperforms the state-of-the-art on the UKB dataset (3.84 without QE or DBA) whereas it was trained for landmarks retrieval and not objects, i.e. in a different retrieval context.': 1.0985816717147827, 'This demonstrates that in spite of insufficient training data, training is still possible and beneficial.': 0.7507232427597046, '- Finally, most findings are not even new or surprising (e.g. aggregate several regions in a multi-scale manner was already achieved by Tolias at al, etc.). So the interest of the paper is limited overall.': 0.8958872556686401, 'In addition, there are some problems in the experiments.': 0.9563431739807129, 'For instance, the tuning experiments are only conducted on the Oxford dataset and using a single network (VGG-19), whereas it is not clear whether these conditions are well representative of all datasets and all networks (it is well known that the Oxford dataset behaves very differently than the Holidays dataset, for instance).': 1.0986061096191406, 'In addition, tuning is performed very aggressively, making it look like the authors are tuning on the test set (e.g. see Table 3).': 1.0982624292373657, 'To conclude, the paper is one year too late with respect to recent developments in the state of the art.': 1.0872385501861572}"
298,https://openreview.net/forum?id=SJQNqLFgl,"{'The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems.': 1.0986123085021973, 'Essentially, it reads like a review paper about modern CNN architectures.': 1.0986123085021973, 'It also proposes a few new architectural ideas inspired by these rules.': 1.0986123085021973, 'These are experimentally evaluated on CIFAR-10 and CIFAR-100, but seem to achieve relatively poor performance on these datasets (Table 1), so their merit is unclear to me.': 1.0986123085021973, ""I'm not sure if such a collection of rules extracted from prior work warrants publication as a research paper."": 1.0986123085021973, 'It is not a bad idea to try and summarise some of these observations now that CNNs have been the model of choice for computer vision tasks for a few years, and such a summary could be useful for newcomers.': 1.0986123085021973, 'However, a lot of it seems to boil down to common sense (e.g. #1, #3, #7, #11).': 1.0984684228897095, 'The rest of it might be more suited for an ""introduction to training CNNs"" course / blog post.': 1.0986123085021973, 'It also seems to be a bit skewed towards recent work that was fairly incremental (e.g. a lot of attention is given to the flurry of ResNet variants).': 1.0986080169677734, 'The paper states that ""it is universal in all convolutional neural networks that the activations are downsampled and the number of channels increased from the input to the final layer"", which is wrong.': 1.0986123085021973, 'We already discussed this previously re: my question about design pattern 5, but I think the answer that was given (""the nature of design patterns is that they only apply some of the time"") does not excuse making such sweeping claims.': 1.098611831665039, 'This should probably be removed.': 1.0986123085021973, '""We feel that normalization puts all the layer\'s input samples on more equal footing, which allows backprop to train more effectively"" (section 3.2, 2nd paragraph) is very vague language that has many possible interpretations and should probably be clarified.': 0.5747342705726624, 'It also seems odd to start this sentence with ""we feel"", as this doesn\'t seem like the kind of thing one should have an opinion about.': 1.098415732383728, 'Such claims should be corroborated by experiments and measurements.': 1.0986123085021973, 'There are several other instances of this issue across the paper.': 1.0985019207000732, ""The connection between Taylor series and the proposed Taylor Series Networks seems very tenuous and I don't think the name is appropriate."": 1.0963993072509766, 'The resulting function is not even a polynomial as all the terms represent different functions': 1.0986123085021973, 'f(x) + g(x)**2 + h(x)**3 + ... is not a particularly interesting object, it is just a nonlinear function of x.': 1.036588191986084, 'Overall, the paper reads like a collection of thoughts and ideas that are not very well delineated, and the experimental results are unconvincing.': 1.080426573753357, 'The authors have grouped recent work in convolutional neural network design (specifically with respect to image classification) to identify core design principles guiding the field at large.': 1.0986123085021973, 'The 14 principles they produce (along with associated references) include a number of useful and correct observations that would be an asset to anyone unfamiliar with the field.': 1.0986123085021973, 'The authors explore a number of architectures on CIFAR-10 and CIFAR-100 guided by these principles.': 1.0985567569732666, 'The authors have collected a quality set of references on the subject and grouped them well which is valuable for young researchers.': 1.0719441175460815, 'Clearly the authors explored a many of architectural changes as part of their experiments and publicly available code base is always nice.': 1.0985347032546997, 'Overall the writing seems to jump around a bit and the motivations behind some design principles feel lost in the confusion.': 1.0986104011535645, 'For example, ""Design Pattern 4: Increase Symmetry argues for architectural symmetry as a sign of beauty and quality"" is presented as one of 14 core design principles without any further justification.': 1.0986123085021973, 'Similarly ""Design Pattern 6: Over-train includes any training method where the network is trained on a harder problem than necessary to improve generalization performance of inference"" is presented in the middle of a paragraph with no supporting references or further explanation.': 1.0986123085021973, 'The experimental portion of this paper feels scattered with many different approaches being presented based on subsets of the design principles.': 1.0986123085021973, 'In general, these approaches either are minor modifications of existing networks (different FractalNet pooling strategies) or are novel architectures that do not perform well.': 1.0986087322235107, 'The exception being the Fractal-of-Fractal network which achieves slightly improved accuracy but also introduces many more network parameters (increased capacity) over the original FractalNet.': 1.0986123085021973, 'Preliminary rating:': 1.0986123085021973, 'It is a useful and perhaps noble task to collect and distill research from many sources to find patterns (and perhaps gaps) in the state of a field; however, some of the patterns presented do not seem well developed and include principles that are poorly explained.': 1.0986114740371704, 'Furthermore, the innovative architectures motivated by the design principles either fall short or achieve slightly better accuracy by introducing many more parameters (Fractal-Of-Fractal networks).': 1.0986121892929077, 'For a paper addressing the topic of higher level design trends, I would appreciate additional rigorous experimentation around each principle rather than novel architectures being presented.': 1.098106026649475, 'The authors take on the task of figuring out a set of design patterns for current deep architectures - namely themes that are recurring in the literature.': 1.0986123085021973, 'If one may say so, a distributed representation of deep architectures.': 1.0986123085021973, 'There are two aspects of the paper that I particularly valued: firstly, the excellent review of recent works, which made me realize how many things I have been missing myself.': 1.0986123085021973, 'Secondly, the ""community service"" aspect of helping someone who starts figure out the ""coordinate system"" for deep architectures - this could potentially be more important than introducing yet-another trick of the trade, as most other submissions may do.': 1.0986123085021973, 'However I think this work is still half-done, and even though working on this project is a great idea, the authors do not yet do it properly.': 1.0986123085021973, 'Firstly, I am not too sure how the choice of these 14 patterns was made.': 1.0923594236373901, 'Maxout for instance (pattern 14) is one of the many nonlinearities (PreLU, ReLU, ...)': 1.0986089706420898, 'and I do not see how it stands on the same grounds as something as general as ""3 Strive for simplicity"".': 0.8403323292732239, 'Similarly some of the patterns are as vague as ""Increase symmetry"" and are backed up by statements such as ""we noted a special degree of elegance in the FractalNet"".': 1.098610758781433, 'I do not see how this leads to a design pattern that can be applied to a new architecture - or if it applies to anything other than the FractalNet.': 1.09861159324646, 'Some other patterns are phrased with weird names ""7 Cover the problem space"" - which I guess stands for dataset augmentation; or ""6 over-train"" which is not backed up by a single reference.': 0.9436106085777283, 'Unless the authors relate it to regularization (text preceding ""overtrain""), which then has no connection to the description of ""over-train"" provided by the authors (""training a network on a harder problem to improve generalization"").': 1.0484007596969604, 'If ""harder problem"" means one where one adds an additional term (i.e. the regularizer), the authors are doing harm to the unexperienced reader, confusing ""regularization"" with something that sounds like ""overfitting"" (i.e. the exact opposite).': 1.098595142364502, 'Furthermore, the extensions proposed in Section 4 seem a bit off tune - in particular I could not figure out': 0.9643247723579407, 'how the Taylor Series networks stem from any of the design patterns proposed in the rest of the paper.': 1.0311938524246216, 'whether the text between 4.1 and 4.1.1 is another of the architecture innovations (and if yes, why it is not in the 4.1.2, or 4.1.0)': 0.43522024154663086, 'and, most importantly, how these design patterns would be deployed in practice to think of a new network.': 0.30503395199775696, 'To be more concrete, the authors mention that they propose the ""freeze-drop-path"" variant from ""symmetry considerations"" to ""drop-path"".': 0.2716819941997528, 'Is this an application of the ""increase symmetry"" pattern?': 0.830966591835022, 'How would ""freeze-drop-path"" be more symmetric that ""drop-path""?': 0.7538898587226868, 'Can this be expressed concretely, or is it some intuitive guess?': 1.0723145008087158, 'If the second, it is not really part of applying a pattern, in my understanding.': 0.6164395213127136, 'If the first, this is missing.': 0.4118567109107971, 'What I would have appreciated more (and would like to see in a revised version) would have been a table of ""design patterns"" on one axis, ""Deep network"" on another, and a breakdown of which network applies which design pattern.': 0.5987710356712341, 'A big part of the previous work is also covered in cryptic language - some minimal explanation of what is taking place in the alternative works would be useful.': 0.24701684713363647}"
299,https://openreview.net/forum?id=SJRpRfKxx,"{'The authors formulate a recurrent deep neural network to predict human fixation locations in videos as a mixture of Gaussians.': 1.0985867977142334, 'They train the model using maximum likelihood with actual fixation data.': 1.0984348058700562, 'Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with the C3D features for action recognition.': 1.0980279445648193, 'quality: I am missing a more thorough evaluation of the fixation prediction performance.': 0.5750049948692322, 'The center bias performance in Table 1 differs significantly from the on in Table 2.': 1.0986120700836182, 'All the state-of-the-art models reported in Table 2 have a performance worse than the center bias performance reported in Table 1.': 1.0986123085021973, 'Is there really no other model better than the center bias?': 1.0986123085021973, 'Additionally I am missing details on how central bias and human performance are modelled.': 1.0986123085021973, 'Is human performance cross-validated?': 1.0986123085021973, 'You claim that your ""results are very close to human performance (the difference is only 3.2%).': 1.098611831665039, 'This difference is actually larger than the difference between Central Bias and your model reported in Table 1.': 1.098610758781433, 'Apart from this, it is dangerous to compare AUC performance differences due to e.g. saturation issues.': 1.098610520362854, 'clarity: the explanation for Table 3 is a bit confusing, also it is not clear why the CONV5 and the FC6 models differ in how the saliency map is used.': 0.637803316116333, 'At least one should also evaluate the CONV5 model when multiplying the input with the saliency map to see how much of the difference comes from the different ways to use the saliency map and how much from the different features.': 1.098602294921875, 'Other issues:': 1.0862371921539307, 'You cite Kümmerer et.': 1.0839742422103882, 'al 2015 as a model which ""learns ... indirectly rather than from explicit information of where humans look"", however the their model has been trained on fixation data using maximum-likelihood.': 0.91474449634552, 'Apart from these issues, I think the paper make a very interesting contribution to spatio-temporal fixation prediction.': 1.0986095666885376, 'If the evaluation issues given above are sorted out, I will happily improve my rating.': 1.0986123085021973, 'This work proposes to a spatiotemporal saliency network that is able to mimic human fixation patterns,': 1.0986123085021973, 'thus helping to prune irrelevant information from the video and improve action recognition.': 0.923751175403595, 'The work is interesting and has shown state-of-the-art results on predicting human attention on action videos.': 1.0986120700836182, 'It has also shown promise for helping action clip classification.': 1.0986123085021973, 'The paper would benefit from a discussion on the role of context in attention.': 1.0933961868286133, 'For instance, if context is important, and people give attention to context, why is it not incorporated automatically in your model?': 1.0986053943634033, 'One weak point is the action recognition section, where the comparison between the two (1)(2) and (3) seems unfair.': 1.0986123085021973, 'The attention weighted feature maps in fact reduce the classification performance, and only improve performance when doubling the feature and associated model complexity by concatenating the weighted maps with the original features.': 1.0986123085021973, 'Is there a way to combine the context and attention without concatenation?': 1.0986123085021973, 'The rational for concatenating the features extracted from the original clip,': 1.0986121892929077, ""and the features extracted from the saliency weighted clip seems to contradict the initial hypothesis that `eliminating or down-weighting pixels that are not important' will improve performance."": 0.06680352240800858, 'The authors should also mention the current state-of-the-art results in Table 4, for comparison.': 0.9194634556770325, '# Other comments:': 1.0929889678955078, '#': 0.5570648312568665, 'Abstract': 1.0986123085021973, ""Typo: `mixed with irrelevant ...'"": 1.0446524620056152, ""``Time consistency in videos ... expands the temporal domain from few frames to seconds'' -"": 0.9109338521957397, 'These two points are not clear, probably need a re-write.': 1.0039081573486328, '# Contributions': 0.5347894430160522, ""1) `The model can be trained without having to engineer spatiotemporal features' - you would need to collect training data from humans though.."": 0.9988948106765747, '# Section 3.1': 1.0815927982330322, 'The number of fixation points is controlled to be fixed for each frame - how is this done?': 1.0962153673171997, 'In practice we freeze the layers of the C3D network to values pretrained by Tran etal.': 1.0975632667541504, 'What happens when you allow gradients to flow back to the C3D layers?': 1.0985827445983887, 'Is it not better to allow the features to be best tuned for the final task?': 1.098276138305664, 'The precise way in which the features are concatenated needs to be clarified in section 3.4.': 0.9218793511390686, 'Minor typo:': 1.0986123085021973, ""`we added them trained central bias'"": 1.0986114740371704, 'This paper proposes a new method for estimating visual attention in videos.': 1.0986111164093018, 'The input clip is first processed by a convnet (in particular, C3D) to extract visual features.': 1.0986123085021973, 'The visual features are then passed to LSTM.': 1.0986099243164062, 'The hidden state at each time step in LSTM is used to generate the parameters in a Gaussian mixture model.': 1.0983812808990479, 'Finally, the visual attention map is generated from the Gaussian mixture model.': 1.0986108779907227, 'Overall, the idea in this paper is reasonable and the paper is well written.': 1.0985304117202759, 'RNN/LSTM has been used in lots of vision problem where the outputs are discrete sequences, there has not been much work on using RNN/LSTM for problems where the output is continuous like in this paper.': 1.0983433723449707, 'The experimental results have demonstrated the effectiveness of the proposed approach.': 1.0985809564590454, 'In particular, it outperforms other state-of-the-art on the saliency prediction task on the Hollywood2 datasets.': 1.098610520362854, 'It also shows improvement over baselines (e.g. C3D + SVM) on the action recognition task.': 1.0920721292495728, 'My only ""gripe"" of this paper is that this paper is missing some important baseline comparisons.': 1.0985110998153687, 'In particular, it does not seem to show how the ""recurrent"" part help the overall performance.': 1.0986123085021973, 'Although Table 2 shows RMDN outperforms other state-of-the-art, it might be due to the fact that it uses strong C3D features (while other methods in Table 2 use traditional handcrafted features).': 1.0986120700836182, 'Since saliency prediction is essentially a dense image labeling problem (similar to semantic segmentation).': 1.0986117124557495, 'For dense image labeling, there has been lots of methods proposed in the past two years, e.g. fully convolution neural network (FCN) or deconvnet.': 1.0986123085021973, 'A straightforward baseline is to simply take FCN and apply it on each frame.': 1.0982590913772583, 'If the proposed method still outperforms this baseline, we can know that the ""recurrent"" part really helps.': 1.0986123085021973}"
300,https://openreview.net/forum?id=SJTQLdqlg,"{'This paper proposes a new memory module for large scale life-long and one-shot learning.': 1.0986123085021973, 'The module is general enough that the authors apply the module to several neural network architectures and show improvements in performance.': 1.0985453128814697, 'Using k-nearest neighbors for memory access is not completely new.': 1.0986123085021973, 'This has been recently explored in Rae et al., 2016 and Chandar et al., 2016.': 1.0930768251419067, 'K-nearest neighbors based memory for one-shot learning has also been explored in [R1].': 1.0986123085021973, 'This paper provides experimental evidence that such an approach can be applied to a variety of architectures.': 1.0984967947006226, 'Authors have addressed all my pre-review questions and I am ok with their response.': 1.0986121892929077, 'Are the authors willing to release the source code to reproduce the results?': 0.40563878417015076, 'At least for omniglot experiments and synthetic task experiments?': 1.0985710620880127, 'References:': 0.9684731364250183, '[R1] Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z. Leibo, Jack Rae, Daan Wierstra, Demis Hassabis:': 0.9719635844230652, 'Model-Free Episodic Control.': 1.098586916923523, 'CoRR abs/1606.04460 (2016)': 1.0986123085021973, 'A new memory module based on k-NN is presented.': 1.0984667539596558, 'The paper is very well written and the results are convincing.': 1.0986121892929077, 'Omniglot is a good sanity test and the performance is surprisingly good.': 1.0984665155410767, 'The artificial task shows us that the authors claims hold and highlight the need for better benchmarks in this domain.': 1.0986123085021973, 'And the translation task eventually makes a very strong point on practical usefulness of the proposed model.': 1.0986123085021973, 'I am not a specialist in memory networks so I trust the authors to double-check if all relevant references have been included (another reviewer mentioned associative LSTM).': 1.0986123085021973, 'But besides that I think this is a very nice and useful paper.': 0.8210577964782715, 'I hope the authors will publish their code.': 1.0986123085021973, 'The paper proposes a new memory module to be used as an addition to existing neural network models.': 1.0986120700836182, 'Pros:': 0.8412134051322937, '* Clearly written and original idea.': 1.0986123085021973, '* Useful memory module, shows nice improvements.': 1.0955835580825806, '* Tested on some big tasks.': 1.0568346977233887, 'Cons:': 1.0986123085021973, '*': 0.5583853721618652, 'No comparisons to other memory modules such as associative LSTMs etc.': 0.9639013409614563}"
301,https://openreview.net/forum?id=SJU4ayYgl,"{'The paper develops a simple and reasonable algorithm for graph node prediction/classification.': 1.0986123085021973, 'The formulations are very intuitive and lead to a simple CNN based training and can easily leverage existing GPU speedups.': 1.0986119508743286, 'Experiments are thorough and compare with many reasonable baselines on large and real benchmark datasets.': 1.0986108779907227, 'Although, I am not quite aware of the literature on other methods and there may be similar alternatives as link and node prediction is an old problem.': 1.098401665687561, 'I still think the approach is quite simple and reasonably supported by good evaluations.': 1.0986121892929077, 'The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph in a convolutional NN implementation.': 1.0538482666015625, 'The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset.': 0.8957729339599609, 'The comparison with baselines on different datasets show a clear jump of performance with the proposed method.': 1.0535926818847656, 'The paper is technically fine and clear, the algorithm seems to scale well, and the results on the different datasets compare very favorably with the different baselines.': 0.41836825013160706, 'The algorithm is simple and training seems easy.': 1.0984318256378174, 'Concerning the originality, the proposed algorithm is a simple adaptation of graph convolutional networks (ref Defferrard 2016 in the paper) to a semi-supervised transductive setting.': 0.9984914660453796, 'This is clearly mentioned in the paper, but the authors could better highlight the differences and novelty wrt this reference paper.': 0.4079436659812927, 'Also, there is no comparison with the family of iterative classifiers, which usually compare favorably, both in performance and training time, with regularization based approaches, although they are mostly used in inductive settings.': 0.9621742367744446, 'Below are some references for this family of methods.': 1.0984841585159302, 'The authors mention that more complex filters could be learned by stacking layers but they limit their architecture to one hidden layer.': 0.820837676525116, 'They should comment on the interest of using more layers for graph classification.': 0.523225724697113, 'Some references on iterative classification Qing Lu and Lise Getoor.': 1.0510796308517456, '2003.': 1.0986123085021973, 'Link-based classification.': 1.0986123085021973, 'In ICML, Vol. 3. 496–503.': 1.0586538314819336, 'Gideon S Mann and Andrew McCallum. 2010.': 0.46721991896629333, 'Generalized expectation criteria for semi-supervised learning with weakly labeled data.': 0.8469617366790771, 'The Journal of Machine Learning Research 11 (2010), 955–984.': 0.5070178508758545, 'David Jensen, Jennifer Neville, and Brian Gallagher.': 1.0986099243164062, '2004.': 1.0986123085021973, 'Why collective inference improves relational classification.': 1.0986082553863525, 'In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 593–598.': 0.8488175272941589, 'Joseph J Pfeiffer III, Jennifer Neville, and Paul N Bennett. 2015.': 1.0380053520202637, 'Overcoming Relational Learning Biases': 1.098606824874878, 'to Accurately Predict Preferences in Large Scale Networks.': 1.0347782373428345, 'In Proceedings of the 24th International Conference on World Wide Web.': 1.0986123085021973, 'International World Wide Web Conferences Steering Committee, 853–': 1.0986123085021973, '863.': 1.0985888242721558, 'Stephane Peters, Ludovic Denoyer, and Patrick Gallinari. 2010.': 0.42330753803253174, 'Iterative annotation of multi-relational social networks.': 1.0984244346618652, 'In Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on.': 0.9729087948799133, 'IEEE, 96–103.': 1.0986123085021973, 'This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.': 1.0986123085021973, 'In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.': 1.0986123085021973, 'This model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.': 1.0984758138656616, 'The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.': 1.09861159324646, 'It is surprising that such a simple model works so much better than all the baselines.': 1.0983203649520874, 'Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this.': 1.0986123085021973, 'Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not.': 1.0986123085021973, 'Even though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.': 1.0936133861541748, 'Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things.': 1.0814741849899292, 'So how would the proposed GCN compare against these methods?': 1.0986123085021973, 'Overall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good.': 1.0986123085021973, 'There are a few questions that still remain, but I feel this paper can be accepted.': 1.0986123085021973}"
302,https://openreview.net/forum?id=SJUdkecgx,"{'This paper presents extensions of the KISSME metric learning method.': 1.0986123085021973, 'First, a subspace learning component is integrated, which avoids the need of ad-hoc pre-processing in the case of high-dimensional data.': 1.0986123085021973, 'Second, the approach is integrated with a CNN model that derives the features from an input image before it is entered into the Mahalanobis distance computation.': 1.0986123085021973, 'The related work section would benefit from a disucssion of previous work that also learns low-rank Mahalanobis metrics, such as e.g. [A].': 1.0986108779907227, 'One might argue that the referenced work, and similar ones, solves a non-convex objective and is therefore of less interest.': 1.0986123085021973, 'This argument, however, seems weak at the moment that CNN training is integrated, since this also involves hihgly non-convex optimisation, yet leads to excellent results.': 1.0826054811477661, 'A discussion on this point would be useful to include in the paper.': 1.0986123085021973, 'In the first experiment, the authors compare to related metric learning work.': 1.0986123085021973, 'They  conclude that the proposed approach is superior, but this seems unjustified as the CNNs used to generate the features by the authors is different from the one used in related work.': 0.4069324731826782, 'Thus it seems to me that no conclusions on the quality of the metric learning approach can be drawn from these experiments.': 1.0986123085021973, 'In the second experiment the authors show that a baseline where a factorisation of the PSD matrix M is absorbed in the low-rank projection matrix W leads to worse results than the proposed over-parametrised method.': 1.0968457460403442, 'It would be useful if the authors could provide an analysis of why this might be the case; the current text does not seem to provide an explanation or hyposthesis.': 0.5345368981361389, 'Is the same cost function used by the proposed method and the pairwise and triplet baseline ?': 1.0986063480377197, 'I found the title of the paper slightly misleading, as the contribution of the paper is not to provide an extension to deep feature training: this has been done extensively in the past.': 0.4199100136756897, 'In my understanding, the contribution lies in the proposed (overparameterized) formulation of learning a low-rank Mahalanobis metric for the KISSME objective function.': 0.5663620233535767, 'Certainly, the extension to deep feature training is interesting, but relatively straightforward as compared to the main contribution.': 1.0986123085021973, 'The title could reflect this more accurately.': 1.0986123085021973, 'The main motivation of the paper to base itself on KISSME is scalability, see introduction.': 1.0986123085021973, 'However, for the case where a CNN is trained it seems that it is likely to be in a large-scale data and iterative training regime, and it is not obvious that KISSME would be more efficient than other metric learning objectives based on a pairwise or triplet loss.': 1.0474339723587036, 'Therefore, comparison with other metric learning  objective functions in both performance and run time would be very useful to get a complete picture of which methods are most effective under what conditions.': 1.0954076051712036, '[A] Guillaumin, M.; Mensink, T.; Verbeek, J. & Schmid, C. Face recognition from caption-based supervision IJCV, 2012, 96, 64-82': 0.4749132990837097, 'KISSME was a promising metric learning approach that got lost in the deep learning flood.': 1.0986119508743286, 'This is a shame since it had very good results at the time, but it was hindered by the quirks of the PCA pre-processing stage.': 1.0862177610397339, 'In a nutshell, this paper reimagines this approach using deep convnets, gets good results on modern benchmark datasets and does not require meticulous preprocessing tricks.': 1.098273754119873, 'It therefore deserves to be published, since it rescues a promising approach from obscurity and does a competent job of illustrating its potential in a modern context, with all the necessary discussion of related work on old (LMNN, MMC, PCCA) and new (deep, triplet-loss based) metric learning approaches.': 1.0873522758483887, 'The paper modifies the KISSME algorithm in two ways: by incorporating a dimensionality reduction step and by integrating it in a loss function for learning deep networks.': 1.0911380052566528, 'Experiments show that the latter solution is more stable than standard approaches to metric learning with deep nets.': 1.09769868850708, 'The paper contribution could be good, but the paper could use some polish.': 0.17603465914726257, ""After carefully reading the authors' response to my original questions, I have to say that I am still confused about several details."": 1.0985904932022095, 'Lemma 1 states that ""log(delta) identifies a Mahalanobis metric ... as M = Proj(...)"".': 1.0986121892929077, 'I still think that the wording of the Lemma is imprecise; the authors should probably simply say that log(delta) results in an approximation of a certain metric, up to a constant term.': 0.49326956272125244, 'The word ""identifies"" definitely does not mean ""approximates"", which, since the word ""approximation"" is contained in the proof of the Lemma, is what is happening here.': 1.0155223608016968, 'The authors have promised to fix this issue in the final version by changing the wording to ""log(\\delta) determines a Mahalanobis metric"", but ""determines"" means that log(\\delta) is a metric, whereas it seems to only *approximate* a metric.': 0.5712711215019226, 'Note also that the proof contains the sentence ""the Mahalanobis distance should approximate"".': 1.0985194444656372, 'It is quite odd to find the verb ""should"" in a mathematical proof.': 1.0986123085021973, 'Ultimately, we are not sure what is being proved since the statement of the lemma is imprecise.': 1.0986123085021973, 'Following up on this discussion, about the relationships between Eq. 4 and 6, the authors comment that ""While, the form of Eq.6 may suggest that it is an approximation to Eq.4, as discussed above, it is indeed the form of metric obtained in the latent space"".': 1.02292001247406, 'I agree that, taken in isolation, this is a valid definition of some metric, but then, at the very least, the paper links this up with the rest of the discussion in a very confusing manner.': 1.0986050367355347, 'Note that Eq. 6 is obtained ""using lemma 1"", which contains the world ""approximation"" in the proof.': 1.0986121892929077, 'While none of these issues necessarily invalidates the *method* and the *results* of the paper, they make the *premises* a little tenuous and confusing.': 1.098597764968872, 'A second contribution of the paper is to incorporate KISSME in metric learning using deep networks and pairwise loss.': 1.0986123085021973, 'Here the authors clarify satisfactorily the difference between standard metric learning in deep networks and their approach in their answers.': 1.0986123085021973, 'I would recommend putting the comparison with the baselines upfront to make this message clearer to the reader (e.g. by moving Section 4.2 before Section 4.1).': 1.0986123085021973, 'Eq. (5) and (6) are supposed to ""reflect better"" Eq.': 1.0986123085021973, '(1) than Eq.': 1.0986123085021973, '(4).': 1.0986123085021973, 'In fact, Eq. (6) is a regularised version of Eq.': 1.0986123085021973, '(4) (by the low-dimensional bottleneck), so it can only be a worse approximation of Eq.': 1.0986123085021973, '(1).': 1.0986123085021973, 'Probably the authors mean that the regularisation can make Eq.': 1.0986123085021973, '(6) preferable to Eq.': 1.0986123085021973, '(4) on the test data.': 1.0986123085021973, 'I would also like to understand better how the method is integrated in deep learning.': 1.0864604711532593, ""As noted by the authors,  with CNNs one can simply learn a projection matrix L as the last FC layer and define the metric to be L'L. Learning can use the usual Siamese scheme with pairwise or triplet losses."": 1.0629069805145264, 'I am rather confused how the proposed algorithm relates to this approach according (Section 3).': 1.0934171676635742, 'My understanding is that one first fixes L and trains the rest of the network optimising the pairwise loss Eq.': 1.0688804388046265, '(14).': 1.0986123085021973, 'Once this is done, one updates L as the square root of the matrix M obtained by the modified KISSME algorithm.': 0.45902690291404724, 'Is that correct?': 1.0962313413619995, 'If so, the authors should compare to the obvious baseline of optimising L directly as part of back-propagation while training the network, as this is a much more direct approach.': 0.7063350677490234, 'I am also confused about the dimensionality reduction role of matrix W. In Eq.': 1.098582148551941, '(5) W^T is used to take the data down from dimension D to dimension d.': 0.566909670829773, 'However, when applied to a CNN, the dimensionality reduction is assumed to be incorporated in the network itself, such that the dimension of the CNN representation is d from the outset.': 0.6878538131713867, 'Hence, in this case the modified KISSME algorithm does not need to perform any dimensionality reduction.': 0.58823561668396, 'Is that so?': 0.41699323058128357, 'Can the authors give us an intuition of what their algorithm brings on top of standard Siamese learning in this case?': 1.0910426378250122, 'Experiments:': 1.0981848239898682, 'Comparison with state of the art on the Car dataset is not very meaningful as every method appear to use a different underlying deep net.': 1.098573923110962, 'The paper uses GoogLeNet, which is significantly better than VGG-M used by Liu et al (2016).': 0.5273098945617676, 'Hence it is not surprising that the authors obtain a better performance.': 1.085453748703003, 'I am more interested in understanding the comparison with the KISSME baseline.': 1.0985749959945679, 'How is this baseline applied to this data?': 1.0983521938323975, 'Is there any fine tuning of the CNN at all?': 1.0986123085021973, 'If not, this may be enough to explain the difference with the proposed JDR-KISSME.': 1.0986123085021973, 'There is one last experiment on CIFAR after conclusion and references.': 1.0986123085021973, 'Should we consider that as well?': 1.0986123085021973}"
303,https://openreview.net/forum?id=SJZAb5cel,"{'The authors propose a transfer learning approach applied to a number of NLP tasks; the set of tasks appear to have an order in terms of complexity (from easy syntactic tasks to somewhat harder semantic tasks).': 1.0986123085021973, 'Novelty: the way the authors propose to do transfer learning is by plugging models corresponding to each task, in a way that respects the known hierarchy (in terms of NLP ""complexity"") of those tasks.': 0.992419421672821, 'In that respect, the overall architecture looks more like a cascaded architecture than a transfer learning one.': 1.0986123085021973, 'There are some existing literature in the area (first two Google results found: https://arxiv.org/pdf/1512.04412v1.pdf, (computer vision) and https://www.aclweb.org/anthology/P/P16/P16-1147.pdf (NLP)).': 1.0983675718307495, 'In addition to the architecture, the authors propose a regularization technique they call ""successive regularization"".': 1.0986123085021973, 'Experiments:': 1.0986123085021973, 'The authors performed a number of experimental analysis to clarify what parts of their architecture are important, which is very valuable;': 1.0986095666885376, 'The information ""transferred"" from one task to the next one is represented both using a smooth label embedding and the hidden representation of the previous task.': 1.0986123085021973, 'At this point there is no analysis of which one is actually important, or if they are redundant (update: the authors mentioned they would add something there).': 1.0986123085021973, 'Also, it is likely one would have tried first to feed label scores from one task to the next one, instead of using the trick of the label embedding': 1.0986123085021973, 'it is unclear what the latter is actually bringing.': 0.3803906738758087, 'The successive regularization does not appear to be important in Table 8; a variance analysis would help to conclude.': 1.0986123085021973, 'this work investigates a joint learning setup where tasks are stacked based on their complexity.': 1.0986123085021973, 'to this end, experimental evaluation is done on pos tagging, chunking, dependency parsing, semantic relatedness, and textual entailment.': 1.0967718362808228, 'the end-to-end model improves over models trained solely on target tasks.': 1.076067566871643, 'although the hypothesis of this work is an important one, the experimental evaluation lacks thoroughness:': 1.0976314544677734, 'first, a very simple multi-task learning baseline': 1.0441663265228271, '[1] should be implemented where there is no hierarchy of tasks to test the hypothesis of the tasks should be ordered in terms of complexity.': 0.4933376908302307, 'second, since the test set of chunking is included in training data of dependency parsing, the results related to chunking with JMT_all are not informative.': 1.097273349761963, 'third, since the model does not guarantee well-formed dependency trees, thus, results in table 4 are not fair.': 1.093763828277588, 'minor issue:': 1.0986123085021973, 'chunking is not a word-level task although the annotation is word-level.': 1.0986123085021973, 'chunking is a structured prediction task where we would like to learn a structured annotation over a sequence [2].': 1.0986123085021973, '[1] http://ronan.collobert.com/pub/matos/2011_nlp_jmlr.pdf': 1.0986123085021973, '[2] http://www.cs.cmu.edu/~nasmith/LSP/': 1.0986123085021973, 'The paper introduce a way to train joint models for many NLP tasks.': 1.0986123085021973, 'Traditionally, we treat these tasks as “pipeline” — the later tasks will depending on the output of the previous tasks.': 1.0986123085021973, 'Here, the authors propose a neural approach which includes all the tasks in one single model.': 1.0986123085021973, 'The higher level tasks takes (1) the predictions from the lower level tasks and (2) the hidden representations of the lower level tasks.': 1.0986123085021973, 'Also proposed in this paper, is the successive regularization.': 1.0986123085021973, 'Intuitively, this means that, when training the high level tasks, we don’t want to change the model in the lower levels by too much so that the lower level tasks can keep a reasonable accuracy of prediction.': 1.0986123085021973, 'On the modeling side, I think the proposed model is very similar comparing to (Zhang and Weiss, ACL 2016) and SPINN (Bowman et al, 2016) in a even simpler way.': 1.0986123085021973, 'The number of the experiments are good.': 1.0986123085021973, 'But I am not sure I am convinced by the numbers in Table 1 since the patterns are not very clear there — sometimes, the performance of the higher level tasks even goes down when training with more tasks (sometimes it does go up, but also not very significant and stable).': 1.0986123085021973, 'The dependency scores, although I don’t think this is a serious problem, comparing the UAS/LAS when the output is not guaranteed to be a well-formed tree isn’t strictly speaking fair.': 1.0986123085021973, 'I admit that the successive regularization make sense intuitively and is a very interesting direction to try.': 1.0986123085021973, 'However, without a careful study of the training schema of such model, the current results on successive regularization do not convince me that it should be the right thing to do in such models (the current results are not strong enough to show that).': 1.0986123085021973, 'The training methods need to be explored here including things as iteratively train on different tasks, and the relationship between the number of training iterations of a task and it’s training set size (and loss on this task etc).': 1.0986123085021973}"
304,https://openreview.net/forum?id=SJ_QCYqle,"{'This paper applies convnet-based object detection techniques to detection of weather events from 3D climate data, additionally exploring the effect of using an unsupervised autoencoder-style objective term.': 1.0980861186981201, 'Pros:': 1.0986123085021973, 'The application of object detection techniques to extreme weather event detection problem is unique, to my knowledge.': 1.0986123085021973, 'The paper is well-written and describes the method well, including a survey of the related work.': 1.0986123085021973, 'The best model makes use of 3D convolutions and unsupervised learning, both of which are relatively unexplored in the detection literature.': 1.0967645645141602, 'Both of these aspects are validated and shown to produce at least small performance improvements over a 2D and/or purely supervised approach.': 1.073019027709961, 'Cons:': 1.0986123085021973, 'The benefits of the 3D convolutional architecture and unsupervised learning end up being a little underwhelming, with 52.92% mAP for the 3D+semi-sup result vs. 51.42% mAP for the 2D+sup result.': 1.097965121269226, 'It’s a bit strange that 3D+sup and 2D+semi-sup are each worse than the 2D+sup base result;  I’d expect each aspect to give a slight improvement over the base result, given that using both together gives the best results': 1.098127841949463, 'perhaps there was not a thorough enough hyperparameter search for these cases.': 1.0986123085021973, 'The paper does acknowledge this and provide potential explanations in Sec. 4.3, however.': 1.0986123085021973, 'As other reviewers pointed out, the use of the 0.1 IoU criterion for true positives is very loose relative to the standard 0.5 criterion.': 0.5916250348091125, 'On the other hand, if the results visualized in Figure 3 are typical, a 0.1 overlap criterion could be reasonable for this domain as the detector does seem to localize events well enough that the system could be used to expedite human review of the climate images for extreme events.': 1.098436713218689, 'Still, it would be useful to also report results at higher overlap thresholds.': 1.0986123085021973, 'Minor: eq 6 should (probably) be the squared L2 norm (i.e. the sum of squares) rather than the L2 norm itself.': 0.7949836850166321, 'Minor: table 4': 1.0986123085021973, 'shouldn’t the semi-supervised models have more parameters than the corresponding supervised ones due to the decoder layers?': 1.0985963344573975, 'Overall, this paper is well-written and applies some interesting underutilized techniques to a relatively unique domain.': 1.0985537767410278, ""The results aren't striking, but the model is ablated appropriately and shown to be beneficial."": 1.0985914468765259, 'For a final version, it would be nice to see results at higher overlap thresholds.': 1.0986121892929077, '[EDIT: The thoughtful author responses addressed my major concerns.': 0.6764578223228455, ""The github links for data and code will be really helpful for reproducing results (I haven't looked carefully, but this is great)."": 1.098569631576538, 'The revision addressed many issues, including the additional results.': 0.6799764037132263, 'As such I am upgrading my rating from a 5 to a 6 and recommend acceptance of the paper.]': 1.0986069440841675, 'The paper proposes to apply deep nets to perform detection and localization of extreme weather events in simulated weather data.': 1.0983726978302002, 'The problem is related to object detection in computer vision in that the input is a 2D “image” (multichannel spatial weather data) or 3D “video” (temporal version of the data) and the output is a bounding box (spatial-temporal localization of a weather event) and class label (weather event type).': 1.0981022119522095, 'It differs from standard object detection in that the input has multiple heterogenous channels and labeled data is scarce.': 1.0985093116760254, 'A simple but quite reasonable deep net is proposed for the task based on similar approaches in computer vision.': 1.0985866785049438, 'While proposal based systems are most popular in vision currently (in particular Faster-RCNN) the proposed approach is simple and a fine starting point.': 1.0886123180389404, 'There is little innovation on the part of the detection system, but as noted, it is a valid application of ideas from computer vision to the task at hand.': 1.0137856006622314, 'The authors propose both a supervised approach (only ground truth bounding box location/label is used) and a semi-supervised approach that additionally incorporates the reconstruction loss as a regularization.': 1.0986123085021973, 'In all cases the losses are fairly standard and again, reasonable.': 0.4697650372982025, 'The only confusing bit is that the “semi-supervised” loss actually has all the labels used for the “supervised” loss and additionally incorporates the reconstruction loss.': 1.0986123085021973, 'Hence, the “semi-supervised” loss is actually stronger, which makes the terminology a bit confusing.': 1.0986123085021973, 'The paper is easy to follow, but notation is sloppy.': 1.0986123085021973, 'For example, above equation 5 it states that “the loss is a weighted combination of reconstruction error and bounding box regression loss”; actually it’s a combination of the supervised and unsupervised loss (Lsup and Lunsup), and Lrec is not defined (although I assume Lrec=Lunsup).': 1.0986123085021973, 'The paper is fairly non-technical, but nevertheless these minor issues should be fixed.': 1.0986123085021973, '(E.g., see also reference to “figure 4 and 4”).': 1.0986123085021973, 'The biggest concern w the paper though is experimental results.': 1.0986123085021973, 'Only a single figure and table of results are shown (figure 4 and table 4).': 1.0986123085021973, 'The metrics are not defined (what is mean average recall?).': 0.9426624178886414, 'Only 2D versus 3D version of the model are shown, and supervised and semi-supervised.': 1.0951621532440186, 'Moreover, numbers seem a bit all over the place, without consistent patterns (e.g., why is 2D supervised better than the seemingly much strong 3D semi-supervised?).': 0.24937230348587036, 'One of the things that is unclear is how many events are actually in the training/testing data, and more importantly, how good are these results in absolute terms?': 0.809504508972168, 'Regardless, the experiments are fairly sparse and ablation studies and more discussion lacking.': 1.0986104011535645, 'It is also unclear if future researchers will be able to reproduce the experimental setting (a commitment to open-source the data or a way to reproduce the experiments would be critical for future authors).': 0.957332193851471, 'Minor nit: the authors use both a classification loss and an “objectness” loss. I’ve never seen both used together like this (normally objectness is used in two-stage object proposal systems where in the first stage class-agnostic proposals are given and in the second stage these are cropped and a class-specific classifier is applied). I strongly suspect removing the objectness loss would not impact results since the classification loss should provide strictly stronger supervisory signal. Regardless, this is a fairly non-standard choice and should be justified (experimentally).': 0.749150812625885, 'Overall this is a borderline paper.': 1.098610758781433, 'I do believe that it is valuable to apply computer vision techniques to a domain that I’ve see little work on in our community.': 1.0986123085021973, 'That being said, I have no expertise on this type of data': 1.0986123085021973, 'it’s possible this deep learning techniques are now routinely used in the climate science literature (I suspect not, though).': 1.0986123085021973, 'Overall, there is little novelty on the algorithmic side in this paper (the equations in section 3 are commonly used in the cv literature).': 1.0986123085021973, 'The use of reconstruction loss to improve results in the data-sparse setting is interesting, but the experimental results are inconclusive.': 1.0986123085021973, 'The experimental validation is generally insufficient.': 1.0986123085021973, 'Reproducibility for future research is difficult unless the data is open-sourced.': 1.0986123085021973, 'Overall, I think this paper is a good start, and with improved experiments and more careful writing I think could eventually make for a decent paper.': 1.0986123085021973, 'This work presents a novel 3D CNN architecture for climate event detection that combines an unsupervised auto-encoder reconstruction loss with YOLO like bounding box prediction.': 1.0986123085021973, 'The approach is trained and evaluated on a large-scale, simulated climate dataset labeled by a costly heuristic approach called TECA.': 1.0986123085021973, 'For the most part, the paper is nicely written (minor comments below) and addresses an important and well motivated problem.': 1.0986123085021973, 'The authors provide sufficient model details to allow reproduction (although public code would be preferred).': 1.0986123085021973, 'I find the experiments a bit unconvincing (see below) but appreciate the attention to model capacity (via number of parameter) when comparing the 2D and 3D model variants.': 1.0986123085021973, 'I am concerned that the evaluation may be insufficient to assess the effectiveness of this method.': 1.0986123085021973, 'An IoU threshold of 0.1 allows for many rather poor detections to count as true positives.': 1.0986123085021973, 'If the goal of this work is to count the number of such instances this is defensible, but for localization this seems overly loose.': 1.0986123085021973, 'Furthermore, the 3D CNN architecture (which is one of the the core novelties of this work compared to past efforts) does not seem capable of producing variable sized boxes (as noted in the last paragraph of page 7), which I imagine results in poorer performance at higher IoU thresholds (as many of the weather events appear small).': 0.6889342069625854, 'The experiments also feel inconclusive about the effect of temporal modeling and semi-supervision.': 0.8616589903831482, 'The temporal component does not seem to matter in the supervised settings (2D 51.45 mAP - 3D 51.00 mAP) but improves somewhat in the semi-supervised case (2D 51.11 mAP - 3D 52.92 mAP).': 0.6873374581336975, 'Whereas the additional unlabeled data seems to hurt in the 2D case but improve results for the 3D model.': 0.6138395071029663, 'Could the authors provide confidence intervals for these numbers?': 0.941459059715271, 'I would like to see further discussion of these trends especially with respect to the effect of the loss weights (alpha, beta, and gamma).': 0.5914416313171387, 'I also note that it is not clear if both the 2D and 3D models were trained for equivalent time periods (seems like no from last paragraph of page 7).': 1.0681830644607544, 'Could a plot of training and validation accuracy for each model be presented for comparison?': 0.503800094127655, 'Finally, is there any baseline approach the authors could report or compare too?': 1.0986123085021973, 'Without one, it is difficult to evaluate the performance of the approach with respect to the difficulty of the problem.': 1.0986123085021973, 'Preliminary Rating:': 1.0986123085021973, 'I think this is an interesting paper that is well motivated but feel the experiments as presented do not seem adequate to support any conclusive trends.': 1.0986123085021973, 'I would like to see the mAP trends across a wider range of IoU values and further discussion of training procedure, loss weight settings, and reasons for lack of bounding box variability in the 3D model (as stated above).': 1.0986120700836182, 'Clarification:': 1.0986123085021973, 'In the paper you say ""While climate models are run on a 3D grid, with the vertical dimension corresponding to 30 levels; we only consider surface quantities (i.e. 2D data) in this study.""': 1.0986037254333496, 'Could you elaborate on what the surface quantities correspond to?': 1.0986123085021973, 'Is it the highest cloud level?': 1.0986123085021973, 'Minor notes:': 1.0986123085021973, 'Please provide years for Prabhat et al. references rather than a and b.': 1.0986123085021973, 'Footnote in 4.2 could be inline text with similar space.': 1.0986123085021973, '4.3 second paragraph the word table is not capitalized like elsewhere.': 1.0986123085021973, '4.3 4th paragraph the word section is not capitalized like elsewhere.': 1.0986123085021973, 'Edit: I appreciate the authors responding to my questions but still feel the relatively poor localization performance at stricter IoU thresholds fails to justify the complexity of the approach.': 1.0986123085021973, 'I encourage the authors to continue pursuing this line of research.': 1.0986123085021973}"
305,https://openreview.net/forum?id=SJc1hL5ee,"{'The paper presents a few tricks to compress a wide and shallow text classification model based on n-gram features.': 1.0986123085021973, 'These tricks include (1) using (optimized) product quantization to compress embedding weights (2) pruning some of the vocabulary elements (3) hashing to reduce the storage of the vocabulary (this is a minor component of the paper).': 1.0986123085021973, 'The paper focuses on models with very large vocabularies and shows a reduction in the size of the models at a relatively minor reduction of the accuracy.': 1.0986123085021973, 'The problem of compressing neural models is important and interesting.': 1.0986123085021973, 'The methods section of the paper is well written with good high level comments and references.': 1.0986123085021973, 'However, the machine learning contributions of the paper are marginal to me.': 1.0986123085021973, 'The experiments are not too convincing mainly focusing on benchmarks that are not commonly used.': 1.0986123085021973, 'The implications of the paper on the state-of-the-art RNN text classification models is unclear.': 1.0986123085021973, 'The use of (optimized) product quantization for approximating inner product is not particularly novel.': 1.0986123085021973, 'Previous work also considered doing this.': 1.0986123085021973, 'Most of the reduction in the model sizes comes from pruning vocabulary elements.': 1.0986123085021973, 'The method proposed for pruning vocabulary elements is simply based on the assumption that embeddings with larger L2 norm are more important.': 1.0986123085021973, 'A coverage heuristic is taken into account too.': 1.0926545858383179, 'From a machine learning point of view, the proper baseline to solve this problem is to have a set of (relaxed) binary coefficients for each embedding vector and learn the coefficients jointly with the weights.': 1.0986123085021973, 'An L1 regularizer on the coefficients can be used to encourage sparsity.': 1.098611831665039, 'From a practical point of view, I believe an important baseline is missing: what if one simply uses fewer vocabulary elements (e.g based on subword units - see https://arxiv.org/pdf/1508.07909.pdf) and retrain a smaller models?': 1.0986104011535645, 'Given the lack of novelty and the missing baselines, I believe the paper in its current form is not ready for publication at ICLR.': 1.0986123085021973, 'More comments:': 0.6366602182388306, 'The title does not make it clear that the paper focuses on wide and shallow text classification models.': 1.0986088514328003, 'Please revise the title.': 1.0985947847366333, 'The paper cites an ArXiv manuscript by Carreira-Perpinan and Alizadeh (2016) several times, which has the same title as the submitted paper.': 1.0975738763809204, 'Please make the paper self-contained and include any supplementary material in the appendix.': 1.085190773010254, 'In Fig 2 does the square mark PQ or OPQ?': 1.098611831665039, 'The paper does not distinguish OPQ and PQ properly at multiple places especially in the experiments.': 0.8751009702682495, 'The paper argues the wide and shallow models are the state of the art in small datasets.': 0.6913473606109619, 'Is this really correct?': 1.098603367805481, 'What about transfer learning?': 1.0986123085021973, 'The paper proposes a series of tricks for compressing fast (linear) text classification models.': 1.098571538925171, 'The paper is clearly written, and the results are quite strong.': 1.0986123085021973, 'The main compression is achieved via product quantization, a technique which has been explored in other applications within the neural network model compression literature.': 1.0986123085021973, 'In addition to the Gong et al. work which was cited, it would be worth mentioning Quantized Convolutional Neural Networks for Mobile Devices (CVPR 2016, https://arxiv.org/pdf/1512.06473v3.pdf), which similarly incorporates fine tuning to mitigate losses due to quantization error.': 1.0986123085021973, ""As such, one criticism of the paper is that it is a more-or-less straightforward application of techniques that have already been shown to be effective elsewhere in the model compression literature, and so isn't particularly surprising or deep from a technical perspective."": 1.0986120700836182, 'However, this is as far as I am aware the first work applying these techniques to text classification, and the results are strong enough that I think it will be of interest to those working on models for text-based tasks.': 1.0986123085021973, 'This paper describes how to approximate the FastText approach such that its memory footprint is reduced by several orders of magnitude, while preserving its classification accuracy.': 1.0986123085021973, 'The original FastText approach was based on a linear classifier on top of bag-of-words embeddings.': 1.0986123085021973, 'This type of method is extremely fast to train and test, but the model size can be quite large.': 1.0986123085021973, 'This paper focuses on approximating the original approach with lossy compression techniques.': 1.0986123085021973, 'Namely, the embeddings and classifier matrices A and B are compressed with Product Quantization, and an aggressive dictionary pruning is carried out.': 1.0986123085021973, 'Experiments on various datasets (either with small or large number of classes) are conducted to tune the parameters and demonstrate the effectiveness of the approach.': 1.0986123085021973, 'With a negligible loss in classification accuracy, an important reduction in term of model size (memory footprint) can be achieved, in the order of 100~1000 folds compared to the original size.': 1.0986123085021973, 'The paper is well written overall.': 1.0986123085021973, 'The goal is clearly defined and well carried out, as well as the experiments.': 1.0986123085021973, 'Different options for compressing the model data are evaluated and compared (e.g. PQ vs LSH), which is also interesting.': 1.0986024141311646, 'Nevertheless the paper does not propose by itself any novel idea for text classification.': 1.0984982252120972, 'It just focuses on adapting existing lossy compression techniques, which is not necessarily a problem.': 1.0986123085021973, 'Specifically, it introduces:': 1.0986123085021973, '- a straightforward variant of PQ for unnormalized vectors,': 1.098609209060669, '- dictionary pruning is cast as a set covering problem (which is NP-hard), but a greedy approach is shown to yield excellent results nonetheless,': 1.0985748767852783, '- hashing tricks and bloom filter are simply borrowed from previous papers.': 1.0960804224014282, 'These techniques are quite generic and could as well be used in other works.': 1.0985336303710938, 'Here are some minor problems with the paper:': 1.0985991954803467, '- it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes).': 1.0634323358535767, '- some parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB.': 1.0986123085021973, 'Overall this looks like a solid work, but with potentially limited impact research-wise.': 1.0986123085021973}"
306,https://openreview.net/forum?id=SJg498clg,"{'The authors introduce a semi-supervised method for neural networks, inspired from label propagation.': 1.0986123085021973, 'The method appears to be exactly the same than the one proposed in (Weston et al, 2008)': 1.0985207557678223, '(the authors cite the 2012 paper).': 0.4745049476623535, 'The optimized objective function in eq (4) is exactly the same than eq (9) in (Weston et al, 2008).': 1.0986002683639526, 'As possible novelty, the authors propose to use the adjacency matrix as input to the neural network, when there are no other features, and show success on the BlogCatalog dataset.': 1.0986123085021973, 'Experiments on text classification use neighbors according to word2vec average embedding to build the adjacency matrix.': 1.0986123085021973, 'Top reported accuracies are not convincing compared to (Zhang et al, 2015) reported performance.': 1.0986123085021973, 'Last experiment is on semantic intent classification, which a custom dataset; neighbors are also found according to a word2vec metric.': 1.0986123085021973, 'In summary, the paper propose few applications to the original (Weston et al, 2008) paper.': 1.0986120700836182, 'It rebrands the algorithm under a new name, and does not bring any scientific novelty, and the experimental section lacks existing baselines to be convincing.': 1.0986123085021973, 'This paper proposes the Neural Graph Machine that adds in graph regularization on neural network hidden representations to improve network learning and take the graph structure into account.': 1.0986123085021973, 'The proposed model, however, is almost identical to that of Weston et al. 2012.': 1.0986111164093018, 'As the authors have clarified in the answers to the questions, there are a few new things that previous work did not do:': 1.0986123085021973, '1. they showed that graph augmented training for a range of different types of networks, including FF, CNN, RNNs etc. and works on a range of problems.': 0.5886921882629395, '2. graphs help to train better networks, e.g. 3 layer CNN with graphs does as well as than 9 layer CNNs': 0.460087388753891, '3. graph augmented training works on a variety of different kinds of graphs.': 1.0574349164962769, 'However, all these points mentioned above seems to simply be different applications of the graph augmented training idea, and observations made during the applications.': 1.0986123085021973, 'I think it is therefore not proper to call the proposed model a novel model with a new name Neural Graph Machine, but rather making it clear in the paper that this is an empirical study of the model proposed by Weston et al. 2012 to different problems would be more acceptable.': 1.0986123085021973, 'The paper proposes a model that aims at learning to label nodes of graph in a semi-supervised setting.': 1.0986123085021973, 'The idea of the model is based on the use of the graph structure to regularize the representations learned at the node levels.': 1.0986121892929077, 'Experimental results are provided on different tasks': 1.0986123085021973, ""The underlying idea of this paper (graph regularization) has been already explored in different papers – e.g 'Learning latent representations of nodes for classifying in heterogeneous social networks'"": 1.0986123085021973, '[Jacob et al. 2014],': 1.0986121892929077, '[Weston et al 2012] where a real graph structure is used instead of a built one.': 0.3113911747932434, ""The experiments lack of strong comparisons with other graph models (e.g Iterative Classification, 'Learning from labeled and unlabeled data on a directed graph', ...)."": 1.0986123085021973, 'So the novelty of the paper and the experimental protocol are not strong enough to accpet the paper.': 1.0960789918899536, 'Pros:': 1.0909439325332642, '* Learning over graph is an important topic': 1.098611831665039, 'Cons:': 1.0985435247421265, '*': 1.0986123085021973, 'Many existing approaches have already exploited the same types of ideas, resulting in very close models': 1.096670150756836, '* Lack of comparison w.r.t existing models': 0.42191392183303833}"
307,https://openreview.net/forum?id=SJgWQPcxl,"{'The paper proposed conditional biGAN and its extension to multi-view biGAN.': 1.0986123085021973, 'The main idea of conditional biGAN is to matching the latent variable distributions of two encoders, each of which are conditioned on the observation (\\tilde{x}) and the output (y), respectively, in addition to standard biGAN formulation.': 1.0986123085021973, 'The description on MV-GAN require more revision.': 1.0781605243682861, 'Specifically, the definition on aggregating model, \\Phi, mapping v and a variable s should be clarified.': 1.0986123085021973, ""Looking at Equation (8), I can't find a term that constrains the output domain of function v to be the same as a data domain."": 1.098611831665039, 'Experimental results are not convincing.': 1.0986123085021973, 'In most generation results, the observation is not very well preserved.': 1.0986082553863525, 'For example, in Figure 6 the second row, background changes significantly from the observation.': 1.0984373092651367, 'We also observe such behavior in digit generation example.': 0.44908571243286133, ""Preserving attributes like gender is interesting but doesn't seem to be a strong indication that the model learn to correlate observation and the output through latent variable."": 1.0594627857208252, 'This paper builds in the bidirectional GAN (BiGAN) to obtain an extension which can handle multiple views of data.': 1.0978821516036987, 'To this end, the authors extend the BiGAN for multiple view aggregation.': 1.0296393632888794, 'This is an easy task and just requires the introduction of the additional distribution and accompanying discriminator.': 1.0986123085021973, 'The main challenging and novel part is in regularizing the model to avoid instabilities.': 1.0986123085021973, 'The authors propose a novel KL divergence based constraint.': 1.0986123085021973, 'As mentioned above, the approach builds quite heavily on previous ones but it has enough novel elements, in particular the constraint for regularization.': 1.0986123085021973, 'This constraint is a reasonable assumption an in practice seems to work well.': 1.0986123085021973, 'One downside is that it comes with a parameter \\lambda which controls its strength and which is not obvious how to find efficiently.': 1.0986123085021973, ""For example, for the MNIST data it takes a very small value, 10^-5 and for the CELEBA it's 10^-3."": 1.0986123085021973, ""It could be that the results are not very sensitive to this value, but there's no discussion concerning this aspect."": 1.0986123085021973, 'Apart from the regularization term, the rest of the model construction is well motivated, I agree with the authors that a multi-view approach employing GANs is an interesting topic to consider.': 1.0986123085021973, 'Presentation is in general good although at parts readability is hindered.': 1.0986123085021973, 'I feel that the notation is unnecessarily complicated, and some parts of the text too.': 1.0986123085021973, ""Furthermore, it wasn't immediately obvious to me what is considered as \\tilde{x} and what is y in the experiments."": 1.0986123085021973, 'Additionally, most of the discussion on the well-studied area of multi-view learning (intro and sec. 6) omits reference to important prior work which is not based on neural networks.': 1.0868866443634033, ""Indeed, some of the issues mentioned as common in today's methods (discrete outputs only, no density estimation...) do not actually exist in many non-neural network approaches."": 1.0986123085021973, 'There are too many works to suggest including in the discussion, but I guess the most relevant ones are in the field of probabilistic and non-linear multi-view learning, which is also what MV-BiGAN is doing.': 1.0913355350494385, 'The experiments presented in the paper are nice illustrations but unfortunately insufficient.': 0.7091042995452881, ""Firstly, they only cover the task of generating (small) images and correspond to non-real world settings (I'd actually consider all of the experiments as toy experiments)."": 1.0915663242340088, 'Furthermore, the most difficult of these experiments (sec. 5.3) is quite unconvincing.': 1.0944212675094604, 'If Fig. 6 shows some of the best results that can be achieved, then this is rather disappointing.': 1.0672463178634644, 'Important details are also missing: what is exactly the attribute vector used?': 1.071893572807312, 'How many instances exist?': 0.9868810772895813, 'The two other experiments on 5.1 and 5.2 are well executed.': 0.4249892830848694, ""It's important that the authors show a comparison with \\lambda=0."": 1.0474493503570557, ""However, beyond showing the validity of the KL term, one can't conclude much about the overall merit of the method as a multi-view learning approach, given the above experiments."": 1.0809459686279297, 'Overall this was a nice paper to read, but seems somehow incomplete.': 0.6167869567871094, 'This paper presents extensions of bidirectional generative adversarial networks to the conditional setting and multi-view setting.': 1.0986123085021973, 'The methods are well-motivated, the mathematical derivations appear to be correct, and the presentation is clear enough to me.': 1.098610758781433, 'I would suggest this paper to be accepted.': 1.0986098051071167, 'However, I find it somewhat limited to only present results for generation tasks.': 1.0985275506973267, 'I think a main advantage of using bi-GAN (rather than the standard GAN) is the additional inference model that can learn useful features.': 1.0986123085021973, 'I am curious about how good the features are for some other supervised (or semi-supervised) learning tasks and what have they really learned.': 1.0986123085021973, 'I also find it interesting that the counterpart of these models under the VAE framework have also been proposed': 1.0985846519470215, 'Kihyuk Sohn and Honglak Lee and Xinchen Yan.': 1.0986123085021973, 'Learning Structured Output Representation using Deep Conditional Generative Models. NIPS 2015.': 1.0986123085021973, '(*** contitional VAE ***)': 1.0986123085021973, 'Weiran Wang, Xinchen Yan, Honglak Lee, and Karen Livescu.': 1.0986123085021973, 'Deep Variational Canonical Correlation Analysis.': 1.0986123085021973, 'In submission to ICLR 2017.': 1.0986123085021973, '(*** sort of multi-view VAE ***)': 1.0986123085021973, 'It would be nice to have discussions and comparisons in future work.': 1.0986123085021973}"
308,https://openreview.net/forum?id=SJiFvr9el,"{'I find the general direction of the work is promising but, in my opinion, the paper has three main drawback.': 1.0986123085021973, 'While the motivation and overall idea seem very reasonable, the derivation is not convincing mathematically.': 1.0986123085021973, 'The experiments are limited and the presentation needs significant improvement.': 1.0986123085021973, 'The writing and wording are in general poorly structured to the point that it is sometimes difficult to follow the proposed ideas.': 1.0986123085021973, 'The overall organization needs improvement and the connection between sections is not properly established.': 1.0986123085021973, 'The paper could be significantly improved by simply re-writing it.': 1.0986123085021973, ""I'm not fully convinced by the motivation for the proposed non-linearity (|c|^2), as described on page 5."": 1.0986123085021973, 'The authors argue that  (Waldspurger, 2016) suggests that higher order nonlinearities might be  beneficial for sparsity.': 1.0986123085021973, ""But unless I'm missing something, that work seems  to suggest that in the general case higher order nonlinearities can be neglected."": 1.0986123085021973, 'Could you please comment on this?': 1.0986123085021973, 'On the other hand, adding a second order term to the descriptor seems': 1.0986123085021973, 'an interesting direction, as long as stability to small variations is preserved (which should be shown experimentally)': 1.0986123085021973, 'The experimental section is rather limited.': 1.0986123085021973, 'The paper would be stronger with a thorough numerical evaluation.': 1.0986123085021973, 'The presented results, in my opinion, do not show convincingly a clear advantage of the proposed method over a standard implementation of the scattering transform.': 1.0986123085021973, 'In order to show the merits of the proposed approach, it would be really helpful to directly compare running times and compression rates.': 1.0986123085021973, 'Questions:': 1.0986123085021973, 'Can you show empirically that the proposed higher order nonlinearity': 1.0986123085021973, 'produces sparser representations than the complex modulus?': 1.0986123085021973, 'Other minor issues:': 1.0986123085021973, 'The proof of Section 2.1, should be preceded by a clear statement in the form of a proposition': 1.0986123085021973, '""Hadamart"" -> Hadamard': 1.0986123085021973, '""Valid set"" -> Validation set': 1.0986123085021973, '""nonzeros coefficients"" -> nonzero coefficients': 1.0986123085021973, 'Figure 3 is difficult to understand.': 1.0986123085021973, 'Please provide more details.': 1.0986123085021973, ""Figure 5 is supposed to show a comparison to a standard implementation of the Scattering network, but it doesn't seem to be such comparison in that figure."": 1.0985629558563232, 'Please explain.': 1.0986123085021973, 'Please verify the references.': 1.0986123085021973, 'The first reference states ""MALLAT"".': 1.0986123085021973, 'This work proposes 3 improvements to scattering networks: (1) a non-linearity that allows Fourier-domain computation, (2) compact-supported (in the Fourier domain) representations, and (3) computing additional variance features.': 1.0986123085021973, 'The technical contributions seem worthwhile, since #1 and #2 may result in better speed, while #3 may improve accuracy.': 1.0985815525054932, 'Unfortunately, they are poorly described and evaluated.': 1.098557710647583, 'If the writing was clear and the evaluation more broad, I would have recommended acceptance since the ideas have merit.': 1.0986123085021973, 'One of the biggest faults of the presentation is that many sentences are overly long and full of unnecessary obfuscating language, e.g. the last paragraph of Section 1 (though unfortunately this permeates the whole paper).': 1.0986123085021973, 'Likewise, most equations are made unnecessarily complicated.': 1.0986108779907227, 'For example, Eq. 5 does not need 4 lines and so many indexes, but just 2:': 1.0986123085021973, 'X_0 = x': 1.0986123085021973, 'X_l = |X_{l-1} *': 1.0986106395721436, 'Psi_l|': 1.0986123085021973, 'with the |.| operator being element-wise.': 1.0981473922729492, 'Most of the hyperparameter dependencies and indexes are not necessary, as well as the repetition of iterations.': 1.0986062288284302, 'The same reasoning can be applied to most Equations 5 to 13.': 1.0986123085021973, 'The argument of cardinality (Eq. 14) does not really help prove that variance is more informative.': 0.9419577121734619, 'In fact, we could just as easily write that the cardinality of S concatenated with any (!)': 1.093970775604248, 'other quantity is >= the cardinality of S. Another argument from machine learning theory would be better.': 0.5198549628257751, 'The authors should strive to make the arguments in the paper less hyperbolic and better substantiated.': 1.0986014604568481, 'The claims about finding invariants of any input (Abstract) and fundamental structures (last paragraph of Section 1.2.1) are not really backed up by any math.': 1.0986071825027466, 'How can we have any guarantees about singling out, for example, semantically relevant representations?': 1.0963718891143799, 'The learning procedures in machine learning give at least some guarantees, while here the feature building seems a bit more heuristic.': 1.0956372022628784, 'This does not take away from the main idea, but this part needs to be better researched.': 1.027829885482788, 'Overview: This work seems very promising, but I believe it should be compared with more baselines, and more precisely described and explained, from a signal processing point of view.': 1.0986121892929077, 'Pros:': 1.0986104011535645, 'New descriptor': 1.0915619134902954, 'Fast implementation': 1.0986121892929077, 'Cons:': 1.0986117124557495, 'a) Lack of rigor': 1.0986123085021973, 'b) Too long accordingly to the content': 0.4057938754558563, 'c)': 1.0985944271087646, 'The computational gain of the algorithm is not clear': 1.0986123085021973, 'd)': 1.010376214981079, 'The work is not compared with its most obvious baseline: a scattering transform': 1.0986123085021973, 'I will detail each cons.': 1.0986123085021973, 'a) Section 1:': 1.0986123085021973, 'The author  motivates the use of scattering transform because it defines a contraction of the space that relies on geometric features.': 1.0985509157180786, '"" The nonlinearity used in the scattering network is the complex modulus which is piecewise linear.""': 1.098611831665039, 'A real modulus is piecewise linear.': 1.0227446556091309, 'A complex modulus has a shape of bell when interpreting C as R^2.': 1.0986123085021973, 'Could you clarify?': 1.0976736545562744, '\\Omega is not introduced.': 1.0986123085021973, 'Could you give a precise reference (page+paper) of this claim: “Higher order nonlinearity refers to |x|^2 instead of |x| as it is usually done in the scattering network.” ?': 1.0986114740371704, 'Section 2:': 1.0986123085021973, 'The motivation of the non-linearity is not clear.': 1.0986123085021973, 'First, this non-linearity might potentially increase a lot the variance of your architecture since it depends on higher moments(up to 4).': 1.0986123085021973, 'I think a fair analysis would be to compute numerically the normalized variance (e.g. divided by the averaged l^2 norm), as a sanity check.': 1.0986123085021973, 'Besides, one should prove that the energy is decreasing.': 1.0986123085021973, 'It is not possible to argue that this architecture is similar to a scattering transform which has precise mathematical foundations and those results are required, since the setting is different.': 1.0986123085021973, 'Permutation is not a relevant variability.': 1.0986123085021973, 'The notion of sparsity during the whole paper sometimes refers to the number of 0 value, either the l^1 norm.': 1.0986114740371704, 'Mathematically, a small value, even 10^-1000 is still a non 0 value.': 1.0986123085021973, 'Did you compute the graph of the figure 4 on the bird dataset?': 1.0935375690460205, 'You might use a ratio instead for clarity.': 1.0966546535491943, 'The wavelet that is defined is not a morlet wavelet ( https://en.wikipedia.org/wiki/Morlet_wavelet ).': 1.0629749298095703, 'It is close to be a gabor wavelet, and actually it has not a 0 averaging.': 0.8786806464195251, 'The measure of the ""sparsity of the filters"" is extremely unclear, is it the ratio between the support of the filter and its size?': 1.0255523920059204, 'A good criteria might be for instance to understand the amount of the energy that has been neglected.': 1.0112637281417847, 'Besides, a filter with compact support has a bad localisation property.': 1.076543927192688, 'However, this topic is not  reached in the paper.': 1.0955489873886108, 'For instance, Cauchy wavelets are not used in many applications.(However in mathematical proofs, they often are)': 0.7469184398651123, 'In Subsection 3.4 you write that V^(l)(x)=Sum_{j>l} S^(j)(x), but also that you do compute only the 2 first order coefficients because they can be neglected.': 0.47628360986709595, 'Besides, you specifically write that adding the variance coefficients improve the representation, whereas they can be obtained as linear combination of S.': 0.9193069338798523, 'You claim you apply only one FFT, whereas you apply several FFTs.': 1.0610723495483398, 'b) From ""One of the great...""': 1.0986123085021973, 'to ""iof the': 1.0986123085021973, 'input.""': 1.0920826196670532, 'section 3.1, the text is not clear.': 0.5706830620765686, 'The motivation is that a convolution in space is slower that performing a convolution in the Fourier domain.': 1.0986123085021973, 'This whole paragraph can be summarised in few sentences.': 1.0986123085021973, 'The section 3.4 is long and corresponds to implementation details.': 1.0986123085021973, 'Maybe it could be removed.': 1.0986123085021973, 'The table 2 seems to indicate that the generation of the filters is one of the bottleneck of your software.': 1.0985931158065796, 'Is this really true?': 1.0986123085021973, 'One of the main claim of the paper is that sparse filters and staying in Fourier domain speed up the computations.': 1.0986123085021973, 'Let us compare the computation of the first order scattering coefficients at scale j with this setting.': 1.0986114740371704, 'One has to compare the complexity to compute sum |x*psi_j| and sum |x*psi_j|^2': 0.785377025604248, 'A downsampling is always performed with wavelets, yet it bears approximation.': 1.0667951107025146, 'In a Fourier resolution implementation, one adjust the degree of approximation and speed of computation with the over sampling parameters.': 1.0974962711334229, 'Assume a FFT of size N costs C*N*log N, then,': 0.9479368925094604, 'Computing \\hat x costs in both case C*N*log N.': 1.098598599433899, 'ST: Then, the signal is multiplied with the fourier transform of the filter, which has a cost of N. In a fourier multi resolution implementation, one periodises the signal by a factor j, such that its size is N/2^j.': 1.0985172986984253, 'Then, the FFT has cost C*N/2^j*log(N/2^j), and modulus has cost say N.': 1.097893238067627, 'Then, one applied the averaging that has complexity N/2^j.': 1.0986123085021973, 'Here: The signal is multiplied with the fourier transport of the filter that has a support of N/2^j.': 1.0976907014846802, 'Then, you convolve it with itself, that has thanks to the padding and the FFT a cost of C*N/2^(j-1)*log(N/2^(j-1))+N/2^(j-1).': 1.094752550125122, 'And you take the 0 frequency.': 1.0986121892929077, 'I might be wrong, since this it not my work to do those calculus, but if you claim that your implementation is theoretically faster, you need to prove it, since I do not know any papers where scattering transform claims to be fastly implemented.': 1.0985175371170044, 'Here, one sees that the difference is not that significant.': 1.0977219343185425, 'Please correct me if I did a mistake.': 1.0986120700836182, 'It is essential to compare your work with the representation of a scattering transform.': 1.0986121892929077, 'First, in term of speed of computation, with a fair implementation (e.g. not MATLAB) and secondly in term of accuracy on the dataset you did use: it is a natural baseline.': 1.097655177116394}"
309,https://openreview.net/forum?id=SJk01vogl,"{'Comments:': 1.0986123085021973, '""This contrasts to adversarial attacks on classifiers, where any inspection of the inputs will reveal the original bytes the adversary supplied,': 1.098610520362854, 'which often have telltale noise""': 1.003711223602295, 'Is this really true?': 1.0986123085021973, 'If it were the case, wouldn\'t it imply that training ""against"" adversarial examples should easily make a classifier robust to adversarial examples (if they all have a telltale noise)?': 1.0986121892929077, 'Pros:': 1.0983481407165527, '-The question of whether adversarial examples exist in generative models, and indeed how the definition of ""adversarial example"" carries over is an interesting one.': 1.0979312658309937, ""-Finding that a certain type of generative model *doesn't have* adversarial examples would be a really significant result, finding that generative models have adversarial examples would also be a worth negative result."": 0.694027841091156, '-The adversarial examples in figures 5 and 6 seem convincing, though they seem much more overt and noisy than the adversarial examples on MNIST shown in (Szegedy 2014).': 0.9942710995674133, ""Is this because it's actually harder to find adversarial examples in these types of generative models?"": 1.0986119508743286, 'Issues:': 1.0986063480377197, '-Paper is significantly over length at 13 pages.': 1.0986121892929077, '-The beginning of the paper should more clearly motivate its purpose.': 1.0986121892929077, '-Paper has ""generative models"" in the title but as far as I can tell the whole paper is concerned with autoencoder-type models.': 1.0970778465270996, 'This is kind of annoying because if someone wanted to consider adversarial attacks on, say, autoregressive models, they might be unreasonably burdened by having to explain how they\'re distinct from a paper called ""adversarial examples for generative models"".': 1.0986040830612183, '-I think that the introduction contains too much background information - it could be tightened.': 1.0986061096191406, 'This paper considers different methods of producing adversarial examples for generative models such as VAE and VAEGAN.': 1.0240727663040161, 'Specifically, three methods are considered: classification-based adversaries which uses a classifier on top of the hidden code, VAE loss which directly uses the VAE loss and the ""latent attack"" which finds adversarial perturbation in the input so as to match the latent representation of a target input.': 1.0986119508743286, 'I think the problem that this paper considers is potentially useful and interesting to the community.': 1.098610520362854, 'To the best of my knowledge this is the first paper that considers adversarial examples for generative models.': 1.0985283851623535, 'As I pointed out in my pre-review comments, there is also a concurrent work of ""Adversarial Images for Variational Autoencoders"" that essentially proposes the same ""latent attack"" idea of this paper with both L2 distance and KL divergence.': 1.0986119508743286, ""Novelty/originality: I didn't find the ideas of this paper very original."": 0.8543358445167542, 'All the proposed three attacks are well-known and standard methods that here are applied to a new problem and this paper does not develop *novel* algorithms for attacking specifically *generative models*.': 1.0986123085021973, 'However I still find it interesting to see how these standard methods compare in this new problem domain.': 1.0986008644104004, 'The clarity and presentation of the paper is very unsatisfying.': 0.9816311597824097, 'The first version of the paper proposes the ""classification-based adversaries"" and reports only negative results.': 1.0986123085021973, 'In the second set of revisions, the core idea of the paper changes and almost an entirely new paper with a new co-author is submitted and the idea of ""latent attack"" is proposed which works much better than the ""classification-based adversaries"".': 1.098402500152588, 'However, the authors try to keep around the materials of the first version, which results in a 13 page long paper, with different claims and unrelated set of experiments.': 1.0967011451721191, '""in our attempts to be thorough, we have had a hard time keeping the length down"" is not a valid excuse.': 1.0934723615646362, 'In short, the paper is investigating an interesting problem and apply and compare standard adversarial methods to this domain, but the novelty and the presentation of the paper is limited.': 1.0986086130142212, 'After the rebuttal:': 1.0974446535110474, 'The paper contains an interesting set of results (mainly produced after the initial submission), but novelty is limited, and presentation is suboptimal.': 0.41763919591903687, 'For me now the biggest problem now is that the title and the content do not correspond.': 0.6809643507003784, 'The authors clearly attack deterministic encoder-decoder models (as described in 3.2), which are not at all the same as generative models, even though many generative models make use of this architecture.': 1.0986123085021973, 'A small experiment with sampling is interesting, but does not change the overall focus of the paper.': 1.0986121892929077, 'This inconsistency in not acceptable.': 0.7906796932220459, 'The whole issue could be resolved for example by simply replacing ""generative models"" by ""encoder-decoder networks"" in the title.': 1.098610758781433, 'Then I would tend towards recommending acceptance.': 1.0985761880874634, 'Initial review:': 0.8632218837738037, 'The paper describes three approaches to generating adversarial examples for deep encoder-decoder generative networks (trained as VAE or VAE-GAN), and shows a comparative analysis of these.': 1.098611831665039, 'While the phenomenon of adversarial examples in discriminative models is widely known and relatively well studied, I am not aware of previous work on adversarial examples for generative networks, so this work is novel (there is a concurrent work by Tabacof et al.': 1.09861159324646, 'which should be cited, though).': 0.6497012376785278, 'The paper has significantly improved since the initial submission; still, I have a number of remarks on presentation and experimental evaluation.': 1.098575472831726, 'I am in the borderline mode, and may change my rating during the discussion phase.': 1.0985428094863892, 'Detailed comments:': 1.0986028909683228, '1)': 1.0986123085021973, 'The paper is 13 pages long - significantly over the recommended page limit of 8 pages.': 1.094800591468811, 'Reviewers have to read multiple papers, multiple versions of each, it is a lot of work.': 0.2661454975605011, 'Large portions of the paper should be shortened and/or moved to the appendix.': 1.0971944332122803, 'It is job of the authors to make the paper concise and readable.': 1.0914158821105957, '""in our attempts to be thorough, we have had a hard time keeping the length down"" is a bad excuse - it may be hard, but has to be done.': 1.0986090898513794, '2) I intentionally avoided term ""generative model"" above because it is not obvious to me if the attacks described by the authors indeed attack generative models.': 1.098588228225708, 'To clarify, the authors train encoder-decoders as generative models (VAE or VAE-GAN), but then remove all stochasticity (sampling) and prior on the latent variables: that is, they treat the models as deterministic encoders-decoders.': 1.0175079107284546, 'It is not a big surprise that a deterministic deep network can be easily tricked; it would be much more interesting to see if the probabilistic aspect of generative models makes them more robust to such attacks.': 0.8054308891296387, 'Am I missing something?': 1.0113260746002197, 'I would like the authors to clarify their view on this and adjust the claims in the paper if necessary.': 1.0685912370681763, '3)': 1.0986123085021973, 'The paper is motivated by possible attacks on a data channel which uses a generative network for compressing information.': 0.8103497624397278, 'Description of the attack scenario in 3.1 does not look convincing to me.': 1.0986123085021973, 'It takes a huge amount of space and I do not think it adds much to the paper.': 1.0986123085021973, 'First, experiments on natural images are necessary to judge if the proposed attack could succeed in a realistic scenario and second, I am not aware of any existing practical applications of VAEs to image compression: attacking JPEG would be much more practical.': 1.0986123085021973, '4) Experiments are limited to MNIST and, in the latest version, SVHN (which is very nice).': 1.0986121892929077, 'While no good generative models of general natural images exist, it is common to evaluate generative models on datasets of faces, so this would be another very natural domain for testing the proposed approach.': 1.0986120700836182, 'Smaller remarks:': 1.0986123085021973, '1) Usage of ""Oracle"" in 3.2 does not look appropriate - oracle typically has access to (part of) ground truth, which is not the case here as far as I understand.': 1.0986121892929077, '2) Beginning of section 4: ""All three methods work for any generative architecture that relies on a learned latent representation z"" - ""are technically applicable to"" would be more correct than ""work for"".': 1.0986123085021973, '3) 4.1: ""confidentally""': 1.0986123085021973}"
310,https://openreview.net/forum?id=SJkXfE5xx,"{'I would like first to apologize for the delay.': 1.0986120700836182, 'Summary: A framework for two-samples statistical test using binary': 1.09861159324646, 'classification is proposed.': 1.097943663597107, 'It allows multi-dimensional sample testing and': 1.0986123085021973, 'an interpretability that other tests lack.': 1.024475336074829, 'A theoritical analysis is': 1.0887802839279175, 'provided and various empirical tests reported.': 1.0963517427444458, 'A very interesting approach.': 1.098513126373291, 'I have however two main concerns.': 1.098610520362854, 'The clarity of the presentation is obscured by too much content.': 1.0919941663742065, 'It would': 1.098150372505188, 'be more interesting if the presentation could be somewhat': 1.0787245035171509, 'self-contained.': 1.0985609292984009, 'You could consider making 2 papers out of this paper.': 1.0986123085021973, 'Seriously, you cram a lot of experiments in this paper.': 1.0986099243164062, 'But the setting': 1.0986090898513794, 'of the experiments is not really explained.': 1.098545789718628, 'We are supposed to have read': 1.0548146963119507, 'Jitkrittum et al., 2016, Radford et al., 2016, Yu et al., 2015, etc.': 1.098360300064087, 'All': 1.0986123085021973, 'this is okay but reduces your public to a very few.': 1.0622564554214478, 'For example, if I am not mistaken, you never explained what SCF is, despite': 1.0975353717803955, 'the fact that its performances are reported.': 1.0980257987976074, 'As a second point, given also that the number of submissions to this conference are exploding,': 1.0986123085021973, 'I would like to challenge you with the following question:': 1.0986121892929077, 'Why is this work significant to the representation learning community?': 1.0986117124557495, 'The submission considers the setting of 2-sample testing from the perspective of evaluating a classifier.': 1.0986123085021973, 'For a classifier between two samples from the same distribution, the distribution of the classification accuracy follows a simple form under the null hypothesis.': 1.0985318422317505, 'As such, a straightforward threshold can be derived for any classifier.': 1.097606897354126, 'Finding a more powerful test then amounts to training a better classifier.': 1.09395170211792, 'One may then focus efforts, e.g. on deep neural networks, for which statistics such as the MMD may be very difficult to characterize.': 1.09861159324646, '+': 1.0986123085021973, 'The approach is sound and very general': 1.09861159324646, 'The paper is timely in that deep learning has had huge impacts in classification and other prediction settings, but has not had as big an impact on statistical hypothesis testing as kernel methods have': 1.0986119508743286, 'The discussion of the relationship to kernel-MMD has not always been as realistic as it could have been.': 1.0986121892929077, 'For example, the kernel-MMD can also be seen as a classifier based approach, so a more fair discussion could be provided.': 1.0986106395721436, 'Also, the form of kernel-MMD used in the comparisons is a bit contradictory to the discussion as well': 1.097639560699463, '*': 0.9329542517662048, 'The linear kernel-MMD is used which is less powerful than the quadradic kernel-MMD (the authors have justified this from the perspective of computation time)': 1.0986111164093018, '* The kernel-MMD is argued against due to its unwieldy distribution under the null, but the linear time kernel-MMD (see also Zaremba et al., NIPS 2013) has a Gaussian distribution under the null.': 1.0980174541473389, ""Arthur Gretton's comment from Dec 14 during the discussion period was very insightful and helpful."": 1.0746644735336304, 'If these insights and additional experiments comparing the kernel-MMD to the classifier threshold on the blobs dataset could be included, that would be very helpful for understanding the paper.': 1.0986123085021973, 'The open review format gives an excellent opportunity to assign proper credit for these experiments and insights by citing the comment.': 1.0986123085021973, '## Paper summary': 1.0986123085021973, 'The paper reconsiders the idea of using a binary classifier to do two-sample testing.': 0.9780013561248779, 'The idea is to split the sample into two disjoint training and test sets, train a classifier on the training set, and use the accuracy on the test set as the test statistic.': 1.0591686964035034, 'If the accuracy is above chance level, one concludes that the two samples are from different distributions i.e., reject H0.': 1.0985875129699707, 'A theoretical result on an asymptotic approximate test power is provided.': 1.068423867225647, 'One implication is that the test is consistent, assuming that the classifier is better than coin tossing.': 0.23352693021297455, 'Experiments on toy problems, evaluation of GANs, and causal discovery verify the effectiveness of the test.': 1.0977041721343994, 'In addition, when the classifier is a neural net, examining the first linear filter layer allows one to see features which are most activated.': 1.0986100435256958, 'The result is an interpretable visual indicator of how the two samples differ.': 0.6305741667747498, '## Review summary': 1.0986123085021973, 'The paper is well written and easy to follow.': 1.0967286825180054, 'The idea of using a binary classifier for a two-sample testing is not new, as made clear in the paper.': 0.7667783498764038, 'The main contributions are the analysis of the asymptotic test power, the use of modern deep nets as the classifier in this context, and the empirical studies on various tasks.': 1.0986123085021973, 'The empirical results are satisfactorily convincing.': 1.096955418586731, 'Although not much discussion is made on why the method works well in practice, overall contributions have a potential to start a new direction of research on model criticisms of generative models, as well as visualization of where a model fails.': 1.0986076593399048, 'I vote for an acceptance.': 1.0843154191970825, '## Major comments / questions': 1.0985567569732666, 'My main concern is on Theorem 1 (asymptotic test power) and its assumptions.': 1.0836002826690674, 'But, I understand that these can be fixed as discussed below.': 1.088341236114502, '* Under H0, the distribution of the test statistic (i.e., sum of 0-1 classification results) follows Binomial(nte, 1/2) as stated.': 1.0985980033874512, 'However, under H1, terms in the sum are independent but *not* identical Bernoulli random variable.': 1.0981476306915283, 'This is because each term depends on a data point z_i, which can be from either P or Q.': 1.0668870210647583, 'So, in the paragraph in Sec3.1: ""... the random variable n_te \\hat{t} follows a Binomial(nte, p)..."" is not correct.': 1.0940135717391968, 'Essentially p depends on z_i.': 1.0978282690048218, 'It should follow a Poisson binomial distribution.': 0.8471909165382385, '* In the same paragraph, for the same reason, the alternative distribution of Binomial(nte, p=p_{risk}) is probably not correct.': 1.09811270236969, 'I guess you mention it to use Moivre-Laplace to get the asymptotic normality.': 0.39730796217918396, 'Anyway, I see no reason why you would need this statement as the Binomial is not required in the proof, but only its asymptotic normality.': 1.0621663331985474, 'A variant of the central limit theorem (instead of the Moivre-Laplace theorem) for independent, non-identical variables would still allow you to conclude the asymptotic normality of the Poisson binomial (with some conditions).': 0.3742711544036865, 'See for example http://stats.stackexchange.com/questions/5347/how-can-i-efficiently-model-the-sum-of-bernoulli-random-variables': 1.0821049213409424, 'The statement in Theorem 1 should be more precise.': 1.088843822479248, 'For instance, ""Assume the assumptions on the classifier given in the previous paragraph.': 0.7943297624588013, 'Then, for large n, the power of C2ST is approximate \\Phi(..).""': 1.0915305614471436, 'The current version is ""The power of C2ST is \\Phi(..).""': 1.0986100435256958, '* The proof of Theorem 1 should be more precise regarding which quantities are exact, which are approximate.': 1.0986056327819824, 'Both the null and alternative normal distributions are approximate for finite nte, for instance.': 1.0982043743133545, '* It is unclear to me why the paper includes independence tests in the experiments.': 1.0986123085021973, 'It does imply that the proposed test can be used to do an independence test.': 0.8111041188240051, ""But, isn't this also true for other two-sample tests where x,y can be stacked together?"": 1.0982855558395386, 'This seems ad-hoc and raises further questions regarding the consistency, what type of dependency can be detected, etc.': 1.0897860527038574, 'These points are not discussed.': 1.0986119508743286, '* Comment: By using classification accuracy as a proxy, one should expect the test power, for a given sample size n, to be lower than a direct statistic like MMD.': 1.0986123085021973, 'A vague analogy would be the t-test (based on the values of the data) vs. the sign test (based on only whether x_i < y_j, not the actual values).': 1.0534816980361938, 'The classifier test is in some sense reminiscent of the sign test i.e., passing data points through a classifier and binarizing the output.': 0.5174084305763245, 'For sufficiently large n, the test can still correctly detect the difference as shown empirically.': 0.8022922873497009, '## Minor comments / questions': 1.0986114740371704, '* Section 2, in the paragraph on the four steps of testing, the random variable for the statistic T is undefined.': 1.0986114740371704, '* In the last paragraph of Sec.2: ""kernel two-sample tests require the prescription of a manually-engineered representation of the data under study, and return values in units that are difficult to interpret.""': 1.0951640605926514, 'This is too vague.': 1.098095178604126, 'Manually-engineered representation?': 1.0986123085021973, ""Wouldn't a neural net require even more of the explicitness of the representation?"": 1.0984214544296265, '* In sec.3.2, it is better to also state the assumptions on the classifier in a slightly less technical way.': 1.0986123085021973, 'Specifically, under H0, you assume that the classifier is not biased, and under H1 you assume that it can learn well (better than coin tossing).': 0.6104403138160706}"
311,https://openreview.net/forum?id=SJqaCVLxx,"{'The authors seems to have proposed a genetic algorithm for learning the features of a convolutional network (LeNet-5 to be precise).': 1.0986123085021973, 'The algorithm is validated on some version of the MNIST dataset.': 0.398208349943161, 'Unfortunately the paper is extremely hard to understand and it is not at all clear what the exact training algorithm is.': 0.45710036158561707, 'Neither do the authors ever motivate why do such a training as opposed to the standard back-prop.': 1.0986123085021973, 'What are its advantages/dis-advantages?': 0.0034582531079649925, 'Furthermore the experimental section is equally unclear.': 1.0981481075286865, 'The authors seem to have merged the training and validation set of the MNIST dataset and use only a subset of it.': 1.0986123085021973, 'It is not clear why is that the case and what subset they use.': 1.0853835344314575, 'In addition, to the best of my understanding, the results reported are RMSE as opposed to classification error.': 1.0975865125656128, 'Why is that the case?': 1.0961319208145142, 'In short, the paper is extremely hard to follow and it is not at all clear what the training algorithm is and how is it better than standard way of training.': 0.7971268892288208, 'The experimental section is equally confusing and unconvincing.': 1.0986121892929077, 'Other comments:': 1.0986120700836182, 'The figures still say LeCun-5': 1.0986123085021973, 'The legends of the plots are not in english.': 0.38234204053878784, ""Hence I'm not sure what is going on there."": 1.0319724082946777, 'The paper is riddled with typos and hard to understand phrasing.': 1.098233699798584, 'Unfortunately, this paper is very difficult to understand.': 0.74300217628479, 'The current version of this paper seems improved compared to the initial version, but still far from a finished level.': 1.0986123085021973, ""I'd encourage the authors to keep editing over the language and presentation."": 1.0986123085021973, 'I also think it would be good to also try answering some of the following questions very clearly in the paper:': 1.0986120700836182, 'What is the advantage, if any, of the proposed algorithm over SGD?': 1.0985850095748901, 'What is the motivation and goal of the work beyond MNIST benchmarking?': 1.0985764265060425, 'Why are few training examples used?': 1.0986123085021973, 'Is this a scenario in which the system might have an advantage?': 1.0982304811477661, 'Concretely describe the genetic algorithms terminology used in the algorithm descriptions, and what each term means in the context of the convolutional network.': 1.0983736515045166, 'Try to make sure that the method, as described, can be understood by a reader without much prior background on genetic algorithms.': 1.0847722291946411, 'A single experiment on MNIST is too small to adequately describe the algorithm performance.': 0.924595832824707, 'Consider using a second or third dataset and/or experimental application.': 1.0984512567520142, ""Much work is still needed on the paper's writing before it can be understood well enough."": 1.098260521888733, 'I hope that some of this might be useful in helping to improve.': 1.087851643562317, 'I would encourage the authors to try to find outside readers, preferably fluent in English, to work with on a frequent basis before resubmitting to another venue.': 0.8589490056037903, 'The paper is still extremely poorly written and presented despite multiple reviewers asking to address that issue.': 1.0986123085021973, 'The frequent spelling mistakes and incoherent sentences and unclear presentation make reading and understanding the paper very difficult and time consuming.': 1.0986123085021973, 'Consider getting help from someone with good english and presentation skills.': 1.0986123085021973}"
312,https://openreview.net/forum?id=SJttqw5ge,"{'Description:': 0.7798075675964355, 'This paper presents a reinforcement learning architecture where, based on ""natural-language"" input, a meta-controller chooses subtasks and communicates them to a subtask controller that choose primitive actions, based on the communicated subtask.': 1.3862941265106201, 'The goal is to scale up reinforcement learning agents to large-scale tasks.': 1.267626404762268, 'The subtask controller embeds the subtask definition (arguments) into vectors by a multi-layer perceptron including an ""analogy-making"" regularization.': 1.3862934112548828, 'The subtask vectors are combined with inputs at each layer of a CNN.': 1.3859518766403198, 'CNN outputs (given the observation and the subtask) are then fed to one of two MLPs; one to compute action probabilities in the policy (exponential falloff of MLP outputs) and the other to compute termination probability (sigmoid from MLP outputs).': 1.3862942457199097, 'The meta controller takes a list of sentences as instructions embeds them into a sequence of subtask arguments (not necessarily a one-to-one mapping).': 1.3862943649291992, 'A context vector is computed by a CNN from the observation, the previous sentence embedding, the previous subtask and its completion state.': 1.3862931728363037, 'The subtask arguments are computed from the context vector through further mechanisms involving instruction retrieval from memory pointers, and hard/soft decisions whether to update the subtask or not.': 1.3862943649291992, 'Training involves policy distillation+actor-critic training for the subtask controller, and actor-critic training for the meta controller keeping the subtask controller frozen.': 1.3862942457199097, 'The system is tested in a grid world where the agent moves and interacts with (picks up/transforms) various item/enemy types.': 1.3854362964630127, 'It is compared to a) a flat controller not using a subtask controller, and b) subtask control by mere concatenation of the subtask embedding to the input with/without the analogy-making regularization.': 1.386194109916687, 'Evaluation:': 1.3797215223312378, 'The proposed architecture seems reasonable, although it is not clear why the specific way of combining subtask embeddings in the subtask controller would be the ""right"" way to do it.': 1.3862943649291992, 'I do not feel the grid world here really represents a ""large-scale task"": in particular the 10x10 size of the grid is very small.': 1.3862943649291992, 'This is disappointing since this was a main motivation of the work.': 1.3460001945495605, 'Moreover, the method is not compared to any state of the art alternatives.': 1.1505659818649292, 'This is especially problematic because the test is not on established benchmarks.': 0.8285906314849854, 'It is not really possible, based on the shown results, to put the performance in context of other works.': 1.3703173398971558, 'This paper presents an architecture and corresponding algorithms for': 1.0565522909164429, 'learning to act across multiple tasks, described in natural language.': 0.7348753213882446, 'The proposed system is hierarchical and is closely related to the options': 0.8989017009735107, 'framework.': 1.3862943649291992, 'However, rather than learning a discrete set of options, it learns': 1.2115761041641235, 'a mapping from natural instructions to an embedding which implicitly (dynamically)': 1.3862943649291992, 'defines an option.': 1.3494855165481567, 'This is a novel and interesting new perspective on options': 1.3862943649291992, 'which had only slightly been explored in the linear setting (see comments below).': 1.3862943649291992, 'I find the use of policy distillation particularly relevant for this setting.': 1.2890862226486206, 'This, on its own, could be a takeaway for many RL readers who might not necessarily': 0.8777465224266052, 'be interested about NLP applications.': 0.9177826046943665, 'In general, the paper does not describe a single, simple, end-to-end,': 1.3818095922470093, 'recipe for learning with this architecture.': 1.3766218423843384, 'It rather relies on many recent': 1.1816314458847046, 'advances skillfully combined: generalized advantage estimation, analogy-making': 1.1633011102676392, 'regularizers, L1 regularization, memory addressing, matrix factorization,': 0.8810440897941589, 'policy distillation.': 0.8245874047279358, 'I would have liked to see some analysis but': 0.7596622705459595, 'understand that it would have certainly been no easy task.': 0.9386004209518433, 'For example, when you say ""while the parameters of the subtask controller are': 1.0789166688919067, 'frozen"", this sounds to me like you\'re having some kind of two-timescale stochastic gradient': 0.7068224549293518, 'descent.': 0.6965906620025635, ""I'm also unsure how you deal with the SMDP structure in your gradient"": 1.3832045793533325, 'updates when you move to the ""temporal abstractions"" setting.': 1.1405057907104492, 'I am inclined to believe that this approach has the potential to scale up to': 1.3862836360931396, 'very large domains, but paper currently does not demonstrate this': 1.1891096830368042, 'empirically.': 1.372986912727356, 'Like any typical reviewer, I would be tempted to say that': 1.3858290910720825, 'you should perform larger experiments.': 1.38625967502594, ""However, I'm also glad that you have"": 1.3859951496124268, 'shown that your system also performs well in a ""toy"" domain.': 0.8884301781654358, 'The characterization': 0.8109322786331177, 'in figure 3 is insightful and makes a good point for the analogy regularizer': 1.386088252067566, 'and need for hierarchy.': 1.3862940073013306, 'Overall, I think that the proposed architecture would inspire other researchers': 0.8987975120544434, 'and would be worth being presented at ICLR.': 0.6958279609680176, 'It also contains novel elements': 0.815015435218811, '(subtask embeddings) which could be useful outside the deep and NLP communities': 0.6242702603340149, 'into the more ""traditional"" RL communities.': 1.2677680253982544, '# Parameterized Options': 0.727805495262146, 'Sutton et.': 1.1305683851242065, 'al (1999) did not explore the concept': 1.3566426038742065, 'of *parameterized* options originally.': 1.3862943649291992, 'It only came later, perhaps first with': 1.2995941638946533, '[""Optimal policy switching algorithms for reinforcement': 1.1072721481323242, 'learning, Comanici & Precup, 2010""] or': 1.343224287033081, '[""Unified Inter and Intra Options Learning Using Policy Gradient Methods"", Levy & Shimkin, 2011].': 1.201794147491455, 'Konidaris also has a line of work  on ""parametrized skills"":': 1.3845235109329224, '[""Learning Parameterized Skills"".': 1.3793796300888062, 'da Silva, Konidaris, Barto, 2012)]': 1.2604033946990967, 'or [""Reinforcement Learning with Parameterized Actions"".': 1.2090758085250854, 'Masson, Ranchod, Konidaris, 2015].': 0.7184808254241943, 'Also, I feel that there is a very important distinction to be made with': 0.7165846824645996, 'the expression ""parametrized options"".': 1.386292576789856, 'In your work, ""parametrized"" comes in': 0.7375091314315796, 'two flavors.': 1.3862943649291992, 'In the spirit of policy gradient methods,': 1.386282205581665, 'we can have options whose policies and termination functions are represented': 1.376716136932373, 'by function approximators (in the same way that we have function approximation': 1.3428148031234741, 'for value functions).': 0.9041412472724915, 'Those options have parameters and we might call them': 1.360874891281128, '""parameterized"" because of that.': 1.3862855434417725, 'This is the setting of Comanicy & Precup (2010),': 0.7338865399360657, 'Levy & Shimkin (2011) Bacon & Precup (2015), Mankowitz, Mann, and': 0.9220708012580872, 'Mannor (2016) for example.': 1.3861124515533447, 'Now, there a second case where options/policies/skills take parameters *as inputs*': 0.9157220125198364, 'and act accordingly.': 1.05696439743042, 'This is what Konidaris & al. means by ""parameterized"", whose': 0.8492797017097473, 'meaning differs from the ""function approximation"" case above.': 1.3556849956512451, 'In your work, the embedding of subtasks arguments is the ""input"" to your options': 0.7461203932762146, 'and therefore behave as ""parameters"" in the sense of Konidaris.': 1.3862943649291992, '# Related Work': 1.3862943649291992, ""I CTRL-F through the PDF but couldn't find references to any of S.R.K. Branavan's"": 0.5668777227401733, 'work.': 1.3862943649291992, ""Branavan's PhD thesis had to do with using control techniques from RL"": 1.386008381843567, 'in order to interpret natural instructions so as to achieve a goal.': 1.3691346645355225, 'For example,': 1.3862943649291992, 'in ""Reinforcement Learning for Mapping Instructions to Actions"", an RL agent': 1.3862783908843994, 'learns from ""Windows troubleshooting articles"" to interact with UI elements': 1.3724126815795898, '(environment) through a Softmax policy (over linear features) learned by policy': 0.7484585642814636, 'gradient methods.': 1.3852152824401855, 'As you mention under ""Instruction execution"" the focus of your work in': 1.3859024047851562, ""on generalization, which is not treated explicitely (afaik) in Branavan's work."": 1.2193270921707153, 'Still, it shares some important algorithmic and architectural similarities which': 1.3780524730682373, 'should be discussed explicitly or perhaps even compared to in your experiments': 1.0014623403549194, '(as a baseline).': 1.3862937688827515, '## Zero-shot and UVFA': 1.1920032501220703, 'It might also want to consider': 0.9900209307670593, '""Learning Shared Representations for Value Functions in Multi-task': 1.3862429857254028, 'Reinforcement Learning"", Borsa, Graepel, Shawe-Taylor]': 1.3039026260375977, 'under the section ""zero-shot tasks generalization"".': 1.065118432044983, '# Minor Issues': 1.3862072229385376, 'I first read the abstract without knowing what the paper would be about': 1.3764808177947998, 'and got confused in the second sentence.': 1.3794203996658325, 'You talk about ""longer sequences of': 1.3652960062026978, 'previously seen instructions"", but I didn\'t know what clearly': 1.0832178592681885, 'meant by ""instructions"" until the second to last sentence where you specify': 1.0739794969558716, '""instructions described by *natural language*.""': 1.3860975503921509, 'You could perhaps': 1.3862943649291992, 're-order the sentences to make it clear in the second sentence that you are': 0.7041948437690735, 'interested in NLP problems.': 1.3862887620925903, 'Zero-generalization: I was familiar with the term ""one-shot"" but not ""zero-shot"".': 1.3638933897018433, 'The way that the second sentence ""[...] to have *similar* zero-shot [...]"" follows': 1.38189697265625, 'from the first sentence might as well hold for the ""one-shot"" setting.': 1.3855243921279907, 'You': 1.3862943649291992, 'could perhaps add a citation to ""zero-shot"", or define it more': 1.1506222486495972, 'explicitly from the beginning and compare it to the one-shot setting.': 1.3862943649291992, 'It could': 1.3862943649291992, 'also be useful if you explain how zero-shot relates to just the notion of': 1.3214530944824219, 'learning with ""priors"".': 1.3498950004577637, 'Under section 3, you say ""cooperate with each other"" which sounds to me very much': 1.3846251964569092, 'like a multi-agent setting, which your work does not explore in this way.': 1.0156028270721436, 'You might want to choose a different terminology or explain more precisely if there': 0.7572959661483765, 'is any connection with the multi-agent setting.': 1.3862942457199097, 'The second sentence of section 6 is way to long and difficult to parse.': 1.2274582386016846, 'You could': 1.3862943649291992, 'probably split it in two or three sentences.': 1.386247992515564, 'This paper can be seen as instantiating a famous paper by the founder of AI John McCarthy on learning to take advice (which was studied in depth by other later researchers, such as Jack Mostow in the card game Hearts).': 1.3862943649291992, 'The idea is that the agent is given high level instructions on how to solve a problem, and must distill from it a low level policy.': 1.3862942457199097, 'This is quite related to how humans learn complex tasks in many domains (e.g., driving, where a driving instructor may provide advice such as ""keep a certain distance from the car in front"").': 1.3862943649291992, 'A fairly complex neural deep learning controller architecture is used, although the details of this system are somewhat confusing in terms of many details that are presented.': 1.3828529119491577, 'A simpler approach might have been easier to follow, at least initially.': 1.0657089948654175, 'The experiments unfortunately are on a rather simplistic 2D maze, and it would have been worthwhile to see how the approach scaled to more complex tasks of the sort usually seen in deep RL papers these days (e.g, Atari, physics simulators etc.).': 1.3862942457199097, 'Nice overall idea, somewhat confusing description of the solution, and an inadequate set of experiments on a less than satisfactory domain of 2D grid worlds.': 1.3862943649291992, 'The paper presents a hierarchical DRL algorithm that solves sequences of navigate-and-act tasks in a 2D maze domain.': 1.3862943649291992, 'During training and evaluation, a list of sub-goals represented by text is given to the agent and its goal is to learn to use pre-learned skills in order to solve a list of sub-goals.': 1.3862943649291992, 'The authors demonstrate that their method generalizes well to sequences of varying length as well as to new combinations of sub-goals (i.e., if the agent knows how to pick up a diamond and how to visit an apple, it can also visit the diamond).': 1.3862943649291992, 'Overall, the paper is of high technical quality and presents an interesting and non-trivial combination of state-of-the-art advancements in Deep Learning (DL) and Deep Reinforcement Learning (DRL).': 1.3862866163253784, 'In particular, the authors presents a DRL agent that is hierarchical in the sense that it can learn skills and plan using them.': 1.3862864971160889, 'The skills are learned using a differential temporally extended memory networks with an attention mechanism.': 1.3862943649291992, 'The authors also make a novel use of analogy making and parameter prediction.': 1.3862943649291992, ""However, I find it difficult to understand from the paper why the presented problem is interesting and why hadn't it bee solved before."": 1.3862873315811157, 'Since the domain being evaluated is a simple 2D maze, using deep networks is not well motivated.': 1.3862943649291992, 'Similar problems have been solved using simpler models.': 1.0336599349975586, 'In particular, there is a reach literature about planning with skills that had been ignored completely by the authors.': 1.3862943649291992, 'Since all of the skills are trained prior to the evaluation of the hierarchical agent, the problem that is being solved is much more similar to supervised learning than reinforcement learning (since when using the pre-trained skills the reward is not particularly delayed).': 1.3862943649291992, 'The generalization that is demonstrated seems to be limited to breaking a sentence (describing the subtask) into words (item, location, action).': 1.3862943649291992, 'The paper is difficult to read, it is constantly switching between describing the algorithm and giving technical details.': 1.3862943649291992, 'In particular, I find it to be overloaded with details that interfere with the general understanding of the paper.': 1.3862943649291992, 'I suggest moving many of the implementation details into the appendix.': 1.3862943649291992, 'The paper should be self-contained, please do not assume that the reader is familiar with all the methods that you use and introduce all the relevant notations.': 1.3862943649291992, 'I believe that the paper will benefit from addressing the problems I described above and will make a better contribution to the community in a future conference.': 1.3862943649291992}"
313,https://openreview.net/forum?id=SJvYgH9xe,"{'EDIT: the revisions made to this paper are very thorough and address many of my concerns, and the paper is also easier to understand.': 1.0571885108947754, 'i recommend the latest version of this paper for acceptance and have increased my score.': 1.065708875656128, 'This paper presents a way of interpreting LSTM models, which are notable for their opaqueness.': 1.0986111164093018, ""In particular, the authors propose decomposing the LSTM's predictions for a QA task into importance scores for words, which are then used to generate patterns that are used to find answers with a simple matching algorithm."": 1.0986123085021973, 'On the WikiMovies dataset, the extracted pattern matching method achieves accuracies competitive with a normal LSTM, which shows the power of the proposed approach.': 1.0985615253448486, 'I really like the motivation of the paper, as interpreting LSTMs is definitely still a work-in-progress, and the high performance of the pattern matching was surprising.': 1.0986123085021973, 'However, several details of the pattern extraction process are not very clear, and  the evaluation is conducted on a very specific task, where predictions are made at every word.': 1.0986117124557495, 'As such, I recommend the paper in its current form as a weak accept but hope that the authors clarify their approach, as I believe the proposed method is potentially useful for NLP researchers.': 1.0986123085021973, 'Comments:': 1.0922884941101074, ""Please introduce in more detail the specific QA tasks you are applying your models on before section 3.3, as it's not clear at that point that the answer is an entity within the document."": 1.0986123085021973, '3.3: is the softmax predicting a 0/1 value (e.g., is this word the answer or not?)': 0.7500547170639038, '3.3: what are the P and Q vectors?': 1.083667516708374, 'do you just mean that you are transforming the hidden state into a 2-dimensional vector for binary prediction?': 0.8500024080276489, 'how does performance of the pattern matching change with different cutoff constant values?': 0.5597696900367737, '5.2: are there questions whose answers are not entities?': 1.0693351030349731, ""how could the proposed approach be used when predictions aren't made at every word?"": 0.40644311904907227, 'is there any extension for, say, sentence-level sentiment classification?': 1.0986123085021973, 'This work proposes a pattern extraction method to both understand what a trained LSTM has learnt and to allow implementation of a hand-coded algorithm that performs similarly to the LSTM.': 1.0986123085021973, 'Good results are shown on one dataset for one model architecture so it is unclear how well this approach will generalize, however, it seems it will be a useful way to understand and debug models.': 1.0986123085021973, 'The questions in WikiMovies seem to be generated from templates and so this pattern matching approach will likely work well.': 1.0986123085021973, ""However, from the experiments it's not clear if this will extend to other types of Q&A tasks where the answer may be free form text and not be a substring in the document."": 1.0986123085021973, 'Is the model required to produce a continuous span over the original document?': 1.0986123085021973, 'The approach also seems to have some deficiencies in how it handles word types such as numbers or entity names.': 1.0986123085021973, 'This can be encoded in the embedding for the word but from the description of the algorithm, it seems that the approach requires an entity detector.': 1.0986123085021973, 'Does this mean that the approach is unable to determine when it has reached an entity from the decomposition of the output of the LSTM?': 1.0986123085021973, ""The results where 'manual pattern matching' where explicit year annotations are used, seem to show that the automatic method is unable to deal with word types."": 1.0986123085021973, 'It would also be good to see an attention model as a baseline in addition to the gradient-based baseline.': 1.0986123085021973, 'Minor comments:': 1.0986123085021973, 'P and Q seem to be undefined.': 1.0986123085021973, ""Some references seem to be bad, e.g. in section 5.1: 'in 1' instead of 'in table 1'."": 1.0986123085021973, ""Similarly above section 7: 'as shown in 3' and in section 7.1."": 1.0986123085021973, ""In the paragraph above section 6.3: 'adam' -> 'Adam'."": 1.0986123085021973, 'This paper proposes a novel method for extracting rule-based classifiers from trained LSTM models.': 1.0986123085021973, 'The proposed method is applied to a factoid question-answering task, where it is demonstrated that the extracted rules perform comparatively to the original LSTM.': 1.0986123085021973, 'The analysis of the extracted rules illustrate the features the LSTM model picks up on.': 1.0986123085021973, 'Analyzing and visualizing the computations carried out by RNNs in order to understand the functions they compute is an important direction of research.': 0.9491943120956421, 'This sort of analysis will help us understand the pitfalls of RNNs, and how we can improve them.': 0.995739758014679, 'Although the approach taken is relatively inflexible - each rule is defined as an ordered sequence of words - the authors experiment with three different scores for picking salient words (state-difference, cell-difference and gradient) and their approach yields comparable performance, which suggests that the extracted rules mimic the RNN closely.': 1.0986123085021973, 'The results are also somewhat surprising, since most of the rules consist only of two or three words.': 1.0986123085021973, 'It would have been interesting to try extend the approach on other natural language processing tasks, such as machine translation.': 1.0986123085021973, 'Presumably the rules learned here will be quite different.': 1.0986123085021973, 'Other comments:': 1.0986123085021973, 'Eq. (12) is over-parametrized with two vectors  and .': 1.0986123085021973, 'The same function can be computed with a single vector.': 1.0986123085021973, 'This becomes clear when you divide both the numerator and denominator by .': 1.0986123085021973, 'Section 4.1.': 1.0986123085021973, 'Is it correct that this section is focused on the forward LSTM?': 1.0986123085021973, 'If so, please clarify it in the text.': 1.0986123085021973, 'In Eq.': 1.0986123085021973, '(13), define .': 1.0986123085021973, 'Eq. (13) is exactly the same as Eq.': 1.0986123085021973, '(15).': 1.0986123085021973, 'Is there a mistake?': 1.0986123085021973, 'In Table 1, third column should have word ""film"" highlighted.': 1.0986123085021973, '""are shown in 2"" -> ""are shown in Table 2"".': 1.0986123085021973, 'Since there are some problems representing numbers, it may help to replace each digit with the hashtag symbol #.': 1.0986123085021973}"
314,https://openreview.net/forum?id=SJx7Jrtgl,"{'This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario.': 1.0986123085021973, 'First the model has to be adapted (with a Gaussian mixture as a prior) and then the inference has to become consistent (by introducing a regularization term).': 1.0986123085021973, 'A general positive point about this paper is that the model construction is kept simple.': 1.0986123085021973, 'Indeed, the assumption about the mixture prior is simple but reasonable, and the inference follows the VAE framework where appropriate with only changing parts that do not conform with the clustering task.': 1.0986123085021973, 'These changes are also motivated by some analysis.': 1.0985755920410156, ""The presentation is also kept simple: the linking to VAE and related methods is made in an clear and honest way, so that it's easy to follow the paper and understand how everything fits together."": 1.0986099243164062, 'Also, the regularization term is a well motivated and reasonable addition.': 1.0560280084609985, ""Given the VAE context in this paper, I'd be interested in seeing a discussion on the variance of the samples in (6)."": 1.096510410308838, 'A negative issue of this paper is that all crucial regularizations rely upon ad-hoc parameters that control their strength, namely \\eta (eq. (3)) and \\alpha (eq. 7).': 1.0986123085021973, 'According to the authors adjusting these parameters is crucial, and there seems to be no principled way of adjusting them.': 0.4057825207710266, 'It also seems that these two parameters interact, since they both regularize z in different ways.': 1.0985594987869263, 'This makes the search space over them grow multiplicatively, since the tuning problem now becomes combinatorial.': 1.0957911014556885, ""The authors mention that they tune the trade-off between these two regularizers, but I'd be interested in a comment concerning how this is done (what's the space of parameters to search on)."": 1.0986096858978271, 'In practical clustering applications, high sensitivity to tuned parameters is undesirable, since one also needs to cross-validate values of K at the same time.': 1.0980144739151, 'I really liked the experiments section.': 1.0832421779632568, 'It is not very exhaustive in terms of comparison, but it is very exploratory in terms of demonstrating the model components, strengths and weaknesses.': 0.9202773571014404, 'This is much more useful than reporting unintuitive percentage improvements relative to arbitrarily selected baselines and datasets.': 1.043323278427124, 'Overall, I am a little concerned about the practicality of this approach, given the tuning it requires.': 1.0962833166122437, 'However, I am in favor of accepting this paper because it makes its strong and weak points very clear through good explanation and demonstration.': 1.0986123085021973, 'Therefore, I expect further research to be built on top of this paper, so that the aforementioned issues will hopefully be alleviated in the future.': 1.0986123085021973, 'Finally, the theoretical intuitions given in the paper (and author comments) improve its usefulness as a scientific manuscript.': 1.0986123085021973, 'The authors posit a mixture of Gaussian prior for variational': 1.0986123085021973, 'auto-encoders.': 1.0986123085021973, 'They also consider a regularization term motivated': 1.0986123085021973, 'from information theory.': 1.0986123085021973, 'The modeling extension is simple and the inference follows': 1.0986123085021973, ""mechanically from what's already standard in the literature."": 1.0986123085021973, 'Instead': 1.0986123085021973, 'of using discrete latent variable samples they collapse the expected': 1.0986123085021973, 'KL; this works for few mixture components and has been considered': 1.0986123085021973, 'before in more general contexts, e.g., Titsias and Lazaro-Gredilla': 1.0986123085021973, '(2015).': 1.0986123085021973, 'It will not scale to many mixture components.': 1.0986123085021973, 'I find the discussion in Section 3.2.2 difficult to parse and, if I': 1.098611831665039, 'understood it correctly, not necessarily correct.': 1.0986123085021973, 'Many arguments are': 1.0986123085021973, 'introduced and few fleshed out.': 1.0986123085021973, 'First, there is a claim that a': 1.0986123085021973, 'multinomial prior with equal class probabilities assigns the same': 1.0986123085021973, 'number of data points to each class on average; this is true a priori': 1.0986123085021973, 'but certainly not true given data.': 1.0986123085021973, 'Second, they claim the KL': 1.0986123085021973, 'regularizer forces the approximate posterior to be close to this': 1.0986123085021973, 'uniform; this is only true for small data, certainly the energy term': 1.0986123085021973, 'in the ELBO (expected log-likelihood) will overpower the regularizer;': 1.0959548950195312, 'is this not the case in a mean-field approximation to a mixture of': 1.0986123085021973, 'Gaussians model?': 1.0986123085021973, 'Third, there is a claim that ""under the mean-field': 1.0986123085021973, 'approximation, this constraint is enforced on each sample""; how does': 1.0986123085021973, 'the mean-field approximation enforce a constraint on the effect of': 1.0986123085021973, 'Monte Carlo sampling?': 1.0986123085021973, 'Fourth, they argue Johnson et al.': 1.0986123085021973, '(2016)': 1.0986123085021973, 'can': 1.0986123085021973, 'overcome this issue partly due to SVI; how does data subsampling': 1.0986123085021973, 'affect this behavior?': 1.0986123085021973, 'Fifth, they derive the exact posterior in': 1.0986123085021973, 'Equation 6; so to what extent are these arguments relevant?': 1.0986123085021973, 'The experiments are limited on toy data and only a few mixture': 1.0986123085021973, 'components are considered (not enough where the collapsed approach': 1.0986123085021973, 'will not scale).': 1.0986123085021973, '+ Titsias, M. K., & Lázaro-Gredilla, M. (2015).': 1.0986123085021973, 'Local Expectation Gradients for Black Box Variational Inference.': 1.0986123085021973, 'In Neural Information Processing Systems.': 1.0986123085021973, 'The authors proposes to a variant of Variational Auto-Encoders using a mixture distribution to enable unsupervised clustering that they combine with an information-theoretical regularization.': 1.0986123085021973, 'To demonstrate the merit of such approach, they perform experiments on a synthetic dataset, MNIST and SVHN.': 1.0986123085021973, 'The use of a mixture of VAE is an incremental idea if novel.': 1.0986123085021973, 'I would like to see the comparison with the more straightforward use of a mixture of gaussians prior.': 1.0986123085021973, 'This model is more complex and I would like to see a justification of this additional complexity.': 1.0986109972000122, 'The results in Table 1 are questionable.': 1.0556385517120361, 'First of all, GMVAE+ seems to outperform other methods with M=1, there should be a run of GMVAE+ with M=10 for proper comparison.': 1.0986123085021973, 'But what I find more disturbing in this table is the variance of the results, especially since you are taking the ""Best Run"".': 1.0986123085021973, 'Was the best run maximizing the validation performance or the test performance ?': 1.0986123085021973, 'Moreover, the average performance (higher) and standard deviation (lower) of Adversarial Autoencoder makes me question the claim that ""we have advanced the state of the art in deep unsupervised clustering both in theory and practice"".': 1.0986121892929077, 'The consistency violation regularization might be interesting.': 0.7737476229667664, 'But the cluster degeneracy problem is, as far as I know, also a problem in plain mixture of gaussians models.': 1.092873454093933, 'So making an experiment on this simpler model on a synthetic dataset should also be done.': 1.0986123085021973, 'In general, I would recommend running more experiments as to solidify your claims.': 1.0986123085021973}"
315,https://openreview.net/forum?id=SJzCSf9xg,"{'This paper proposes a new idea to help defending adversarial examples by training a complementary classifier to detect them.': 1.0986123085021973, 'The results of the paper show that adversarial examples in fact can be easily detected.': 1.0986123085021973, 'Moreover, such detector generalizes well to other similar or weaker adversarial examples.': 1.0986123085021973, 'The idea of this paper is simple but non-trivial.': 1.0986123085021973, 'While no final scheme is proposed in the paper how this idea can help in building defensive systems, it actually provides a potential new direction.': 1.0986123085021973, 'Based on its novelty, I suggest an acceptance.': 1.0986123085021973, 'My main concern of this paper is about its completeness.': 1.0986123085021973, 'No effective method is reported in the paper to defend the dynamic adversaries.': 1.0986123085021973, 'It could be difficult to do so, but rather the paper doesn’t seem to put much effort to investigate this part.': 1.0986123085021973, 'How difficult it is to defend the dynamic adversaries is an important and interesting question following the conclusions of this paper.': 1.0986123085021973, 'Such investigation may essentially help improve our understanding of adversarial examples.': 1.0986123085021973, 'That being said, the novelty of this paper is still significant.': 0.4573539197444916, 'Minor comment:': 1.0945031642913818, 'The paper needs to improve its clarity.': 1.0737909078598022, 'Some important details are skipped in the paper.': 1.0986119508743286, 'For example, the paper should provide more details about the dynamic adversaries and the dynamic adversary training method.': 1.0985777378082275, 'This paper explores an important angle to adversarial examples: the detection of adversarial images and their utilization for trainig more robust networks.': 1.0986123085021973, 'This takes the competition between adversaries and models to a new level.': 1.09860098361969, 'The paper presents appealing evidence for the feasibility of robustifying networks by employing the a detector subnetwork that is trained particularly for the purpose of detecting the adversaries in a terget manner rather than just making the networks themselves robust to adversarial examples.': 1.0986123085021973, 'The jointly trained primary/detector system is evaluated in various scenarios including the cases when the adversary generator has access to the model and those where they are generated in a generic way.': 1.0986123085021973, 'The results of the paper show good improvements with the approach and present well motived thorough analyses to back the main message.': 1.0986121892929077, 'The writing is clear and concise.': 1.0972803831100464, 'I reviewed the manuscript on December 5th.': 0.06381602585315704, 'Summary:': 1.0986123085021973, 'The authors investigate the phenomenon of adversarial perturbations and ask whether one may build a system to independently detect an adversarial data point': 1.010261058807373, 'if one could detect an adversarial example, then might prevent a machine from automatically processing it.': 0.4296051561832428, 'Importantly, the authors investigate whether it is possible to build an adversarial detector which is resilient to adversarial examples built against *both* the classifier and the detector.': 0.810382604598999, 'Their results suggest that training a detector in this more difficult setting still yields gains but does not entirely resolve the problem of detecting adversarial examples.': 1.0986121892929077, 'Major comments:': 1.0986061096191406, 'The authors describe a novel approach for dealing with adversarial examples from a security standpoint': 0.7333520650863647, 'namely, build an independent system to detect the adversary so a human might intervene in those cases.': 0.5057129263877869, 'A potential confound of this approach is that an adversary might respond by constructing adversarial examples to fool *both* the original classifier and the new detector.': 1.0985716581344604, 'If that were possible, then this approach is moot since an attacker could always outwit the original system.': 0.8884047269821167, ""To their credit, the authors show that building a 'dynamic' detector to detect adversarial examples but also be resilient to an adversary mitigates this potential escalation (worse case from 55% to 70% detection rate)."": 1.0986123085021973, ""Even though the 'dynamic' detector  demonstrates positive gains, I am concerned about overall scores."": 1.0986108779907227, 'Detecting adversarial examples at this rate would not be a reliable security procedure.': 0.536185085773468, ""My second comment is about 'model transferability'."": 0.9904438257217407, ""My definition of 'model transferability' is different then the one used in the paper."": 1.0985990762710571, 'My definition means that one constructs an adversarial example on one network and measures how well the adversarial examples attack a second trained model': 0.406726211309433, 'where the second model has been trained with different initial conditions.': 1.0986120700836182, ""(The author's definition of 'transferability' is based on seeing how well the detector generalizes across training methods)."": 1.0949158668518066, ""'Model transferability' (per my definition) is quite important because it measures how general an adversarial example is across all models"": 1.0983668565750122, 'and not specific to a given trained model.': 0.3067336082458496, ""Different methods have different levels of 'model transferability' (Kurakin et al, 2016) and I am concerned how well the detector they built would be able to detect adversarial examples across *all models* and not just the trained model in question."": 1.0986123085021973, 'In other words, a good detector would be able to detect adversarial examples from any network and not just one particularly trained network.': 1.0906387567520142, 'This question seems largely unaddressed in this paper but perhaps I missed some subtle point in their descriptions.': 1.0970731973648071, 'Minor comments:': 1.0986123085021973, 'If there were any points in the bottom-left of the Figure 2 left, then this would be very important to see': 1.0984225273132324, 'perhaps move the legend to highlight if the area contains no points.': 0.26881712675094604, 'X-axis label is wrong in Figure 2 right.': 1.0705386400222778, 'Measure the transferability of the detector?': 1.0986123085021973, 'How is \\sigma labeled on Figure 5?': 1.0986032485961914, ""Whenever an image is constructed to be an 'adversary', has the image actually been tested to see if it is adversarial?"": 1.0986123085021973, 'In other words, does the adversarial image actually result in a misclassification by the original network?': 1.0986123085021973}"
316,https://openreview.net/forum?id=Sk-oDY9ge,"{'The paper presents an application of deep learning to genomic SNP data': 1.0986123085021973, 'with a comparison of possible approaches for dealing with the very': 1.0986123085021973, 'high data dimensionality.': 1.0986123085021973, 'The approach looks very interesting but the': 1.0986123085021973, 'experiments are too limited to draw firm conclusions about the': 1.0986123085021973, 'strengths of different approaches. The presentation would benefit from': 1.0986123085021973, 'more precise math.': 1.0646240711212158, 'Quality:': 1.0844022035598755, 'The basic idea of the paper is interesting and the applied deep': 1.0985994338989258, 'learning methodology appears reasonable.': 0.42649197578430176, 'The experimental evaluation': 1.0986123085021973, 'is rather weak as it only covers a single data set and a very limited': 1.0188194513320923, 'number of cross validation folds.': 0.31962570548057556, 'Given the significant variation in': 1.0984477996826172, 'the performances of all the methods, it seems the differences between': 0.9768050312995911, 'the better-performing methods are probably not statistically': 0.8987768888473511, 'significant.': 0.7133510708808899, 'More comprehensive empirical validation': 1.0986123085021973, 'could clearly': 0.6827600002288818, 'strengthen the paper.': 1.0923038721084595, 'Clarity:': 1.098610758781433, 'The writing is generally good both in terms of the biology and ML, but': 1.0986123085021973, 'more mathematical rigour would make it easier to understand precisely': 0.38221022486686707, 'what was done.': 0.3234268128871918, 'The different architectures are explained on an': 0.2643924951553345, 'intuitive level and might benefit from a clear mathematical': 0.5035265684127808, 'definition.': 1.098078727722168, 'I was ultimately left unsure of what the ""raw end2end""': 0.9471858143806458, 'model is - given so few parameters it cannot work on raw 300k': 0.9596105813980103, 'dimensional input': 0.8072519302368164, 'but I could not figure out what kind of embedding': 0.7315254211425781, 'was used.': 1.047001600265503, 'The results in Fig. 3 might be clearer if scaled so that maximum for': 0.9777040481567383, 'each class is 1 to avoid confounding from different numbers of': 0.8977066874504089, 'subjects in different classes.': 1.0926032066345215, 'In the text, please use the standard': 0.4060959815979004, 'italics math font for all symbols such as N, N_d, ...': 0.9991035461425781, 'Originality:': 1.0985444784164429, 'The application and the approach appear quite novel.': 1.0986123085021973, 'Significance:': 1.0986123085021973, 'There is clearly strong interest for deep learning in the genomics': 1.0986123085021973, 'area and the paper seeks to address some of the major bottlenecks': 1.0986123085021973, 'here.': 1.0986123085021973, 'It is too early to tell whether the specific techniques proposed': 1.0986123085021973, 'in the paper will be the ultimate solution, but at the very least the': 0.5967815518379211, 'paper provides interesting new ideas for others to work on.': 1.0749088525772095, 'Other comments:': 1.0986123085021973, 'I think releasing the code as promised would be a must.': 0.6781818866729736, 'The paper addresses the important problem (d>>n) in deep learning.': 1.0986123085021973, 'The proposed approach, based on lower-dimensional feature embeddings, is reasonable and makes applying deep learning methods to data with large d possible.': 1.0986123085021973, 'The paper is well written and the results show improvements over reasonable baselines.': 1.0986123085021973, 'The problem addressed here is practically important (supervised learning with n<<d), and as far as I know, the approach is novel.': 1.0986123085021973, 'I thought their proposed solution was innovative, and I enjoyed the paper.': 1.0986123085021973, 'The presentation is clear and it has nice experiments.': 0.41296857595443726, 'Comments/questions:': 0.9883552193641663, '1) What is the dimensionality of the feature embeddings?': 0.20511996746063232, '2) SNPtoVec still requires training a very fat autoencoder network on X': 0.2157672792673111, ""I suppose this doesn't contribute to the size of the final run-time model and overfitting is avoided because the parameters are fit in an unsupervised manner?"": 1.0986123085021973}"
317,https://openreview.net/forum?id=Sk2Im59ex,"{'This paper presents an unsupervised image transformation method that maps a sample from source domain to target domain.': 1.0986123085021973, 'The major contribution lies in that it does not require aligned training pairs from two domains.': 1.0986123085021973, 'The model is based on GANs.': 1.0986123085021973, 'To make it work in the unsupervised setting, this paper decomposes the generation function into two modules: an encoder that identify a common feature space between two domains and an decoder that generates samples in the target domain.': 1.0986123085021973, 'To avoid trivial solutions, this paper proposed two additional losses that penalize 1) the feature difference between a source sample and its transformed sample and 2) the pixel difference between a target sample and its re-generated sample.': 1.0986123085021973, 'This paper presents extensive experiments on transferring SVHN digit images to MNIST style and transferring face images to emoji style.': 1.0986123085021973, '+The proposed learning method enables unsupervised domain transfer that could be impactful in broad problem contexts.': 1.0986123085021973, '+This paper presents careful ablation studies to analyze the effects of different components of the system, which is helpful for understanding the paper.': 1.0986123085021973, '+The transferred images are visually impressive and quantitative results also show the image identities are preserved across domains to some degree.': 1.0986123085021973, 'It will be more interesting to show results in other domains such as texts and images.': 1.0986123085021973, 'In addition to the face identities, it is also of great interest to analyze how well the facial attributes are preserved when mapping to target domain.': 1.0986123085021973, 'Update: thank you for running more experiments, and add more explanations in the manuscript.': 1.0986123085021973, 'They addressed most of my concerns, so I updated the score accordingly.': 1.0986123085021973, 'The work aims at learning a generative function G that can maps input from source domain to the target domain, such that a given representation function f remain unchanged accepting inputs from either domain.': 1.0986123085021973, 'The criteria is termed f-constancy.': 1.0986123085021973, 'The proposed method is evaluated on two visual domain adaptation tasks.': 1.0986123085021973, 'The paper is relatively easy to follow, and the authors provided quite extensive experimental results on the two datasets.': 1.0986123085021973, 'f-constancy is the main novelty of the work.': 1.0986123085021973, 'It seems counter-intuitive to force the function G to be of g o f, i.e., starting from a restricted function f which might have already lost information.': 1.0986123085021973, 'As in the face dataset, f is learned to optimize the performance of certain task on some external dataset.': 1.0986123085021973, 'It is not clear if an input from the source or target domain can be recovered from applying G as in equation (5) and (6).': 1.0986123085021973, 'Also, the f function is learned with a particular task in mind.': 1.0986123085021973, 'As in the two experiments, the representation function f is learned to identify the digits in the source SVHN dataset or the identity of some face dataset.': 1.0986123085021973, 'As a result, the procedure has to be repeated if we were to perform domain adaptation for the same domains but for different tasks, such as recognizing expressions instead of identity.': 1.0986123085021973, 'Do the authors have insight on why the baseline method proposed in equation (1) and (2) perform so poorly?': 1.0986123085021973, 'Figure 5 shows some visual comparison between style transfer and the proposed method.': 1.0986123085021973, 'It is not clear though which method is better.': 1.0986123085021973, 'Will it be possible to apply style transfer to generate emojis from photos and repeat the experiments shown in table 4?': 1.0986123085021973, ""Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before."": 1.0986123085021973, 'This paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result.': 1.0986123085021973, 'Pros:': 1.0986123085021973, '1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f.': 1.0986123085021973, '2. The proposed method produces visually appealing results on several datasets': 1.0976018905639648, '3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task': 0.9999245405197144, '4. The paper is well-written and easy to read': 1.0623891353607178, 'Cons:': 1.0986123085021973, '1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution)': 0.6643030643463135, '2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain  2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f.': 1.0474433898925781, '3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches.': 1.0584814548492432, 'I would also like to point out that using super-resolved outputs as opposed to the actual model’s outputs can produce a false impression of the visual quality of the transferred samples.': 1.0985291004180908, 'I’d suggest moving original outputs from the appendix into the main part.': 1.0986123085021973}"
318,https://openreview.net/forum?id=Sk2iistgg,"{'The paper proposes a nonlinear regularizer for solving ill-posed inverse problems.': 1.0986113548278809, 'The latent variables (or causal factors) corresponding to the observed data are assumed to lie near a low dimensional subspace in an RKHS induced by a predetermined kernel.': 1.0986123085021973, 'The proposed regularizer can be seen as an extension of the linear low-rank assumption on the latent factors.': 1.0986123085021973, 'A nuclear norm penalty on the Cholesky factor of the kernel matrix is used as a relaxation for the dimensionality of the subspace.': 1.0986123085021973, 'Empirical results are reported on two tasks involving linear inverse problems': 1.0986123085021973, 'missing feature imputation, and estimating non-rigid 3D structures from a sequence of 2D orthographic projections': 1.0430808067321777, 'and the proposed method is shown to outperform linear low-rank regularizer.': 1.036483645439148, 'The clarity of the paper has scope for improvement (particularly, Introduction) - the back and forth b/w dimensionality reduction techniques and inverse problems is confusing at times.': 1.0986123085021973, 'Clearly defining the ill-posed inverse problem first and then motivating the need for a regularizer (which brings dimensionality reduction techniques into the picture) may be a more clear flow in my opinion.': 1.0986123085021973, 'The motivation behind relaxation of rank() in Eq 1 to nuclear-norm in Eq 2 is not clear to me in this setting.': 1.0986121892929077, 'The relaxation does not yield a convex problem over S,C (Eq 5) and also increases the computations (Algo 2 needs to do full SVD of K(S)': 1.0985957384109497, 'every time).': 1.0986123085021973, 'The authors should discuss pros/cons over the alternate approach that fixes the rank of C (which can be selected using cross-validation, in the same way as  is selected), leaving just the first two terms in Eq 5.': 1.097597599029541, 'For this simpler objective, an interesting question to ask would be': 0.4802856743335724, 'are there kernel functions for which it can solved in a scalable manner?': 1.0986117124557495, 'The proposed alternating optimization approach in the current form is computationally intensive and seems hard to scale to even moderate sized data': 1.0986121892929077, 'in every iteration one needs to compute the kernel matrix over S and perform full SVD over the kernel matrix (Algo 2).': 1.0046753883361816, 'Empirical evaluations are also not extensive': 1.0986123085021973, '(i) the dataset used for feature imputation is old and non-standard, (ii) for structure estimation from motion on CMU dataset, the paper only compares with linear low-rank regularization, (iii) there is no comment/study on the convergence of the alternating procedure (Algo 1).': 0.3762449026107788, 'This paper considers an alternate formulation of Kernel PCA with rank constraints incorporated as a regularization term in the objective.': 1.0986121892929077, 'The writing is not clear.': 1.0986123085021973, 'The focus keeps shifting from estimating “causal factors”, to nonlinear dimensionality reduction to Kernel PCA to ill-posed inverse problems.': 1.0986123085021973, 'The problem reformulation of Kernel PCA uses somewhat standard tricks and it is not clear what are the advantages of the proposed approach over the existing methods as there is no theoretical analysis of the overall approach or empirical comparison with existing state-of-the-art.': 1.0986123085021973, 'Not sure what the authors mean by “causal factors”.': 1.0986123085021973, 'There is a reference to it in Abstract and in Problem formulation on page 3 without any definition/discussion.': 1.0986123085021973, 'In KPCA, I am not sure why one is interested in step (iii) outlined on page 2 of finding a pre-image for each': 1.098610281944275, 'Authors outline two key disadvantages of the existing KPCA approach.': 1.0986123085021973, 'The first one, that of low-dimensional manifold assumption not holding exactly, has received lots of attention in the machine learning literature.': 1.0986123085021973, 'It is common to assume that the data lies near a low-dimensional manifold rather than on a low-dimensional manifold.': 1.0986123085021973, 'Second disadvantage is somewhat unclear as finding “a data point (pre-image) corresponding to each projection in the input space” is not a standard step in KPCA.': 1.0986123085021973, 'On page 3, you never define , , .': 1.0986123085021973, 'Clearly, they cannot be cartesian products.': 1.0986123085021973, 'I have to assume that notation somehow implies N-tuples.': 1.0798625946044922, 'On page 3, Section 2,  and  are sets.': 1.0947391986846924, 'What do you mean by': 1.0986123085021973, 'On page 5,  is never defined.': 1.0986123085021973, 'Experiments: None of the standard algorithms for matrix completion such as OptSpace or SVT were considered': 1.0984702110290527, 'Experiments: There is no comparison with alternate existing approaches for Non-rigid structure from motion.': 1.076324224472046, 'Proof of the main result Theorem 3.1:': 0.9902809858322144, 'To get from (16) to (17) using the Holder inequality (as stated) one would end up with a term that involves sum of fourth powers of weights w_{ij}.': 1.069704294204712, 'Why would they equal to one using the orthonormal constraints?': 1.06986665725708, 'It would be useful to give more details here, as I don’t see how the argument goes through at this point.': 0.46602341532707214, 'This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space.': 1.0661849975585938, 'The authors proposed an iterative minimization approach in order to obtain a local optimum of a relaxed problem.': 1.0259952545166016, 'The paper contains errors and the experimental evaluation is not convincing.': 1.0985920429229736, 'Only old techniques are compared against in very toy datasets.': 1.0974111557006836, 'The authors claim state-of-the-art, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches.': 0.4361032247543335, 'The experimental evaluation should demonstrate robustness to more complex noise and outliers, as this was one of the motivations in the introduction.': 0.6607292294502258, 'The authors do not address the out-of-sample problem.': 0.8990516662597656, 'This is a problem of kernel-based methods vs LVMs, and thus should be address here.': 0.7757865786552429, 'The paper contains errors:': 1.0986123085021973, 'The last paragraph of section 1 says that this paper proposes a closed form solution to robust KPCA.': 0.9461482763290405, 'This is simply wrong, as the proposed approach consists of iteratively solving iterativey a set of closed form updates  and Levenberg-Marquard optimizationd.': 1.0986123085021973, 'This is not any more closed form!': 1.0986123085021973, 'In the same paragraph (and later in the text) the authors claim that the proposed approach can be trivially generalized to incorporate other cost functions.': 1.0986123085021973, 'This is not true, as in general there will be no more inner loop closed form updates and the authors will need to solve a much more complex optimization problem.': 1.0986123085021973, 'The third paragraph of section 2 claims that this paper presents a novel energy minimization framework to solve problems of the general form of eq.': 1.0986123085021973, '(2).': 1.0986123085021973, 'However, this is not what the authors solve at the end.': 1.0986123085021973, 'They solve a different problem that has been subject to at least two relaxations.': 1.0986123085021973, 'It is not clear how solving for a local optima of this double relaxed problem is related to the original problem they want to solve.': 1.0986123085021973, 'The paper says that Geiger et al defined non linearities on a latent space of pre-defined dimensionality.': 1.0986123085021973, 'This is wrong.': 1.0986123085021973, 'This paper discovers the dimensionality of the latent space by means of a regularizer that encourages the singular values to be sparse.': 1.0986121892929077, 'Thus, it does not have a fixed dimensionality, the latent space is just bounded to be smaller or equal than the dimensionality of the original space.': 1.0986123085021973, 'It is not clear to me why the author say for LVMs such as GPLVM that ""the latent space is learned a priority with clean training data"".': 1.0986123085021973, 'One can use different noise models within the GP framework.': 1.0986123085021973, 'Furthermore, the proposed approach assumes Gaussian noise (see eq. 6), which is also the trivial case for GP-based LVMs.': 1.0986123085021973, 'It is not clear what the authors mean in the paper by ""pre-training"" or saying that techniques do not have a training phase.': 1.0986123085021973, 'KPCA is trained via a closed-form update, but there is still training.': 1.0986123085021973}"
319,https://openreview.net/forum?id=Sk36NgFeg,"{'This paper aims to characterize the perceptual ability of a neural network under different input conditions.': 1.0986123085021973, 'This is done by manipulating the input image x in various ways (e.g. downsamplig, foveating), and training an auto-encoder to reconstruct the original full-resolution image.': 1.0986123085021973, 'MSE and qualitative results are shown and compared for the different input conditions.': 1.0986123085021973, 'Unfortunately, this paper seems to lack focus, presenting a set of preliminary inspections with few concrete conclusions.': 1.0986123085021973, 'For example, at the end of sec 4.4, ""This result is not surprising, given that FOV-R contains additional information ....': 1.0986123085021973, 'These results suggests that a small number of foveations containing rich details might be all these neural networks need...."".': 1.0986123085021973, 'But this hypothesis is left dangling:': 1.0986123085021973, 'What detailed regions are needed, and from where?': 1.0986123085021973, 'For what sort of tasks?': 1.0986123085021973, ""Secondly, it isn't clear to me what reconstruction behaviors are caused by a fundamental perception of the input, and what are artifacts of the autoencoder and pixelwise l2 loss?"": 1.0986123085021973, 'A prime example is texture, which the autoencoder fails to recover.': 1.0986123085021973, 'But with a pixelwise loss, the network must predict high-frequency textures nearly pixel-for-pixel at training time; if this is impossible, then it will generate a pixelwise average of the training samples': 1.0986123085021973, 'a flat region.': 1.0986123085021973, ""So then the network's inability to reconstruct textures is due to a problem generating them, specifically averaging from the training loss, not necessarily an issue in perceiving textures."": 1.0986123085021973, ""A network trained a different way (perhaps an adversarial network) may infer a texture is there, even if it wouldn't be able to generate it in a pixelwise l2 sense."": 1.0986123085021973, 'Similarly, the ability to perform color reconstruction given a color glimpse I think has much to do with disambiguating the color of an object/scene:': 1.0986123085021973, 'If there is an ambiguity, the network won\'t know which to ""choose"" (white flower or yellow flower?) and output an average, which is why there are so many sepia tones.': 1.0986123085021973, 'However, in its section on this, the paper only measures the reconstruction error for different amounts of color given, and does not drill very far into any hypotheses for why this behavior occurs.': 1.0986123085021973, 'There are some interesting measurements here, such as the amount of color needed in the foveation to reconstruct a color image, and the discussion on global features, which may start to get at a mechanism by which glimpses may propagate to an entire reconstruction.': 1.0986123085021973, ""But overall it's hard to know what to take away from this paper."": 1.0986123085021973, 'What are larger concrete conclusions that can be garnered from the details, and what mechanisms bring them about?': 1.0986123085021973, 'Can these be more thoroughly explored with more focus?': 1.0986123085021973, 'I like the idea the paper is exploring.': 1.0986123085021973, 'Nevertheless I see some issues with the analysis:': 1.0986123085021973, 'To get a better understanding of the quality of the results, I think at least some state-of-the-art comparisons should be included (e.g. by setting d times': 0.41335582733154297, 'd pixel patches too their average and applying a denoising autoencoder).': 1.0986123085021973, 'If they perform significantly better, then this indicates that the presented model is not yet taking all the information from the input image that could be used.': 1.0983619689941406, 'SCT-R and FOV-R are supposed to test how much information can be restored from the Fovea alone as opposed to the Fovea together with low resolution periphery.': 0.4946092665195465, 'However, there is an additional difference between the two conditions: According to the paper, in SCT-R, part of the image was set to zero, while in FOV-R it was removed alltogether.': 1.074475884437561, 'With only one or two hidden layers, I could easily imagine this making a difference.': 1.0986123085021973, 'On page 4, you compare the performance of FOV-R (1% error) with that of DS-D (1.5%) and attribute this to information about the periphery that the autoencoder extracts from the fovea.': 0.7244768142700195, 'While this might be the case, at least part of the reduced error will be due to the fact that the fovea is (hopefully) perfectly reconstructed.': 1.0177003145217896, 'To answer the actual question ""how much additional information about the periphery can be extracted from the fovea"", you should consider calculating the error only in the periphery, i.e. the part of the image where DS-D and FOV-R got exactly the same input for.': 1.0986082553863525, 'Then any decreased error is only due to the additional fovea information.': 1.0986123085021973, 'Other issues:': 1.0986123085021973, 'The images in Figure 2 (a) and (b) in the rows ""factor 2"", ""factor 4"", ""factor 8"" look very blurry.': 1.0986121892929077, 'There seems some interpolation to be going on (although slighly different than the bilinear interpolation).': 1.0986123085021973, 'This makes it hard to asses how much information is in these images.': 1.0986123085021973, 'I think it would be much more insightfull to print them with ""nearest"" interpolation.': 1.0986123085021973, 'Figure 3 caption too vague.': 1.0986123085021973, 'Maybe add something like footnote 2?': 1.0986123085021973, 'Often figures appear too early in paper which leads to lots of distance between text and figures.': 1.0986123085021973, ""This paper is motivated by the ability that human's visual system can recognize contents of environment by from critical features, and tried to investigate whether neural networks can also have this kind of ability."": 1.0986123085021973, 'Specifically, the paper proposed to use Auto-Encoder (AE) as the network to reconstruct the low fidelity of visual input.': 0.9732546806335449, 'Moreover, similar to Mnih et al. (2014),  the paper also proposed to use a recurrent fashion to mimic the sequential behavior the  human visual system.': 0.6171861290931702, 'I think the paper is well motivated.': 1.0986123085021973, 'However, there are several concerns:': 1.0986123085021973, '1. The baselines of the paper are too weak. Nearest neighbor, bilinear, bicubic and cubic interpolations without any learning procedure are of course performed worse than AE based models. The author should compare with the STOA methods such as https://arxiv.org/abs/1609.04802': 1.0983105897903442, '2. Can the experiments based on AE support the idea that artificial neural networks can perceive an image from low fidelity? AE is only a kind of neural network, can the conclusion extend to other kind of networks? I think it would be much better if the authors can provide a more general conclusion.': 1.0983012914657593}"
320,https://openreview.net/forum?id=Sk8J83oee,"{'This paper proposes an extension of the GAN framework known as GAP whereby multiple generators and discriminators are trained in parallel.': 1.0985766649246216, 'The generator/discriminator pairing is shuffled according to a periodic schedule.': 1.0986120700836182, 'Pros:': 1.081081509590149, '+': 1.0985960960388184, 'The proposed approach is simple and easy to replicate.': 0.8443526029586792, 'Cons:': 1.098400592803955, 'The paper is confusing to read.': 1.0976370573043823, 'The results are suggestive but do not conclusively show a performance win for GAP.': 1.0981569290161133, 'The main argument of the paper is that GAP leads to improved convergence and improved coverage of modes.': 1.0944474935531616, 'The coverage visualizations are suggestive but there still is not enough evidence to conclude that GAP is in fact improving coverage.': 1.0986015796661377, 'And for convergence it is difficult to assess the effect of GAP on the basis of learning curves.': 1.0986043214797974, 'The proposed GAM-II metric is circular in that model performance depends on the collection of baselines the model is being compared with.': 1.0986078977584839, 'Estimating likelihood via AIS seems to be a promising way to evaluate, as does using the Inception score.': 1.098515272140503, ""Perhaps a more systematic way to determine GAP's effect would be to set up a grid search of hyperparameters and train an equal number of GANs and GAP-GANs for each setting."": 1.0986119508743286, 'Then a histogram over final Inception scores or likelihood estimates of the trained models would help to show whether GAP tended to produce better models.': 1.0986123085021973, 'Overall the approach seems promising but there are too many open questions regarding the paper in its current form.': 1.0979561805725098, '* Section 2: ""Remark that when...""': 1.0986123085021973, '=> seems like a to-do.': 1.0954821109771729, '* Section A.1: The proposed metric is not described in adequate detail.': 1.098604440689087, 'This paper proposes to address the mode collapsing problem of GANs by training a large set of generators and discriminators, pairing them each up with different ones at different times throughout training.': 1.0986123085021973, 'The idea here is that no one generator-discriminator pair can be too locked together since they are all being swapped.': 1.0986121892929077, 'This idea is nice and is addressing an important issue with GAN training.': 1.0922023057937622, 'However, I think the paper is lacking in experimental results.': 1.0974750518798828, 'In particular:': 0.5340433120727539, 'The authors need to do more work to motivate the GAM metric.': 0.4177086353302002, 'It is not intuitively obvious to me that the GAM metric is a good way of evaluating the generator networks since it relies on the prediction of the discriminator networks which can fixate on artifacts.': 1.0986123085021973, 'Perhaps the authors could explore if the GAM metric correlates with inception scores or human evaluations.': 1.098584532737732, ""Currently the only quantitative evaluation uses this criterion and it really isn't clear it's a relevant quantity to be measuring."": 1.098611831665039, 'Related to the above comment, the authors need to compare more to other methods.': 1.0986123085021973, 'Why not evaluate inception scores and compare with previous methods.': 1.0986123085021973, 'Similarly, generation quality is not compared with previous methods.': 1.0986123085021973, ""It's not obvious that the sample quality is any better with this method."": 1.0986123085021973, 'And now just repeating questions from pre-review section:': 1.0986123085021973, 'If, instead of swapping, you were to simply train K GANs on K splits of the data, or K GANs with differing initial conditions (but without swapping)': 0.7087162733078003, 'do you see any improvement in results?': 0.9950273036956787, 'Similarly, how about if you train larger capacity models with dropout in G and D?': 1.098588228225708, 'Since dropout essentially averages many models it would be interesting to see if the effects are the same.': 1.0985966920852661, 'In figure 6 it appears that the validation costs remain the same as parallelization increase, but the training cost goes up and that is why the gap is shrinking.': 1.0986121892929077, 'Does this really imply better generalization?': 1.0985878705978394, 'In summary, interesting paper that addresses an important issue with GAN training, but compelling results are missing.': 1.098607063293457, 'This paper proposes Generative Adversarial Parallelization (GAP), one schedule to train N Generative Adversarial Networks (GANs) in parallel.': 0.9815978407859802, 'GAP proceeds by shuffling the assignments between the N generators and the N discriminators at play every few epochs.': 1.0332666635513306, 'Therefore, GAP forces each generator to compete with multiple discriminators at random.': 1.0105472803115845, 'The authors claim that such randomization reduces undesired ""mode collapsing behaviour"", typical of GANs.': 1.068655252456665, 'I have three concerns with this submission.': 1.0863929986953735, '1) After training the N GANs for a sufficient amount of time, the authors propose to choose the best generator using the GAM metric.': 0.8391424417495728, 'I oppose to this because of two reasons.': 0.9433196187019348, 'First, a single GAN will most likely be unable to express the full richness of the true data begin modeled.': 0.9786273241043091, 'Said differently, a single generator with limited power will either describe a mode well, or describe many modes poorly.': 0.37033671140670776, 'Second, GAM relies on the scores given by the discriminators, which can be ill-posed (focus on artifacts).': 0.35079485177993774, 'Since there is There is nothing wrong with mode collapsing when this happens under control.': 1.0971122980117798, 'Thus, I believe that a better strategy would be to not choose and combine all generators into a mixture.': 0.4054122269153595, 'Of course, this would require a way to decide on mixture weights.': 0.9409425258636475, 'This can be done, for instance, using rejection sampling based on discriminator scores.': 1.0258994102478027, '2)': 1.0986123085021973, 'The authors should provide a theoretical (or at least conceptual) comparison to dropout.': 0.5703712105751038, 'In essence, this paper has a very similar flavour: every generator is competing against all N discriminators, but at each epoch we drop N-1 for every generator.': 1.0986080169677734, 'Related to the previous point, after training dropout keeps all the neurons, effectively approximating a large ensemble of neural networks.': 1.098610758781433, '3)': 1.0986123085021973, 'The qualitative results are not convincing.': 0.43004488945007324, 'Most of the figures show only results about GAP.': 1.0986121892929077, 'How do the baseline samples look like?': 1.0986123085021973, 'The GAN and LAPGAN papers show very similar samples.': 1.0985442399978638, 'On the other hand, I do not find Figures 3 and 4 convincing: for instance, the generator in Figure 3 was most likely under-parametrized.': 0.7817447185516357, 'As a minor comment, I would remove Figure 2.': 1.0986123085021973, 'This is because of three reasons: it may be protected by copyright, it occupies a lot of space, and it does not add much value to the explanation.': 1.0986123085021973, 'Also, the indices (i_t) are undefined in Algorithm 1.': 1.0986123085021973, 'Overall, this paper shows good ideas, but it needs further work in terms of conceptual development and experimental evaluation.': 1.0986123085021973}"
321,https://openreview.net/forum?id=Sk8csP5ex,"{'This paper extend the Spin Glass analysis of Choromanska et al. (2015a) to Res Nets which yield the novel dynamic ensemble results for Res Nets and the connection to Batch Normalization and the analysis of their loss surface of Res Nets.': 1.0986099243164062, 'The paper is well-written with many insightful explanation of results.': 1.0986090898513794, 'Although the technical contributions extend the Spin Glass model analysis of the ones by Choromanska et al. (2015a), the updated version could eliminate one of the unrealistic assumptions and the analysis further provides novel dynamic ensemble results and the connection to Batch Normalization that gives more insightful results about the structure of Res Nets.': 1.0986123085021973, 'It is essential to show this dynamic behaviour in a regime without batch normalization to untangle the normalization effect on ensemble feature.': 1.0986123085021973, 'Hence authors claim that steady increase in the L_2 norm of the weights will maintain the this feature but setting for Figure 1 is restrictive to empirically support the claim.': 1.0986096858978271, 'At least results on CIFAR 10 without batch normalization for showing effect of L_2 norm increase and results that support claims about Theorem 4 would strengthen the paper.': 1.0986011028289795, 'This work provides an initial rigorous framework to analyze better the inherent structure of the current state of art Res Net architectures and its variants which can stimulate potentially more significant results towards careful understanding of current state of art models (Rather than always to attempting to improve the performance of Res Nets by applying intuitive incremental heuristics, it is important to progress on some solid understanding too).': 1.0986123085021973, 'This paper shows how spin glass techniques that were introduced in Choromanska et al. to analyze surface loss of deep neural networks can be applied to deep residual networks.': 1.0986123085021973, 'This is an interesting contribution but it seems to me that the results are too similar to the ones in Choromanska et al.': 1.0986123085021973, 'and thus the novelty is seriously limited.': 1.0944421291351318, 'Main theoretical techniques described in the paper were already introduced and main theoretical results mentioned there were in fact already proved.': 1.098611831665039, 'The authors also did not get rid of lots of assumptions from Choromanska et al.': 1.073925495147705, '(path-independence, assumptions about weights distributions, etc.).': 1.0986123085021973, 'Summary:': 1.0986123085021973, 'In this paper, the authors study ResNets through a theoretical formulation of a spin glass model.': 1.0986123085021973, 'The conclusions are that ResNets behave as an ensemble of shallow networks at the start of training (by examining the magnitude of the weights for paths of a specific length) but this changes through training, through which the scaling parameter C (from assumption A4) increases, causing it to behave as an ensemble of deeper and deeper networks.': 1.0986123085021973, 'Clarity:': 1.0986123085021973, 'This paper was somewhat difficult to follow, being heavy in notation, with perhaps some notation overloading.': 1.0986123085021973, 'A summary of some of the proofs in the main text might have been helpful.': 1.0986123085021973, 'Specific Comments:': 1.0986123085021973, ""In the proof of Lemma 2, I'm not sure where the sequence beta comes from (I don't see how it follows from 11?)"": 1.0986123085021973, 'The ResNet structure used in the paper is somewhat different from normal with multiple layers being skipped?': 1.0986123085021973, '(Can the same analysis be used if only one layer is skipped?': 1.0986123085021973, 'It seems like the skipping mostly affects the number of paths there are of a certain length?)': 1.0986123085021973, 'The new experiments supporting the scale increase in practice are interesting!': 1.0986123085021973, ""I'm not sure about Theorems 3, 4 necessarily proving this link theoretically however, particularly given the simplifying assumption at the start of Section 4.2?"": 1.0986123085021973}"
322,https://openreview.net/forum?id=SkB-_mcel,"{'The work introduces a new regularization for learning domain-invariant representations with neural networks.': 0.43726518750190735, 'The regularization aims at matching the higher order central moments of the hidden activations of the NNs of the source and target domain.': 0.28834205865859985, 'The authors compared the proposed method vs MMD and two state-of-art NN domain adaptation algorithms on the Amazon review and office datasets, and showed comparable performance.': 1.0877381563186646, 'The idea proposed is simple and straightforward, and the empirical results suggest that it is quite effective.': 1.0986123085021973, 'The biggest limitation I can see with the proposed method is the assumption that the hidden activations are independently distributed.': 1.0932400226593018, 'For example, this assumption will clearly be violated for the hidden activations of convolutional layers, where neighboring activations are dependent.': 0.4111151099205017, 'I guess this is why the authors start with the output of dense layers for the image dataset.': 1.0986123085021973, 'Do the authors have insight on if it is beneficial to start adaptation from lower level?': 1.0986123085021973, 'If so, do the authors have insight on how to relax the assumption?': 1.0986111164093018, 'In these scenarios, if MMD has an advantage as it does not make this assumption?': 0.5875983238220215, 'Figure 3 does not seems to clearly support the boost of performance shown in table 2.': 1.0689806938171387, 'The only class where the new regularization brings the source and target domain closer seem to be the mouse class pointed by the authors.': 1.0986123085021973, 'Is the performance improvement only coming from this single class?': 1.0986114740371704, 'This paper proposed a new metric central moment discrepancy (CMD) for matching two distributions, with applications to domain adaptation.': 1.0986121892929077, 'Compared to a more well-known variant, MMD, CMD has the benefit of not over penalizing the mean, and therefore can focus more on the shape of distribution around the center.': 1.0986121892929077, 'In terms of discriminative power (the ability to tell two distributions apart), MMD and CMD should be equivalent, but in practice I can understand that CMD may be better as MMD tries to match the raw moments which may over penalize data that are not zero centered.': 1.0986123085021973, 'In the paper CMD is used only up to Kth order, and not all the central moments are used, but rather only the diagonal entries are considered in the CMD objective, I think this is mostly motivated for computation efficiency.': 1.0985963344573975, 'A natural comparison with MMD therefore can be made, by also explicitly include raw moments up to Kth order.': 1.0986123085021973, 'Another thing to compare against is to include all moments, not just the diagonal terms, in the objective.': 1.097335934638977, 'This is computationally expensive, but can be done for e.g. 1st and 2nd orders.': 1.0941346883773804, 'Since the experiments only compare CMD in the above form with kernelized MMD, the claim that explicit moment matching is helpful is not very well supported.': 1.0986120700836182, 'To make this a solid claim CMD should be compared against MMD with explicit raw moments.': 1.0986123085021973, 'The claim that the kernel parameter in MMD is hard to tune and CMD does not have such parameters only applies to kernel MMD, not explicit MMD.': 1.0986123085021973, 'For kernel MMD, there are also studies on how to set these parameters, for example:': 1.098611831665039, 'Sriperumbudur et al.  Kernel choice and classifiability for rkhs embeddings of probability distributions.': 1.0924445390701294, 'Gretton et al.': 1.0985530614852905, 'A kernel two-sample test.': 0.4072173833847046, 'and also using multiple kernels (Li et al. 2015) which removes the need to tune them.': 0.7095039486885071, 'Tuning the beta directly like done in this paper is usually not the way MMD is tuned.': 0.40947967767715454, 'At least simple heuristics like dividing |x-y|^2 by dimensionality or mean pairwise distance first should be applied first before trying beta in the way done in this paper.': 1.0918720960617065, 'Overall I think CMD could be better than MMD, and could have applications in many domains.': 1.0927951335906982, 'But it also has the problem of not easily kernelizable (you can argue this both ways though).': 1.0971451997756958, 'The experiments demonstrating that CMD is better could be done more convincinly.': 1.0986123085021973, 'Variational auto-encoders, adversarial networks, and kernel scoring rules like MMD have recently gained popularity as methods for learning directed generative models and for other applications like domain adaptation.': 1.0986087322235107, 'This paper gives an additional method along the scoring rules direction that uses the matching of central moments to match two probability distributions.': 1.0971171855926514, 'The technique is simple, and in the case of domain adaptation, highly effective.': 0.48323580622673035, 'CMD seems like a very nice and straightforward solution to the domain adaptation problem.': 1.0982834100723267, 'The method is computationally straightforward to implement, and seems quite stable with respect to the tuning parameters when compared to MMD.': 0.9492061734199524, 'I was skeptical reading through this, especially given the fact that you only use K=5 in your experiments, but the results seem quite good.': 1.0986106395721436, 'The natural question that I have now is: how will this method do in training generative models?': 1.0986123085021973, 'This is beyond the scope of this paper, but it’s the lowest hanging fruit.': 1.0982873439788818, 'Below I give more detailed feedback.': 1.0986123085021973, 'One way to speed up MMD is to use a random Fourier basis as was done in “Fastmmd: Ensemble of circular discrepancy for efficient two-sample test” by Zhao and Meng, 2015.': 1.0986119508743286, 'There are also linear time estimators, e.g., in “A Kernel Two-Sample Test“ by Gretton et al., 2012.': 1.0986013412475586, 'I don’t think you need to compare against these approaches since you compare to the full MMD, but they should be cited.': 1.09837007522583, 'The paper “Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy” by Sutherland et al. submitted to ICLR 2017 as well, discusses techniques for optimizing the kernel used in MMD and is worth citing in section 3.': 0.7226883172988892, 'How limiting is the assumption that the distribution has independent marginals?': 0.10813215374946594, 'The sample complexity of MMD depends heavily on the dimensionality of the input space - do you have any intuitions about the sample complexity of CMD?': 1.0804520845413208, ""It seems like it's relatively insensitive based on the results in Figure 4, but I would be surprised if this were the case with 10,000 hidden units."": 0.3612976670265198, 'I mainly ask this because with generative models, the output space can be quite high-dimensional.': 1.085935354232788, 'I’m concerned that the central moments won’t be numerically stable at higher orders when backpropagating.': 1.0918467044830322, 'This doesn’t seem to be a problem in the experimental results, but perhaps the authors could comment a bit on this?': 1.0254303216934204, 'I’m referring to the fact that ck(X) can be very large for k >= 3.': 1.0984638929367065, 'Proposition 1 alleviates my concerns that the overall objective is unstable, I’m referring specifically to the individual terms within.': 0.9936646819114685, 'Figure 3 is rather cluttered, and aside from the mouse class it’s not clear to me from the visualization that the CMD regularizer is actually helping.': 0.7596710920333862, 'It would be useful to remove some of the classes for the purpose of visualization.': 1.094165563583374, 'I would like some clarification about the natural geometric interpretations of K=5.': 1.098512053489685, 'Do you mean that the moments up to K=5 have been well-studied?': 0.983176052570343, 'Do you have any references for this?': 1.048995852470398, 'Why does K >= 6 not have a natural geometric interpretation?': 1.0966519117355347, 'Figure 4 should have a legend': 0.6269341111183167}"
323,https://openreview.net/forum?id=SkBsEQYll,"{'This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.': 1.093119502067566, 'Although the paper presents this notion as new, basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities.': 1.0986123085021973, 'Even when looking at the specific case of word/context embeddings, the method is not novel either: this method is almost identical to one of the similarity functions presented in ""A Simple Word Embedding Model for Lexical Substitution"" (Melamud et al., 2015).': 1.0986121892929077, 'The novelty claim must be more accurate and position itself with respect to existing work.': 1.0986123085021973, 'In addition, I think the evaluation could be done better.': 1.0986123085021973, 'There are plenty of benchmarks for word embeddings in context, for example:': 1.0986123085021973, '* http://veceval.com/ (Nayak et al., RepEval 2016).': 1.098609447479248, '* Lexical Substitution in Context': 1.0986123085021973, 'And many higher-level tasks where word similarity in context could be a game-changer:': 1.0986123085021973, '* Semantic Text Similarity': 1.0986120700836182, '* Recognizing Textual Entailment / Natural Language Inference': 1.0985932350158691, 'I was disappointed that none of these were even brought up.': 1.0986123085021973, 'this paper proposes to use feed-forward neural networks to learn similarity preserving embeddings.': 1.0986123085021973, 'They also use the proposed idea to represent out-of-vocabulary words using the words in given context.': 1.0986123085021973, 'First, considering the related work [1,2] the proposed approach brings marginal novelty.': 1.0986123085021973, 'Especially': 1.0986123085021973, 'Context Encoders is just a small improvement over word2vec.': 1.0986123085021973, 'Experimental setup should provide more convincing results other than visualizations and non-standard benchmark for NER evaluation with word vectors [3].': 1.0986123085021973, '[1] http://papers.nips.cc/paper/5477-scalable-non-linear-learning-with-adaptive-polynomial-expansions.pdf': 1.0985167026519775, '[2] http://deeplearning.cs.cmu.edu/pdfs/OJA.pca.pdf': 1.0986123085021973, '[3] http://www.anthology.aclweb.org/P/P10/P10-1040.pdf': 1.0986123085021973, 'This paper introduces a similarity encoder based on a standard feed-forward neural network with the aim of generating similarity-preserving embeddings.': 1.0986123085021973, 'The approach is utilized to generate a simple extension of the CBOW word2vec model that transforms the learned embeddings by their average context vectors.': 1.0986123085021973, 'Experiments are performed on an analogy task and named entity recognition.': 1.0986123085021973, 'While this paper offers some reasonable intuitive arguments for why a feed-forward neural network can generate good similarity-preserving embeddings, the architecture and approach is far from novel.': 1.0986123085021973, 'As far as I can tell, the model is nothing more than the most vanilla neural network trained with SGD on similarity signals.': 1.0986123085021973, 'Slightly more original is the idea to use context embeddings to augment the expressive capacity of learned word representations.': 0.40545451641082764, ""Of course, using explicit contextual information is not a new idea, especially for tasks like word sense disambiguation (see, e.g., 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should also be cited), but the specific method used here is original, as far as I know."": 0.6734420657157898, 'The evaluation of the method is far from convincing.': 0.5861029624938965, 'The corpora used to train the embeddings are far smaller than would ever be used in practice for unsupervised or semi-supervised embedding learning.': 1.0986123085021973, 'The performance on the analogy task says little about the benefit of this method for larger corpora, and, as the authors mentioned in the comments, they expect ""the gain will be less significant, as the global context statistics brought in by the ConEc can also be picked up by word2vec with more training.""': 1.064257264137268, 'The argument can be made (and the authors do claim) that extrinsic evaluations are more important for real-world applications, so it is good to see experiments on NER.': 1.0985968112945557, 'However, again the embeddings were trained on a very small corpus and I am not convinced that the observed benefit will persist when trained on larger corpora.': 1.0986123085021973, 'Overall, I believe this paper offers little novelty and weak experimental evidence supporting its claims.': 1.0986123085021973, 'I cannot recommend it for acceptance.': 1.0986123085021973}"
324,https://openreview.net/forum?id=SkCILwqex,"{'This paper proposes the Layerwise Origin Target Synthesis (LOTS) method, which entails computing a difference in representation at a given layer in a neural network and then projecting that difference back to input space using backprop.': 1.0986123085021973, 'Two types of differences are explored: linear scalings of a single input’s representation and difference vectors between representations of two inputs, where the inputs are of different classes.': 1.0986123085021973, 'In the former case, the LOTS method is used as a visualization of the representation of a specific input example, showing what it would mean, in input space, for the feature representation to be supressed or magnified.': 1.0986123085021973, 'While it’s an interesting computation to perform, the value of the visualizations is not very clear.': 1.0986123085021973, 'In the latter case, LOTS is used to generate adversarial examples, moving from an origin image just far enough toward a target image to cause the classification to flip.': 1.0986123085021973, 'As expected, the changes required are smaller when LOTS targets a higher layer (in the limit of targetting the last layer, results similar to the original adversarial image results would be obtained).': 1.0986123085021973, 'The paper is an interesting basic exploration and would probably be a great workshop paper.': 1.0986123085021973, 'However, the results are probably not quite compelling enough to warrant a full ICLR paper.': 1.0986123085021973, 'A few suggestions for improvement:': 1.0986123085021973, '- Several times it is claimed that LOTS can be used as a method for mining for diverse adversarial examples that could be used in training classifiers more robust to adversarial perturbation. But this simple experiment of training on LOTS generated examples isn’t tried. Showing whether the LOTS method outperforms, say, FGS would go a long way toward making a strong paper.': 1.0986123085021973, '- How many layers are in the networks used in the paper, and what is their internal structure? This isn’t stated anywhere. I was left wondering whether, say, in Fig 2 the CONV2_1 layer was immediately after the CONV1_1 layer and whether the FC8 layer was the last layer in the network.': 1.0986056327819824, '- In Fig 1, 2, 3, and 4, results of the application of LOTS are shown for many intermediate layers but miss for some reason applying it to the input (data) layer and the output/classification (softmax) layer. Showing the full range of possible results would reinforce the interpreatation (for example, in Fig 3, are even larger perturbations necessary in pixel space vs CONV1 space? And does operating directly in softmax space result in smaller perturbations than IP2?)': 1.0983601808547974, '- The PASS score is mentioned a couple times but never explained at all. E.g. Fig 1 makes use of it but does not specify such basics as whether higher or lower PASS scores are associated with more or less severe perturbations. A basic explanation would be great.': 1.0983339548110962, '- 4.2 states “In summary, the visualized internal feature representations of the origin suggest that lower convolutional layers of the VGG Face model have managed to learn and capture features that provide semantically meaningful and interpretable representations to human observers.” I don’t see that this follows from any results. If this is an important claim to the paper, it should be backed up by additional arguments or results.': 1.0985229015350342, '1/19/17 UPDATE AFTER REBUTTAL:': 1.098595142364502, ""Given that experiments were added to the latest version of the paper, I'm increasing my review from 5 -> 6."": 1.09746515750885, 'I think the paper is now just on the accept side of the threshold.': 1.0985321998596191, 'The paper presents a new exciting layerwise origin-target synthesis method both for generating a large number of diverse adversarials as well as for understanding the robustness of various layers.': 1.0980814695358276, 'The methodology is then used to visualize the amount of perturbation necessary for producing a change for higher level features.': 0.972228467464447, 'The approach to match the features of another unrelated image is interesting and it goes beyond producing adversarials for classification.': 1.0985981225967407, 'It can also generate adversarials for face-recognition and other models where the result is matched with some instance from a database.': 1.0986121892929077, 'Pro: The presented approach is definitely sound, interesting and original.': 1.0986123085021973, ""Con: The analyses presented in this paper are relatively shallow and don't touch the most obvious questions."": 1.0986123085021973, 'There is not much experimental quantitative evidence for the efficacy of this method compared with other approaches to produce adversarials.': 1.0986123085021973, 'The visualization is not very exciting and it is hard to any draw any meaningful conclusions from them.': 1.0986123085021973, 'It would definitely improve the paper if it would present some interesting conclusions based on the new ideas.': 1.0986123085021973, 'This paper presents a relatively novel way to visualize the features / hidden units of a neural network and generate adversarial examples.': 1.0986123085021973, 'The idea is to do gradient descent in the pixel space, from a given hidden unit in any layer.': 1.0986123085021973, 'This can either be done by choosing a pair of images and using the difference in activations of the unit as the thing to do gradient descent over or just the activation itself of the unit for a given image.': 1.0986123085021973, 'In general this method seems intriguing, here are some comments:': 1.0986123085021973, 'It’s not clear that some of the statements at the beginning of Sec 4.1 are actually true, re: positive/negative signs and how that changes (or does not change) the class.': 1.0986123085021973, 'Mathematically, I don’t see why that would be the case?': 1.0986123085021973, 'Moreover the contradictory evidence from MNIST vs. faces supports my intuition.': 1.0986123085021973, 'The authors use the PASS score through the paper, but only given an intuition + citation for it.': 1.0986123085021973, 'I think it’s worth explaining what it actually does, in a sentence or two.': 1.0986123085021973, 'The PASS score seems to have some, but not complete, correlation with L_2, L_\\{infty} or visual estimation of how “good” the adversarial examples are.': 1.0986123085021973, 'I am not sure what the take-home message from all these numbers is.': 1.0986123085021973, '“In general, LOTS cannot produce high quality adversarial examples at the lower layers” (sec 5.2) seems false for MNIST, no?': 1.0986123085021973, 'I would have liked this work to include more quantitative results (e.g., extract adversarial examples at different layers, add them to the training set, train networks, compare on test set), in addition to the visualizations present.': 1.0986123085021973, 'That to me is the main drawback of the paper, in addition to basically no comparisons with other methods (it’s hard to judge the merits of this work in vacuum).': 1.0986123085021973, 'EDIT after rebuttal:': 1.0986123085021973, 'thanks to the authors for addressing the experimental validation concerns.': 1.0986123085021973, 'I think this makes the paper more interesting, so revising my score accordingly.': 1.0986123085021973}"
325,https://openreview.net/forum?id=SkC_7v5gx,"{'The paper is about channel sparsity in Convolution layer.': 1.0986123085021973, 'The paper is well written and it elaborately discussed and investigated different approaches for applying sparsity.': 1.0985254049301147, 'The paper contains detailed literature review.': 1.0986120700836182, 'In result section, it showed the approach gives good results using 60% sparsity with reducing number of parameters, which can be useful in some embedded application with limited resource i.e. mobile devices.': 1.0986123085021973, 'The main point is that the paper needs more detailed investigation on different dropout schedule.': 1.0986038446426392, 'As mentioned implementation details section, they deactivate the connections by applying masks to parameter tensors, which is not helpful in speeding up the training and computation in convolution layer.': 1.0986121892929077, 'They can optimize implementation to reduce computation time.': 1.0945879220962524, 'This paper aims to improve efficiency of convolutional networks by using a sparse connection structure in the convolution filters at each layer.': 1.09844172000885, 'Experiments are performed using MNIST, CIFAR-10 and ImageNet, comparing the sparse connection kernels against dense convolution kernels with about the same number of connections, showing the sparse structure (with more feature maps) generally performs better for similar numbers of parameters.': 1.098569631576538, 'Unfortunately, any theoretical efficiencies are not realized, since the implementation enforces the sparse structure using a zeroing mask on the weights.': 1.0986106395721436, 'In addition, although the paper mentions that this method can be implemented efficiently and take advantage of contiguous memory reads/writes of current architectures, I still find it unclear whether this would be the case:  The number of activation units is no smaller than when using a dense convolution of same dimension, and these activations (inputs and outputs) must be loaded/stored.': 1.0986123085021973, 'The fact that the convolution is sparse saves only on the multiply/addition operation cost, not memory access for the activations, which can often be the larger amount of time spent.': 1.0986075401306152, 'The section on incremental training is interesting, but feels short and preliminary, and any gains here also have yet to be realized.': 1.0986098051071167, 'The precision is no better than for the original network, and as mentioned above, the implementation of the sparse structure is no faster than the original.': 1.0985926389694214, 'Overall, the method and evaluations show that the basic approach has promise.': 1.0927679538726807, 'However, it is unclear how real gains (in either speed or accuracy) might actually be found with it.': 1.0984903573989868, 'Without this last step, it still seems incomplete to me for a conference paper.': 1.0986123085021973, 'The paper experiments with channel to channel sparse neural networks.': 1.0986123085021973, 'The paper is well written and the analysis is useful.': 1.0986123085021973, 'The sparse connection is not new but has not been experimented on large-scale problems like ImageNet.': 1.0986123085021973, 'One of the reasons for that is the unavailability of fast implementations of randomly connected convolutional layers.': 1.0986123085021973, 'The results displayed in figures 2, 3, 4, and, 5 show that sparse connections need the same number of parameters as the dense networks to reach to the best performance on the given tasks, but can provide better performance when there is a limited budget for the #parameters and #multiplyAdds.': 1.0986123085021973, 'This paper is definitely informative but it does not reach to the conference acceptance level, simply because the idea is not new, the sparse connection implementation is poor, and the results are not very surprising.': 1.0986123085021973}"
326,https://openreview.net/forum?id=SkJeEtclx,"{'The authors propose a ""hierarchical"" attention model for video captioning.': 1.0986123085021973, 'They introduce a model composed of three parts: the temporal modeler (TEM) that takes as input the video sequence and outputs a sequential representation of the video to the HAM; the hierarchical attention/memory mechanism (HAM) implements a soft-attention mechanism over the sequential video representation; and finally a decoder that generates a caption.': 1.0986123085021973, 'Related to the second series of questions above, it seems as though the authors have chosen to refer to their use of an LSTM (or equivalent RNN) as the output of the Bahdanau et al (2015) attention mechanism as a hierarchical memory mechanism.': 1.0986123085021973, 'I am actually sympathetic to this terminology in the sense that the recent popularity of memory-based models seems to neglect the memory implicit in the LSTM state vector, but that said, this seems to seriously misrepresent the significance fo the contribution of this paper.': 1.0986123085021973, 'I appreciate the ablation study presented in Table 1.': 1.0986123085021973, 'Not enough researchers bother with this kind of analysis.': 1.0986123085021973, 'But it does show that the value of the contributions is not actually clear.': 1.0986123085021973, 'In particular the case for the TEM is quite weak.': 1.0986123085021973, 'Regarding the quantitative evaluation presented in Table 2, the authors are carving out a fairly specific set of features to describe the set of ""fair"" comparators from the literature.': 1.0986123085021973, 'Given the variability of the models and alternate training datasets that are in use, I would find it more compelling if the authors just set about trying to achieve the best results they can, if that includes the fine-tuning of the frame model, so be it.': 1.0986123085021973, 'The value of this work is as an application paper, so the discovery and incorporation of elements that can significantly improve performance would seems warranted.': 1.0986123085021973, 'Overall, at this point, I do not see a sufficient contribution to warrant publication in ICLR.': 1.0986123085021973, 'The paper proposes an attention-based approach for video description.': 1.0986123085021973, 'The approach uses three LSTMs and two attention mechanisms to sequentially predict words from a sequence of frames.': 1.0986123085021973, 'In the LSTM-encoder of the frames (TEM), the first attention approach predicts a spatial attention per frame, and computes the weighted average.': 1.0986123085021973, 'The second LSTM (HAM) predicts an attention over the hidden states of the encoder LSTM.': 1.0986123085021973, 'The third LSTM which run temporally in parallel to the second LSTM generates the sentence, one word at a time.': 1.0986123085021973, 'Strength:': 1.0986123085021973, '===============': 1.0986123085021973, 'The paper works on a relevant and interesting problem.': 1.0986123085021973, 'Using 2 layers of attention in the proposed way have to my knowledge not been used before for video description.': 1.0986123085021973, 'The exact architecture is thus novel (but the work claims much more without sufficient attribution, see blow)': 1.0986123085021973, 'The experiments are evaluated on two datasets, MSVD and Charades, showing performance on the level of related work for MSVD and improvements for Charades.': 1.0986123085021973, 'Weaknesses:': 1.0986123085021973, '1. Claims about the contribution/novelty of the model seem not to hold:': 1.0986123085021973, '1.1. One of the main contributions is the Hierarchical Attention/Memory (HAM):': 1.0986123085021973, '1.1.1. It is not clear to me how the presented model (Eq 6-8), are significantly different from the presented model in Xu et al / Yao et al. While Xu et al. attends over spatial image locations and Yao et al. attend over frames, this model attends over encoded video representations h_v^i. A slight difference might be that Xu et al. use the same LSTM to generate, while this model uses an additional LSTM for the decoding.': 1.0986123085021973, '1.1.2. The paper states in section 3.2 “we propose f_m to memorize the previous attention”, however H_m^{t’-1} only consist of the last hidden state. Furthermore, the model f_m does not have access to the “attention” \\alpha. This was also discussed in comments by others, but remains unclear.': 1.0986123085021973, '1.1.3. In the discussion of comments the authors claim that “attention not only is a function a current time step but also a function of all previous attentions and network states.”: While it is true that there is a dependency but that is true also for any LSTM, however the model does not have access to the previous network states as H_g^{t’-1} only consist of the last hidden state, as well as H_m^{t’-1} [at least that is what the formulas say and what Figure 1 suggests].': 1.0986123085021973, '1.1.4. The authors claim to have multi-layer attention in HAM, however it remains unclear where the multi-layer comes from.': 1.0986123085021973, '1.2. The paper states that in section 3.1. “[CNN] features tend to discard the low level information useful in modeling the motion in the video (Ballas et al., 2016).” This suggests that the approach which follows attacks this problem. However, it cannot model motion as attention \\rho between frames is not available when predicting the next frame. Also, it is not clear how the model can capture anything “low level” as it operates on rather high level VGG conv 5 features.': 1.0986123085021973, '2. Related work: The difference of HAM to Yao et al. and Xu et al. should be made more clear / or these papers should be cited in the HAM section.': 1.0986123085021973, '3. Conceptual Limitation of the model: The model has two independent attention mechanisms, a spatial one, and a temporal one. The spatial (within a frame) is independent of the sentence generation. It thus cannot attend to different aspects of the frames for different words which would make sense. E.g. if the sentence is “the dog jumps on the trampoline”, the model should focus on the dog when saying “dog” and on the trampoline when saying “trampoline”, however, as the spatial attention is fixed this is difficult. Also, the encoder model does not have an explicitly way to look at different aspects in the frame during the encoding so might likely get stuck and always predict the same spatial attention for all frames (or it might e.g. always attend to the dog which moves around, but never on the scene).': 0.8089240193367004, '4. Eq 11 contradicts Fig 1: How is the model exactly receiving the previous word as input. Eq. 11 suggests it is the softmax. If this is the case, the authors should emphasize this in the text as this is unusual. More common would be to use the ground truth previous word during training (which Fig 11 suggests) and the “hardmax”, i.e. the highest predicted previous word encoded as one-hot vector at test time.': 0.8495758771896362, '5. Clarity:': 1.0986109972000122, '5.1. It would be helpful if the same notation would be used in Eq 2-5 and 6-9. Why is a different notation required?': 1.0984488725662231, '5.2. It would be helpful if Fig 1 could contain more details or additional figures for the corresponding parts would be added. If space is a problem, e.g. the well-known equations for LSTM, softmax (Eq 2), and log likelihood loss (Eq 12) could be omitted or inlined.': 0.4517720639705658, '6. Evaluation:': 1.0986123085021973, '6.1. The paper claims that the “the proposed architecture outperforms all previously proposed methods and leads to a new state of the art results”.': 1.0986088514328003, '6.1.1. For the MSVD dataset this clearly is wrong, even given the same feature representation. Pan et al. (2016 a) in Table 2 achieve higher METEOR (33.10).': 1.0986109972000122, '6.1.2. For this strong claim, I would also expect that it outperforms all previous results independent of the features used, which is also wrong again, Yu et al achieve higher performance in all compared metrics.': 1.0986101627349854, '6.1.3. For Charades dataset, this claim is also too bold as hardly any methods have been evaluated on this dataset, so at least all the ablations reported in Table 1 should also be reported for the Charades dataset, to make for this dataset any stronger claims.': 1.0985801219940186, '6.2. Missing qualitative results of attention: The authors should show qualitative results of the attention, for both attention mechanisms to understand if anything sensible is happening there. How diverse is the spatial and the temporal attention? Is it peaky or rather uniform?': 0.7499388456344604, '6.3. Performance improvement is not significant over model ablations: The improvements over Att+No TEM is only 0.5 Meteor, 0.7 Blue@4 and the performance drops for CIDEr by 1.7.': 0.7055786848068237, '6.4. Missing human evaluation: I disagree with the authors that a human evaluation is not feasible. 1. An evaluation on a subset of the test data is not so difficult. 2. Even if other authors do not provide their code/model [and some do], they are typically happy to share the predicted sentences which is sufficient and even better for human evaluation [if not I would explicitly mention that some authors did not share sentences, as this seems clearly wrong]. 3. For model ablations the sentences are available to the authors.': 1.0406137704849243, '7. Several of the comments raised by reviewers/others have not yet been incorporated in a revised version of the paper and/or are still not clear from the explanations given. E.g. including SPICE evaluation and making fixes seems trivial.': 0.9996515512466431, '8. Hyperparameters are inconsistent: Why are the hyperparemters inconsistent between the ablation analysis (40 frames are sampled) and the performance comparison (8 frames)? Should this not be selected on the validation set? What is the performance of all the ablations with 8 frames?': 1.096763014793396, 'Other (minor/discussion points)': 1.0986123085021973, 'Equation 10: what happens with h_m, and h_g, the LSTM formulas provided only handle two inputs.': 1.0986123085021973, 'Are h_m and h_g concatenated.': 1.0621050596237183, 'There is a section 4.1 but no 4.2.': 1.0983397960662842, 'The paper states in section 4.1 “our proposed architecture can alone not only learn a representation for video that can model the temporal structure of a video sequence, but also a representation that can effectively map visual space to the language space.”': 1.0983951091766357, 'However, this seems to be true also for many/most other approaches, e.g.': 1.0985937118530273, '[Venugopalan et al. 2015 ICCV]': 1.0986123085021973, 'Summary:': 1.0986123085021973, 'While the paper makes strong claims w.r.t.': 1.0872788429260254, 'to the approach and results, the approach lacks novelty and the results are not convincing over related work and ablations.': 1.0986123085021973, 'Furthermore, improved clarity and visualizations of the model and attention results would benefit the paper.': 1.0986119508743286, 'This paper addresses video captioning with a TEM-HAM architecture, where a HAM module attends over attended outputs of the TEM module when generating the description.': 1.0957585573196411, 'This gives a kind of 2-level attention.': 1.0986123085021973, 'The model is evaluated on the Charades and MSVD datasets.': 0.45681941509246826, '1. Quality/Clarify: I found this paper to be poorly written and relatively hard to understand. As far as I can tell the TEM module of Section 3.1 is a straight-forward attention frame encoder of Bahdanau et al. 2015 or Xu et al. 2015. The decoder of Section 3.3 is a standard LSTM with log likelihood. The HAM module of Section 3.2 is the novel module but is not very well described. It looks to be an attention LSTM where the attention is over the TEM LSTM outputs, but the attention weights are additionally conditioned on the decoder state. There are a lot of small problems with the description, such as notational discrepancy in using \\textbf in equations and then not using it in the text. Also, I spent a long time trying to understand what f_m is. The authors say:': 0.6127673983573914, '""In order to let the network remember what has been attended before and the temporal': 1.0986123085021973, 'structure of a video, we propose f_m to memorize the previous attention and encoded version of an': 0.7070982456207275, 'input video with language model.': 1.0986123085021973, 'Using f_m not only enables the network to memorize previous': 1.0986123085021973, 'attention and frames, but also to learn multi-layer attention over an input video and corresponding': 1.0986123085021973, 'language.""': 1.0986123085021973, 'Where one f_m is bold and the other f_m is not.': 1.0986123085021973, 'Due to words such as ""we propose f_m"" assumed this was some kind of a novel technical contribution I couldn\'t find any details about but it is specified later in Section 3.3 at the end that f_m is in fact just an LSTM.': 1.0986123085021973, ""It's not clear why this piece of information is in Section 3.3, which discusses the decoder."": 1.0986123085021973, 'The paper is sloppy in other parts.': 1.0986032485961914, 'For example in Table 1 some numbers have 1 significant digit and some have 2.': 0.7660512924194336, 'The semantics of the horizontal line in Table 2 are not explained in text.': 0.48678669333457947, '2. Experimental results: The ablation study shows mixed results when adding TEM and HAM to the model. Looking at METEOR which was shown to have the highest correlation to humans in the COCO paper compared to the other evaluation criteria, adding TEM+HAM improves the model from 31.20 to 31.70. It is not clear how significant this improvement is, especially given that the test set is only 670 videos. I have doubts over this result. In Table 2, the METEOR score of Pan et al. 2016a is higher [33.10 vs. 31.80], but this discrepancy is not addressed in text. This is surprising because the authors explicitly claim ""state of the art results"".': 1.0971862077713013, ""3. Originality/Significance: The paper introduces an additional layer of attention over a more standard sequence to sequence setup, which is argued to alleviate the burden on the LSTM's memory. This is moderately novel but I don't believe that the experimental results make it sufficiently clear that it is also worth doing. If the paper made the standard model somehow simpler instead of more complex I would be more inclined to judge it favorably."": 1.0943214893341064, 'Minor:': 1.0986123085021973, 'In response to the author\'s comment ""not sure what causes to think of RBM.': 1.0986123085021973, ""We don't model any part of our architecture using RBM."": 1.0986123085021973, ""We'd be appreciated if you please elaborate more about your confusion about figure 1 so we can address it accordingly."": 1.0986123085021973, '"", I created a diagram to hopefully make this more clear:  https://imgur.com/a/4MJaG .': 1.0986123085021973, 'It is very common to use 2 rows of circles with lines between them pairwise to denote an RBM.': 1.0986123085021973, 'The lines indicate undirected edges of the graphical model.': 1.0986123085021973, 'A typical RBM diagram can have, for example, 3 circles on top and 4 circles on the bottom joined with edges.': 1.0986123085021973, 'Your diagram has 4 circles on the top and 5 circles on the bottom joined with edges.': 1.0986123085021973, 'However, not all of your circles from the top and bottom are connected, which further adds to the difficulty.': 1.0986123085021973, 'In particular, your last circle on the bottom is only connected to 2 circles on the top.': 1.0986123085021973, 'If the authors are trying to denote a neural network I would advise using arrows instead of lines.': 1.0986123085021973, 'However, since the HAM module uses an LSTM just like the encoder and the decoder it is unclear why this module has a visually distinct appearance at all.': 1.0986123085021973}"
327,https://openreview.net/forum?id=SkXIrV9le,"{'This paper proposes a generative model of videos composed of a background and a set of 2D objects (sprites).': 1.0986121892929077, 'Optimization is performed under a VAE framework.': 1.0974000692367554, ""The authors' proposal of an outer product of softmaxed vectors (resulting in a 2D map that is delta-like), composed with a convolution, is a very interesting way to achieve translation of an image with differentiable parameters."": 1.0986123085021973, 'It seems to be an attractive alternative to more complicated differentiable resamplers (such as those used by STNs) when only translation is needed.': 1.0986061096191406, 'Below I have made some comments regarding parts of the text, especially the experiments, that are not clear.': 1.0986069440841675, 'The experimental section in particular seems rushed, with some results only alluded to but not given, not even in the appendix.': 1.0982593297958374, 'For an extremely novel and exotic proposal, showing only synthetic experiments could be excused.': 1.0986123085021973, ""However, though there is some novelty in the method, it is disappointing that there isn't even an attempt at trying to tackle a problem with real data."": 1.0983630418777466, 'I suggest as an example aerial videos (such as those taken from drone platforms), since the planar assumption that the authors make would most probably hold in that case.': 1.0986123085021973, 'I also suggest that the authors do another pass at proof-reading the paper.': 1.0986123085021973, 'There are missing references (""Fig. ??': 1.0986123085021973, '""), unfinished sentences (caption of Fig. 5), and the aforementioned issues with the experimental exposition.': 0.9520706534385681, 'This paper presents an approach to modeling videos based on a decomposition into a background + 2d sprites with a latent hidden state.': 1.0986123085021973, 'The exposition is OK, and I think the approach is sensible, but the main issue with this paper is that it is lacking experiments on non-synthetic datasets.': 1.0986121892929077, ""As such, while I find the graphics inspired questions the paper is investigating interesting, I don't think it is clear that this work introduces useful machinery for modeling more general videos."": 1.0983048677444458, 'I think this paper is more appropriate as a workshop contribution in its current form.': 1.0986123085021973, 'This paper presents a generative model of video sequence data where the frames are assumed to be generated by a static background with a 2d sprite composited onto it at each timestep.': 1.0979275703430176, 'The sprite itself is allowed to dynamically change its appearance and location within the image from frame to frame.': 1.0978091955184937, 'This paper follows the VAE (Variational Autoencoder) approach, where a recognition/inference network allows them to recover the latent state at each timestep.': 1.0986073017120361, 'Some results are presented on simple synthetic data (such as a moving rectangle on a black background or the “Moving MNIST” data.': 1.0983271598815918, 'However, the results are preliminary and I suspect that the assumptions used in the paper are far too strong too be useful in real videos.': 1.09861159324646, 'On the Moving MNIST data, the numerical results are not competitive to state of the art numbers.': 1.0985426902770996, 'The model itself is also not particularly novel and the work currently misses some relevant citations.': 1.0605696439743042, 'The form of the forward model, for example, could be viewed as a variation on the DRAW paper by Gregor et al (ICML 2014).': 1.0234520435333252, 'Efficient Inference in Occlusion-Aware Generative Models of Images by Huang & Murphy (ICLR) is another relevant work, which used a variational auto-encoder with a spatial transformer and an RNN-like sequence model to model the appearance of multiple sprites on a background.': 1.0474135875701904, 'Finally, the exposition in this paper is short on many details and I don’t believe that the paper is reproducible from the text alone.': 1.0390716791152954, 'For example, it is not clear what the form of the recognition model is…  Low-level details (which are very important) are also not presented, such as initialization strategy.': 1.0982496738433838}"
328,https://openreview.net/forum?id=SkYbF1slg,"{'This paper presents an information theoretic framework for unsupervised learning.': 1.093947172164917, 'The framework relies on infomax principle, whose goal is to maximize the mutual information between input and output.': 1.0986086130142212, 'The authors propose a two-step algorithm for learning in this setting.': 1.0979126691818237, 'First, by leveraging an asymptotic approximation to the mutual information, the global objective is decoupled into two subgoals whose solutions can be expressed in closed form.': 1.0986123085021973, 'Next, these serve as the initial guess for the global solution, and are refined by the gradient descent algorithm.': 1.0986121892929077, 'While the story of the paper and the derivations seem sound, the clarity and presentation of the material could improve.': 1.098456859588623, 'For example, instead of listing step by step derivation of each equation, it would be nice to first give a high-level presentation of the result and maybe explain briefly the derivation strategy.': 1.0986123085021973, 'The very detailed aspects of derivations, which could obscure the underlying message of the result could perhaps be postponed to later sections or even moved to an appendix.': 1.0986123085021973, 'A few questions that the authors may want to clarify:': 1.0986123085021973, '1. Page 4, last paragraph: ""from above we know that maximizing I(X;R) will result in maximizing I(Y;R) and I(X,Y^U)"". While I see the former holds due to equality in 2.20, the latter is related via a bound in 2.21. Due to the possible gap between I(X;R) and I(X,Y^U), can your claim that maximizing of the former indeed maximizes the latter be true?': 0.3805839717388153, '2. Paragraph above section 2.2.2: it is stated that, dropout used to prevent overfitting may in fact be regarded as an attempt to reduce the rank of the weight matrix. No further tip is provided why this should be the case. Could you elaborate on that?': 0.4997318983078003, '3. At the end of page 9: ""we will discuss how to get optimal solution of C for two specific cases"". If I understand correctly, you actually are not guaranteed to get the optimal solution of C in either case, and the best you can guarantee is reaching a local optimum. This is due to the nonconvexity of the constraint 2.80 (quadratic equality). If optimality cannot be guaranteed, please correct the wording accordingly.': 0.7683840394020081, 'This paper proposes a hierarchical infomax method.': 1.0977729558944702, 'My comments are as follows:': 0.5247825384140015, '(1) First of all, this paper is 21 pages without appendix, and too long as a conference proceeding.': 1.0986121892929077, 'Therefore, it is not easy for readers to follow the paper.': 1.0986123085021973, 'The authors should make this paper as compact as possible while maintaining the important message.': 1.0986123085021973, '(2) One of the main contribution in this paper is to find a good initialization point by maximizing I(X;R).': 0.9773247838020325, 'However, it is unclear why maximizing I(X;\\breve{Y}) is good for maximizing I(X;R) because Proposition 2.1 shows that I(X;\\breve{Y}) is an “upper” bound of I(X;R)': 1.098526120185852, '(When it is difficult to directly maximize a function, people often maximize some tractable “lower” bound of it).': 0.4952392578125, 'Minor comments:': 1.0986123085021973, '(1) If (2.11) is approximation of (2.8), “\\approx” should be used.': 1.043383240699768, '(2) Why K_1 instead of N in Eq.(2.11)?': 0.543137788772583, '(3) In Eq.(2.12), H(X) should disappear?': 1.097926139831543, '(4) Can you divide Section 3 into subsections?': 1.098488688468933, 'This is an 18 page paper plus appendix which presents a mathematical derivation for infomax for an actual neural population with noise.': 1.098323941230774, 'The original Bell & Sejnowski infomax framework only considered the no noise case.': 1.0986123085021973, 'Results are shown for natural image patches and the mnist dataset, which qualitatively resemble results obtained with other methods.': 1.0986123085021973, 'This seems like an interesting and potentially more general approach to unsupervised learning.': 1.0986123085021973, 'However the paper is quite long and it was difficult for me to follow all the twists and turns.': 1.0986123085021973, 'For example the introduction of the hierarchical model was confusing and it took several iterations to understand where this was going.': 1.0986123085021973, ""'Hierarchical' is probably not the right terminology here because it's not like a deep net hierarchy, it's just decomposing the tuning curve function into different parts."": 1.0986123085021973, 'I would recommend that the authors try to condense the paper so that the central message and important steps are conveyed in short order, and then put the more complete mathematical development into a supplementary document.': 1.0986123085021973, 'Also, the authors should look at the work of Karklin & Simoncelli 2011 which is highly related.': 1.0986123085021973, 'They also use an infomax framework for a noisy neural population to derive on and off cells in the retina, and they show the conditions under which orientation selectivity emerges.': 1.0986123085021973}"
329,https://openreview.net/forum?id=SkgSXUKxx,"{'Summary': 1.0986123085021973, '===': 1.0986123085021973, 'This paper extends and analyzes the gradient regularizer of Hariharan and': 1.0986123085021973, 'Girshick 2016.': 1.0986123085021973, 'In that paper a regularizer was proposed which penalizes': 1.0986123085021973, 'gradient magnitudes and it was shown to aid low-shot learning performance.': 1.0986123085021973, 'This work shows that the previous regularizer is equivalent to a direct penalty': 1.0986123085021973, 'on the magnitude of feature values weighted differently per example.': 1.0986123085021973, 'The analysis goes to to provide two examples where a feature penalty': 1.0986123085021973, 'favors a better representation.': 1.0986123085021973, 'The first example addresses the XOR': 1.0986123085021973, 'problem, constructing a network where a feature penalty encourages': 1.0986123085021973, 'a representation where XOR is linearly separable.': 1.0986123085021973, 'The second example analyzes a 2 layer linear network, showing improved stability': 1.0986123085021973, 'of a 2nd order optimizer when the feature penalty is added.': 1.0986123085021973, 'One last bit of analysis shows how this regularizer can be interpreted as': 1.0986123085021973, 'a Gaussian prior on both features and weights.': 1.0986123085021973, 'Since the prior can be': 1.0986123085021973, 'interpreted as having a soft whitening effect, the feature regularizer': 1.0986123085021973, 'is like a soft version of Batch Normalization.': 1.0986123085021973, 'Experiments show small improvements on a synthetic XOR test set.': 1.0986123085021973, 'On the Omniglot dataset feature regularization is better than most baselines,': 1.0986123085021973, 'but is worse than Moment Matching Networks.': 1.0986123085021973, 'An experiment on ImageNet similar': 1.0986123085021973, 'to Hariharan and Girshick 2016 also shows effective low-shot learning.': 1.0986123085021973, 'Strengths': 1.0986123085021973, '*': 1.0986123085021973, 'The core proposal is a simple modification of Hariharan and Girshick 2016.': 1.0986123085021973, 'The idea of feature regularization is analyzed from multiple angles': 1.0986123085021973, 'both theoretically and empirically.': 1.0986123085021973, 'The connection with Batch Normalization could have broader impact.': 1.0986123085021973, 'Weaknesses': 1.0986123085021973, 'In section 2 the gradient regularizer of Hariharan and Girshick is introduced.': 1.0986031293869019, 'While introducing the concept, some concern is expressed about the motivation:': 1.0986011028289795, '""And it is not very clear why small gradients on every sample produces': 0.47906801104545593, 'good generalization experimentally.""': 0.1433299481868744, 'This seems to be the central issue to me.': 0.5351271033287048, 'The paper details some related analysis, it does not offer a clear answer to': 0.4872865676879883, 'this problem.': 0.5705007910728455, '* The purpose and generality of section 2.1 is not clear.': 0.8097741007804871, 'The analysis provides a specific case (XOR with a non-standard architecture)': 0.9352636337280273, 'where feature regularization intuitively helps learn a better representation.': 0.9078260660171509, 'However, the intended take-away is not clear.': 0.05672858655452728, 'The take-away may be that since a feature penalty helps in this case it': 1.06402587890625, 'should help in other cases.': 1.0976905822753906, 'I am hesitant to buy that argument because of the': 1.0831760168075562, 'specific architecture used in this section.': 0.3934882879257202, 'The result seems to rely on the': 1.0926539897918701, 'choice of an x^2 non-linearity, which is not often encountered in recent neural': 0.26833367347717285, 'net literature.': 1.0556128025054932, 'The point might also be to highlight the difference between a weight': 0.3124640882015228, 'penalty and a feature penalty because the two seem to encourage': 0.2795066237449646, 'different values of b in this case.': 0.294802725315094, 'However, there is no comparison to': 0.9547249674797058, 'a weight penalty on b in section 2.1.': 0.8302854299545288, '* As far as I can tell, eq. 3 depends on either assuming an L2 or cross-entropy': 0.9635872840881348, 'loss.': 1.0986003875732422, 'A more general class of losses for which eq. 3 holds is not provided.': 0.9499129056930542, 'This': 1.0986123085021973, 'should be made clear before eq. 3 is presented.': 1.0986123085021973, '* The Omniglot and ImageNet experiments are performed with Batch Normalization,': 1.0986123085021973, 'yet the paper points out that feature regularization may be similar in effect': 0.4391036629676819, 'to Batch Norm.': 1.0968867540359497, 'Since the ResNet CNN baseline includes Batch Norm and there are': 0.4180922508239746, 'clear improvements over that baseline, the proposed regularizer has a clear': 0.40795600414276123, 'additional positive effect.': 0.26846906542778015, 'However, results should be provided without': 1.0985616445541382, 'Batch Norm so a 1-1 comparison between the two methods can be performed.': 0.40547478199005127, 'The ImageNet experiment should be more like Hariharan and Girshick.': 1.0111346244812012, 'In particular, the same split of classes should be used (provided in': 0.6950670480728149, 'the appendix) and performance should be measured using n > 1 novel examples': 0.4882080852985382, 'per class (using k nearest neighbors).': 0.7170079350471497, 'Minor:': 0.8501691222190857, 'A brief comparison to Matching Networks is provided in section 3.2, but the': 0.39068591594696045, 'performance of Matching Networks should also be reported in Table 1.': 0.7432161569595337, '* From the approach section: ""Intuitively when close to convergence, about half': 0.628354012966156, 'of the data-cases recommend to update a parameter to go left, while': 0.6292483806610107, 'the other half recommend to go right.""': 0.7961289882659912, 'Could the intuition be clarified?': 0.44256770610809326, 'There are many directions in high': 0.6172013878822327, 'dimensional space and many ways to divide them into two groups.': 0.7734852433204651, '* Is the SGM penalty of Hariharan and Girshick implemented for this paper': 0.6584282517433167, 'or using their code?': 0.2963449954986572, 'Either is acceptable, but clarification would be appreciated.': 0.9562629461288452, '* Should the first equal sign in eq. 13 be proportional to, not equal to?': 0.8109774589538574, 'The work is dense in nature, but I think the presentation could be improved.': 0.4993075728416443, 'In particular, more detailed derivations could be provided in an appendix': 0.4361482858657837, 'and some details could be removed from the main version in order to increase': 0.513137698173523, 'focus on the results (e.g., the derviation in section 2.2.1).': 0.3958205282688141, 'Overall Evaluation': 1.0307869911193848, 'This paper provides an interesting set of analyses, but their value is not clear.': 1.0697877407073975, 'There is no clear reason why a gradient or feature regularizer should improve': 1.0880903005599976, 'low-shot learning performance.': 1.0985230207443237, 'Despite that, experiments support that conclusion,': 1.0986123085021973, 'the analysis is interesting by itself, and the analysis may help lead to a': 1.0986123085021973, 'clearer explanation.': 1.0986123085021973, 'The work is a somewhat novel extension and analysis of Hariharan and Girshick 2016.': 1.0977768898010254, 'Some points are not completely clear, as mentioned above.': 0.949336588382721, 'This paper proposes analysis of regularization, weight Froebius-norm and feature L2 norm, showing that it is equivalent to another proposed regularization, gradient magnitude loss.': 1.0986121892929077, 'They then argue that: 1) it is helpful to low-shot learning, 2) it is numerically stable, 3) it is a soft version of Batch Normalization.': 1.0985655784606934, 'Finally, they demonstrate experimentally that such a regularization improves performance on low-shot tasks.': 1.0986099243164062, 'First, this is a nice analysis of some simple models, and proposes interesting insights in some optimization issues.': 1.0986123085021973, 'Unfortunately, the authors do not demonstrate, nor argue in a convincing manner, that such an analysis extends to deep non-linear computation structures.': 1.0986123085021973, 'I feel like the authors could write a full paper about ""results can be derived for φ(x) with convex differentiable non-linear activation functions such as ReLU"", both via analysis and experimentation to measure numerical stability.': 1.0986123085021973, 'Second, the authors again show an interesting correspondance to batch normalization, but IMO fail to experimentally show its relevance.': 1.0960865020751953, 'Finally, I understand the appeal of the proposed method from a numerical stability point of view, but am not convinced that it has any effect on low-shot learning in the high dimensional spaces that deep networks are used for.': 1.0986117124557495, 'I commend the authors for contributing to the mathematical understanding of our field, but I think they have yet to demonstrate the large scale effectiveness of what they propose.': 1.0986123085021973, 'At the same time, I feel like this paper does not have a clear and strong message.': 1.08610200881958, 'It makes various (interesting) claims about a number of things, but they seem more or less disparate, and only loosely related to low-shot learning.': 1.083130121231079, 'notes:': 1.082218885421753, '""an expectation taken with respect to the empirical distribution generated by the training set"", generally the training set is viewed as a ""montecarlo"" sample of the underlying, unknown data distribution \\mathcal{D}.': 0.7429518699645996, '""we can see that our model learns meaningful representations"", it gets a 6.5% improvement on the baseline, but there is no analysis of the meaningfulness of the representations.': 1.0092887878417969, '""Table 13.2"" should be ""Table 2"".': 1.0983530282974243, 'please be mindful of formatting, some citations should be parenthesized and there are numerous extraneous and missing spacings between words and sentences.': 0.4514690339565277, 'The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net.': 1.0986007452011108, 'Although the equations suggest a weighting per example, dropping this weight (alpha_i) works equally well.': 1.0985932350158691, 'The proposed approach relates to Batch Norm and weight decay.': 1.0974905490875244, 'Experiments are given on ""low-shot"" settting.': 0.9432822465896606, 'There seem to be two stories in the paper: feature penalty as a soft batch norm version, and low-shot learning; why is feature penalty specifically adapted to low-shot learning and not a more classical supervised task?': 1.0986123085021973, ""Regarding your result on Omniglot, 91.5, I believe it is still about 2% worse than the Matching Networks, which you refer to but don't put in Table 1."": 1.0986123085021973, 'Why?': 0.4066316485404968, 'Overall, the idea is simple but feels like preliminary: while it is supposed to be a ""soft BN"", BN itself gets better performance than feature penalty, and both together give even better results.': 1.0986123085021973, 'Is something still missing in the explanation?': 1.0983242988586426, 'edits after revised version:': 0.5310283303260803, 'Thank you for adding more information to the paper.': 0.821132481098175, 'I feel it is still too long but hopefully you can reduce it to 9 pages as promised.': 1.0948561429977417, ""However, I'm still not convinced the paper is ready to be accepted, mainly for the following reasons:"": 1.096219539642334, 'on Omniglot, the paper is still significantly far from the current state of the art.': 1.098511815071106, 'the new experiments do not really confirm/infirm the relationship with BN.': 1.0651111602783203, 'you added an explanation of why FP works for low-shot setting, by showing it controls the VC dimension and hence is good to control overfitting with a small number of training examples, but this discussion is basic and does not really shed more light than the obvious.': 0.6304295659065247, ""I'm pushing up your score from 4 to 5 for the improved version, but I still think it is below acceptance level."": 1.0986123085021973}"
330,https://openreview.net/forum?id=SkgewU5ll,"{'This paper addresses the problem of data sparsity in the healthcare domain by leveraging hierarchies of medical concepts organized in ontologies.': 1.097962498664856, 'The paper focuses on sequential prediction given a patient’s medical record (a sequence of medical codes, some of which might occur very rarely).': 1.0986119508743286, 'Instead of simply assigning each medical code an independent embedding before feeding it to an RNN, the proposed approach assigns each node in the medical ontology a “basic” embedding, and composes a “final” embedding for each medical code by taking a learned weighted average (via an attention mechanism) of the medical code’s ancestors in the ontology.': 1.0743067264556885, 'Notably, the paper is well written and the approach is quite intuitive.': 1.0902214050292969, 'I have the following comments:': 1.0986123085021973, 'Why is the patient’s visit taken as just the sum of medical codes found in the visit, and not say the average or a learned weighted average?': 0.41237562894821167, 'Wouldn’t this bias for/against the number of codes in the visit?': 1.0646637678146362, 'I don’t see why basic embeddings are not fine tuned as well.': 1.0986123085021973, 'Did you find that to hurt performance?': 1.0986123085021973, 'Do you have an explanation for that?': 1.0986123085021973, 'Looking at Figure 2, the results seem very close and the figures are not very clear (figure (b) top is missing).': 1.0979149341583252, 'Also, I am wondering how significant the differences are so it would be nice to comment on that.': 1.098531723022461, 'Finally, I think this is an interesting application paper applying well-established deep learning techniques.': 1.0986123085021973, 'The paper deals with an important issue that arises when applying deep learning models in domains with scarce data resources.': 1.0986123085021973, 'However, I would like the authors to comment on what there paper offers as new insights to the ICLR community and why they think ICLR is a good avenue for their work.': 0.8810672760009766, ""I read the authors' response and maintain my rating."": 1.0986123085021973, 'This paper introduces an approach for integrating a direct acyclic graph structure of the data into word / code embeddings, in order to leverage domain knowledge and thus help train an RNN with scarce data.': 1.0807048082351685, 'It is applied to codes of medical visits.': 1.0986123085021973, 'Each code is part of an ontology, which can be represented by a DAG, where codes correspond to leaf nodes, and where different codes may share common ancestors (non-leaf nodes) in the DAG.': 0.7967365980148315, 'Instead of embedding merely the leaf nodes, one can also embed the non-leaf nodes, and the embeddings of the code and its ancestors can be combined using a convex sum.': 1.0983350276947021, 'That convex sum can be seen as an attention mechanism over the representation.': 1.0986123085021973, 'The attention weights depend on the embeddings and the weights of an MLP, meaning that the model can separate learning the code embeddings and the interaction between the codes.': 0.6418923735618591, 'Embedding codes are pretrained using GloVe, then fine-tuned.': 1.0986121892929077, 'The model is properly evaluated on two medical datasets, with several variations to isolate the contribution of the DAG (GRAM or GRAM+ vs. RNN or RandomDAG) and of pretraining the embeddings (RNN+ vs RNN, GRAM+ vs GRAM).': 1.0983518362045288, 'Both are shown to help achieve the best performance and the evaluation methodology seems thorough.': 1.0986123085021973, 'The paper is also well written, and the case for MLP attention instead of a plain dot product of embeddings was made by the authors.': 1.0986043214797974, 'My only two comments would be:': 1.0986123085021973, '1) Why is there a softmax in equation 4, given that the loss is multivariate cross-entropy (in the predicted visit, several codes could be equal to 1), not a a single-class cross-entropy?': 1.0981906652450562, '2) What is the embedding dimension m?': 1.0986123085021973, 'SUMMARY.': 1.0986123085021973, 'This paper presents a method for enriching medical concepts with their parent nodes in an ontology.': 1.0986123085021973, 'The method employs an attention mechanism over the parent nodes of a medical concept to create a richer representation of the concept itself.': 1.0986123085021973, 'The rationale of this is that for  infrequent medical concepts the attention mechanism will rely more on general concepts, higher in the ontology hierarchy, while for frequent ones will focus on the specific concept.': 1.0986123085021973, 'The attention mechanism is trained together with a recurrent neural network and the model accuracy is tested on two tasks.': 1.0986123085021973, 'The first task aims at prediction the diagnosis categories at each time step, while the second task aims at predicting whether or not a heart failure is likely to happen after the T-th step.': 1.0986123085021973, 'Results shows that the proposed model works well in condition of data insufficiency.': 1.0986123085021973, 'OVERALL JUDGMENT': 1.0986123085021973, 'The proposed model is simple but interesting.': 1.0986123085021973, 'The ideas presented are worth to expand but there are also some points where the authors could have done better.': 1.0986123085021973, 'The learning of the representation of concepts in the ontology is a bit naive, for example the authors could have used some kind of knowledge base factorization approach to learn the concepts, or some graph convolutional approach.': 1.0986123085021973, 'I do not see why the the very general factorization methods for knowledge bases do not apply in the case of ontology learning.': 1.0986123085021973, 'I also found strange that the representation of leaves are fine tuned while the inner nodes are not, it is a specific reason to do so?': 1.0986123085021973, 'Regarding the presentation, the paper is clear and the qualitative evaluation is insightful.': 1.0986123085021973, 'DETAILED COMMENTS': 1.0986123085021973, 'Figure 2.': 1.0986123085021973, 'Please use the same image format with the same resolution.': 1.0986123085021973}"
331,https://openreview.net/forum?id=SkhU2fcll,"{'The paper proposed a nice framework leveraging Tucker and Tensor train low-rank tensor factorization to induce parameter sharing for multi-task learning.': 1.0986123085021973, 'The framework is nice and appealing.': 0.9201616048812866, 'However, MTL is a very well studied problem and the paper considers simple task for different classification, and it is not clear if we really need ``Deep Learning"" for these simple datasets.': 1.0986123085021973, 'A comparison with existing shallow MTL is necessary to show the benefits of the proposed methods (and in particular being deep) on the dataset.': 1.0986123085021973, 'The authors ignore them on the basis of speculation and it is not clear if the proposed framework is really superior to simple regularizations like the nuclear norm.': 1.0986123085021973, 'The idea of nuclear norm regularization can also be extended to deep learning as gradient descent are popular in all methods.': 1.0986123085021973, 'The paper proposed a tensor factorization approach for MTL to learn cross task structures for better generalization.': 1.0985915660858154, 'The presentation is clean and clear and experimental justification is convincing.': 1.0986123085021973, 'As mentioned, including discussions on the effect of model size vs. performance would be useful in the final version and also work in other fields related to this.': 1.0986030101776123, 'One question on Sec. 3.3, to build the DMTRL, one DNN per-task is trained with the same architecture.': 1.098516583442688, 'How important is this pretraining?': 1.081432819366455, 'Would random initialization also work here?': 1.0986123085021973, 'If the data is unbalanced, namely, some classes have very few examples, how would that affect the model?': 1.0986123085021973, 'This paper proposed a deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network with tensor factorization and end-to-end knowledge sharing.': 1.0986123085021973, 'This approach removed the requirement of a user-deﬁned multi-task sharing strategy in conventional approach.': 1.0986123085021973, 'Their experimental results indicate that their approach can achieve higher accuracy with fewer design choices.': 1.0986076593399048, 'Although factorization ideas have been exploited in the past for other tasks I think applying it to MTL is interesting.': 1.0984922647476196, 'The only thing I want to point out is that the saving of parameter is from the low-rank factorization.': 1.0835990905761719, ""In the conventional MTL each layer's weight size can also be reduced if SVD is used."": 1.0986123085021973, 'BTW, recent neural network MTL was explored first (earlier than 2014, 2015 work cited) in speech recognition community.': 1.0975441932678223, 'see, e.g.,': 0.7604025602340698, 'Huang, J.T., Li, J., Yu, D., Deng, L. and Gong, Y., 2013, May.': 0.380079448223114, 'Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers.': 1.0986123085021973, 'In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (pp.': 1.0972241163253784, '7304': 1.0986123085021973, '-7308': 1.0986123085021973, ').': 1.098266839981079, 'IEEE.': 1.0986123085021973}"
332,https://openreview.net/forum?id=SkkTMpjex,"{'In this paper, the authors present a partially asynchronous variant of the K-FAC method.': 0.6931295394897461, 'The authors adapt/modify the K-FAC method in order to make it computationally tractable for optimizing deep neural networks.': 0.39263594150543213, 'The method distributes the computation of the gradients and the other quantities required by the K-FAC method (2nd order statistics and Fisher Block inversion).': 0.6926307082176208, 'The gradients are computed in synchronous manner by the ‘gradient workers’ and the quantities required by the K-FAC method are computed asynchronously by the ‘stats workers’ and ‘additional workers’.': 0.6917624473571777, 'The method can be viewed as an augmented distributed Synchronous SGD method with additional computational nodes that update the approximate Fisher matrix and computes its inverse.': 0.6878132224082947, 'The authors illustrate the performance of the method on the CIFAR-10 and ImageNet datasets using several models and compare with synchronous SGD.': 0.5304687023162842, 'The main contributions of the paper are:': 0.6931471824645996, '1) Distributed variant of K-FAC that is efficient for optimizing deep neural networks.': 0.6931471824645996, 'The authors mitigate the computational bottlenecks of the method (second order statistic computation and Fisher Block inverses) by asynchronous updating.': 0.692994236946106, '2)': 0.6931471824645996, 'The authors propose a “doubly-factored” Kronecker approximation for layers whose inputs are too large to be handled by the standard Kronecker-factored approximation.': 0.6409542560577393, 'They also present (Appendix A) a cheaper Kronecker factored approximation for convolutional layers.': 0.690545916557312, '3) Empirically illustrate the performance of the method, and show:': 0.6931471824645996, 'Asynchronous Fisher Block inversions do not adversely affect the performance of the method (CIFAR-10)': 0.6924929022789001, 'K-FAC is faster than Synchronous SGD (with and without BN, and with momentum) (ImageNet)': 0.6931232213973999, 'Doubly-factored K-FAC method does not deteriorate the performance of the method (ImageNet and ResNet)': 0.6918816566467285, 'Favorable scaling properties of K-FAC with mini-batch size': 0.6931471824645996, 'Pros:': 0.6931471824645996, 'Paper presents interesting ideas on how to make computationally demanding aspects of K-FAC tractable.': 0.6931466460227966, 'Experiments are well thought out and highlight the key advantages of the method over Synchronous SGD (with and without BN).': 0.692284345626831, 'Cons:': 0.6931471824645996, '“…it should be possible to scale our implementation to a larger distributed system with hundreds of workers.”': 0.6917673945426941, 'The authors mention that this should be possible, but fail to mention the potential issues with respect to communication, load balancing and node (worker) failure.': 0.6656174659729004, 'That being said, as a proof-of-concept, the method seems to perform well and this is a good starting point.': 0.6923619508743286, 'Mini-batch size scaling experiments: the authors do not provide validation curves, which may be interesting for such an experiment.': 0.6878664493560791, 'Keskar et.': 0.6931471824645996, 'al. 2016': 0.6931471824645996, '(On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima) provide empirical evidence that large-batch methods do not generalize as well as small batch methods.': 0.6896315813064575, 'As a result, even if the method has favorable scaling properties (in terms of mini-batch sizes), this may not be effective.': 0.242278054356575, 'The paper is clearly written and easy to read, and the authors do a good job of communicating the motivation and main ideas of the method.': 0.693070113658905, 'There are a few minor typos and grammatical errors.': 0.6931471824645996, 'Typos:': 0.6931471824645996, '“updates that accounts for” — “updates that account for”': 0.6931471824645996, '“Kronecker product of their inverse” — “Kronecker product of their inverses”': 0.6931471824645996, '“where P is distribution over” — “where P is the distribution over”': 0.6931471824645996, '“back-propagated loss derivativesas” — “back-propagated loss derivatives as”': 0.6931471824645996, '“inverse of the Fisher” — “inverse of the Fisher Information matrix”': 0.6931471824645996, '“which amounts of several matrix” — “which amounts to several matrix”': 0.6895885467529297, '“The diagram illustrate the distributed” — “The diagram illustrates the distributed”': 0.6931250095367432, '“Gradient workers computes” — “Gradient workers compute”': 0.6931468844413757, '“Stat workers computes” — “Stat workers compute”': 0.6931435465812683, '“occasionally and uses stale values” — “occasionally and using stale values”': 0.693008303642273, '“The factors of rank-1 approximations” — “The factors of the rank-1 approximations”': 0.6931435465812683, '“be the first singular value and its left and right singular vectors” — “be the first singular value and the left and right singular vectors … , respectively.”': 0.17472724616527557, '“\\Psi is captures” — “\\Psi captures”': 0.6931038498878479, '“multiplying the inverses of the each smaller matrices” — “multiplying the inverses of each of the smaller matrices”': 0.6832363605499268, '“which is a nested applications of the reshape” — “which is a nested application of the reshape”': 0.671042263507843, '“provides a computational feasible alternative” — “provides a computationally feasible alternative”': 0.027424119412899017, '“according the geometric mean” — “according to the geometric mean”': 0.5048694610595703, '“analogous to shrink” — “analogous to shrinking”': 0.6924766898155212, '“applied to existing model-specification code” — “applied to the existing model-specification code”': 0.6930738687515259, '“: that the alternative parametrization” — “: the alternative parameterization”': 0.6241535544395447, 'Minor Issues:': 0.6931453347206116, 'In paragraph 2 (Introduction) the authors mention several methods that approximate the curvature matrix.': 0.5872355103492737, 'However, several methods that have been developed are not mentioned.': 0.6931190490722656, 'For example:': 0.6931471824645996, '1) (AdaGrad) Adaptive Subgradient Methods for Online Learning and Stochastic Optimization (http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)': 0.5962114930152893, '2) Stochastic Quasi-Newton Methods for Nonconvex Stochastic Optimization (https://arxiv.org/abs/1607.01231)': 0.680938720703125, '3) adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs (http://link.springer.com/chapter/10.1007/978-3-319-46128-1_1)': 0.477471262216568, '4)': 0.6915357708930969, 'A Self-Correcting Variable-Metric Algorithm for Stochastic Optimization (http://jmlr.org/proceedings/papers/v48/curtis16.html)': 0.6612048149108887, '5) L-SR1: A Second Order Optimization Method for Deep Learning (https://openreview.net/pdf?id=By1snw5gl)': 0.6797155141830444, 'Page 2, equation s = WA, is there a dimension issue in this expression?': 0.6929771900177002, 'x-axis for top plots in Figures 3,4,5,7 (Updates x XXX) appear to be a headings for the lower plots.': 0.6881981492042542, '“James Martens.': 0.6931471824645996, 'Deep Learning via Hessian-Free Optimization” appears twice in References section.': 0.6931471824645996, 'The paper proposes an asynchronous distributed K-FAC method for efficient optimization of': 0.6931465864181519, 'deep networks.': 0.6931471824645996, 'The authors introduce interesting ideas that many computationally demanding': 0.6931471824645996, 'parts of the original K-FAC algorithm can be efficiently implemented in distributed fashion.': 0.6931471824645996, 'The': 0.6931471824645996, 'gradients and the second-order statistics are computed by distributed workers separately and': 0.6931471824645996, 'aggregated at the parameter server along with the inversion of the approximate Fisher matrix': 0.6931471824645996, 'computed by a separate CPU machine.': 0.6931471824645996, 'The experiments are performed in CIFAR-10 and ImageNet': 0.6931471824645996, 'classification problems using models such as AlexNet, ResNet, and GoogleReNet.': 0.6931471824645996, 'The paper includes many interesting ideas and techniques to derive an asynchronous distributed': 0.6931471824645996, 'version from the original K-FAC.': 0.6931471824645996, 'And the experiments also show good results on a few': 0.6931471824645996, 'interesting cases.': 0.6931471824645996, 'However, I think the empirical results are not thorough and convincing': 0.6931471824645996, 'enough yet.': 0.6931471824645996, 'Particularly, experiments on various and large number of GPU workers (in the same machine,': 0.6931471824645996, 'or across multiple workers) are desired.': 0.6931471824645996, 'For example, as pointed by the authors in the answer of a comment,': 0.6931471824645996, 'Chen et.al.': 0.6931471824645996, '(Revisiting Distributed Synchronous SGD, 2015) used 100 workers to test their distributed deep': 0.6931471824645996, 'learning algorithm.': 0.6931471824645996, 'Even considering that the authors have a limitation in computing resource under the': 0.6931471824645996, 'academic research setting, the maximum number of 4 or 8 GPUs seems too limited as the only test case of': 0.6931471824645996, 'demonstrating the efficiency of a distributed learning algorithm.': 0.6931471824645996}"
333,https://openreview.net/forum?id=Skn9Shcxe,"{'Thank you for an interesting angle on highway and residual networks.': 1.0986123085021973, 'This paper shows a new angle to how and what kind of representations are learnt at each layer in the aforementioned models.': 1.0986121892929077, 'Due to residual information being provided at a periodic number of steps, each of the layers preserve feature identity which prevents lesioning unlike convolutional neural nets.': 1.0986123085021973, 'Pros': 1.0986123085021973, 'the iterative unrolling view was extremely simple and intuitive, which was supported by theoretical results and reasonable assumptions.': 1.0985304117202759, 'Figure 3 gave a clear visualization for the iterative unrolling view': 0.5100221037864685, 'Cons': 1.0986027717590332, 'Even though, the perspective is interesting few empirical results were shown to support the argument.': 1.0986123085021973, 'The major experiments are image classification and language models trained on mutations of character-aware neural language models.': 1.0986123085021973, 'Figure 4 and 5 could be combined and enlarged to show the effects of batch normalization.': 1.0986123085021973, 'The paper describes an alternative view on hierarchical feature representations in deep neural networks.': 1.0985716581344604, 'The viewpoint of refining representations is well motivated and is in agreement with the success of recent model structures like ResNets.': 1.0985925197601318, 'Pros:': 1.0986123085021973, 'Good motivation for the effectiveness of ResNets and Highway networks': 1.0983552932739258, 'Convincing analysis and evaluation': 1.0985034704208374, 'Cons:': 1.0986123085021973, 'The effect of this finding of the interpretation of batch-normalization is only captured briefly but seems to be significant': 1.0986123085021973, 'Explanation of findings in (Zeiler & Fergus (2014))': 0.11714375764131546, 'using UIE viewpoint missing': 1.0986123085021973, 'Remarks:': 1.0974475145339966, 'Missing word in line 223: ""that it *is* valid""': 1.0986123085021973, 'This paper provides a new perspective to understanding the ResNet and Highway net.': 1.098156452178955, 'The new perspective assumes that the blocks inside the networks with residual or skip-connection are groups of successive layers with the same hidden size, which performs to iteratively refine their estimates of the same feature instead of generate new representations.': 1.0986123085021973, 'Under this perspective, some contradictories with the traditional representation view induced by ResNet and Highway network and other paper can be well explained.': 1.0986123085021973, 'The pros of the paper are:': 1.09490966796875, '1. A novel perspective to understand the recent progress of neural network is proposed.': 0.9495558738708496, '2. The paper provides a quantitatively experimentals to compare ResNet and Highway net, and shows contradict results with several claims from previous work. The authors also give discussions and explanations about the contradictories, which provides a good insight of the disadvantages and advantages between these two kind of networks.': 0.3679761290550232, 'The main cons of the paper is that the experiments are not sufficient.': 0.6703653335571289, 'For example, since the main contribution of the paper is to propose the “unrolled iterative estimation"" and the stage 4 of Figure 3 seems not follow the assumption of ""unrolled iterative estimation"" and the authors says: ""We note that stage four (with three blocks) appears to be underestimating the representation values, indicating a probable weak link in the architecture."".': 1.0986123085021973, 'Thus, it would be much better to do experiments to show that under some condition, the performance of stage 4 can follow the assumption.': 1.0986120700836182, 'Moreover, the paper should provide more experiments to show the evidence of ""unrolled iterative estimation"", not comparing ResNet with Highway Net.': 1.0984529256820679, 'The lack of experiments on this point is the main concern from myself.': 1.096104621887207}"
334,https://openreview.net/forum?id=SkpSlKIel,"{'SUMMARY': 1.0986123085021973, 'This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units.': 1.0986123085021973, 'The main contribution of the paper is to show that approximating a strongly convex differentiable function is possible with much less units when using a network with one more hidden layer.': 1.0612218379974365, 'PROS': 1.098610758781433, 'The paper presents an interesting combination of tools and arrives at a nice result on the exponential superiority of depth.': 1.0985616445541382, 'CONS': 0.40617629885673523, 'The main result appears to address only strongly convex univariate functions.': 1.0973546504974365, 'SPECIFIC COMMENTS': 1.0975935459136963, 'Thanks for the comments on L. Still it would be a good idea to clarify this point as far as possible in the main part.': 1.098131537437439, 'Also, I would suggest to advertise the main result more prominently.': 1.0986061096191406, 'I still have not read the revision and maybe you have already addressed some of these points there.': 1.0986106395721436, 'The problem statement is close to that from [Montufar, Pascanu, Cho, Bengio NIPS 2014], which specifically arrives at exponential gaps between deep and shallow ReLU networks, albeit from a different angle.': 1.098124384880066, 'I would suggest to include that paper it in the overview.': 1.0979188680648804, 'In Lemma 3, there is an i that should be x': 1.098595380783081, ""In Theorem 4, ``\\tilde f'' is missing the (x)."": 1.0984200239181519, 'Theorem 11, the lower bound always increases with L ?': 1.098456621170044, 'In Theorem 11, \\bf x\\in [0,1]^d?': 1.0275049209594727, 'The main contribution of this paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network that uses O(log(1/eps)) layers and O(poly log(1/eps))': 1.0986123085021973, 'hidden units where the activation functions can be either ReLU or binary step or any combination of them.': 1.092368483543396, 'The paper is well written and clear.': 1.0367850065231323, 'The arguments and proofs are easy to follow.': 0.6004927754402161, 'I only have two questions:': 0.6755872964859009, '1- It would be great to have similar results without binary step units.': 0.8954159617424011, 'To what extent do you find the binary step unit central to the proof?': 1.0986123085021973, '2- Is there an example of piecewise smooth function that requires at least poly(1/eps) hidden units with a shallow network?': 0.7853169441223145, 'This paper shows:': 1.0976027250289917, '1. Easy, constructive proofs to derive e-error upper-bounds on neural networks with O(log 1/e) layers and O(log 1/e) ReLU units.': 0.3972941040992737, '2. Extensions of the previous results to more general function classes, such as smooth or vector-valued functions.': 0.4144926369190216, '3. Lower bounds on the neural network size, as a function of its number of layers. The lower bound reveals the need of exponentially many more units to approximate functions using shallow architectures.': 0.7283560037612915, 'The paper is well written and easy to follow.': 1.072066307067871, 'The technical content, including the proofs in the Appendix, look correct.': 1.0986123085021973, 'Although the proof techniques are simple (and are sometimes modifications of arguments by Gil, Telgarsky, or Dasgupta), they are brought together in a coherent manner to produce sharp results.': 1.0986123085021973, 'Therefore, I am leaning toward acceptance.': 1.0986123085021973}"
335,https://openreview.net/forum?id=Skq89Scxx,"{'This heuristic to improve gradient descent in image classification is simple and effective, but this looks to me more like a workshop track paper.': 1.0986121892929077, 'Demonstration of the algorithm is limited to one task (CIFAR) and there is no theory to support it, so we do not know how it will generalize on other tasks': 1.0985662937164307, 'Working on DNNs for NLP, I find some observations in the paper opposite to my own experience.': 1.0976967811584473, 'In particular, with architectures that combine a wide variety of layer types (embedding, RNN, CNN, gating), I found that ADAM-type techniques far outperform simple SGD with momentum, as they save searching for the right learning rate for each type of layer.': 1.0986123085021973, 'But ADAM only works well combined with Poliak averaging, as it fluctuates a lot from one batch to another.': 1.0986123085021973, 'Revision:': 1.0986123085021973, 'the authors substantially improved the contents of the paper, including experiments on another set than Cifar': 1.0986123085021973, 'the workshop track has been modified to breakthrough work, so my recommendation for it is not longer appropriate': 1.0986123085021973, 'I have therefore improved my rating': 1.0986123085021973, 'This an interesting investigation into learning rate schedules, bringing in the idea of restarts, often overlooked in deep learning.': 1.0986123085021973, 'The paper does a thorough study on non-trivial datasets, and while the outcomes are not fully conclusive, the results are very good and the approach is novel enough to warrant publication.': 1.0986123085021973, 'I thank the authors for revising the paper based on my concerns.': 1.0986123085021973, 'Typos:': 1.0986123085021973, '“flesh” -> “flush”': 1.0986123085021973, 'This paper describes a way to speed up convergence through sudden increases of otherwise monotonically decreasing learning rates.': 1.0986123085021973, 'Several techniques are presented in a clear way and parameterized method is proposed and evaluated on the CIFAR task.': 1.0986123085021973, 'The concept is easy to understand and the authors chose state-of-the-art models to show the performance of their algorithm.': 1.0986123085021973, 'The relevance of these results goes beyond image classification.': 1.0986123085021973, 'Pros:': 1.0986123085021973, 'Simple and effective method to improve convergence': 1.0986123085021973, 'Good evaluation on well known database': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'Connection of introduction and topic of the paper is a bit unclear': 1.0986123085021973, 'Fig 2, 4 and 5 are hard to read.': 1.0986123085021973, 'Lines are out of bounds and maybe only the best setting for T_0 and T_mult would be clearer.': 1.0986123085021973, ""The baseline also doesn't seem to converge"": 1.0986123085021973, 'Remarks:': 1.0986123085021973, 'An loss surface for T_0 against T_mult would be very helpful.': 1.0986123085021973, 'Also understanding the relationship of network depth and the performance of this method would add value to this analysis.': 1.0986123085021973}"
336,https://openreview.net/forum?id=SkqMSCHxe,"{'This paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future based on video and vehicle state input.': 1.0986117124557495, 'The architecture allows several RNNs to compete to make the best predictions, with only the best prediction receiving back propagation training at each time step.': 1.0986121892929077, 'Preliminary experimental results show that this scheme can yield reduced prediction error.': 1.0986123085021973, 'It is not clear how the best-performing RNN is chosen for each time point at test time.': 1.0985643863677979, 'That is, how is the “integrated prediction” obtained in Fig.': 0.6177302002906799, '7? Is the prediction the one with minimum error over all of the output layers?': 1.0986109972000122, 'If so, this means the prediction cannot be made until you already know the value to be predicted.': 1.0986121892929077, 'It seems possible that a larger generic RNN might be able to generate accurate predictions.': 1.0985995531082153, 'If I understand correctly, the competitive architectures have many more parameters than the baseline.': 1.06686532497406, 'Is the improved performance here due to the competitive scheme, or just a larger model?': 1.0977025032043457, 'A large amount of additional work is required to sustain the claim that this scheme is successfully extracting driver ‘intentions’.': 1.0986123085021973, 'It would be interesting to see if the scheme, suitably extended, can automatically infer the intention to stop at a stop sign vs slowing but not stopping due to a car in front, say, or to pass a car vs simply changing lanes.': 1.0986123085021973, 'Adding labels to the dataset may enable this comparison more clearly.': 0.7293980121612549, 'More generally, the intention of the driver seems more related to the goals they are pursuing at the moment; there is a fair amount of work in inverse reinforcement learning that examines this problem (some of it in the context of driving style as well).': 1.0986123085021973, 'This paper proposes a neural network architecture for car state prediction while driving based on competitive learning.': 1.0986123085021973, 'Competitive learning creates several duplicates of the baseline neural architecture and during training only updates the architecture with minimum loss.': 1.0986123085021973, 'The experiments compare the competitive learning approach to a single baseline architecture on a driving benchmark task.': 1.0986123085021973, 'The paper is understandable but could benefit from some copy editing.': 1.0986123085021973, 'The competitive learning approach seems rather adhoc and this paper feels quite incomplete without significant discussion and comparisons to ensembling.': 1.0986123085021973, 'Much recent work has shown that duplicating and ensembling neural architectures can produce gains, and it’s not clear why competitive learning is better than ensembling, it seems less theoretically sound to me.': 1.0986123085021973, 'There is a huge confound in the experiments due to the competitive learning architecture having many more free parameters than the baseline architecture.': 1.0986123085021973, 'Again I think comparing to ensembling with the same number of architectures duplicated and perhaps comparing to a single baseline with larger hidden layers to make the total number of free parameters comparable is critical to validating the proposed approach.': 1.0986123085021973, 'The graphical model of the driving process depicted in figure 1 seems nonsensical.': 1.0986106395721436, 'If e is observed then all variables are known given the dependencies shown.': 1.0986120700836182, 'Further, it is at best very poor notation to say that the driving action d decided at time t affects the vehicle state s at that same time.': 1.0986123085021973, 'It should be that s_t depends on d_(t-1).': 1.0986123085021973, 'Also, according to this figure the driving decision d does not depend on the observed vehicle state x which also seems invalid.': 1.0986123085021973, 'Odd to have a paragraph break in abstract': 1.0986123085021973, 'Figure 1 caption should include a brief explanation of the variables shown': 1.0986123085021973, 'Authors propose a competitive learning architecture that learn different RNN predictors independently, akin to a committee of experts which are chosen with a hard switch at run-time.': 1.0986123085021973, 'This work is applied to the task of predictive different driving behaviors from human drivers, and combines behaviors at test time, often switching behaviors within seconds.': 1.0986123085021973, 'Prediction loss is lower than the similar but non-competitive architecture used as a baseline.': 1.0986123085021973, 'It is not very clear how to interpret the results, what is the real impact of the model.': 1.0986123085021973, 'If behaviors switch very often, can this really be seen as choosing the best driving mode for a given situation?': 1.0986123085021973, 'Maybe the motivation needs to be rephrased a little to be more convincing?': 1.0986123085021973, 'The competitive approach presented is interesting but not really novel, thus the impact of this paper for a conference such as ICLR may be limited.': 1.0986123085021973}"
337,https://openreview.net/forum?id=Sks3zF9eg,"{'Summary:': 1.0971063375473022, 'In this paper, the authors explore the advantages/disadvantages of using a sin activation function.': 1.0986123085021973, 'They first demonstrate that even with simple tasks, using sin activations can result in complex to optimize loss functions.': 1.0986123085021973, 'They then compare networks trained with different activations on the MNIST dataset, and discover that the periodicity of the sin activation is not necessary for learning the task well.': 1.0986120700836182, 'They then try different algorithmic tasks, where the periodicity of the functions is helpful.': 1.0986123085021973, 'Pros:': 1.08865487575531, 'The closed form derivations of the loss surface were interesting to see, and the clarity of tone on the advantages *and* disadvantages was educational.': 1.0986123085021973, 'Cons:': 1.0985631942749023, 'Seems like more of a preliminary investigation of the potential benefits of sin, and more evidence (to support or in contrary) is needed to conclude anything significant': 1.0986123085021973, 'the results on MNIST seem to indicate truncated sin is just as good, and while it is interesting that tanh maybe uses more of the saturated part, the two seem relatively interchangeable.': 0.9226067066192627, 'The toy algorithmic tasks are hard to conclude something concrete from.': 1.0961205959320068, 'Authors propose using periodic activation functions (sin) instead of tanh for gradient descent training of neural networks.': 1.0986123085021973, ""This change goes against common sense and there would need to be strong evidence to show that it's a good idea in practice."": 1.098584771156311, 'The experiments show slight improvement (98.0 -> 98.1) for some MNIST configurations.': 1.0986123085021973, 'They show strong improvement (almost 100% higher accuracy after 1500 iterations) on a toy algorithmic task.': 1.0983562469482422, ""It's not clear that this activation function is good for a broad class of algorithmic tasks or just for the two they present."": 1.0986123085021973, 'Hence evidence shown is insufficient to be convincing that this is a good idea for practical tasks.': 1.0974265336990356, 'An interesting study of using Sine as activation function showing successful training of models using Sine.': 1.098576545715332, 'However the scope of tasks this is applied to is a bit too limited to be convincing.': 1.0985970497131348, 'Maybe showing good results on more important tasks in addition to current toy tasks would make a stronger case?': 1.0986123085021973}"
338,https://openreview.net/forum?id=Sks9_ajex,"{'The paper proposes a new way of transferring knowledge.': 1.0986062288284302, 'I like the idea of transferring attention maps instead of activations.': 1.0817092657089233, 'However, the experiments don’t show a big improvement compared with knowledge distillation alone and I think more experiments are required in IMAGENET section.': 1.0970460176467896, 'I would consider updating the score if the authors extend the last section 4.2.2.': 1.0986123085021973, 'The paper presented a modified knowledge distillation framework that minimizes the difference of the sum of statistics across the a feature map between the teacher and the student network.': 1.0986123085021973, 'The authors empirically demonstrated the proposed methods outperform the fitnet style distillation baseline.': 1.0986123085021973, 'Pros:': 1.0986123085021973, '+': 1.0986123085021973, 'The author evaluated the proposed methods on various computer vision dataset': 1.0986123085021973, 'The paper is in general well-written': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'The method seems to be limited to the convolutional architecture': 1.0986123085021973, 'The attention terminology is misleading in the paper.': 1.0986117124557495, 'The proposed method really just try to distill the summed squared(or other statistics e.g. summed lp norm) of  activations in a hidden feature map.': 1.0986123085021973, 'The gradient-based attention transfer seems out-of-place.': 1.0986123085021973, 'The proposed gradient-based methods are never compared directly to nor are used jointly with the ""attention-based"" transfer.': 1.0986123085021973, 'It seems like a parallel idea added to the paper that does not seem to add much value.': 1.0986123085021973, 'It is also not clear how the induced 2-norms in eq.(2) is computed.': 1.0986123085021973, 'Q is a matrix \\in \\mathbb{R}^{H \\times W}  whose induced 2-norm is its largest singular value.': 1.098609447479248, 'It seems computationally expensive to compute such cost function.': 1.0986123085021973, 'Is it possible the authors really mean the Frobenius norm?': 0.626629650592804, 'Overall, the proposed distillation method works well in practice but the paper has some organization issues and unclear notation.': 1.098611831665039, 'This paper proposes to investigate attention transfers between a teacher and a student network.': 1.0986123085021973, 'Attention transfer is performed by minimising the l2 distance between the teacher/student attention maps at different layers, in addition to minimising the classification loss and optionally a knowledge distillation term.': 1.0986123085021973, 'Authors define several activation based attentions (sum of absolute feature values raise at the power p or max of values raised at the power p).': 1.0986123085021973, 'They also propose a gradient based attention (derivative of the Loss w.r.t. inputs).': 1.0986086130142212, 'They evaluate their approaches on several datasets (CIFAR, Cub/Scene, Imagenet) showing that attention transfers  does help improving the student network test performance.': 1.0986123085021973, 'However, the student networks performs worst than the teacher, even with attention.': 1.0986123085021973, 'Few remarks/questions:': 1.0886852741241455, 'in section 3 authors  claim that networks with higher accuracy have a higher spatial correlation between the object and the attention map.': 1.0986121892929077, 'While Figure 4 is compelling, it would be nice to have quantitative results showing that as well.': 1.0986123085021973, 'how did you choose the hyperparameter values, it would be nice to see what is the impact of .': 1.0544567108154297, 'it would be nice to report teacher train and validation loss in Figure 7 b)': 1.0986123085021973, 'from the experiments, it is not clear what at the pros/cons of the different attention maps': 1.0986123085021973, 'AT does not lead to better result than the teacher.': 1.0986123085021973, 'However, the student networks have less parameters.': 1.0719096660614014, 'It would be interesting to characterise the corresponding speed-up.': 1.0964233875274658, 'If you keep the same architecture between the student and the teacher, is there any benefit to the attention transfer?': 1.0898758172988892, 'In summary:': 1.0986123085021973, 'Clearly written and well motivated.': 1.0986123085021973, 'Consistent improvement of the student with attention compared to the student alone.': 1.0986123085021973, 'Students have worst performances than the teacher models.': 1.0986123085021973, 'It is not clear which attention to use in which case?': 1.0986123085021973, 'Somewhat incremental novelty relatively to Fitnet': 1.0986123085021973}"
339,https://openreview.net/forum?id=SkuqA_cgx,"{'This paper introduces a new dataset to evaluate word representations.': 1.0986123085021973, 'The task considered in the paper, called outlier detection (also known as word intrusion), is to identify which word does not belong to a set of semantically related words.': 1.0986123085021973, 'The task was proposed by Camacho-Collados & Navigli (2016) as an evaluation of word representations.': 1.0986123085021973, 'The main contribution of this paper is to introduce a new dataset for this task, covering 5 languages.': 1.0986123085021973, 'The dataset was generated automatically from the Wikidata hierarchy.': 1.0986123085021973, 'Entities which are instances of the same category are considered as belonging to the same cluster, and outliers are sampled at various distances in the tree.': 1.0986123085021973, 'Several heuristics are then proposed to exclude uninteresting clusters from the dataset.': 1.0986123085021973, 'Developing good ressources to evaluate word representations is an important task.': 1.0986123085021973, 'The new dataset introduced in this paper might be an interesting addition to the existing ones (however, it is hard to say by only reviewing the paper).': 1.0986123085021973, 'I am a bit concerned by the lack of discussion and comparison with existing approaches (besides word similarity datasets).': 1.0986123085021973, 'In particular, I believe it would be interesting to discuss the advantages of this evaluation/dataset, compared to existing ones such as word analogies.': 1.0986123085021973, 'The proposed evaluation also seems highly related to entity typing, which is not discussed in the paper.': 1.0986123085021973, 'Overall, I believe that introducing ressources for evaluating word representations is very important for the community.': 1.0986123085021973, 'However, I am a bit ambivalent about this submission.': 1.0986123085021973, 'I am not entirely convinced that the proposed dataset have clear advantages over existing ressources.': 1.0986123085021973, 'It also seems that existing tasks, such as entity typing, already capture similar properties of word representations.': 1.0986123085021973, 'Finally, it might be more relevant to submit this paper to LREC than to ICLR.': 1.0986123085021973, 'This paper describes a new benchmark for word representations: spotting the “odd one out”.': 1.0986123085021973, 'The authors build upon an idea recently presented at the RepEval workshop, but are able to collect a significantly larger amount of examples by relying on existing ontologies.': 1.0986123085021973, 'Although the innovation is relatively incremental, it is an important step in defining challenging benchmarks for general-purpose word representations.': 0.8911862969398499, 'While humans are able to perform this task almost flawlessly (given adequate domain knowledge), the experiments in this paper show that current embeddings fall short.': 1.0978703498840332, 'The technical contribution is thorough; the dataset construction appears logical, and the correlation analysis is convincing.': 0.7997483611106873, 'I would like to see it accepted at ICLR.': 1.0986123085021973, 'First, let me praise the authors for generating and releasing an NLP data set: a socially useful task.': 1.0986123085021973, 'The authors use an algorithm to generate a 500-cluster-per-language data set in semantic similarity.': 1.0986123085021973, 'This brings up a few points.': 1.0986123085021973, ""1. If the point of using the algorithm is to be scalable, why release such a small data set? It's roughly the same order of magnitude as the data sets released in the SemEval tasks over the recent years. I would have expected something orders of magnitude larger."": 1.0986123085021973, '2. The authors hand checked a small subset of the clusters: they found one where it was ambiguous, and should probably have been removed. Mechanical Turk can scale pretty well': 0.41693925857543945, 'why not post-facto filter all of the clusters using MT?': 0.4711384177207947, 'This is (in effect)': 1.0954307317733765, 'how ImageNet was created, and it has millions of items.': 1.0986111164093018, '3. Evaluating data set papers is an tricky issue. What makes a data set ""good"" or publishable? There are a number of medium-sized NLP data sets released every year (e.g., through SemEval). Those are designed to address tasks in NLP that people find interesting. I don\'t know of a data set that exactly addresses the task that the authors propose: the task is trying to address the idea of semantic similarity, which has had multiple data sets thrown at it since SemEval 2012. I wish that the paper had included comparisons to show that the particular task / data combination is better suited for analyzing semantic similarity than other existing data sets.': 0.9435860514640808, 'Two final notes:': 1.0986123085021973, 'A.': 1.0986123085021973, ""This paper doesn't seem very well-suited to ICLR."": 1.0986123085021973, 'New NLP data sets may be indirectly useful for evaluating word embeddings (and hence representations).': 1.0986123085021973, ""But, I didn't learn much from the paper: GloVe is empirically less good for semantic similarity than other embeddings?"": 1.0986123085021973, 'If true, why?': 1.0986123085021973, 'That would be interesting.': 1.0986123085021973, 'B.': 1.0986123085021973, 'The first proposal for the ""put a word into a cluster and see if it stands out"" task (in the context of human evaluation of topic models), is': 1.0985846519470215, 'Jonathan Chang, Jordan Boyd-Graber, Chong Wang, Sean Gerrish, and David M. Blei.': 1.0974680185317993, 'Reading Tea Leaves: HowHumans Interpret Topic Models.': 1.0986123085021973, 'Neural Information Processing Systems, 2009': 1.0986123085021973, 'which deserves a citation, I think.': 1.08987557888031}"
340,https://openreview.net/forum?id=Skvgqgqxe,"{'I have not much to add to my pre-review comments.': 1.0986123085021973, ""It's a very well written paper with an interesting idea."": 1.0986123085021973, 'Lots of people currently want to combine RL with NLP.': 1.0986123085021973, 'It is very en vogue.': 1.0986123085021973, 'Nobody has gotten that to work yet in any really groundbreaking or influential way that results in actually superior performance on any highly relevant or competitive NLP task.': 1.0963982343673706, 'Most people struggle with the fact that NLP requires very efficient methods on very large datasets and RL is super slow.': 1.0986008644104004, ""Hence, I believe this direction hasn't shown much promise yet"": 1.0986123085021973, ""and it's not yet clear it ever will due to the slowness of RL."": 1.0986123085021973, 'But many directions need to be explored and maybe eventually they will reach a point where they become relevant.': 1.098611831665039, 'It is interesting to learn the obviously inherent grammatical structure in language though sadly again, the trees here do not yet capture much of what our intuitions are.': 1.0980385541915894, ""Regardless, it's an interesting exploration, worthy of being discussed at the conference."": 1.0981475114822388, 'The paper proposes to use reinforcement learning to learn how to compose the words in a sentence, i.e. parse tree, that can be helpful for the downstream tasks.': 0.9700393676757812, 'To do that, the shift-reduce framework is employed and RL is used to learn the policy of the two actions SHIFT and REDUCE.': 1.0698128938674927, 'The experiments on four datasets (SST, SICK, IMDB, and SNLI) show that the proposed approach outperformed the approach using predefined tree structures (e.g. left-to-right, right-to-left).': 1.072213888168335, 'The paper is well written and has two good points.': 1.0986109972000122, 'Firstly, the idea of using RL to learn parse trees using downstream tasks is very interesting and novel.': 1.0984044075012207, 'And employing the shift-reduce framework is a very smart choice because the set of actions is minimal (shift and reduce).': 1.0986121892929077, 'Secondly, what shown in the paper somewhat confirms the need of parse trees.': 0.9086626172065735, 'This is indeed interesting because of the current debate on whether syntax is helpful.': 1.0980535745620728, 'I have the following comments:': 1.0986123085021973, ""it seems that the authors weren't aware of some recent work using RL to learn structures for composition, e.g. Andreas et al (2016)."": 1.0986106395721436, 'because different composition functions (e.g. LSTM, GRU, or classical recursive neural net) have different inductive biases, I was wondering if the tree structures found by the model would be independent from the composition function choice.': 1.0985642671585083, 'because RNNs in theory are equivalent to Turing machines, I was wondering if restricting the expressiveness of the model (e.g. reducing the dimension) can help the model focus on discovering more helpful tree structures.': 0.6596406698226929, 'Ref:': 1.0917128324508667, 'Andreas et al.': 1.0986123085021973, 'Learning to Compose Neural Networks for Question Answering.': 0.9583730101585388, 'NAACL 2016': 0.9495570659637451, 'In this paper, the authors propose a new method to learn hierarchical representations of sentences, based on reinforcement learning.': 1.0986123085021973, 'They propose to learn a neural shift-reduce parser, such that the induced tree structures lead to good performance on a downstream task.': 1.0986109972000122, 'They use reinforcement learning (more specifically, the policy gradient method REINFORCE) to learn their model.': 1.0985512733459473, 'The reward of the algorithm is the evaluation metric of the downstream task.': 1.0950638055801392, 'The authors compare two settings, (1) no structure information is given (hence, the only supervision comes from the downstream task) and (2) actions from an external parser is used as supervision to train the policy network, in addition to the supervision from the downstream task.': 1.0986123085021973, 'The proposed approach is evaluated on four tasks: sentiment analysis, semantic relatedness, textual entailment and sentence generation.': 1.0986109972000122, 'I like the idea of learning tree representations of text which are useful for a downstream task.': 1.0986123085021973, 'The paper is clear and well written.': 1.0986123085021973, 'However, I am not convinced by the experimental results presented in the paper.': 1.0986123085021973, 'Indeed, on most tasks, the proposed model is far from state-of-the-art models:': 1.0985997915267944, '- sentiment analysis, 86.5 v.s. 89.7 (accuracy);': 1.0986123085021973, '- semantic relatedness, 0.32 v.s. 0.25 (MSE);': 1.0986123085021973, '- textual entailment, 80.5 v.s. 84.6 (accuracy).': 1.0986123085021973, 'From the results presented in the paper, it is hard to know if these results are due to the model, or because of the reinforcement learning algorithm.': 1.0985910892486572, 'PROS:': 1.0986123085021973, '- interesting idea: learning structures of sentences adapted for a downstream task.': 1.0986123085021973, '- well written paper.': 1.0986123085021973, 'CONS:': 1.0986123085021973, '- weak experimental results (do not really support the claim of the authors).': 1.0986123085021973, 'Minor comments:': 1.0986123085021973, 'In the second paragraph of the introduction, one might argue that bag-of-words is also a predominant approach to represent sentences.': 1.0980867147445679, 'Paragraph titles (e.g. in section 3.2) should have a period at the end.': 1.0986123085021973, 'UPDATE': 1.0986123085021973, 'I am still not convinced by the results presented in the paper, and in particular by the fact that one must combine the words in a different way than left-to-right to obtain state of the art results.': 0.6834703087806702, 'However, I do agree that this is an interesting research direction, and that the results presented in the paper are promising.': 1.0986119508743286, 'I am thus updating my score from 5 to 6.': 1.0986123085021973}"
341,https://openreview.net/forum?id=SkwSJ99ex,"{'One of the main idea of this paper is to replace pooling layers with convolutions of stride 2 and retraining the model.': 1.0986123085021973, 'Authors merge this into a new layer and brand it as a new type of layer.': 1.09861159324646, 'This is very misleading and adding noise to the field.': 1.0986031293869019, 'And using strided convolutions rather than pooling is not actually novel (e.g. https://arxiv.org/abs/1605.02346).': 1.0986123085021973, 'While the speed-up obtained are good, the lack of novelty and the rebranding attempt make this paper not a good fit for ICLR.': 1.0986098051071167, 'This paper proposes to reduce model size and evaluation time of deep CNN models on mobile devices by converting multiple layers into single layer and then retraining the converted model.': 1.0986123085021973, 'The paper showed that the computation time can be reduced by 3x to 5x with only 0.4% accuracy loss on a specific model.': 1.0986121892929077, 'Reducing model sizes and speeding up model evaluation are important in many applications.': 1.0986123085021973, 'I have several concerns:': 1.0986123085021973, '1. There are many techniques that can reduce model sizes. For example, it has been shown by several groups that using the teacher-student approach, people can achieve the same and sometimes even better accuracy than the teacher (big model) using a much smaller model. However, this paper does not compare any one of them.': 1.0754810571670532, ""2. The technique proposed in this paper is limited in its applicability since it's designed specifically for the models discussed in the paper."": 0.4377919137477875, '3. Replacing several layers with single layer is a relatively standard procedure. For example, the mean variance normalization layer and batch normalization layer can all be absorbed without retraining or losing accuracy.': 0.3886181116104126, 'BTW, the DNN low-rank approximation technique was first proposed in speech recognition.': 1.0986067056655884, 'e.g.,': 1.0985746383666992, 'Xue, J., Li, J. and Gong, Y., 2013, August.': 0.9469619989395142, 'Restructuring of deep neural network acoustic models with singular value decomposition.': 1.0986123085021973, 'In INTERSPEECH (pp. 2365-2369).': 1.0891717672348022, 'This paper looks at the idea of fusing multiple layers (typically a convolution and a LRN or pooling layer) into a single convolution via retraining of just that layer, and shows that simpler, faster models can be constructed that way at minimal loss in accuracy.': 0.9276753664016724, 'This idea is fine.': 1.0986123085021973, 'Several issues:': 1.0986123085021973, ""The paper introduces the concept of a 'Deeprebirth layer', and for a while it seems like it's going to be some new architecture."": 1.0985651016235352, 'Mid-way, we discover that 1)': 1.0986123085021973, ""it's just a convolution 2)"": 1.0730808973312378, ""it's actually a different kind of convolution depending on whether one fuses serial or parallel pooling layers."": 0.9720869064331055, ""I understand the desire to give a name to the technique, but in this case naming the layer itself, when it's actually multiple things, non of which are new architecturally, confuses the argument a lot."": 1.0986123085021973, 'There are ways to perform this kind of operator fusion without retraining, and some deep learning framework such as Theano and the upcoming TensorFlow XLA implement them.': 1.0986123085021973, 'It would have been nice to have a baseline that implements it, especially since most of the additional energy cost from non-fused operators comes from the extra intermediate memory writes that operator fusion removes.': 1.0986123085021973, 'Batchnorm can be folded into convolution layers without retraining by scaling the weights.': 1.0986123085021973, 'Were they folded into the baseline figures reported in Table 7?': 1.0986123085021973, 'At the time of writing, the authors have not provided the details that would make this research reproducible, in particular how the depth of the fused layers relates to the depth of the original layers in each of the experiments.': 1.0986123085021973, 'Retraining: how much time (epochs) does the retraining take?': 1.0986123085021973, 'Did you consider using any form of distillation?': 1.0986123085021973, 'Interesting set of experiments.': 1.0986123085021973, 'This paper needs a lot of improvements to be suitable for publication.': 1.0986123085021973, 'Open-sourcing: having the implementation be open-source always enhances the usefulness of such paper.': 1.0986123085021973, 'Not a requirement obviously.': 1.0986123085021973}"
342,https://openreview.net/forum?id=SkxKPDv5xl,"{'The paper proposed a novel SampleRNN to directly model waveform signals and achieved better performance both in terms of objective test NLL and subjective A/B tests.': 1.0985995531082153, 'As mentioned in the discussions, the current status of the paper lack plenty of details in describing their model.': 1.0736348628997803, 'Hopefully, this will be addressed in the final version.': 0.41404327750205994, ""The authors attempted to compare with wavenet model, but they didn't manage to get a model better than the baseline LSTM-RNN, which makes all the comparisons to wavenets less convincing."": 1.0985920429229736, 'Hence, instead of wasting time and space comparing to wavenet, detailing the proposed model would be better.': 1.0986123085021973, 'The paper introduces SampleRNN, a hierarchical recurrent neural network model of raw audio.': 1.0986088514328003, 'The model is trained end-to-end and evaluated using log-likelihood and by human judgement of unconditional samples, on three different datasets covering speech and music.': 1.0984249114990234, 'This evaluation shows the proposed model to compare favourably to the baselines.': 1.094032645225525, 'It is shown that the subsequence length used for truncated BPTT affects performance significantly, but interestingly, a subsequence length of 512 samples (~32 ms) is sufficient to get good results, even though the features of the data that are modelled span much longer timescales.': 1.0981297492980957, 'This is an interesting and somewhat unintuitive result that I think warrants a bit more discussion.': 0.0927010327577591, 'The authors have attempted to reimplement WaveNet, an alternative model of raw audio that is fully convolutional.': 1.0049747228622437, 'They were unable to reproduce the exact model architecture from the original paper, but have attempted to build an instance of the model with a receptive field of about 250ms that could be trained in a reasonable time using their computational resources, which is commendable.': 1.0986056327819824, 'The architecture of the Wavenet model is described in detail, but it found it challenging to find the same details for the proposed SampleRNN architecture (e.g. which value of ""r"" is used for the different tiers, how many units per layer, ...).': 0.9174445867538452, 'I think a comparison in terms of computational cost, training time and number of parameters would also be very informative.': 0.7250474095344543, 'Surprisingly, Table 1 shows a vanilla RNN (LSTM) substantially outperforming this model in terms of likelihood, which is quite suspicious as LSTMs tend to have effective receptive fields of a few hundred timesteps at best.': 0.6448005437850952, 'One would expect the much larger receptive field of the Wavenet model to be reflected in the likelihood scores to some extent.': 1.0983468294143677, 'Similarly, Figure 3 shows the vanilla RNN outperforming the Wavenet reimplementation in human evaluation on the Blizzard dataset.': 0.4418552815914154, 'This raises questions about the implementation of the latter.': 1.0955467224121094, 'Some discussion about this result and whether the authors expected it or not would be very welcome.': 1.0985006093978882, 'Table 1 and Figure 4 also show the 2-tier SampleRNN outperforming the 3-tier model in terms of likelihood and human rating respectively, which is very counterintuitive as one would expect longer-range temporal correlations to be even more relevant for music than for speech.': 1.0864243507385254, 'This is not discussed at all, I think it would be useful to comment on why this could be happening.': 0.8264026045799255, ""Overall, this an interesting attempt to tackle modelling very long sequences with long-range temporal correlations and the results are quite convincing, even if the same can't always be said of the comparison with the baselines."": 1.0406103134155273, 'It would be interesting to see how the model performs for conditional generation, seeing as it can be more easily be objectively compared to models like Wavenet in that domain.': 1.0689289569854736, 'Other remarks:': 1.0986123085021973, 'upsampling the output of the models is done with r separate linear projections.': 1.0986123085021973, 'This choice of upsampling method is not motivated.': 1.0820730924606323, 'Why not just use linear interpolation or nearest neighbour upsampling?': 1.0986121892929077, 'What is the advantage of learning this operation?': 1.0986123085021973, ""Don't the r linear projections end up learning largely the same thing, give or take some noise?"": 1.0986099243164062, 'The third paragraph of Section 2.1.1 indicates that 8-bit linear PCM was used.': 1.0986120700836182, 'This is in contrast to Wavenet, for which an 8-bit mu-law encoding was used, and this supposedly improves the audio fidelity of the samples.': 0.9117041230201721, 'Did you try this as well?': 1.0986123085021973, 'Section 2.1 mentions the discretisation of the input and the use of a softmax to model this discretised input, without any reference to prior work that made the same observation.': 0.9749703407287598, 'A reference is given in 2.1.1, but it should probably be moved up a bit to avoid giving the impression that this is a novel observation.': 1.097276210784912, 'Pros:': 1.0986123085021973, 'The authors are presenting an RNN-based alternative to wavenet, for generating audio a sample at a time.': 1.0986123085021973, 'RNNs are a natural candidate for this task so this is an interesting alternative.': 1.0986123085021973, 'Furthermore the authors claim to make significant improvement in the quality of the produces samples.': 1.0986123085021973, 'Another novelty here is that they use a quantitative likelihood-based measure to assess them model, in addition to the AB human comparisons used in the wavenet work.': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'The paper is lacking equations that detail the model.': 1.0986123085021973, 'This can be remedied in the camera-ready version.': 1.0986123085021973, 'The paper is lacking detailed explanations of the modeling choices:': 1.0986123085021973, ""It's not clear why an MLP is used in the bottom layer instead of (another) RNN."": 1.0986123085021973, ""It's not clear why r linear projections are used for up-sampling, instead of feeding the same state to all r samples, or use a more powerful type of transformation."": 1.0986123085021973, 'As the authors admit, their wavenet implementation is probably not as good as the original one, which makes the comparisons questionable.': 1.0986123085021973, 'Despite the cons and given that more modeling details are provided, I think this paper will be a valuable contribution.': 1.0986123085021973}"
343,https://openreview.net/forum?id=SkyQWDcex,"{'1. the QA model is not novel, very similar to the existing model.': 1.0978161096572876, '2. The IQA model is very confusing. If it needs human interactive in the training process, how could it be practical to ask human to join the training in each iteration? It sounds impractical. If the human interactive questions are predefined, then it is not interactive at all, since it is not based on the current state of model output.': 0.9476879835128784, 'This work describes': 1.0986123085021973, '1: a two stage encoding of stories in bAbI like setups, where a GRU is used to encode a sentence, word by word, conditioned on a sentence level GRU, and the sentence level GRU keeps track of a sentence level encoding.': 1.090150237083435, 'Each is used': 1.0944433212280273, '2: modifying the bAbI tasks so it is necessary to ask a question to correctly solve the problem': 1.0986123085021973, 'I am not convinced by the papers results:': 1.0986119508743286, '1:   The new architecture does not do significantly better than DMN+, and in my view, is similar to DMN+.': 1.0980284214019775, 'What problem with DMN+ does your architecture solve?': 1.0986123085021973, '2:  There are now several papers doing the second thing, for example ""Dialog-based Language Learning"" by Weston and  ""Learning End-to-End Goal-Oriented Dialog"" by Bordes and Weston, and I think doing it more carefully and in more compelling ways.': 1.0986109972000122, 'In the current work, the correct answer to the question seems given independent of the what the agent asks, so any model that can output ""unknown"" and then input the extra response has an advantage.': 1.0941641330718994, 'Essentially all of the architectures that are used to solve bAbI can be modified to do this...': 1.0986123085021973, 'Indeed, the enc-dec* accuracies in appendix A show that this sort of module can be appended to any other model.': 1.0986123085021973, 'All of the standard models can be trained to output questions as a sequence of words.': 1.0986123085021973, ""Furthermore, I suspect you could generate the questions  in the authors' setting just by enumerating all the questions that occur in training, and taking a softmax over them, instead of generating word-by-word."": 1.0985937118530273, 'This paper proposes an ""interactive"" version of the bAbI dataset by adding supporting questions/answers to the dataset in cases where there is not enough information to answer the question.': 1.0986123085021973, 'Interactive QA is certainly an interesting problem and is well-motivated by the paper.': 1.0986123085021973, ""However, I don't feel like the bAbI extension is adequately explained."": 1.0986123085021973, 'For example, the baseline DMN and MemN2N models on the IQA task are ""take both statements and question as input and then': 1.0986123085021973, 'estimate an answer.""': 1.0986123085021973, 'Their task is then fundamentally more difficult from the CAN\'s because they do not distinguish ""feedback"" from the original context; perhaps a more fair approach would be to treat **every** question (both supporting and original questions) as individual instances.': 1.0986123085021973, 'Also, how were the supporting questions and the user feedback generated?': 1.0986123085021973, 'How many templates / words were used to create them?': 1.0986123085021973, 'The dataset creation details are missing, and if space is an issue, a lot of basic exposition on things like GRU / sentence encodings can be cut (or at least greatly shortened) and replaced with pointers to the original papers.': 1.0986123085021973, 'Another issue I had is that the model attempts to generate these synthetic questions; if there are just one or two templates, why not just predict the values that fill these templates?': 1.0986123085021973, 'So instead of generating ""Which bedroom, master one or guest one?"" with an RNN decoder, just predict ""which"" or ""which bedroom""... isn\'t this sufficient?': 1.0986123085021973, 'In the end, these just seem like more supporting facts, not actual interaction with users, and the fact that it is run on only three of the original twenty tasks make the conclusions hard to trust.': 1.0986123085021973, 'In conclusion, I think the paper has a strong idea and motivation, but the experiments are not convincing for the paper to be accepted at ICLR.': 1.0986123085021973}"
344,https://openreview.net/forum?id=Sy1rwtKxg,"{'This paper propose a parallel mechanism for stochastic gradient descent method (SGD) in case of gradient can be computed via linear operations (including least square linear regression and polynomial regression problems).': 1.0986065864562988, 'The motivation is to recover the same effect compared with sequential SGD, by using a proposed sound combiner.': 1.0978480577468872, 'To make such combiner more efficient, the authors also use a randomized projection matrix to do dimension reduction.': 0.9805692434310913, 'Experiments shows the proposed method has better speedup than previous methods like Hogwild! and Allreduce.': 1.0986077785491943, 'I feel that there might be some fundamental misunderstanding on SGD.': 1.0986123085021973, ""''The combiner matrixM  generate above can be quite large and expensive to compute."": 1.0986123085021973, 'The sequential SGD algorithm maintains and updates the weight vector w , and thus requires O(f)  space and time, where f  is the number of features.': 1.0986123085021973, 'In contrast,M  is a f f  matrix and consequently, the space and time complexity of parallel SGD is O(f^2) .': 1.0986114740371704, 'In practice, this would mean that we would need O(f) processors to see constant speedups, an infeasible proposition particularly for datasets that can have': 1.0985783338546753, 'thousands if not millions of features.""': 0.40033435821533203, 'I do not think one needs O(f^2) space and complexity for updating M_i * v, where v is an f-dimensional vector.': 1.0986121892929077, ""Note that M_i is a low rank matrix in the form of (I - a_i a_i')."": 1.098598599433899, ""The complexity and space can be reduced to O(f) if compute it by O(v - a_i (a_i' v)) equivalently."": 1.0982146263122559, 'If M_i is defined in the form of the product of n number of rank 1 matrices.': 1.0985761880874634, 'The complexity and space complexity is O(fn).': 1.0985819101333618, ""In the context of this paper, n should be much smaller than f. I seriously doubt that all author's assumptions, experiments, and strategies in this paper are based on this incorrect assumption on space and complexity of SGD."": 1.0985699892044067, 'Why one can have speedup is unclear for me.': 1.0394396781921387, 'It is unclear what computations are in parallel and why this sequential algorithms can bring speedup if M_i*v is computed in the most efficient way.': 1.0986123085021973, 'I suggest authors to make the following changes to make this paper more clear and theoretically solid': 1.0986088514328003, 'provide computational complexity per step of the proposed algorithm': 1.0986123085021973, 'convergence rate analysis (convergence analysis is not enough): we would like to see how the dimension reduction can affect the complexity.': 1.0986123085021973, 'Overall, the idea in this paper is interesting and the paper is well-written and well-motivated.': 1.0986123085021973, 'However, I think it is not ready to publish in ICLR for the following reasons:': 1.0986123085021973, 'This paper is not related to representation learning.': 1.0986123085021973, 'It may be more suitable for a general machine learning or data mining conference.': 1.0986123085021973, 'The proposed approach can only work for a small class of models and cannot apply to popular formulations,  such as SVM, logistic regression, and neural network.': 1.0986123085021973, 'It is unclear why we want to use SGD for this specific type of formulations.': 1.0986123085021973, 'For model like linear regression, the authors should compare their methods with linear programming approaches.': 1.0986123085021973, 'Also, it is unclear why we need to develope parallel algorithm for linear regressio problems as they are relatively easy to solve unless the data are big (see next comment).': 1.0986123085021973, 'The dataset used in the paper are relatively small and can be only used for proving the concept.': 1.0986123085021973, 'Most datasets considered in the paper can be solved in a few second using a single core CPU.': 1.0986123085021973, 'Hogwild! is suitable for sparse dataset because of its asynchronized nature.': 1.0986123085021973, 'On data that are very sparse, the proposed approach is only slightly better or is worse than Hogwild.': 1.0986123085021973, 'For dense dataset, it is unclear why we need to use SYMSGD instead of simply parallelizing the gradient computation using GPUs.': 1.0986123085021973, 'Put them together, the experiment results are not convincing.': 1.0986123085021973, 'This paper describes a correction technique to combine updates from multiple SGD to make it statistically equivalent to sequential technique.': 1.0980572700500488, 'Comments': 1.0986123085021973, '1) The proposed method is novel and interesting to allow update to be corrected even when the update is delayed.': 1.0986123085021973, '2) The proposed theory can only be applied to square loss setting (with linear update rule), making it somewhat limited.': 1.098537802696228, 'This paper would be much more interesting to ICLR community, if the technique is applicable to general objective function and settings of deep neural networks.': 1.0986121892929077, '3) The resulting technique requires book-keeping of a dimensional reduced combiner matrix, which causes more computation in terms of complexity.': 1.0968066453933716, 'The authors argue that the overhead can be canceled with SIMD support for symbolic update.': 1.0986123085021973, 'However, the normal update of SGD might also benefit from SIMD, especially when the dataset is dense.': 1.0985968112945557, 'Overall, even though the practical value of this work is limited by 2) and 3), the technique(specifically the correction rule) proposed in the paper could be of interest to people scaling up learning.': 1.09861159324646, 'I would encourage the author to extend the method to the cases of non-linear objective function which could make it more interesting to the ICLR community': 1.0986123085021973}"
345,https://openreview.net/forum?id=Sy2fzU9gl,"{'The paper is attacking a classical and very hard problem: non-linear PCA i.e. learning the principal components a.k.a independent factors, a.k.a. orthogonal geodesics in the highly non-linear latent manifold of a given data-set.': 1.386293888092041, 'Moreover, the paper is hoping for these factors to be humanly-interpretable.': 1.3862894773483276, 'The problem is so hard that is likely unsolvable in unsupervised fashion for real-life datasets.': 1.3862943649291992, 'After all, even we humans are able to zero-in the the type of, say, legs of chairs (Fig 3 in the paper), only after we have identified the object as a chair and have rotated and translated it.': 1.3862943649291992, 'The importance of this problem is hard to overstate, so we hope the paper is accepted, on the grounds of asking so fundamental a question.': 1.3862941265106201, 'The paper correctly points out the inability of VAE to disentangle important factors even on toy data-sets.': 1.3862943649291992, 'This of course has been known for awhile, e.g., Fig 4.': 1.386044979095459, 'on reference [1], from almost 2 years ago.': 1.386247992515564, 'On the back of so much hope and expectation built-up in the introduction, the solution put forward in the paper strikes us as a curious but a hardly useful toy.': 1.3862943649291992, 'Slapping a large multiplicative factor on the generative error term of a VAE is not going to work for any real-life datasets.': 1.3862943649291992, 'Generation without reconstruction leads to schizophrenia, as every respectable psychiatrist will  testify.': 1.3859301805496216, 'Physicists have attacked this problem considerably earlier than DeepMind and their scalable and conceptual solutions have been discussed at length in references [1], section 1.6, section 4, and most of reference [2].': 0.7985390424728394, 'In short, for spatial, color, time symmetries, symmetry statistics are produced by smaller specialized nets like spatial transformers and used to augment the latent variables, to aid the decoder.': 1.386291742324829, 'As demonstrated in reference [2], Figures 3,4, this is indispensable in order to handle distortion, e.g., spatial transformation.': 1.3554342985153198, 'Note that this approach is completely unsupervised.': 1.3859132528305054, 'This of course does not handle complex factors like ""type of legs"" but that is a hell of an ambitious goal, and while we hope to be proven wrong, probably way beyond reach of machines and even many humans (for real-life data-sets that is!)': 1.3862943649291992, 'By complete luck, the authors may have hit upon something else of fundamental importance: the letter ""beta"" for their multiplicative coefficient is of course reserved in statistical physics for the inverse of the temperature.': 0.819750964641571, 'This requires the separation of generative ""energy"" and ""entropy"" and is  partially addressed in section 2.9 of reference [1].': 1.2743117809295654, 'The correct definition of ""generative temperature"" is not published yet, but used extensively in experiments and can be privately communicated upon request.': 0.7751687169075012, 'When worked out correctly, the beta does not multiply the generative error, as in this paper, so it would be very interesting to repeat the toy experiments here with the ""correct"" physical model instead.': 1.3862943649291992, 'A question to the authors: creating a new metric (""disentanglement"") as u do is commendable but after hundred years of PCA, ICA, etc , is there no some proxy that can be used instead?': 0.7739472985267639, 'Also, could you not for example simply distort the dataset along  different factors and then look for the quality of the reconstructed images (as in the Introduction of reference [2])?': 0.8300075531005859, '[1] https://arxiv.org/pdf/1508.06585v5.pdf': 1.357424259185791, '[2] https://arxiv.org/pdf/1511.02841v3.pdf': 0.9445083141326904, 'The paper proposes beta-VAE which strengthen the KL divergence between the recognition model and the prior to limit the capacity of latent variables while sacrificing the reconstruction error.': 1.3862943649291992, 'This allows the VAE model to learn more disentangled representation.': 1.386282205581665, ""The main concern is that the paper didn't present any quantitative result on log likelihood estimation."": 1.3861730098724365, 'On the quality of generated samples, although the beta-VAE learns disentangled representation, the generated samples are not as realistic as those based on generative adversarial network, e.g., InfoGAN.': 1.3849754333496094, 'Beta-VAE learns some interpretable factors of variation, but it still remains unclear why it is a better (or more efficient) representation than that of standard VAE.': 1.3862943649291992, 'In experiment, what is the criteria for cross-validation on hyperparameter \\beta?': 1.3862943649291992, 'There also exists other ways to limit the capacity of the model.': 1.3862943649291992, 'The simplest way is to reduce the latent variable dimension.': 1.3862943649291992, 'I am wondering how the proposed beta-VAE is a better model than the VAE with reduced, or optimal latent variable dimension.': 1.215227723121643, 'Summary': 0.7912013530731201, '===': 1.3862943649291992, 'This paper presents Beta-VAE, an augmented Variational Auto-Encoder which': 1.3786879777908325, 'learns disentangled representations.': 1.3861818313598633, 'The VAE objective is derived': 1.3840739727020264, 'as an approximate relaxation of a constrained optimization problem where': 1.3068640232086182, 'the constraint matches the latent code of the encoder to a prior.': 1.3862478733062744, 'When KKT multiplier beta on this constraint is set to 1 the result is the': 1.3857195377349854, 'original VAE objective, but when beta > 1 we obtain Beta-VAE, which simply': 0.6931620240211487, 'increases the penalty on the KL divergence term.': 0.7169868350028992, 'This encourages the model to': 1.3853778839111328, 'learn a more efficient representation because the capacity of the latent': 0.9936000108718872, 'representation is more limited by beta.': 1.3862943649291992, 'The distribution of the latent': 1.3862943649291992, 'representation is rewarded more when factors are independent because': 1.3771415948867798, 'the prior (an isotropic Gaussian) encourages independent factors, so the': 1.3861342668533325, 'representation should also be disentangled.': 1.376960277557373, 'A new metric is proposed to evaluate the degree of disentanglement.': 1.3857024908065796, 'Given': 1.3862943649291992, 'a setting in which some disentangled latent factors are known, many examples': 1.2124478816986084, 'are generated which differ in all of these factors except one.': 1.3410152196884155, 'These examples': 0.7650311589241028, 'are encoded into the learned latent representation and a simple classifier': 1.1463158130645752, 'is used to predict which latent factor was kept constant.': 1.383556604385376, 'If the learned': 1.3848127126693726, 'representation does not disentangle the constant factor then the classifier': 0.9258379936218262, 'will more easily confuse factors and its accuracy will be lower.': 0.9993166923522949, 'This': 1.3862608671188354, 'accuracy is the final number reported.': 1.3862943649291992, 'A synthetic dataset of 2D shapes with known latent factors is created to': 0.8336707353591919, 'test the proposed metric and Beta-VAE outperforms a number of baselines': 0.7140899896621704, '(notably InfoGAN and the semi-supervised DC-IGN).': 1.015580415725708, 'Qualitative results show that Beta-VAE learns disentangled factors': 1.181426763534546, 'on the 3D chairs dataset, a dataset of 3D faces, and the celebA dataset': 0.6763725876808167, 'of face images.': 1.3862943649291992, 'The effect of varying Beta is also evaluated using the proposed': 1.373431921005249, 'metric and the latent factors learned on the 2D shapes dataset are explored': 0.8780122399330139, 'in detail.': 1.3862943649291992, 'Strengths': 1.334617018699646, '* Beta-VAE is simple and effective.': 1.362105131149292, '*': 1.3862943649291992, 'The proposed metric is a novel way of testing whether ground truth factors': 1.38603675365448, 'of variation have been identified.': 1.1003444194793701, '* There is extensive comparison to relevant baselines.': 1.383914828300476, 'Weaknesses': 1.3862943649291992, '* Section 3 describes the proposed disentanglement metric, however I feel': 1.374166488647461, 'I need to read the caption of the associated figure (I thank for adding': 1.3268826007843018, 'that) and Appendix 4 to understand the metric intuitively or in detail.': 0.9092933535575867, 'It would be easier to read this section if a clear intuition preceeded': 0.8318474292755127, 'a detailed description': 1.3862910270690918, 'and I think more space should be devoted to this': 0.7183509469032288, 'in the paper.': 1.3862943649291992, '* Appendix 4: Why was the bottom 50% of the resulting scores discarded?': 1.386018991470337, 'As indicated in pre-review comments, the disentanglement metric is similar': 1.385810375213623, 'to a measure of correlation between latent features.': 1.3635998964309692, 'Could the proposed metric': 1.3862918615341187, 'be compared to a direct measure of cross-correlation between latent factors': 1.3862508535385132, 'estimated over the 2D shapes dataset?': 0.7181715965270996, '* The end of section 4.2 observes that high beta values result in low': 1.3854917287826538, 'disentanglement, which suggests the most efficient representation is not': 1.3373870849609375, 'disentangled.': 1.3862069845199585, 'This seems to disagree with the intuition from the approach': 1.3859279155731201, 'section that more efficient representations should be disentangled.': 1.363280177116394, 'It would': 1.3862943649291992, 'be nice to see discussion of potential reasons for this disagreement.': 1.3828359842300415, 'The writing is somewhat dense.': 1.2121459245681763, 'Overall Evaluation': 1.3862413167953491, 'The core idea is novel, simple and extensive tests show that it is effective.': 1.1864941120147705, 'The proposed evaluation metric is novel might come into broader use.': 1.2962945699691772, 'The main downside to the current version of this paper is the presentation,': 1.199893593788147, 'which provides sufficient detail but could be more clear.': 1.3648477792739868, 'This paper proposes the beta-VAE, which is a reasonable but also straightforward generalization of the standard VAE.': 1.3857282400131226, 'In particular, a weighting factor beta is added for the KL-divergence term to balance the likelihood and KL-divergence.': 1.3862898349761963, 'Experimental results show that tuning this weighting factor is important for learning disentangled representations.': 1.3862943649291992, 'A linear-classifier based protocol is proposed for measuring the quality of disentanglement.': 1.3862943649291992, 'Impressive illustrations on manipulating latent variables are shown in the paper.': 0.8830601572990417, 'Learning disentangled representations without supervision is an important topic.': 1.3627475500106812, 'Showing the effectiveness of VAE for this task is interesting.': 1.3862357139587402, 'Generalizing VAE with a weighting factor is straightforward (though reformulating VAE is also interesting), the main contribution of this paper is on the empirical side.': 1.3845789432525635, 'The proposed protocol for measuring disentangling quality is reasonable.': 1.3862943649291992, 'Establishing protocol is one important methodology contribution of this paper, but the presentation of Section 3 is still not good.': 1.3015319108963013, 'Little motivation is provided at the beginning of Section 3.': 1.3859888315200806, 'Figure 2 is a summary of the algorithm, which is helpful, but it still necessary to intuitively explain the motivation at the first place (e.g., what you expect if a factor is disentangled, and why the performance of a classifier can reflect such an expectation).': 1.3862943649291992, 'Moreover, 1) z_diff appeared without any definition in the main text.': 1.3862943649291992, '2) Use “decoding” for x~Sim(v,w) may make people confuse the ground truth sampling procedure w ith the trained decoder.': 0.9059620499610901, 'The illustrative figures on traversing the disentangled factor are impressive, though image generation quality is not as good as InfoGAN (not the main point of this paper).': 1.3862930536270142, 'However, 1) it will be helpful to discuss if the good disentangling quality only attribute to the beta factor and VAE framework.': 1.3862898349761963, 'For example, the training data in this paper seems to be densely sampled for the visualized factors.': 1.3845845460891724, 'Does the sampling density play a critical role?': 1.3862601518630981, '2) Not too many qualitative results are provided for each experiment?': 1.3836336135864258, 'Adding more figures (e.g., in appendix) to cover more factors and seeding images can strength the conclusions drawn in this paper.': 1.385274887084961, '3)': 1.3862943649291992, 'Another detailed question related to the generalizability of the model: are the seeding image for visualizing faces from unseen subjects or subjects in the training set?': 1.386156439781189, '(maybe I missed something here.)': 1.382327675819397, 'Quantitative results are only presented for the synthesized 2D shape.': 1.2463208436965942, 'What hinders this paper from reporting quantitative numbers on real data (e.g., the 2D and 3D face data)?': 1.3857554197311401, 'One possible reason is that not all factors can be disentangled for real data, but it is still feasible to pick up some well-defined factor to measure the quantitative performance.': 1.364245891571045, 'Quantitative performance is only measured by the proposed protocol.': 0.8106892108917236, 'Since the effectiveness of the protocol is something the paper need to justify, reporting quantitative results using simpler protocol is helpful both for demonstrating the disentangling quality and for justifying the proposed protocol (consistency with other measurement).': 1.3862156867980957, 'A simple experiment is facial identity recognition and pose estimation using disentangled features on a standard test set (like in Reed et al, ICML 2014).': 1.3862640857696533, 'In Figure 6 (left), why ICA is worse than PCA for disentanglement?': 1.3862922191619873, 'Is it due to the limitation of the ICA algorithm or some other reasons?': 0.581421971321106, 'In Figure 6 (right), what is “factor change accuracy”?': 1.3836615085601807, 'According to Appendix A.4 (which is not referred to in the main text), it is the “Disentanglement metric score”.': 1.0857566595077515, 'Is that right?': 0.8672037124633789, 'If so Figure 6 (right) shows the reconstruction results for the best disentanglement metric score.': 1.3830772638320923, 'Then, 1) how about random generation or traversing along a disentangled factor?': 1.3823671340942383, '2) more importantly, how is the reconstruction/generation results when the disentanglement metric score is suboptimal.': 1.2218470573425293, 'Overall, the results presented in this paper are very interesting, but there are many details to be clarified.': 1.164414405822754, 'Moreover, more quantitative results are also needed.': 0.9357438087463379, 'I hope at least some of the above concerns can be addressed.': 1.253746509552002}"
346,https://openreview.net/forum?id=Sy4tzwqxe,"{'The paper proposes two methods for what is called wild variational inference.': 1.0986003875732422, 'The goal is to obtain samples from the variational approximate distribution q': 1.0986123085021973, 'without requiring to evaluate the density q(z) by which it becomes possible to': 0.42018061876296997, 'consider more flexible family of distributions.': 1.0664417743682861, 'The authors apply the proposed': 1.0796922445297241, 'method to the problem of optimizing the hyperparamter of the SGLD sampler.': 1.006118893623352, 'The experiments are performed on a 1-d mixture of gaussian distribution and': 1.0986037254333496, 'Bayesian logistic regression tasks.': 1.0863099098205566, 'The key contribution seems to connect the previous findings in SVGD and KSD': 1.0986104011535645, 'to the concept of inference networks, and to use them for hyperparameter': 1.0663905143737793, 'optimization of SGLD.': 0.7040344476699829, 'This can not only be considered as a rather simple': 1.0959850549697876, 'connection/extension, but also the toyish experiments are not enough to convince': 1.0736273527145386, 'readers on the significance of the proposed model.': 1.080823540687561, ""Particularly, I'm wondering"": 0.06256745755672455, 'how the particle based methods can deal with the multimodality (not the simple': 1.0879325866699219, '1d gaussian mixture case) in general.': 0.7593263983726501, 'Also, the method seems still to require to evaluate': 1.0985743999481201, 'the true gradient of the target distribution (e.g., the posterior distribution) for': 1.0744911432266235, 'each z ~ q.': 0.9984879493713379, 'This seems to be a computational problem for large dataset settings.': 1.0984728336334229, 'In the experiments, the authors compare the methods for the same number of': 0.6743438243865967, 'update steps.': 0.5957814455032349, 'But, considering the light computation of SGLD per update, I think': 1.0924408435821533, 'SGLD can make much more updates per unit time than the proposed methods,': 1.098588228225708, 'particularly for large datasets.': 1.0557432174682617, 'The Bayesian logistic regression on 54 dimensions': 1.0800365209579468, 'seems also a quite simple experiment, considering that its posterior is close to': 0.23988556861877441, 'a Gaussian distribution.': 1.0948786735534668, 'Also, including Hamiltonian Monte Carlo (HMC) with': 0.7678725123405457, 'automatic hyperparameter tuning mechanism (like, no u-turn sampler) would be': 1.048519253730774, 'interesting.': 1.0600906610488892, 'The paper is written very unclearly.': 0.942915678024292, 'Especially, it is not clear what is the exact': 0.9374827146530151, 'contributions of the paper compared to the other previous works including the': 1.0796114206314087, ""authors' works."": 1.089089274406433, 'The main message is quite simple but most of the pages are': 1.0986123085021973, 'spent to explain previous works.': 1.0986123085021973, ""Overall, I'd like to suggest to have more significant high-dimension, large scale"": 1.0986123085021973, 'experiments, and to improve the writing.': 1.0986123085021973, 'The authors propose two variational methods based on the theme of posterior approximations which may not have a tractable density.': 1.0986123085021973, 'The first is from another ICLR submission on ""amortized SVGD""': 1.0986123085021973, '(Wang and Liu, 2016), where here the innovation is in using SGLD as the inference network.': 1.0986123085021973, 'The second is from a NIPS paper (Ranganath et al., 2016) on minimizing the Stein divergence with a parametric approximating family, where here the innovation is in defining their test functions to be an RKHS, obtaining an analytic solution to the inner optimization problem.': 1.0980110168457031, 'The methodology is incremental.': 1.0986123085021973, 'Everything up to Section 3.2 is essentially motivation, background, or related work.': 1.0986123085021973, 'The notion of a ""wild variational approximation"" was already defined in Ranganath et al. (2016), termed a ""variational program"".': 1.0986123085021973, 'It would be useful for the authors to comment on the difference, if any.': 1.0986123085021973, 'Section 3.2 is at first interesting because it analytically solves the maximum problem that is faced in Ranganath et al. (2016).': 1.0986123085021973, 'However, this requires use of a kernel which will certainly not scale in high dimensions, so it is then equivalent in practice to having chosen a very simple test function family.': 1.0986123085021973, 'To properly scale to high dimensions would require a deeper kernel and also learning its parameters; this is not any easier than parameterizing the test function family as a neural network to begin with, which Ranganath et al. (2016) do.': 1.0976135730743408, 'Section 4 introduces a Langevin inference network, which essentially chooses the variational approximation as an evolving sequence of Markov transition operators as in Salimans et al. (2015).': 1.0986123085021973, 'I had trouble understanding this for a while because I could not understand what they mean by inference network.': 1.0986123085021973, 'None of it is amortized in the usual inference network sense, which is that the parameters are given by the output of a neural network.': 1.0986123085021973, 'Here, the authors simple define global parameters of the SGLD chain which are used across all the latent variables (which is strictly worse?).': 1.0986123085021973, '(What then makes it an ""inference network""?)': 1.0986123085021973, 'Is this not the variational approximation used in Salimans et al. (2015), but using a different objective to train it?': 1.0986123085021973, 'The experiments are limited, on a toy mixture of Gaussians posterior and Bayesian logistic regression.': 1.0986123085021973, 'None of this addresses the problems one might suspect on high-dimensional and real data, such as the lack of scalability for the kernel, the comparison to Salimans et al. (2015) for the Langevin variational approximation, and any note of runtime or difficulty of training.': 1.0986123085021973, 'Minor comments': 1.0986123085021973, ""+ It's not clear if the authors understood previous work on expressive variational families or inference networks."": 1.0986123085021973, 'For example, they argue Rezende & Mohamed, 2015b; Tran et al., 2015; Ranganath et al., 2015 require handcrafted inference networks.': 1.0986123085021973, 'However, all of them assume use of any neural network for amortized inference.': 1.0986123085021973, 'None of them even require an inference network.': 1.0986123085021973, 'Perhaps the authors mean handcrafted posterior approximations, which to some extent is true; however, the three mentioned are all algorithmic in nature: in Rezende & Mohamed (2015), the main decision choice is the flow length; Tran et al. (2015), the size of the variational data; Ranganath et al. (2015), the flow length on the auxiliary variable space.': 1.0985016822814941, 'Each works well on different problems, but this is also true of variational objectives which admit intractable q (as the latter two consider, as does Salimans et al. (2015)).': 1.0986080169677734, ""The paper's motivation could be better explained, and perhaps the authors could be clearer on what they mean by inference network."": 1.056878685951233, '+ I also recommend the authors not term a variational inference method based on the class of approximating family.': 1.0945011377334595, 'While black box variational inference in Ranganath et al. (2014) assumes a mean-field family, the term itself has been used in the literature to mean any variational method that imposes few constraints on the model class.': 1.0986123085021973, 'The authors propose methods for wild variational inference, in which the': 1.0986123085021973, 'variational approximating distribution may not have a directly accessible': 1.0986123085021973, 'density function.': 1.0986123085021973, ""Their approach is based on the Stain's operator, which acts"": 1.0986123085021973, 'on a given function and returns a zero mean function with respect to a given': 1.0986123085021973, 'density function which may not be normalized.': 1.0986123085021973, 'Quality:': 1.0986123085021973, 'The derviations seem to be technically sound.': 1.0986123085021973, 'However, my impression is that': 1.0986123085021973, 'the authors are not very careful and honest at evaluating both the strengths': 1.0986123085021973, 'and weaknesses of the proposed work.': 1.0986123085021973, 'How does the method perform in cases in': 1.0986123085021973, 'which the distribution to be approximated is high dimensional?': 1.0986123085021973, 'The logistic': 1.0986123085021973, 'regression problem considered only has 54 dimensions.': 1.0986123085021973, 'How would this method': 1.0986123085021973, 'perform in a neural network in which the number of weights is goint to be way': 1.0986123085021973, 'much larger?': 1.0986123085021973, 'The logistic regression model is rather simple and its posterior': 1.0986123085021973, 'will be likely to be close to Gaussian.': 1.0986123085021973, 'How would the method perform in more': 1.0986123085021973, 'complicated posteriors such as the ones of Bayesia neural networks?': 1.0986123085021973, 'Clarity:': 1.0986123085021973, 'The paper is not clearly written.': 1.0986123085021973, 'I found it very really hard to follow and not': 1.0986123085021973, 'focused.': 1.0986123085021973, ""The authors describe way too many methods: 1) Stein's variational"": 1.0986123085021973, 'gradient descent (SVGD), 2) Amortized SVGD, 3) Kernelized Stein discrepancy': 0.7312045097351074, '(KSD), 4)': 1.005122423171997, 'Lavengin inference network, not to mention the introduction to': 1.0981556177139282, ""Stein's discrepancy."": 1.018671989440918, 'I found very difficult to indentify the clear': 0.829846203327179, 'contributions of the paper with so many different techniques.': 0.6932179927825928, 'Originality:': 0.3522348403930664, 'It is not clear how original the proposed contributions are.': 1.0969935655593872, 'The first of the': 1.0957778692245483, 'proposed methods is also discussed in': 0.623292863368988, 'Wang, Dilin and Liu, Qiang.': 0.716080367565155, 'Learning to draw samples: With application to': 1.053001880645752, 'amortized mle for generative adversarial learning.': 0.8732402920722961, 'Submitted to ICLR 2017, 2016': 0.5796212553977966, 'How does this work differ from that one?': 1.0675477981567383, 'Significance:': 1.0220199823379517, 'It is very hard to evaluate the importance of proposed methods.': 1.097414255142212, 'The authors': 1.0985758304595947, 'only report results on a 1d toy problem with a mixture of Gaussians and on a': 1.0555696487426758, 'logistic regression model with dimension 54.': 1.0683389902114868, 'In both cases the distributions to': 0.47637680172920227, 'be approximated are very simple and of low dimension.': 0.7328303456306458, 'In the regression case': 1.0697983503341675, 'the posterior is also likely to be close to Gaussian and therefore not clear': 1.0983999967575073, 'what advances the proposed method would provide with respect to other more': 0.6937179565429688, 'simple approaches.': 1.0958452224731445, 'The authors do not compare with simple variational': 1.0583208799362183, 'approaches based on Gaussian approximations.': 1.073685646057129}"
347,https://openreview.net/forum?id=Sy6iJDqlx,"{'In this paper a well known soft mixture of experts model is adapted for, and applied to, a specific type of transfer learning problem in reinforcement learning (RL), namely transfer of action policies and value functions between similar tasks.': 1.0846201181411743, 'Although not treated as such, the experimental setup is reminiscent of hierarchical RL works, an aspect which the paper does not consider at length, regrettably.': 1.070743441581726, 'One possible implication of this work is that architecture and even learning algorithm choices could simply be stated in terms of the objective of the target task, rather than being hand-engineered by the experimenter.': 1.08461332321167, 'This is clearly an interesting direction of future work which the paper illuminates.': 1.0986123085021973, 'Pros:': 1.0986123085021973, 'The paper diligently explains how the network architecture fits in with various widely used reinforcement learning setups, which does facilitate continuation of this work.': 1.0946868658065796, 'The experiments are good proofs of concept, but do not go beyond that i.m.h.o.': 1.0986123085021973, 'Even so, this work provides convincing clues that collections of deep networks, which were trained on not entirely different tasks, generalize better to related tasks when used together rather than through conventional transfer learning (e.g. fine-tuning).': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'As the paper well recounts in the related work section, libraries of fixed policies have long been formally proposed for reuse while learning similar tasks.': 1.09794282913208, 'Indeed, it is well understood in hierarchical RL literature that it can be beneficial to reuse libraries of fixed (Fernandez & Veloso 2006) or jointly learned policies which may not apply to the entire state space, e.g. options (Pricop et. al).': 1.0986123085021973, 'What is not well understood is how to build such libraries, and this paper does not convincingly shed light in that direction, as far as I can tell.': 1.0986062288284302, 'The transfer tasks have been picked to effectively illustrate the potential of the proposed architecture, but the paper does not tackle negative transfer or compositional reuse in well known challenging situations outlined in previous work (e.g. Parisotto et.': 1.0986123085021973, 'al 2015, Rusu el.': 0.5899288654327393, 'al 2015, 2016).': 1.0986123085021973, 'Since the main contributions are of an empirical nature, I am curious how the results shown in figures 6 & 7 look plotted against wall-clock time, since relatively low data efficiency is not a limitation for achieving perfect play in Pong (see Mnih. et al, 2015).': 1.0986090898513794, 'It would be more illuminating to consider tasks where final performance is plausibly limited by data availability.': 0.9895150661468506, 'It would also be interesting if the presented results were achieved with reduced amounts of computation, or reduced representation sizes compared to learning from scratch, especially when one of the useful source tasks is an actual policy trained on the target task.': 1.0984631776809692, 'Finally, it is perhaps underwhelming that it takes a quarter of the data required for learning Pong from scratch just to figure out that a perfect Pong policy is already in the expert library.': 1.0963616371154785, 'Simply evaluating each expert for 10 episodes and using an  average-score-weighted majority vote to mix action choices would probably achieve the same final performance for a smaller fraction of the data.': 1.086317539215088, 'This paper studies the problem of transferring solutions of existing tasks to tackle a novel task under the framework of reinforcement learning and identifies two important issues of avoiding negative transfer and being selective transfer.': 1.0986113548278809, 'The proposed approach is based on a convex combination of existing solutions and the being-learned solution to the novel task.': 1.0986123085021973, 'The non-negative weight of each solution implies that the solution of negative effect is ignored and more weights are allocated to more relevant solution in each state.': 1.0986123085021973, 'This paper derives this so-called ""A2T"" learning algorithm for policy transfer and value transfer for REINFORCE and ACTOR-CRITIC algorithms and experiments with synthetic Chain World and Puddle World simulation and': 1.0986123085021973, 'Atari 2600 game Pong.': 1.0986123085021973, '+This paper presents a novel approach for transfer reinforcement learning.': 1.0986123085021973, '+The experiments are cleverly designed to demonstrate the ability of the proposed method.': 1.0986123085021973, 'An important aspect of transfer learning is that the algorithm can automatically figure out if the existing solutions to known tasks are sufficient to solve the novel task so that it can save the time and energy of learning-from-scratch.': 1.0986123085021973, 'This issue is not studied in this paper as most of experiments have a learning-from-scratch solution as base network.': 1.0986123085021973, 'It will be interesting to see how well the algorithm performs without base network.': 1.0986123085021973, 'In addition, from Figure 3, 5 and 6, the proposed algorithm seems to accelerate the learning speed, but the overall network seems not better than the solo base network.': 1.0986123085021973, 'It will be more convincing to show some example that existing solutions are complementary to the base network.': 1.0986123085021973, 'If ignoring the base network, the proposed network can be considered as ensemble reinforcement learning that take advantages of learned agents with different expertise to solve the novel task.': 1.0986123085021973, 'The paper tackles important problems in multi-task reinforcement learning: avoid negative transfer and allow finer selective transfer.': 1.0848846435546875, 'The method is based on soft attention mechanism, very general, and demonstrated to be applicable in both policy gradient and value iteration methods.': 1.092297077178955, ""The introduction of base network allows learning new policy if the prior policies aren't directly applicable."": 1.0986123085021973, 'State-dependent sub policy selection allows finer control and can be thought of assigning state space to different sub policies/experts.': 1.0897666215896606, 'The tasks are relatively simplistic but sufficient to demonstrate the benefits.': 1.0986123085021973, 'One limitation is that the method is simple and the results/claims are mostly empirical.': 1.0986123085021973, 'It would be interesting to see extensions to option-based framework, stochastic hard attention mechanism, sub-policy pruning, progressive networks.': 1.0892534255981445, 'In figure 6, the read curve seems to perform worse than the rest in terms of final performance.': 1.0985350608825684, 'Perhaps alternative information to put with figures is the attention mask activation statistics during learning, so that we may observe that it learns to turn off adversarial sub-policies and rely on newly learned base policy mostly.': 1.0986123085021973, 'This is also generally good to check to see if any weird co-adaptation is happening.': 1.0986123085021973}"
348,https://openreview.net/forum?id=Sy7m72Ogg,"{'The authors present a method for adaptively setting the step size for SGD by treating the learning rate as an action in an MDP whose reward is the change in loss function.': 1.0986123085021973, 'The method is presented against popular adaptive first-order methods for training deep networks (Adagrad, Adam, RMSProp, etc).': 1.0986123085021973, 'The results are interesting but difficult to assess in a true apples-to-apples manner.': 1.0985983610153198, 'Some specific comments:': 1.0986121892929077, 'What is the computational overhead of the actor-critic algorithm relative to other algorithms?': 0.8920329213142395, 'No plots with the wall-time of optimization are presented, even though the success of methods like Adagrad was due to their wall-time performance, not the number of iterations.': 1.0986123085021973, 'Why was only a single learning rate learned?': 1.0986123085021973, 'To accurately compare against other popular first order methods, why not train a separate RL model for each parameter, similar to how popular first-order methods adaptively change the learning rate for each parameter.': 1.0986123085021973, 'Since learning is a non-stationary process, while RL algorithms assume a stationary environment, why should we expect an RL algorithm to work for learning a learning rate?': 1.0974780321121216, 'In figure 6, how does the proposed method compare to something like early stopping?': 1.0986123085021973, 'It may be that the actor-critic method is overfitting less simply because it is worse at optimization.': 1.0986123085021973, 'The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning.': 1.0986123085021973, 'The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.': 1.0986123085021973, 'I have two main concerns.': 1.0975528955459595, 'One is the lack of comparisons to similar recently proposed methods - ""Learning Step Size Controllers for Robust Neural Network Training"" by Daniel et al. and ""Learning to learn by gradient descent by gradient descent"" by Andrychowicz et al.': 1.098597526550293, 'The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are.': 1.098610758781433, 'Their work does use more prior knowledge as the authors stated, but why is this a bad thing?': 1.0986067056655884, 'My second concern is with the experiments.': 0.6894115209579468, 'Some of the numbers reported for the other methods are surprisingly low.': 1.0985932350158691, 'For example, why is RMSprop so bad in Table 2 and Table 3?': 1.0985465049743652, 'These results suggest that the methods are not being tuned properly, which reinforces the need for comparisons on standard architectures with previously reported results.': 1.0985443592071533, 'For example, if the baselines used a better architecture like a ResNet or, for simplicty, Network in Network from this list:': 1.0986123085021973, 'http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html': 0.3195036053657532, ""there wouldn't be a question of how well they are being tuned."": 0.6487945914268494, 'Beating a previously published result on a well known architecture would be much more convincing.': 1.0982400178909302, 'In the question response the authors mention and compare other works such as ""Learning to Learn by Gradient Descent by Gradient Descent"", but the goal of current work and that work is quite different.': 1.098611831665039, 'That work is a new form of optimization algorithm which is not the case here.': 1.0986123085021973, 'And bayesian hyper-parameter optimization methods aim for multiple hyper-parameters but this work only tune one hyper-parameter.': 1.0986123085021973, 'The network architecture used for the experiments on CIFAR-10 is quite outdated and the performances are much poorer than any work that has published in last few years.': 1.0986123085021973, 'So the comparison are not valid here, as if the paper claim the advantage of their method, they should use the state of the art network architecture and see if their claim still holds in that setting too.': 1.0986123085021973, 'As discussed before, the extra cost of hyper-parameter optimizers are only justified if the method could push the SOTA results in multiple modern datasets.': 1.0986123085021973, 'In summary, the general idea of having an actor-critic network as a meta-learner is an interesting idea.': 1.0986123085021973, ""But the particular application proposed here does not seems to have any practical value and the reported results are very limited and it's hard to draw any conclusion about the effectiveness of the method."": 1.0986123085021973}"
349,https://openreview.net/forum?id=Sy8gdB9xx,"{'This paper offers a very interesting empirical observation regarding the memorization capacity of current large deep convolutional networks.': 1.0986123085021973, 'It shows they are able to perfectly memorize full training-set input-to-label mapping, even with random labels (i.e. when label has been rendered independent of input), using the same architecture and hyper-parameters as used for training with correct labels, except for a longer time to convergence.': 1.0986121892929077, 'Extensive experiments support the main argument of the paper.': 1.0986123085021973, 'Reflexions and observations about finite-sample expressivity and implicit regularization with linear models fit logically within the main theme and are equally thought-provoking.': 1.0952157974243164, 'While this work doesn’t propose much explanations for the good generalization abilities of what it clearly established as overparameterized models,': 1.0986123085021973, 'it does compel the reader to think about the generalization problem from a different angle than how it is traditionally understood.': 1.0621821880340576, 'In my view, raising good questions and pointing to apparent paradox is the initial spark that can lead to fundamental progress in understanding.': 1.0985976457595825, 'So even without providing any clear answers, I think this work is a very valuable contribution to research in the field.': 1.0975216627120972, 'Detailed question: in your solving of Eq. 3 for MNIST and CIFAR10, did you use integer y class targets, or a binary one-versus all approach yielding 10 discriminant functions (hence a different alpha vector for each class)?': 1.0986106395721436, 'This paper presents a set of experiments where via clever use of randomization and noise addition authors demonstrate the enormous modeling (memorization) power of the deep neural networks with large enough capacity.': 1.0986123085021973, 'Yet these same models have very good generalization behavior even when all obvious explicit or implicit regularizers are removed.': 1.0986123085021973, 'These observations are used to argue that classical theory (VC dimension, Rademacher complexity, uniform stability) is not able to explain the generalization behavior of deep neural networks, necessitating novel theory.': 1.0986120700836182, 'This is a very interesting and thought provoking body of work and I am in complete accord with the observations and conclusions of the paper.': 1.0986119508743286, 'The classical generalization theory is indeed often at a loss with complex enough model families.': 1.0986123085021973, 'As the authors point out, once model families reach a point where they have capacity to memorize train sets, the classical theory does not yield useful results that could give insight into generalization behavior of these models, leaving one to empirical studies and observations.': 1.0986123085021973, 'A minor clarification comment: On page 2, “ … true labels were replaced by random labels.”': 1.0933810472488403, 'Please state that random labels came from the same set as the true labels, to clarify the experiment.': 1.0986123085021973, 'the authors of this work shed light on the generalization properties of deep neural networks.': 1.0966206789016724, 'Specifically, the consider various regularization methods (data augmentation, weight decay, and dropout).': 1.0986123085021973, 'They also show that quality of the labels, namely label noise also significantly affects the generalization ability of the network.': 1.098532795906067, 'There are a number of experimental results, most of which are intuitive.': 1.0886503458023071, 'Here are some specific questions that were not addressed in the paper:': 1.0986080169677734, '1. Given two different DNN architectures with the same number of parameters, why do certain architectures generalize better than others? In other words, is it enough to consider only the size (# of parameters) of the network and the size of the input (number of samples and their dimensionality), to be able to reason about the generalization properties of a given network?': 0.6810997724533081, '2. Does it make sense to study the stability of predictions given added dropout during inference?': 1.0216584205627441, 'Finally, provided a number of experiments and results, the authors do not draw a conclusion or offer a strong insight into what is going on with generalization in DNNs or how to proceed forward.': 1.0985931158065796}"
350,https://openreview.net/forum?id=SyCSsUDee,"{'The paper introduces supervised deep learning with layer-wise reconstruction loss (in addition to the supervised loss) and class-conditional semantic additive noise for better representation learning.': 1.0986123085021973, 'Total correlation measure and additional insights from auto-encoder are used to derive layer-wise reconstruction loss and is further combined with supervised loss.': 1.0986123085021973, 'When combining with supervised loss the class-conditional additive noise model is proposed, which showed consistent improvement over the baseline model.': 1.0986123085021973, 'Experiments on MNIST and CIFAR-10 datasets while changing the number of training examples per class are done extensively.': 1.0986123085021973, 'The derivation of Equation (3) from total correlation is hacky.': 1.0986123085021973, 'Moreover, assuming graphical model between X, Y and Z, it should be more carefully derived to estimate H(X|Z) and H(Z|Y).': 1.0986121892929077, 'The current proposal, encoding Z and Y from X and decoding from encoded representation is not really well justified.': 1.098611831665039, 'Is \\sigma in Equation 8 trainable parameter or hyperparameter?': 1.0986123085021973, 'If it is trainable how it is trained?': 1.0986101627349854, 'If it is not, how are they set?': 1.0986123085021973, 'Does j correspond to one of the class?': 1.098609209060669, 'The proposed feature augmentation sounds like simply adding gaussian noise to the pre-softmax neurons.': 1.0986019372940063, 'That being said, the proposed method is not different from gaussian dropout (Wang and Manning, ICML 2013) but applied on different layers.': 1.098609209060669, 'In addition, there is a missing reference (DisturbLabel: Regularizing CNN on the Loss Layer, CVPR 2016) that applied synthetic noise process on the loss layer.': 1.0986098051071167, 'Experiments should be done for multiple times with different random subsets and authors should provide mean and standard error.': 1.0985668897628784, 'Overall, I believe the proposed method is not very well justified and has limited novelty.': 1.0975892543792725, 'The paper presents a new regularization technique for neural networks, which seeks to maximize correlation between input variables, latent variables and outputs.': 1.0589385032653809, 'This is achieved by defining a measure of total correlation between these variables and decomposing it in terms of entropies and conditional entropies.': 1.0819872617721558, 'Authors explain that they do not actually maximize the total correlation, but a lower-bound of it that ignores simple entropy terms, and only considers conditional entropies.': 1.0985757112503052, 'It is not clearly explained what is the rationale for discarding these entropy terms.': 1.0983123779296875, 'Entropies measures are applying to probability distributions (i.e. this implies that the variables in the model should be random).': 1.0983030796051025, 'The link between the conditional entropy formulation and the reconstruction error is not made explicit.': 1.0370029211044312, 'In order to link these two views, I would have expected, for example, a noise model for the units of the network.': 0.21021145582199097, 'Later in the paper, it is claimed that the original ladder network is not suitable for supervised learning with small samples, and some empirical results seek to demonstrate this.': 1.0846621990203857, 'But a more theoretical explanation why it is the case would have been welcome.': 0.9848409295082092, 'The MNIST results are shown for a particular convolutional neural network architecture, however, most ladder network results for this dataset have been produced on standard fully-connected architectures.': 1.0986123085021973, 'Results for such neural network architecture would have been desirable for more comparability with original ladder neural network results.': 1.0968910455703735, 'This paper introduces a maximum total correlation procedure, adds a target and then adds noise perturbations.': 1.0986123085021973, 'Technical issues:': 1.0986123085021973, 'The move from (1) to (2) is problematic.': 1.0986123085021973, 'Yes it is a lower bound, but by igoring H(Z), equation (2) ignores the fact that H(Z) will potentially vary more significantly that H(Z|Y).': 1.0985814332962036, 'As a result of removing H(Z), the objective (2) encourages Z that are low entropy as the H(Z) term is ignored, doubly so as low entropy Z results in low entropy Z|Y.': 1.0980256795883179, 'Yes the -H(X|Z) mitigates against a complete entropy collapse for H(Z), but it still neglects critical terms.': 1.0986123085021973, 'In fact one might wonder if this is the reason that semantic noise addition needs to be done anyway, just to push up the entropy of Z to stop it reducing too much.': 1.0986123085021973, 'In (3) arbitrary balancing paramters lamda_1 and lambda_2 are introduced ex-nihilo - they were not there in (2).': 1.0986123085021973, 'This is not ever justified.': 1.0986123085021973, 'Then in (5), a further choice is made by simply adding L_{NLL} to the objective.': 1.0986123085021973, 'But in the supervised case, the targets are known and so turn up in H(Z|Y).': 1.0986123085021973, 'Hence now H(Z|Y) should be conditioned on the targets.': 1.0985873937606812, 'However instead another objective is added again without justification, and the conditional entropy of Z is left disconnected from the data it is to be conditioned on.': 1.0985928773880005, 'One might argue the C(X,Y,Z) simply acts as a prior on the networks (and hence implicitly on the weights) that we consider, which is then combined with a likelihood term, but this case is not made.': 0.8739803433418274, 'In fact there is no explicit probabilistic or information theoretic motivation for the chosen objective.': 1.096200942993164, 'Given these issues, it is then not too surprising that some further things need to be done, such as semantic noise addition to actually get things working properly.': 1.0898959636688232, 'It may be the form of noise addition is a good idea, but given the troublesome objective being used in the first place, it is very hard to draw conclusions.': 0.9922950863838196, 'In summary, substantially better theoretical justification of the chosen model is needed, before any reasonable conclusion on the semantic noise modelling can be made.': 0.8824375867843628}"
351,https://openreview.net/forum?id=SyEiHNKxx,"{'I would definitely love to have this and use it for my research.': 1.0986123085021973, 'A great tool.': 1.0986123085021973, 'However, the paper lacks detail.': 1.0986123085021973, 'In particular, I feel that it is impossible for someone to reimplement the research-mostly because of the lack of detail.': 1.0986123085021973, 'However, replicability is a crucial part of science.': 1.0985994338989258, 'Other publications proposing software (e.g. the tensorflow, theano and edward papers) come along with open source code.': 1.0986123085021973, 'This is not the case here and therefore the picture is quite incomplete.': 1.0986123085021973, 'I am not convinced that ICLR is the right venue: robotics conferences such as IROS and ICRA might appreciate it much more.': 1.0986121892929077, 'Nevertheless, this is just an encouragement to the authors to interact with those communities.': 1.0986123085021973, 'A differentiable physics engine is indeed a wonderful thing to have.': 0.43198564648628235, 'The key selling point of the proposed software is its speed, however there is no comparison to other physics engines.': 1.0986121892929077, 'Besides describing the engine\'s speed in rather creative units (e.g. ""model seconds per day""), the reader has no idea if this is fast or slow.': 1.0986123085021973, ""Todorov's engine (my simulator of choice) computes a dynamics step and its derivatives wrt both states and controls (using finite-differences) in less than 1ms for a *full humanoid* model (his code is available here mujoco.org/book/programming.html#saDerivative)."": 1.0986123085021973, ""I think this actually faster than the engine described in this paper, but I can't be sure."": 1.09861159324646, 'Because this engine is so limited in what it can collide (sphere/sphere and sphere/plane), it would be trivial to build the example models in several other popular engines (e.g. ODE, Bullet and MuJoCo) and simply compare the performance.': 1.09861159324646, 'Until this comparison is done I consider the paper to be incomplete.': 1.0981743335723877, 'This paper creates a physics simulator using theano, and uses it to learn a neural network policy by back propagating gradients through the simulation.': 1.0986123085021973, 'The approach is novel, and is motivated by being able to learn policies for robotics.': 1.0986123085021973, 'My two key reservations with the paper are as follows:': 1.09861159324646, '1. The method is motivated by learning policies for robotics. However, the proposed method is *only* useful for robotics if the learned policy can transfer the real world. Transferring policies from simulation to real-world is an open research problem, and is particularly challenging with less realistic simulators.': 0.3300701975822449, '2. They key novelty/benefit of this approach over other model-based approaches is that the simulator is differentiable. However, the only empirical comparison in the paper is to a model-free approach (CMA-ES). To appropriately demonstrate the approach, it should be compared to other model-based approaches, which do not require analytic derivatives of the model.': 0.5853303074836731, 'For the reader to fully understand the pros and cons of the approach, the paper should also include quantitative comparison between the speed of the proposed simulator, and that of standard simulation platforms.': 1.0986121892929077, 'Because the idea is interesting and novel, I think it lies above the acceptance threshold.': 1.0986123085021973, 'However, it would be significantly improved with the aforementioned comparisons.': 1.0986049175262451, 'Lastly, the writing of the paper could be improved, as it is rather informal and/or imprecise in a number of places.': 1.0986113548278809, 'Here are some examples:': 1.0986114740371704, '“we model the use of a neural network as a general controller for a robot” - can be more concisely phrased as something like “we model the robot controller using a neural network” or “the robot controller is modeled using a neural network""': 1.083543300628662, '“In previous research, finding a gradient…” - This is a run-on sentence.': 1.0986121892929077, '“We basically jam this entire equation into” - This sentence is informal and imprecise.': 1.09860360622406, '“deep learning neural network” - the word “learning” should be omitted': 1.09804105758667, '“one big RNN, where we unfold over time” - should be “…RNN, which we unfold over time” or “…RNN, unfolded over time”': 0.5211282968521118, 'The writing would also be improved by making it more concise and fitting the paper into 8 pages.': 1.096954584121704}"
352,https://openreview.net/forum?id=SyJNmVqgg,"{'Paper is easy to follow, Idea is pretty clear and makes sense.': 1.0986123085021973, 'Experimental results are hard to judge, it would be nice to have other baselines.': 1.0986123085021973, ""For faster training convergence, the question is how well tuned SGD is, I didn't"": 1.0986123085021973, 'see any mentioning of learning rate schedule.': 1.0986123085021973, 'Also, it would be important to test': 1.0986123085021973, 'this on other data sets.': 1.0986123085021973, 'Success with filtering training data could be task dependent.': 1.0984841585159302, 'Final review: The writers were very responsive and I agree the reviewer2 that their experimental setup is not wrong after all and increased the score by one.': 1.0986119508743286, 'But I still think there is lack of experiments and the results are not conclusive.': 1.098611831665039, 'As a reader I am interested in two things, either getting a new insight and understanding something better, or learn a method for a better performance.': 1.0986123085021973, 'This paper falls in the category two, but fails to prove it with more throughout and rigorous experiments.': 1.0986123085021973, 'In summary the paper lacks experiments and results are inconclusive and I do not believe the proposed method would be quite useful and hence not a conference level publication.': 1.0986123085021973, 'The paper proposes to train a policy network along the main network for selecting subset of data during training for achieving faster convergence with less data.': 1.0986123085021973, 'Pros:': 1.0986123085021973, ""It's well written and straightforward to follow"": 0.8716480731964111, 'The algorithm has been explained clearly.': 1.0968568325042725, 'Cons:': 1.0986123085021973, 'Section 2 mentions that the validation accuracy is used as one of the feature vectors for training the NDF.': 1.0986049175262451, 'This invalidates the experiments, as the training procedure is using some data from the validation set.': 0.6380650997161865, 'Only one dataset has been tested on.': 1.0986108779907227, 'Papers such as this one that claim faster convergence rate should be tested on multiple datasets and network architectures to show consistency of results.': 1.0986121892929077, 'Especially larger datasets as the proposed methods is going to use less training data at each iteration, it has to be shown in much larger scaler datasets such as Imagenet.': 1.0986123085021973, 'As discussed more in detail in the pre-reviews question, if the paper is claiming faster convergence then it has to compare the learning curves with other baselines such Adam.': 1.0986121892929077, 'Plain SGD is very unfair comparison as it is almost never used in practice.': 1.0986123085021973, 'And this is regardless of what is the black box optimizer they use.': 1.0901544094085693, 'The case could be that Adam alone as black box optimizer works as well or better than Adam as black box + NDF.': 1.0986123085021973, 'This work proposes to augment normal gradient descent algorithms with a ""Data Filter"", that acts as a curriculum teacher by selecting which examples the trained target network should see to learn optimally.': 0.40479788184165955, 'Such a filter is learned simultaneously to the target network, and trained via Reinforcement Learning algorithms receiving rewards based on the state of training with respect to some pseudo-validation set.': 1.0323467254638672, 'Stylistic comment, please use the more common style of ""(Author, year)"" rather than ""Author (year)"" when the Author is *not* referred to or used in the sentence.': 0.656099796295166, 'E.g. ""and its variants such as Adagrad Duchi et al. (2011)"" should be ""such as Adagrad (Duchi et al., 2011)"", and  ""proposed in Andrychowicz et al. (2016),"" should remain so.': 0.5132670998573303, 'I think the paragraph containing ""What we need to do is, after seeing the mini-batch Dt of M training instances, we dynamically determine which instances in Dt are used for training and which are filtered."" should be clarified.': 0.8707879185676575, 'What is ""seeing""?': 1.0986123085021973, 'That is, you should mention explicitly that you do the forward-pass first, then compute features from that, and then decide for which examples to perform the backwards pass.': 1.0978477001190186, 'There are a few choices in this work which I do not understand:': 1.0986123085021973, 'Why wait until the end of the episode to update your reinforce policy (algorithm 2), but train your actor critic at each step (algorithm 3)?': 1.0984348058700562, 'You say REINFORCE has high variance, which is true, but does not mean it cannot be trained at each step (unless you have some experiments that suggest otherwise, and if so they should be included or mentionned in the paper).': 1.0986109972000122, 'Similarly, why not train REINFORCE with the same reward as your Actor-Critic model?': 1.0986123085021973, 'And vice-versa?': 1.0534700155258179, 'You claim several times that a limitation of REINFORCE is that you need to wait for the episode to be over, but considering your data is i.i.d., you can make your episode be anything from a single training step, one D_t, to the whole multi-epoch training procedure.': 1.098542332649231, 'I have a few qualms with the experimental setting:': 0.9116406440734863, 'is Figure 2 obtained from a single (i.e. one per setup) experiment?': 0.46296536922454834, 'From different initial weights?': 0.7747787237167358, 'If so, there is no proper way of knowing whether results are chance or not!': 1.0986123085021973, 'This is a serious concern for me.': 1.0986123085021973, 'with most state-of-the-art work using optimization methods such as Adam and RMSProp, is it surprising that they were not experimented with.': 1.0986123085021973, 'it is not clear what the learning rates are; how fast should the RL part adapt to the SL part?': 1.0986123085021973, 'Its not clear that this was experimented with at all.': 1.0986123085021973, 'the environment, i.e. the target network being trained, is not stationnary at all.': 1.0986123085021973, 'It would have been interesting to measure how much the policy changes as a function of time.': 1.0986123085021973, 'Figure 3, could both be the result of the policy adapting, or of the policy remaining fixed and the features changing (which could indicate a failure of the policy to adapt).': 1.0986123085021973, 'in fact it is not really adressed in the paper that the environment is non-stationary, given the current setup, the distribution of features will change as the target network progresses.': 1.0986123085021973, 'This has an impact on optimization.': 1.0986123085021973, 'how is the ""pseudo-validation"" data, target to the policy, chosen?': 1.0986123085021973, 'It should be a subset of the training data.': 1.0986123085021973, 'The second paragraph of section 3.2 suggests something of the sort, but then your algorithms suggest that the same data is used to train both the policies and the networks, so I am unsure of which is what.': 1.0986123085021973, 'Overall the idea is novel and interesting, the paper is well written for the most part, but the methodology has some flaws.': 1.0986123085021973, 'Clearer explanations and either more justification of the experimental choices or more experiments are needed to make this paper complete.': 1.0986123085021973, 'Unless the authors convince me otherwise, I think it would be worth waiting for more experiments and submitting a very strong paper rather than presenting this (potentially powerful!)': 1.0986123085021973, 'idea with weak results.': 1.0986123085021973}"
353,https://openreview.net/forum?id=SyK00v5xx,"{'This is a good paper with an interesting probabilistic motivation for weighted bag of words models.': 1.0986123085021973, 'The (hopefully soon) added comparison to Wang and Manning will make it stronger.': 1.0981603860855103, 'Though it is sad that for sufficiently large datasets, NB-SVM still works better.': 1.098590612411499, 'In the second to last paragraph of the introduction you describe a problem of large cooccurrence counts which was already fixed by the Glove embeddings with their weighting function f.': 1.0986114740371704, 'Minor comments:': 1.0986123085021973, '""The capturing the similarities""': 1.0863380432128906, 'typo in line 2 of intro.': 0.7953263521194458, '""Recently, (Wieting et al.,2016) learned""': 0.34993287920951843, 'use citet instead of parenthesized citation': 0.40904122591018677, 'This paper proposes a simple way to reweight the word embedding in the simple composition function for sentence representation.': 1.0986123085021973, 'This paper also shows the connection between this new weighting scheme and some previous work.': 1.0986121892929077, 'Here are some comments on technical details:': 0.8767976760864258, 'The word ""discourse"" is confusing.': 0.9960917234420776, 'I am not sure whether the words ""discourse"" in ""discourse vector c_s"" and the one in ""most frequent discourse"" have the same meaning.': 1.0986123085021973, 'Is there any justification about  related to syntac?': 1.0986121892929077, 'Not sure what thie line means: ""In fact the new model was discovered by our detecting the common component c0 in existing embeddings.""': 1.0986123085021973, 'in section ""Computing the sentence embedding""': 1.0240001678466797, 'Is there any explanation about the results on sentiment in Table 2?': 1.0977283716201782, 'This paper presents a new theoretically-principled method of representing sentences as vectors.': 1.0986123085021973, 'The experiments show that vectors produced by this method perform well on similarity and entailment benchmarks, surpassing some RNN-based methods too.': 1.0986123085021973, 'Overall, this is an interesting empirical result, especially since the model is not order-sensitive (as far as I can tell).': 1.0986123085021973, 'I would like to see some more discussion on why such a simple model does better than LSTMs at capturing similarity and entailment.': 1.0986123085021973, 'Could this be an artifact of these benchmarks?': 1.0986123085021973}"
354,https://openreview.net/forum?id=SyOvg6jxx,"{'This paper introduces a new way of extending the count based exploration approach to domains where counts are not readily available.': 1.0986123085021973, 'The way in which the authors do it is through hash functions.': 1.0985932350158691, 'Experiments are conducted on several domains including control and Atari.': 1.098610758781433, 'It is nice that the authors confirmed the results of Bellemare in that given the right ""density"" estimator, count based exploration can be effective.': 1.0986117124557495, ""It is also great the observe that given the right features, we can crack games like Montezuma's revenge to some extend."": 1.0922025442123413, 'I, however, have several complaints:': 1.0986123085021973, 'First, by using hashing, the authors did not seem to be able to achieve significant improvements over past approaches.': 1.0986114740371704, 'Without ""feature engineering"", the authors achieved only a fraction of the performance achieved in Bellemare et al. on Montezuma\'s Revenge.': 1.098473072052002, 'The proposed approaches In the control domains, the authors also does not outperform VIME.': 1.0986109972000122, 'So experimentally, it is very hard to justify the approach.': 1.0945481061935425, 'Second, hashing, although could be effective in the domains that the authors tested on, it may not be the best way of estimating densities going forward.': 1.0986123085021973, 'As the environments get more complicated, some learning methods, are required for the understanding of the environments instead of blind hashing.': 1.0986123085021973, 'The authors claim that the advantage of the proposed method over Bellemare et al. is that one does not have to design density estimators.': 1.098610281944275, 'But I would argue that density estimators have become readily available (PixelCNN, VAEs, Real NVP, GANs) that they can be as easily applied as can hashing.': 1.0969270467758179, 'Training the density estimators is not difficult problem as more.': 1.098254919052124, 'The paper proposes a new exploration scheme for reinforcement learning using locality-sensitive hashing states to build a table of visit counts which are then used to encourage exploration in the style of MBIE-EB of Strehl and Littman.': 1.0986065864562988, 'Several points are appealing about this approach: first, it is quite simple compared to the current alternatives (e.g. VIME, density estimation and pseudo-counts).': 1.0942293405532837, 'Second, the paper presents results across several domains, including classic benchmarks, continuous control domains, and Atari 2600 games.': 1.0982067584991455, 'In addition, there are results for comparison from several other algorithms (DQN variants), many of which are quite recent.': 1.095280408859253, 'The results indicate that the approach clearly improves over the baseline.': 0.7029101252555847, 'The results against other exploration algorithms are not as clear (more dependent on the individual domain/game), but I think this is fine as the appeal of the technique is its simplicity.': 1.0986123085021973, 'Third, the paper presents results on the sensitivity to the granularity of the abstraction.': 0.5572771430015564, 'I have only one main complaint, which is it seems there was some engineering involved to get this to work, and I do not have much confidence in the robustness of the conclusions.': 1.0985993146896362, 'I am left uncertain as to how the story changes given slight perturbations over hyper-parameter values or enabling/disabling of certain choices.': 1.0986123085021973, 'For example, how critical was using PixelCNN (or tying the weights?) or noisifying the output in the autoencoder, or what happens if you remove the custom additions to BASS?': 1.0985876321792603, 'The granularity results show that the choice of resolution is sensitive, and even across games the story is not consistent.': 1.0966389179229736, 'The authors decide to use state-based counts instead of state-action based counts, deviating from the theory, which is odd because the reason to used LSH in the first place is to get closer to what MBIE-EB would advise via tabular counts.': 1.0867505073547363, 'There are several explanations as to why state-based versus state-action based counts perform similarly in Atari; the authors do not offer any.': 1.098511815071106, 'Why?': 1.0986123085021973, 'It seems like the technique could be easily used in DQN as well, and many of the variants the authors compare to are DQN-based, so omitting DQN here again seems strange.': 1.0620155334472656, 'The authors justify their choice of TRPO by saying it ensures safe policy improvement, though it is not clear that this is still true when adding these exploration bonuses.': 1.09566330909729, ""The case study on Montezuma's revenge, while interesting, involves using domain knowledge and so does not really fit well with the rest of the paper."": 1.0955537557601929, 'So, in the end, simple and elegant idea to help with exploration tested in many domains, though I am not certain which of the many pieces are critical for the story to hold versus just slightly helpful, which could hurt the long-term impact of the paper.': 1.0659292936325073, 'After response:': 1.0986123085021973, 'Thank you for the thorough response, and again my apologies for the late reply.': 0.5443367958068848, 'I appreciate the follow-up version on the robustness of SimHash and state counting vs. state-action counting.': 1.0985865592956543, 'The paper addresses an important problem (exploration), suggesting a ""simple"" (compared to density estimation) counting method via hashing.': 0.9543403387069702, 'It is a nice alternative approach to the one offered by Bellemare et al.': 0.8471899032592773, 'If discussion among reviewers were possible, I would now try to assemble an argument to accept the paper.': 1.0985832214355469, ""Specifically, I am not as concerned about beating the state of the art in Montezuma's as Reviewer3 as the merit of the current paper is one the simplicity of the hashing and on the wide comparison of domains vs. the baseline TRPO."": 0.9434053301811218, 'This paper shows that we should not give up on simple hashing.': 1.0915658473968506, 'There still seems to be a bunch of fiddly bits to get this to work, and I am still not confident that these results are easily reproducible.': 1.0985920429229736, 'Nonetheless, it is an interesting new contrasting approach to exploration which deserves attention.': 1.0986123085021973, 'Not important for the decision: The argument in the rebuttal concerning DQN & A3C is a bit of a straw man.': 0.5272566080093384, 'I did not mention anything at all about A3C, I strictly referred to DQN, which is less sensitive to parameter-tuning than A3C. Also, Bellemare 2016 main result on Montezuma used DQN.': 1.0966976881027222, 'Hence the omission of these techniques applied to DQN still seems a bit strange (for the Atari experiments).': 1.0985380411148071, 'The figure S9 from Mnih et al. points to instances of asynchronous one-step Sarsa with varied thread counts.. of course this will be sensitive to parameters: it is both asynchronous online algorithms *and* the parameter varied is the thread count!': 1.0175498723983765, ""This is hardly indicative of DQN's sensitivity to parameters, since DQN is (a) single-threaded (b) uses experience replay, leading to slower policy changes."": 1.0804314613342285, 'Another source of stability, DQN uses a target network that changes infrequently.': 1.0985115766525269, 'Perhaps the authors made a mistake in the reference graph in the figure?': 1.0985560417175293, '(I see no Figure 9 in https://arxiv.org/pdf/1602.01783v2.pdf , I assume the authors meant Figure S9)': 0.48622649908065796, 'This paper proposed to use a simple count-based exploration technique in high-dimensional RL application (e.g., Atari Games).': 1.0986123085021973, 'The counting is based on state hash, which implicitly groups (quantizes) similar state together.': 1.0583044290542603, 'The hash is computed either via hand-designed features or learned features (unsupervisedly with auto-encoder).': 1.0986117124557495, 'The new state to be explored receives a bonus similar to UCB (to encourage further exploration).': 1.098332405090332, 'Overall the paper is solid with quite extensive experiments.': 1.098580002784729, 'I wonder how it generalizes to more Atari games.': 1.0986123085021973, 'Montezuma’s Revenge may be particularly suitable for approaches that implicitly/explicitly cluster states together (like the proposed one), as it has multiple distinct scenarios, each with small variations in terms of visual appearance, showing clustering structures.': 1.0986123085021973, 'On the other hand, such approaches might not work as well if the state space is fully continuous (e.g. in RLLab experiments).': 1.0940884351730347, 'The authors did not answer my question about why the hash code needs to be updated during training.': 1.0986100435256958, 'I think it is mainly because the code still needs to be adaptive for a particular game (to achieve lower reconstruction error) in the first few iterations .': 1.0986123085021973, 'After that stabilization is the most important.': 1.0986101627349854, 'Sec. 2.3 (Learned embedding) is quite confusing (but very important).': 1.0985854864120483, 'I hope that the authors could make it more clear (e.g., by writing an algorithm block) in the next version.': 1.0984116792678833}"
355,https://openreview.net/forum?id=SyQq185lg,"{'This paper proposes to learn decomposition of sequences (such as words) for speech recognition.': 1.0986123085021973, 'It addresses an important issue and I forsee it being useful for other applications such as machine translation.': 1.0986123085021973, 'While the approach is novel and well-motivated, I would very much like to see a comparison against byte pair encoding (BPE).': 1.0984786748886108, 'BPE is a very natural (and important) baseline (i.e. dynamic vs fixed decomposition).': 1.0986047983169556, 'The BPE performance should be obtained for various BPE vocab sizes.': 0.679182231426239, 'Minor points': 1.0981909036636353, 'Did the learned decompositions correspond to phonetically meaningful units?': 1.0986082553863525, ""From the example in the appendix it's hard to tell if the model is learning phonemes or just most frequent character n-grams."": 1.0986119508743286, 'Any thoughts on applications outside of speech recognition?': 1.0973894596099854, 'If this is shown to be effective in other domains it would be a really strong contribution (but this is probably outside the scope for now).': 1.0986123085021973, 'Interesting paper which proposes jointly learning automatic segmentation of words to sub words and their acoustic models.': 1.0986123085021973, 'Although the training handles the word segmentation as hidden variable which depends also on the acoustic representations, during the decoding only maximum approximation is used.': 1.0986123085021973, 'The authors present nice improvements over character based results, however they did not compare results with word segmentation which does not assume the dependency on acoustic.': 1.0986123085021973, 'Obviously, only text based segmentation would result in two (but simpler) independent tasks.': 1.0986123085021973, 'In order to extract such segmentation several publicly open tools are available and should be cited.': 1.0986123085021973, 'Some of those tools can also exploit the unigram probabilities of the words to perform their segmentations.': 1.0986123085021973, 'It looks that the improvements come from the longer acoustical units - longer acoustical constraints which could lead to less confused search -, pointing towards full word models.': 1.0986123085021973, 'In another way, less tokens are more probable due to less multiplication of probabilities.': 1.0985469818115234, 'As a thought experiment for an extreme case: if all the possible segmentations would be possible (mixture of all word fragments, characters, and full-words), would the proposed model use word fragments at all?': 1.0986123085021973, '(WSJ is a closed vocabulary task).': 1.0564346313476562, 'It would be good to show that the sub word model could outperform even a full-word model (no segmentation).': 1.0986123085021973, 'Your model estimates p(z_t|x,z<t;\\theta), but during training both p(z_t|x,y,z<t;\\theta) and p(z_t|x,z<t;\\theta) are needed.': 1.0620486736297607, 'Their connections and calculations, in general Section 2 should be more detailed.': 0.6000152826309204, 'The experimental setup should be presented in a replicable way.': 0.6634665727615356, 'The authors use one sample of Z during the gradient calculation.': 1.0986037254333496, 'Would not it be more consistent then to use the maximum instead of sampling?': 1.0971744060516357, 'Or is even getting the maximum is computationally too expensive?': 1.0983043909072876, 'Considering the left-to-right approximation, this might be possible and more consistent with your decoding, please address this issue in details.': 1.0986121892929077, 'This submission proposes to learn the word decomposition, or word to sub-word sequence mapping jointly with the attention based sequence-to-sequence model.': 1.0986123085021973, 'A particular feature of this approach is that the decomposition is not static, instead, it also conditions on the acoustic input, and the mapping is probabilistic, i.e., one word may map to multiple sub-word sequences.': 1.0986123085021973, 'The authors argue that the dynamic decomposition approach can more naturally reflect the acoustic pattern.': 1.0985194444656372, 'Interestingly, the motivation behind this approach is analogous to learning the pronunciation mixture model for HMM based speech recognition, where the probabilistic mapping from a word to its pronunciations also conditions on the acoustic input, e.g.,': 1.0986123085021973, 'I. McGraw, I. Badr, and J. Glass, ""Learning lexicons form speech using a pronunciation mixture model,"" in IEEE Transactions on Audio, Speech, and Language Processing, 2013': 1.0790650844573975, 'L. Lu, A. Ghoshal, S. Renals, ""Acoustic data-driven pronunciation lexicon for large vocabulary speech recognition"", in Proc.': 1.0976752042770386, 'ASRU': 1.0986123085021973, 'R. Singh, B. Raj, and R. Stern, ""Automatic generation of subword units for speech recognition systems,""  in IEEE Transactions on Speech and Audio Processing, 2002': 0.4222095012664795, 'It would be interesting to put this work in the context by linking it to some previous works in the HMM framework.': 1.098549485206604, 'Overall, the paper is well written, and it is theoretically convincing.': 1.094193935394287, 'The experimental study could be more solid, e.g., it is reasonable to have a word-level baseline, as the proposed approach lies in between the character-level and word-level systems.': 1.0986113548278809, 'the vocabulary size of the WSJ si284 dataset is 20K at maximum, which is not very large for the softmax layer, and it is a closed vocabulary task.': 1.0900418758392334, 'I guess the word-level system may be also competitive to the numbers reported in this paper.': 1.0986123085021973, 'Furthermore, can you explain what is the computational bottleneck of the proposed approach?': 1.0985909700393677, 'You downsampled the data by the factor of 4 using an RNN, and it still took around 5 days to converge.': 1.0986121892929077, 'To me, it is a bit expensive, especially given that you only take one sample when computing the gradient.': 0.6075436472892761, 'Table 2 is a little bit misleading, as CTC with language model and seq2seq with a language model model from Bahdanau et al. is much closer to the best number reported in this Table 2, while you may only get a very small improvement using a language model.': 1.098609209060669, 'Finally, ""O(5) days to converge"" sounds a bit odd to me.': 1.0986123085021973}"
356,https://openreview.net/forum?id=SyVVJ85lg,"{'This paper introduces an analytical performance model to estimate the training and evaluation time of a given network for different software, hardware and communication strategies.': 1.0986123085021973, 'The paper is very clear.': 1.0986104011535645, 'The authors included many freedoms in the variables while calculating the run-time of a network such as the number of workers, bandwidth, platform, and parallelization strategy.': 1.0986121892929077, 'Their results are consistent with the reported results from literature.': 1.0986121892929077, 'Furthermore, their code is open-source and the live demo is looking good.': 1.0985749959945679, 'The authors mentioned in their comment that they will allow users to upload customized networks and model splits in the coming releases of the interface, then the tool can become very useful.': 1.0986123085021973, 'It would be interesting to see some newer network architectures with skip connections such as ResNet, and DenseNet.': 1.0985896587371826, 'In PALEO the authors propose a simple model of execution of deep neural networks.': 1.0984858274459839, 'It turns out that even this simple model allows to quite accurately predict the computation time for image recognition networks both in single-machine and distributed settings.': 1.0986123085021973, 'The ability to predict network running time is very useful, and the paper shows that even a simple model does it reasonably, which is a strength.': 1.0986123085021973, 'But the tests are only performed on a few networks of very similar type (AlexNet, Inception, NiN) and only in a few settings.': 1.0986121892929077, 'Much broader experiments, including a variety of models (RNNs, fully connected, adversarial, etc.) in a variety of settings (different batch sizes, layer sizes, node placement on devices, etc.) would probably reveal weaknesses of the proposed very simplified model.': 1.0986123085021973, 'This is why this reviewer considers this paper borderline': 1.0495922565460205, ""it's a first step, but a very basic one and without sufficiently large experimental underpinning."": 0.4049500524997711, ""More experiments were added, so I'm updating my score."": 1.0986123085021973, 'This paper is technically sound.': 1.097326636314392, 'It highlights well the strengths and weaknesses of the proposed simplified model.': 1.0986123085021973, 'In terms of impact, its novelty is limited, in the sense that the authors did seemingly the right thing and obtained the expected outcomes.': 1.0986123085021973, 'The idea of modeling deep learning computation is not in itself particularly novel.': 1.0986123085021973, 'As a companion paper to an open source release of the model, it would meet my bar of acceptance in the same vein as a paper describing a novel dataset, which might not provide groundbreaking insights, yet be generally useful to the community.': 1.0986123085021973, ""In the absence of released code, even if the authors promise to release it soon, I am more ambivalent, since that's where all the value lies."": 1.0986123085021973, 'It would also be a different story if the authors had been able to use this framework to make novel architectural decisions that improved training scalability in some way, and incorporated such new insights in the paper.': 1.0986123085021973, 'UPDATED: code is now available.': 1.0986123085021973, 'Revised review accordingly.': 1.0986123085021973}"
357,https://openreview.net/forum?id=SyW2QSige,"{'Pros:': 1.090900182723999, '*': 1.0986123085021973, 'The general idea behind the paper seems pretty novel and potentially quite cool.': 1.0986123085021973, 'The specific technical implementation seems pretty reasonable and well-thought through.': 1.0986123085021973, 'The general types of the tasks that they try out their approach on spans a wide and interesting spectrum of cognition abilities.': 1.0986123085021973, 'The writing is pretty clear.': 1.0211237668991089, 'I basically felt like I could replicate much of what they did from their paper descriptions.': 1.0986123085021973, 'Cons:': 1.089368462562561, 'The evaluation of the success of these ideas, as compared to other possible approaches, or as compared to human performance on similar tasks, is extremely cursory.': 1.0986121892929077, '* The specific tasks that they try are quite simple.': 1.0986123085021973, ""I really don't know whether their approach is better than a bunch of simpler things on these tasks."": 1.0986123085021973, 'Taking these two cons together, it feels like the authors basically get the implementation done and working somewhat, and then just wrote up the paper.': 1.0986123085021973, '(I know how it feels to be under a deadline without a complete set of results.)': 1.098412275314331, 'If the authors had used their approach to solve an obviously hard problem that previously was completely unsolved, even the type of cursory evaluation level chosen here would have been fine.': 1.0986123085021973, 'Or if they had done a very thorough evaluation of a bunch of standard models on each task (and humans too, ideally), and compared their model to those results, that would have been great.': 1.0986123085021973, ""But given the complexity of their methods and the fact that the tasks are either not well-known benchmarks or very challenging as such, it's really hard to tell how much of an advance is made here."": 1.0986123085021973, 'But it does seem like a potentially fruitful research direction.': 1.0986123085021973, 'This paper proposes a setting to learn models that will seek information (e.g., by asking question) in order to solve a given task.': 1.0976697206497192, 'They introduce a set of tasks that were designed for that goal.': 1.08548903465271, 'They show that it is possible to train models to solve these tasks with reinforcement learning.': 0.6433953046798706, 'One key motivation for the tasks proposed in this work are the existence of games like 20Q or battleships where an agent needs to ask questions to solve a given task.': 0.8832313418388367, 'It is quite surprising that the authors do not actually consider these games as potential tasks to explore (beside the Hangman).': 0.41219958662986755, 'It is also not completely clear how the tasks have been selected.': 0.5266057848930359, 'A significant amount of work has been dedicated in the past to understand the property of games like 20Q (e.g., Navarro et al., 2010) and how humans solve them.': 0.46848800778388977, 'It would interesting to see how the tasks proposed in this work distinguish themselves from the ones studied in the existing literature, and how humans would perform on them.': 0.8790705800056458, 'In particular, Cohen & Lake, 2016m have recently studied the 20 questions games in their paper “Searching large hypothesis spaces by asking questions” where they both evaluate the performance of humans and computer.': 0.7307782769203186, 'I believe that this paper would really benefits from a similar study.': 0.7568019032478333, 'Developing the ability of models to actively seek for information to solve a task is a very interesting but challenging problem.': 1.0723271369934082, 'In this paper, all of the  tasks require the agent to select a questions from a finite set of clean and informative possibilities.': 0.9769406318664551, 'This allows a simpler analysis of how a given agent may perform but at the cost of a reducing the level of noise that would appear in more realistic settings.': 0.7921229004859924, 'This paper also show that by using a relatively standard mix of deep learning models and reinforcement learning, they are able to train agents that can solve these tasks in the way it was intended to.': 1.0985666513442993, 'This validates their empirical setting but also may exhibit some of the limitation of their approach; using relatively toy-ish settings with perfect information and a fixed number of questions may be too simple.': 1.0986032485961914, 'While it is interesting to see that their agent are able to perform well on all of their tasks, the absence of baselines limit the conclusions we can draw from these experiments.': 1.0986073017120361, 'For example in the Hangman experiment, it seems that the frequency based model obtains promising performance.': 1.098565697669983, 'It would interesting to see how good are baselines that may use the co-occurrence of letters or the frequency of character n-grams.': 1.0986120700836182, 'Overall, this paper explores a very interesting direction of research and propose a set of promising tasks to test the capability of a model to learn from asking question.': 1.0986119508743286, 'However, the current analysis of the tasks is a bit limited, and it is hard to draw any conclusion from them.': 1.0941290855407715, 'It would be good if the paper would focus more on how humans perform on these tasks, on strong simple baselines and on more tasks related to natural language (since it is one of the motivation of this work) rather than on solving them with relatively sophisticated models.': 1.098480463027954, 'This paper proposed to use Generalized Advantage Estimation (GAE) to optimize DNNs for information seeking tasks.': 1.0961129665374756, 'The task is posed as a reinforcement learning problem and the proposed method explicitly promotes information gain to encourage exploration.': 1.0986120700836182, 'Both GAE and DNN have been used for RL before.': 0.9884986877441406, 'The novelty in this paper seems to be the explicit modeling of information gain.': 1.0958898067474365, 'However, there is insufficient empirical evidence to demonstrate the benefit and generality of the proposed method.': 0.8542323112487793, ""An apple to apple comparison to previous RL framework that doesn't model information gain is missing."": 1.09861159324646, 'For example, the cluttered MNIST experiment tried to compare against Mnih et al. (2014) (which is a little out dated) with two settings.': 1.0986114740371704, 'But in both setting the input to the two methods are different.': 0.4412500858306885, 'Thus it is unclear what contributed to the performance difference.': 1.0915379524230957, 'The experiment section is cluttered and hard to read.': 1.0986123085021973, 'A table that summarizes the numbers would be much better.': 1.0986123085021973}"
358,https://openreview.net/forum?id=SyWvgP5el,"{'This paper explores ensemble optimisation in the context of policy-gradient training.': 1.0986123085021973, 'Ensemble training has been a low-hanging fruit for many years in the this space and this paper finally touches on this interesting subject.': 0.782185971736908, 'The paper is well written and accessible.': 1.0986123085021973, 'In particular the questions posed in section 4 are well posed and interesting.': 1.0947140455245972, 'That said the paper does have some very weak points, most obviously that all of its results are for a very particular choice of domain+parameters.': 1.0986121892929077, 'I eagerly look forward to the journal version where these experiments are repeated for all sorts of source domain/target domain/parameter combinations.': 1.0986123085021973, '<rant': 1.0911659002304077, 'Finally a stylistic comment that the authors can feel free to ignore.': 1.0986047983169556, ""I don't like the trend of every paper coming up with a new acronymy wEiRDLY cAsEd name."": 1.0982885360717773, 'Especially here when the idea is so simple.': 0.8961918354034424, 'Why not use words?': 1.0984874963760376, 'English words from the dictionary.': 0.7025639414787292, 'Instead of ""EPOpt"" and ""EPOpt-e"", you can write ""ensemble training"" and ""robust ensemble training"".': 0.4386937618255615, 'Is that not clearer?': 1.0598764419555664, '/>': 1.0986123085021973, 'Paper addresses systematic discrepancies between simulated and real-world policy control domains.': 1.09857177734375, 'Proposed method contains two ideas: 1) training on an ensemble of models in an adversarial fashion to learn policies that are robust to errors and 2) adaptation of the source domain ensemble using data from a (real-world) target domain.': 1.09861159324646, '> Significance': 1.0986123085021973, 'Paper addresses and important and significant problem.': 0.4947642683982849, 'The approach taken in addressing it is also interesting': 0.6908353567123413, '> Clarity': 1.0986123085021973, 'Paper is well written, but does require domain knowledge to understand.': 1.0986123085021973, 'My main concerns were well addressed by the rebuttal and corresponding revisions to the paper.': 1.0906031131744385, 'The paper looks at the problem of transferring a policy learned in a simulator to  a target real-world system.': 1.0986087322235107, 'The proposed approach considers using an ensemble of simulated source domains, along with adversarial training, to learn a robust policy that is able to generalize to several target domains.': 1.0986119508743286, 'Overall, the paper tackles an interesting problem, and provides a reasonable solution.': 1.0985628366470337, 'The notion of adversarial training used here does not seem the same as other recent literature (e.g. on GANs).': 1.0986123085021973, 'It would be useful to add more details on a few components, as discussed in the question/response round.': 1.0986123085021973, 'I also encourage including the results with alternative policy gradient subroutines, even if they don’t perform well (e.g. Reinforce), as well as results with and without the baseline on the value function.': 1.098610758781433, 'Such results are very useful to other researchers.': 0.16700336337089539}"
359,https://openreview.net/forum?id=SyZprb5xg,"{'SUMMARY': 1.0986123085021973, 'This paper presents a study of the number of hidden units and training examples needed to learn functions from a particular class.': 1.0986123085021973, 'This class is defined as those Boolean functions with an upper bound on the variability of the outputs.': 1.0986123085021973, 'PROS': 1.0986077785491943, 'The paper promotes interesting results from the theoretical computer science community to investigate the efficiency of representation of functions with limited variability in terms of shallow feedforward networks with linear threshold units.': 1.0986123085021973, 'CONS': 0.6150158643722534, 'The analysis is limited to shallow networks.': 1.0986123085021973, 'The analysis is based on piecing together interesting results, however without contributing significant innovations.': 1.0986123085021973, 'The presentation of the main results and conclusions is somewhat obscure, as the therein appearing terms/constants do not express a clear relation between increased robustness and decreasing number of required hidden units.': 1.0986123085021973, 'COMMENTS': 1.0986123085021973, ""In the abstract one reads ``The universal approximation theorem for neural networks says that any reasonable function is well-approximated by a two-layer neural network with sigmoid gates but it does not provide good bounds on the number of hidden-layer nodes or the weights.''"": 1.0986123085021973, 'In page 1 the paper points the reader to a review article.': 1.0112775564193726, 'It could be a good idea to include also more recent references.': 1.0986123085021973, 'Given the motivation presented in the abstract of the paper it would be a good idea to also comment of works discussing the classes of Boolean functions representable by linear threshold networks.': 1.0986123085021973, 'For instance the paper': 1.0986123085021973, '[Hyperplane Arrangements Separating Arbitrary Vertex Classes in n-Cubes.': 1.0986123085021973, 'Wenzel, Ay, Paseman] discusses various classes of functions that can be represented by shallow linear threshold networks and provides upper and lower bounds on the number of hidden units needed for representing various types of Boolean functions.': 1.098495364189148, 'In particular that paper also provides lower bounds on the number of hidden units needed to define a universal approximator.': 1.0986123085021973, 'It certainly would be a good idea to discuss the results on the learning complexity in terms of measures such as the VC-dimension.': 1.0986123085021973, 'Thank you for the explanations regarding the constants.': 1.0986123085021973, 'So if the noise sensitivity is kept constant, larger values of epsilon are associated with a smaller value of delta and of 1/epsilon.': 1.0986123085021973, 'Nonetheless, the description in Theorem 2 is in terms of poly(1/epsilon, 1/delta), which still could increase?': 1.0980124473571777, 'Also, in Lemma 1 reducing the sensitivity at a constant noise increases the bound on k?': 1.0986123085021973, 'The fact that the descriptions are independent of n seems to be related to the definition of the noise sensitivity as an expectation over all inputs.': 1.0986123085021973, 'This certainly deserves more discussion.': 1.0986123085021973, 'One good start could be to discuss examples of functions with an upper bound on the noise sensitivity (aside from the linear threshold functions discussed in Lemma 2).': 1.0986123085021973, 'Also, reverse statements to Lemma 1 would be interesting, describing the noise sensitivity of juntas specifically, even if only as simple examples.': 1.0986123085021973, ""On page 3 ``...variables is polynomial in the noise-sensitivity parameters'' should be inverse of?"": 1.0986123085021973, 'MINOR COMMENTS': 1.0986123085021973, 'On page 5 Proposition 1 should be Lemma 1?': 1.0986123085021973, ""This work finds a connection between Bourgain's junta problem, the existing results in circuit complexity, and the approximation of a boolean function using two-layer neural net."": 1.0986123085021973, 'I think that finding connections between different fields and applying the insights gained is a valid contribution.': 1.0985047817230225, 'For this reason, I recommend acceptance.': 0.980612576007843, 'But my current major concern is that this work is only constrained to the domain of boolean hypercube, which is far from what is done in practice (continuous domain).': 1.0986123085021973, 'Indeed, the authors could argue that understanding the former is a first step, but if the connection is only suitable for this case and not adaptable to more general scenarios, then it probably would have limited interest.': 1.0986073017120361, 'The approximation capabilities of neural networks have been studied before for approximating different classes of functions.': 1.0986123085021973, 'The goal of this paper is to provide an analog of the approximation theorem for the class of noise-stable functions.': 1.0986123085021973, 'The class of functions that are noise-stable and their output does not significantly depend on an individual input seems an interesting class and therefore I find the problem definition interesting.': 1.0986123085021973, 'The paper is well-written and it is easy to follow the proofs and arguments.': 1.0986123085021973, 'I have two major comments:': 1.0986123085021973, '1- Presentation: The way I understand this arguments is that the noise-stability measures the ""true"" dimensionality of the data based on the dependence of the function on different dimensions.': 1.0985803604125977, 'Therefore, it is possible to restate and prove an analog to the approximation theorems based on ""true"" dimensionality of data.': 1.0986123085021973, 'It is also unclear when the stability based bounds are tighter than dimension based bounds as both of them grow exponentially.': 1.0986123085021973, 'I find these discussions interesting but unfortunately, the authors present the result as some bound that does not depend on the dimension and a constant (!??)': 1.0986123085021973, 'that grows exponentially with (1/eps).': 1.0986123085021973, 'This is not entirely the right picture because the epsilon in the stability could itself depend on the dimension.': 1.0986123085021973, 'I believe in most problems (1/epsilon) grows with the dimension.': 1.0986123085021973, '2- Contribution: Even though the connection is new and interesting, the contribution of the paper is not significant enough.': 1.0986123085021973, 'The presented results are direct applications of previous works and most of the lemmas in the paper are restating the known results.': 1.0986123085021973, 'I believe more discussions and results need to be added to make this a complete work.': 1.0986123085021973}"
360,https://openreview.net/forum?id=Syfkm6cgx,"{'This paper empirically studies the invariance, equivariance and equivalence properties of representations learned by convolutional networks under various kinds of data augmentation.': 1.0978370904922485, 'Additional loss terms are presented which can make a representation more invariant or equivariant.': 1.097971796989441, 'The idea of measuring invariance, equivariance and equivalence of representations is not new (Lenc & Vedaldi).': 1.0983237028121948, 'The authors are the first to systematically study the effect of data augmentation on these properties, but it is unclear in what way the results are surprising, interesting, or useful.': 1.0986108779907227, 'It is not really surprising that data augmentation increases invariance, or that training with the same augmentation leads to more similar representations than training with different augmentations.': 1.0985788106918335, 'Regarding the presented method to increase invariance and equivariance: while it could be that a representation will generalize better if it is invariant or equivariant, it is not clear why one would want to increase in/equivariance if it does not indeed lead to improvements in performance.': 1.0986113548278809, 'The paper presents no evidence that training for increased invariance / equivariance leads to substantial improvements in performance.': 1.098548173904419, 'Combined with the fact that the loss (eq. 6) would substantially increase the computational burden, I don’t think this technique will be very useful.': 1.0986123085021973, 'Minor comments:': 1.0986123085021973, 'R^{nxn} should be R^{n \\times n}': 1.0985503196716309, 'In eq. 2: ‘equivaraince’': 1.0986086130142212, 'In 3.3, argmax is not properly formatted': 1.0986123085021973, 'I think data augmentation was already considered essential before Krizhevsky et al.': 1.0287450551986694, 'Not really correct to attribute this to them.': 0.9332922697067261, 'About the claim “This is related to the idea of whether CNNs collapse (invariance) or linearize (equivariance) view manifolds of 3D objects”.': 1.09238862991333, 'The idea that equivariance means that the manifold (orbit) is linearized, is incorrect.': 1.0985219478607178, 'A linear representation M_g can create nonlinear manifolds.': 1.0986008644104004, 'A simple example is given by a rotation matrix in 2D (clearly linear), generating a nonlinear manifold (the circle).': 1.098609447479248, 'Equivariance in eq. 2 should be called “non-equivariance”.': 0.9699671268463135, 'If the value is low, the representation is equivariant, while if it is high it is non-equivariant.': 0.257589191198349, '“Eq. 2  also uses the paradigm that”, uses the word paradigm in a strange manner': 1.0419793128967285, 'In the definition of x’_ij, should one of the g_j be inverted?': 0.4510497450828552, 'Otherwise it seems like the transformation is applied twice, instead of being undone.': 1.0986123085021973, 'This work presents an empirical study of the influence of different types of data augmentation on the performance of CNNs.': 1.0986121892929077, 'It also proposes to incorporate additional loss functions to encourage approximate invariance or equivariance, and shows there are some benefits.': 1.0928983688354492, 'The paper reads well and the objectives are clear.': 1.0986123085021973, 'The study of invariances in CNNs is a very important topic, and advances in this area are greatly appreciated.': 1.0985931158065796, 'The paper splits itself in two very different parts': 1.0794553756713867, 'the empirical study of equivariances in existing CNNs, and the proposal of equivariance objectives.': 1.0976208448410034, 'However, taken separately each of these two parts could be better executed.': 1.0986098051071167, ""On the empirical study, its breath is relatively limited, and it's hard to draw any far-reaching conclusions from it:"": 1.0986123085021973, 'Only one network is studied; at least one other architecture would have made for better generalization.': 1.0986123085021973, 'Only one layer (fc7) is studied; this presents issues as the top layer is the most invariant.': 1.0986123085021973, 'At least one convolutional layer (possibly more) should have been considered.': 1.097105622291565, 'The reliance on the scanned text dataset does not help; however the ImageNet results are definitely very encouraging.': 1.0986123085021973, 'It is nice to see how performance degrades with the degree of transformations, and the authors do interpret the results, but it would be better to see more analysis.': 1.0986095666885376, 'There is only a limited set of conclusions that can be drawn from evaluating networks with jittered data.': 1.0986123085021973, 'If the authors could propose some other interesting ways to assess the invariance and equivariance, they would potentially draw more insightful conclusions from it.': 1.0985991954803467, 'On the proposed loss function, only a very quick treatment of it is given (Section 4, half a page).': 1.0986123085021973, 'It does not differ too much from known invariance/equivariance objectives studied in the literature previously, e.g. Decoste and Scholkopf, ""Training Invariant Support Vector Machines"", Machine Learning, 2002.': 1.098509669303894, ""I'm not sure that dividing the paper into these two different contributions is the best approach; they both feel a bit incomplete, and a full treatment of only one of them would make for an overall better paper."": 1.098611831665039, 'This paper is an extension of Lenc&Vedaldi15 paper, showing CNN representations at FC7 layer are to certain extent equivariant to various classes of transformations and that training with a certain group of transformation makes the representations more equivalent.': 1.0986123085021973, 'Authors performed a large amount of experiments, training over 30 networks with different forms of jitter, which is quite impressive.': 1.0986051559448242, 'However it is rather difficult to find a main message of this work.': 0.4287196099758148, 'Yes, authors measured the properties on a different layer than the Lenc&Vedaldi15, however it is hard to find some novel insights other than the known fact that jitter helps to achieve invariance.': 1.098369836807251, 'The evaluation seems to be mostly correct, however the paper does not seem to be solving the task advertised in its title really well.': 1.0985854864120483, 'Major issues are in the experiments with the representation distances:': 1.0083482265472412, '*': 0.37198975682258606, 'The selection of only FC7 is a bit controversial - it is followed only by a single classification layer (FC8) to the common output - class likelyhoods.': 1.0986123085021973, 'Because the FC8 is just a linear projections, what the equivalence map does is just to re-project the FC8 weights of the attached network to the weights of the original network.': 1.098603367805481, 'Probably performing similar experiments but on more layers may be more useful (as the networks are already trained).': 1.0986123085021973, 'The experiment with representation distance is missing what is the classification error on the testing dataset.': 1.0986114740371704, 'This would answer whether the representations are actually compatible up to linear transformation at all...': 1.0986123085021973, '* It is not clear for the experiment with K-NN whether this is measured per each test set example?': 0.9660742282867432, 'After training the equivalence map?': 0.4416932463645935, 'More clear would be to show that networks trained on similar group of jitter transformations are more compatible on the target task.': 0.7497026920318604, 'The proposed method does not seem to improve equivariance consistently on all tasks.': 0.8073946833610535, 'Especially with \\lambda_1 and \\lambda_2 having such small values, the loss is basically equal to simple data jitter as it just adds up the loss of the original and transformed image.': 0.6588650941848755, 'Maybe the issue is in the selection of the FC7 layer?': 1.0986080169677734, 'In general, this paper shows some interesting results on the FC7 equivariance, but it does not seem to be drawing many interesting new observations out of these experiments.': 1.0959187746047974, 'Due to some issues with the equivalence experiments and the finetuning of equivariance, I would not recommend acceptance of this manuscript.': 1.0986123085021973, 'However, refining the experiments on already trained networks and restructuring this manuscript into more investigative work may lead to interesting contribution to the field.': 1.0986121892929077, 'There are also few minor issues:': 1.0986123085021973, '* It is not experimentally verified that the new criterion for equivariance mapping helps to gain better results.': 1.0986123085021973, 'The angles on page 1 and 5 are missing units (degrees?).': 1.0981870889663696, '* On page three, ""In practice, it is difficult... "", it is not M_g which is maximised/minimised, but the loss over the M_g': 0.30716565251350403, '* Page 4, footnote 2 - if you are just halving the activations, it is hard to call it a dropout as this constant factor can be passed to the following/preceding weights': 0.47576791048049927, '* Is the network for RVL-CDIP the same architecture as Alexnet?': 0.8751847743988037, '* On page 7, Figure 3a+3b - in my opinion, turning the diagonal elements to white is really misleading, and probably even incorrect, as the distance between the same representations should be zero (which is also a way how to verify that the experiments are performed correctly).': 0.24281162023544312}"
361,https://openreview.net/forum?id=SygGlIBcel,"{'This paper proposes an extension of neural network language (NLM) models to better handle large vocabularies.': 1.0986123085021973, 'The main idea is to obtain word embeddings by combining character-level embeddings with a convolutional network.': 1.0970637798309326, 'The authors compare word embeddings (WE),character embeddings (CE) as well a combined character and word embeddings (CWE).': 1.0978522300720215, ""It's quite obvious how CE or CWE embeddings can be used at the input of an NLM, but this is more tricky at the output layer."": 1.0982301235198975, 'The authors propose to use NCE to handle this problem.': 0.7858923673629761, 'NCE allows to speed-up training, but has no impact on inference during testing: the full softmax output layer must be calculated and normalized (which can be very costly).': 1.07781982421875, 'It was not clear to me how the network is used during TESTING with an open-vocabulary.': 1.0986119508743286, 'Since the NLM is only used during reranking, the unnormalized probability of the requested word could be obtained at the output.': 1.0986123085021973, 'However, when reranking n-best lists with the NLM feature, different sentences are compared and I wonder whether this does work well without proper normalization.': 1.0986121892929077, 'In addition, the authors provide perplexities in Table 2 and Figures 2 and 3.': 1.0774927139282227, 'This needs normalization, but it is not clear to me how this was performed.': 1.0968772172927856, 'The authors mention a 250k output vocabulary.': 1.0985636711120605, 'I doubt that the softmax was calculated over 250k values.': 1.0986123085021973, 'Please explain.': 1.09352707862854, 'The model is evaluated by reranking n-best lists of an SMT systems for the IWSLT 2016 EN/CZ task.': 1.0984816551208496, 'In the abstract, the authors mention a gain of 0.7 BLEU.': 0.9531805515289307, 'I do not agree with this claim.': 0.4246172606945038, 'A vanilla word-based NLM, i.e. a well-known model, achieves already a gain of 0.6 BLEU.': 1.0973433256149292, 'Therefore, the new model proposed in this paper brings only an additional improvement of 0.1 BLEU.': 0.6222653388977051, 'This is not statistically significant.': 0.8543186783790588, 'I conjecture that a similar variation could be obtained by just training several models with different initializations, etc.': 1.098611831665039, 'Unfortunately, the NLM models which use a character representation at the output do not work well.': 1.064234972000122, 'There are already several works which use some form of character-level representations at the input.': 1.0942590236663818, 'Could you please discuss the computational complexity during training and inference.': 1.0985050201416016, 'Minor comments': 0.5762733221054077, '- Figure 2 and 3 have the caption ""Figure 4"". This is misleading.': 0.9416807889938354, '- the format of the citations is unusual, eg.': 1.0343356132507324, '""While the use of subword units Botha & Blunsom (2014)""': 1.0877315998077393, '->': 1.0986123085021973, '""While the use of subword units (Botha & Blunsom, 2014)""': 1.098463535308838, 'this paper proposes a model for representing unseen words in a neural language model.': 1.0986123085021973, 'the proposed model achieves poor results in LM and a slight improvement over a baseline model.': 1.0986123085021973, 'this work needs a more comprehensive analysis:': 1.0986123085021973, ""there's no comparison with related work trying to address the same problem"": 0.7195979952812195, 'an intrinsic evaluation and investigation of why/how their work should be better are missing.': 1.095159649848938, 'to make a bolder claim, more investigation should be done with other morphologically rich languages.': 1.0553752183914185, 'Especially for MT, in addition to going from En-> Language_X, MRL_X -> En or MRL_X -> MRL_Y should be done.': 1.097306489944458, 'In this submission, an interesting approach to character-based language modeling is pursued that retains word-level representations both in the context, and optionally also in the output.': 0.8771911859512329, 'However, the approach is not new, cf.': 0.05049000680446625, '(Kim et al. 2015) as cited in the submission, as well as (Jozefowicz et al. 2016).': 1.0006189346313477, 'Both Kim and Jozefowicz already go beyond this submission by applying the approach using RNNs/LSTMs.': 1.0986090898513794, 'Also, Jozefowicz et al. provide a comparative discussion of different approaches to character-level modeling, which I am missing here, at least by discussing this existing work.': 0.9807774424552917, 'THe remaining novelty of the approach then would be its application to machine translation, although it remains somewhat unclear, inhowfar reranking of N-best lists can handle the OOV problem - the translation-related part of the OVV problem should be elaborated here.': 1.0969988107681274, 'That said, some of the claims of this submission seems somewhat exaggerated, like the statement in Sec. 2.3: ""making the notion of vocabulary obsolete"", whereas the authors e.g. express doubts concerning the interpretation of perplexity w/o an explicit output vocabulary.': 0.9210243821144104, 'For example modeling of especially frequent word forms still can be expected to contribute, as shown in e.g. arXiv:1609.08144': 1.0986123085021973, 'Sec. 2.3: You claim that the objective requires a finite vocabulary.': 1.0981403589248657, 'This statement only is correct if the units considered are limited to full word forms.': 0.79671311378479, 'However, using subwords and even individual characters, implicitly larger and even infinite vocabularies can be covered with the log-likelihood criterion.': 0.9962431788444519, 'Even though this require a model different from the one proposed here, the corresponding statement should qualified in this respect.': 1.0985928773880005, 'The way character embeddings are used for the output should be clarified.': 0.9188529253005981, 'The description in Sec. 2.4 is not explicit enough in my view.': 0.768345296382904, 'Concerning the configuration of NCE, it would be desirable to get a better idea of how you arrived at your specific configuration and parameterization described in Sec. 3.4.': 0.5891743302345276, 'Sec. 4.1: you might want to mention that (Kim et al. 2015) came to similar conclusions w.r.t.': 0.3654319643974304, 'the performance of using character embeddings at the output, and discuss the suggestions for possible improvements given therein.': 0.9955212473869324, 'Sec. 4.2: there are ways to calculate and interpret perplexity for unknown words, cf.': 1.098559021949768, '(Shaik et al. IWSLT 2013).': 1.098592758178711, 'Sec. 4.4 and Table 4: the size of the full training vocabulary should be provided here.': 0.8445945382118225, 'Minor comments:': 1.0986123085021973, 'p. 2, bottom: three different input layer -> three different input layers (plural)': 0.5065000057220459, 'Fig. 1: fonts within the figure are way too small': 1.0986121892929077, 'p. 3, first item below Fig.': 1.0986123085021973, '1: that we will note WE -> that we will denote WE': 1.0986123085021973, ""Sec. 2.3: the parameters estimation -> the parameter estimation (or: the parameters' estimation)"": 1.0986123085021973, 'p. 5, first paragraph: in factored way -> in a factored way': 1.0986123085021973, 'p. 5, second paragraph: a n-best list, a nk-best list -> an n-best list, an nk-best list': 1.0982452630996704, 'Sec. 4.2, last sentence: Despite adaptive gradient, -> verb and article missing': 1.0986123085021973}"
362,https://openreview.net/forum?id=SygvTcYee,"{'The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) (Carreira-Perpinan and Wang, 2012).': 1.3862943649291992, 'This method decomposes the optimization into training individual layers and updating the auxiliary coordinates.': 1.3862943649291992, 'The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing the parameters to move between machines.': 1.3862943649291992, 'Relatively good speedup factors are reported especially on larger datasets and a theoretical model of performance is presented that matches with the experiments.': 1.3862943649291992, 'My main concern is that even though the method is presented as a general framework for nested functions, experiments focus on a restricted family of models (i.e. binary autoencoders with linear or kernel encoders and linear decoders) with only two components.': 1.3862943649291992, 'While the speedup factors are encouraging, it is hard to get a sense of their importance as the binary autoencoder model considered is not well studied by other researchers and is not widely used.': 1.383059024810791, 'I encourage the authors to apply this framework to more generic architectures and problems.': 1.3862943649291992, 'Questions:': 1.3862943649291992, '1- Does this framework apply to some form of generic multi-layer neural network?': 1.3862943649291992, 'If so, some experimental results are useful.': 1.3862943649291992, '2- What is the implication of applying this framework to more than two components (an encoder and a decoder) and non-linear components?': 0.9536141157150269, '3- It is desired to see a plot of performance as a function of time for different setups to demonstrate the speedup after convergence.': 1.3720507621765137, 'It seems the paper only focuses on the speedup factors per iteration.': 1.3806662559509277, 'For example, increasing the mini-batch size may improve the speed per iteration but may hurt the convergence speed.': 1.3214956521987915, '4- Did you consider a scenario where the dataset is too big that storing the data and auxiliary variables on multiple machines simultaneously is not possible?': 1.1098668575286865, 'The paper cites an ArXiv manuscript with the same title by the authors multiple times.': 0.30857932567596436, 'Please make the paper self-contained and include any supplementary material in the appendix.': 0.5175679922103882, 'I believe without applying this framework to a more generic architecture beyond binary autoencoders, this paper does not appeal to a wide audience at ICLR, hence weak reject.': 0.9265130758285522, 'UPDATE:': 1.3084739446640015, 'I looked at the arxiv version of the paper.': 1.3862801790237427, 'It is much longer and appears more rigorous.': 1.3862943649291992, 'Fig 3 there is indeed more insightful.': 1.3862943649291992, 'However, I am reviewing the submission and my overall assessment does not change.': 1.2633970975875854, 'This is not a minor incremental contribution, and if you want to compress it into a conference submission of this type, I would recommend choosing message you want to convey, and focus on that.': 1.3859299421310425, 'As you say, ""...ICLR submission focus on the ParMAC algorithm...': 1.3806809186935425, '"", I would focus on this properly - and remove or move to appendix all extensions and theoretical remarks, and have an extra page on explaining the algorithm.': 1.3862698078155518, 'Additionally, make sure to clearly explain the relation of the arxiv paper, in particular that the submission was a compressed version.': 1.3862943649291992, 'ORIGINAL REVIEW:': 1.3622210025787354, 'The submission proposes ParMAC, based on MAC (Method of Auxiliary Coordinates), formulating a distributed variant of the idea.': 1.373805046081543, 'Related Work: In the part on convex ERM and methods, I would recommend citing general communication efficient frameworks, COCOA (Ma et al.) and AIDE (Reddi et al.).': 1.0787813663482666, 'I believe these works are most related to the practical objectives authors of this paper set, while number of the papers cited are less relevant.': 1.2944371700286865, 'Section 2, explaining MAC, is quite clearly written, but I do not find part on MAC and EM particularly useful.': 1.3862943649291992, 'Section 3 is much less clearly written.': 1.3862943649291992, 'I have trouble following notation, particularly in the speedups part, as different symbols were introduced at different places.': 1.3854392766952515, 'Perhaps a quick summary or paragraph on notation in the introduction would be helpful.': 0.8729340434074402, 'In paragraph 2, you write as if reader knew how data/anything is distributed, but this was not mentioned yet; it is specified later.': 1.3862839937210083, 'It is not clear what is meant by ""submodel"".': 1.386218547821045, 'Perhaps a more precise example pointing back to eqs (1) & (2) would be useful.': 1.206989049911499, 'As far as I understand from what is written, there are P independent sets of submodels, that traverse the machines in circular fashion.': 1.1781141757965088, ""I don't understand how are they initialized (identically?), and more importantly I don't understand what would be a single output of the algorithm (averaging?"": 0.7746257185935974, 'does not seem to make sense).': 1.3862884044647217, 'Since this is not addressed, I suppose I get it wrong, leaving me to guess what was actually meant.': 1.3573486804962158, 'The fact that I am not able to understand what is actually happening, I see as major issue.': 1.3862943649291992, ""I don't like the later paragraphs on extensions, model for speedup, convergence and topologies."": 1.3862943649291992, ""I don't understand whether these are novel contributions or not, as the authors refer to other work for details."": 1.3862943649291992, 'If these are novel, the explanation is not sufficient, particularly speedup part, which contains undefined quantities, e.g. T(P)': 1.3862943649291992, ""(or I can't find it)."": 1.3862943649291992, 'If this is not novel, It does not provide enough explanation to understand anything more, compared with a its version compressed to 1/4 of its size and referring to the other work.': 1.1276218891143799, ""The statement that we can recover the original convergence guarantees seems strong and I don't see why it should be trivial to show (but author point to other work which I did not look at)."": 1.1771478652954102, 'In topologies part, claiming that something does ""true SGD"", without explaining what is ""true SGD"" seems very strange.': 1.3237955570220947, 'Other statements in this section seem also very vague and unjustified/unexplained.': 0.7373831272125244, ""Experimental section seems to suggest that the method is interesting for binary autoencoders, but I don't see how would I conclude anything about any other models."": 0.34305140376091003, 'ParMAC is also not compared to alternative methods, only with itself, focusing on scaling properties.': 1.3862195014953613, 'Conclusion contains statements that are too strong or misleading based on what I saw.': 1.386285424232483, 'In particular, ""we analysed its parallel speedup and convergence"" seems ungrounded.': 1.3861528635025024, 'Further, the claim ""The convergence properties of MAC remain essentially unaltered in ParMAC"" is unsupported, regardless of the meaning of ""essentially unchanged"".': 1.127548336982727, ""In summary, the method seems relevant for particular model class, binary autoencoders, but clarity of presentation is insufficient - I wouldn't be able to recreate the algorithm used in experiments - and the paper contains a number of questionable claims."": 1.3714966773986816, 'This paper proposes an extension of the MAC method in which subproblems are trained on a distributed cluster arranged in a circular configuration.': 1.3862943649291992, 'The basic idea of MAC is to decouple the optimization between parameters and the outputs of sub-pieces of the model (auxiliary coordinates); optimization alternates between updating the coordinates given the parameters and optimizing the parameters given the outputs.': 1.3829385042190552, 'In the circular configuration.': 1.3862942457199097, 'Because each update is independent, they can be massively parallelized.': 1.3862309455871582, 'This paper would greatly benefit from more concrete examples of the sub-problems and how they decompose.': 1.3862941265106201, 'For instance, can this be applied effectively for deep convolutional networks, recurrent models, etc?': 1.3862800598144531, ""From a practical perspective, there's not much impact for this paper beyond showing that this particular decoupling scheme works better than others."": 1.386293888092041, 'There also seem to be a few ideas worth comparing, at least:': 1.384724497795105, 'Circular vs. parameter server configurations': 1.3862943649291992, 'Decoupled sub-problems vs. parallel SGD': 1.3862943649291992, ""Parallel SGD also has the benefit that it's extremely easy to implement on top of NN toolboxes, so this has to work a lot better to be practically useful."": 1.3862943649291992, ""Also, it's a bit hard to understand what exactly is being passed around from round to round, and what the trade-offs would be in a deep feed-forward network."": 1.3862943649291992, 'Assuming you have one sub-problem for every hidden unit, then it seems like:': 1.3862943649291992, '1. In the W step, different bits of the NN walk their way around the cluster, taking SGD steps w.r.t. the coordinates stored on each machine. This means passing around the parameter vector for each hidden unit.': 1.3862943649291992, ""2. Then there's a synchronization step to gather the parameters from each submodel, requiring a traversal of the circular structure."": 1.3862943649291992, ""3. Then each machine updates it's coordinates based on the complete model for a slice of the data. This would mean, for a feed-forward network, producing the intermediate activations of each layer for each data point."": 1.3862943649291992, 'So for something comparable to parallel SGD, you could do the following: put a mini-batch of size B on each machine with ParMAC, compared to running such mini-batches in parallel.': 1.3862943649291992, 'Completing steps 1-2-3 above would then be roughly equivalent to one synchronized PS type implementation step (distribute model to workers, get P gradients back, update model.)': 1.2856823205947876, 'It would be really helpful to see how this compares in practice.': 1.2698949575424194, ""It's hard for me to understand intuitively why the proposed method is theoretically any better than parallel SGD (except for the issue of non-smooth function optimization); the decoupling also can fundamentally change the problem since you're not doing back-propagation directly anymore, so that seems like it would conflate things as well and it's not necessarily going to just work for other types of architectures."": 1.3817840814590454, 'This paper proposes a novel approach ParMAC, a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models which is based on the composition of multiple processing layers (i.e., deep nets).': 1.3861509561538696, 'The basic idea of MAC to optimise the nested objective function, which is traditionally learned using methods based on the chain-rule gradients but inconvenient and is hard to parallelise, is to break nested functional relationships judiciously by introducing new variables ( the auxiliary coordinates) as equality constraints, and then to optimise a penalised function using alternating optimisation over the original parameters (W step) and over the coordinates (Z step).': 1.381803035736084, 'The minimisation (W step) updates the parameters by splitting the nested model into independent submodels and training them using existing algorithms, and the coordination (Z step) ensures that corresponding inputs and outputs of submodels eventually match.': 1.3862943649291992, 'In this paper, the basic assumptions of ParMAC are that with large datasets in distributed systems, it is imperative to minimise data movement over the network because of the communication time generally far exceeds the computation time in modern architectures.': 1.3862943649291992, 'Thus, the authors propose the ParMAC to translate the parallelism inherent in MAC into a distributed system by data parallelism and model parallelism.': 1.3862943649291992, 'They also analyse its parallel speedup and convergence, and demonstrated it with MPI-based implementation to optimise binary autoencoders.': 1.3862943649291992, 'The proposed ParMAC is tested on 3 colour image retrieval datasets.': 1.3862943649291992, 'The organization of the paper is well written, and the presentation is clear.': 1.3862943649291992, 'My questions are included in the following:': 1.3862943649291992, 'The MAC framework solves the original problem approximately.': 1.3862943649291992, 'If people use the sigmoid function to smooth the stepwise function, the naive optimization methods can be easier applied.': 1.3862943649291992, 'What is the difference between these two?': 1.3862943649291992, 'Or why do we want to use a new approach to solve it?': 1.3862943649291992, 'The authors do not compare their ParMAC model with other distributed approaches for the same nested function optimization problem.': 1.3862943649291992}"
363,https://openreview.net/forum?id=Syoiqwcxx,"{'The main merit of this paper is to draw again attention to how crucial initialization of deep network *can* be; and to counter the popular impression that modern architectures and improved gradient descent techniques make optimization local minima and saddle points no longer a  problem.': 1.0986123085021973, 'While the paper provides interesting counter-examples that showcase how bad initialization mixed with particular data can lead the optimization to get stuck at a poor solution, these feel like contrived artificial constructs.': 1.0986123085021973, 'More importantly the paper does not consider popular heuristics that likely help to avoid getting stuck, such as: non-saturating activation functions (e.g. leaky RELU), batch-norm, skip connections (resnet), that can all be thought of as contributing to keep the gradients flowing.': 1.0986123085021973, 'The paper puts up a big warning sign about potential initialization problems (with standard RELU nets), but without proposing new solutions or workarounds, nor carrying out a systematic analysis of how this picture is affected by most commonly used current heuristic techniques (in architecture, initialization and training).': 1.0986123085021973, 'Such a broader scope analysis, especially if it did lead to insights of practical relevance, could much increase the value of the paper for the reader.': 1.0986123085021973, 'This paper studies the error surface of deep rectifier networks, giving specific examples for which the error surface has local minima.': 1.0986123085021973, 'Several experimental results show that learning can be trapped at apparent local minima by a variety of factors ranging from the nature of the dataset to the nature of the initializations.': 1.0981916189193726, 'This paper develops a lot of good intuitions and useful examples of ways that training can go awry.': 1.098177433013916, 'Even though the examples constructed in this paper are contrived, this does not necessarily remove their theoretical importance.': 1.0986123085021973, 'It is very useful to have simple examples where things go wrong.': 1.0986123085021973, 'However the broader theoretical framing of the paper appears to be going after a strawman.': 1.0986123085021973, '“The underlying easiness of optimizing deep networks does not simply rest just in the emerging structures due to high dimensional spaces, but is rather tightly connected to the intrinsic characteristics of the data these models are run on.”': 1.0986123085021973, 'I believe this perspective is already contained in several of the works cited as not belonging to this perspective.': 1.0986123085021973, 'Choromanska et al., for instance, analyze Gaussian inputs, and so clearly make claims based on characteristics of the data the models are run on.': 1.0986123085021973, 'More broadly, the loss function is determined jointly by the dataset and the model parameters, and so no account of the error surface can be separated from dataset properties.': 1.0986123085021973, 'It is not clear to me what ‘emerging structures due to high dimensional spaces’ are, or what they could be, that would make them independent of the dataset and initial model parameters.': 1.0986123085021973, 'The emerging structure of the error surface is necessarily related to the dataset and model parameters.': 1.0986123085021973, 'Again, a key worry with this paper is that it is aiming at a strawman: replica methods characterize average behavior for infinite systems, so it is not surprising that specific finite sized systems might yield poor optimization landscapes.': 1.0986123085021973, 'The paper seems surprised that training can be broken with a bad initialization, but initialization is known to be critical, even for linear networks: saddle points are not innocuous, with bad initializations dramatically slowing learning (e.g. Saxe et al. 2014).': 1.0983271598815918, 'It seems like the proof of proposition 5 may have an error.': 1.0986123085021973, 'Suppose cdf_b(0) = 0 and cdf_W(0)=1/2.': 1.0986123085021973, 'We have P(learning fails) >': 1.0986123085021973, '= 1 - 1/2^{h^2(k-1)}, meaning that the probability of failure _increases_ as the number of hidden units increases.': 1.0967392921447754, 'It seems like it should rather be (ignoring the bias) p(fails) >': 1.0985958576202393, '= 1 - [ 1 - p(w<0)^h^2]^{k-1}.': 1.0985640287399292, 'In this case the limit as k-> infinity depends on how h scales with k, so it is no longer necessarily true that “one does not have a globally good behaviour of learning regardless of the model size.”': 1.0976217985153198, 'The paper also appears to insufficiently distinguish between local minima and saddle points.': 1.0986123085021973, 'Section 3.1 states it shows training being stuck in a local minimum, but this is based on training with a fixed budget of epochs.': 1.098591923713684, 'It is not possible to tell whether this result reflects a genuine local minimum or a saddle point based on simulation results.': 1.0986123085021973, 'It may also be the case that, while rectifiers suffer from genuine blind spots, sigmoid or soft rectifier nonlinearities may not.': 1.0985132455825806, 'On the XOR problem with two hidden nodes, for instance, it was thought that were local minima but in fact there are none (e.g. L. Hamey, “Analysis of the error surface of the XOR network with two hidden nodes,” 1995).': 1.0982469320297241, 'If the desire is simply to show that training does not converge for particular finite problems, much simpler counterexamples can be constructed and would suffice: set all hidden unit weights to zero, for instance.': 1.0986071825027466, 'In the response to prereview questions, the authors write ‘If the “complete characterization”': 1.0986123085021973, '[of the error surface] was indeed universally valid, we would not be able to break the learning with the initialization’': 0.5996053814888, 'but, as mentioned previously, the basic results for even deep linear networks show that a bad initialization (at or near a saddle point) will break learning.': 1.0928043127059937, 'Again, it seems this paper is attacking a straw man along the lines of “nothing can possibly go wrong with neural network training.”': 1.0986123085021973, 'No prior theoretical result claims this.': 1.0986123085021973, 'The Figure 2 explanation seems counterintuitive to me.': 1.0986123085021973, 'Simply scaling the input, if the weight matrices are initialized with zero biases, will not change the regions over which each ReLU activates.': 1.0756932497024536, 'That is, this manipulation does not achieve the goal of concentrating “most of the data points in very few linear regions.”': 1.0984464883804321, 'A far more likely explanation is that the much weaker scaling has not been compensated by the learning algorithm, but the algorithm would converge if run longer.': 1.0986101627349854, 'The response notes that training has been conducted for an order of magnitude longer than required for the unscaled input to converge, but the scaling on the data is not one but five orders of magnitude—and indeed the training does converge without issue for scaling up to four orders of magnitude.': 0.7030836343765259, 'The response notes that Adam should compensate for the scaling factor, but this depends on the details of the Adam implementation—the epsilon factor used to protect against division by zero, for example.': 1.0984448194503784, 'This paper contains many interesting results, but a variety of small technical concerns remain.': 1.0986123085021973, 'The paper studies some special cases of neural networks and datasets where optimization fails.': 1.0986123085021973, 'Most of the considered models and datasets are however highly constructed and do not follow the basic hyperparameters selection and parameter initialization heuristics.': 1.0986123085021973, 'This reduces the practical relevance of the analysis.': 1.0986123085021973, 'The experiment ""bad initialization on MNIST"" shows that for very negative biases or weights drawn from a non-centered distribution, all ReLU activations are ""off"" for all data points, and thus, optimization is prevented.': 1.0986123085021973, 'This never occurs in practice, because using proper initialization heuristics avoid these cases.': 1.0986123085021973, 'The ""jellyfish"" dataset constructed by the authors is demonstrated to be difficult to fit by a small model.': 1.0986123085021973, 'However, the size/depth of the considered model is unsuitable for this problem.': 1.0986123085021973, 'Proposition 4 assumes that we can choose the mean from which the weight parameters are initialized.': 1.0986123085021973, 'This is typically not the case in practice as most initialization heuristics draw weight parameters from a distribution with mean 0.': 1.0986123085021973, 'Proposition 5 considers infinitely deep ReLU networks.': 1.0986123085021973, 'Very deep networks would however preferably be of type ResNet.': 1.0986123085021973}"
364,https://openreview.net/forum?id=SypU81Ole,"{'This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models.': 1.0986123085021973, ""This paper is somewhat challenging to assess since it doesn't propose a new algorithm, model, application etc."": 1.0986123085021973, 'On the one hand these techniques will be highly relevant to the generative modeling community and I think this paper deserves a wide audience.': 1.0986123085021973, 'The techniques proposed are simple, well explained, and of immediate use to those working on generative models.': 1.0986123085021973, ""However, I'm not sure the paper is appropriate for an ICLR conference track as it doesn't provide any greater theoretical insights into sampling generative models and there are no comparisons / quantitative evaluations of the techniques proposed."": 1.0986123085021973, ""Overall, I'm very much on the fence since I think the techniques are useful and this paper should be read by those interested in generating modeling."": 1.0986123085021973, 'I would be willing to increase my core if the author could present a case for why ICLR is an appropriate venue for this work.': 1.0986123085021973, 'This paper proposed a set of different things under the name of ""sampling generative models"", focusing on analyzing the learned latent space and synthesizing desirable output images with certain properties for GANs.': 1.0984535217285156, 'This paper does not have one single clear message or idea, but rather proposed a set of techniques that seem to produce visually good looking results.': 1.059063196182251, 'While this paper has some interesting ideas, it also has a number of problems.': 1.0986123085021973, 'The spherical interpolation idea is interesting, but after a second thought this does not make much sense.': 1.0986123085021973, 'The proposed slerp interpolation equation (page 2) implicitly assumes that the two points q1 and q2 lie on the same sphere, in which case the parameter theta is the angle corresponding to the great arc connecting the two points on the sphere.': 1.0986096858978271, 'However, the latent space of a GAN, no matter trained with a uniform distribution or a Gaussian distribution, is not a distribution on a sphere, and many points have different distances to the origin.': 0.40583276748657227, ""The author's justification for this comes from the well known fact that in high dimensional space, even with a uniform distribution most points lie on a thin shell in the unit cube."": 1.0983760356903076, 'This is true because in high-dimensional space, the outer shell takes up most of the volume in space, and the inner part takes only a very small fraction of the space, in terms of volume.': 0.7487010955810547, 'This does not mean the density of data in the outer shell is greater than the inner part, though.': 1.0986123085021973, 'In a uniform distribution, the data density should be equal everywhere, a point on the outer shell is not more likely than a point in the inner part.': 1.0986108779907227, 'Under a Gaussian model, the data density is on the other hand higher in the center and much lower on the out side.': 1.0986123085021973, 'If we have a good model of data, then sampling the most likely points from the model should give us plausible looking samples.': 1.098532795906067, 'In this sense, spherical interpolation should do no better than the normally used linear interpolation.': 1.0986123085021973, 'From the questions and answers it seems that the author does not recognize this distinction.': 1.0986123085021973, 'The results shown in this paper seem to indicate that spherical interpolation is better visually, but it is rather hard to make any concrete conclusions from three pairs of examples.': 1.0986056327819824, 'If this is really the case then there must be something else wrong about our understanding of the learned model.': 1.0986123085021973, 'Aside from these, the J-diagram and the nearest neighbor latent space traversal both seems to be good ways to explore the latent space of a learned model.': 1.0986123085021973, 'The attribute vector section on transforming images to new ones with desired attributes is also interesting, and it provides a few new ways to make the GAN latent space more interpretable.': 1.0979394912719727, 'Overall I feel most of the techniques proposed in this paper are nice visualization tools.': 1.0986123085021973, 'The contributions however, are mostly on the design of the visualizations, and not much on the technical and model side.': 1.0986123085021973, 'The spherical interpolation provides the only mathematical equation in the paper, yet the correctness of the technique is arguable.': 1.0986123085021973, 'For the visualization tools, there are also no quantitative evaluation, maybe these results are more art than science.': 1.0986123085021973, 'In this paper the authors propose various techniques to sample visualizations from generative models with high dimensional latent spaces like VAEs and GANs.': 1.0986104011535645, 'For example, the authors highlight the well known but often not sufficiently appreciated fact that the probability mass of high dimensional Gaussian distributions concentrates near a thin hyper-shell with a certain radius.': 1.0591562986373901, 'They therefore propose to use spherical interpolations (great arcs) instead of the commonly used linear interpolations.': 1.0986123085021973, 'In a similar spirit they propose a visualisation for analogies and techniques to reinforce structure in VAE latent spaces.': 1.093878149986267, 'I find it hard to give clear recommendation for this paper: On the one hand I enjoyed reading it and I might want use some of the proposals (e.g. spherical interpolations; J-diagrams) in future work of mine.': 1.0918585062026978, 'On the other hand, it’s obvious that this paper is not a typical machine learning paper; it does not propose a new model, or training method, or provide (theoretical/empirical) insight and it does not have the scientific quality and depth I’ve seen in many other ICLR submissions.': 1.0986119508743286, 'But it does more than just describing useful “tricks”.': 1.0985190868377686, ""And all things considered I think this paper deserves a wider audience (but  I'm not convinced that ICLR is the right venue)"": 1.0985978841781616}"
365,https://openreview.net/forum?id=Sys6GJqxl,"{'The paper presents an interesting and very detailed study of targeted and non-targeted adversarial examples in CNNs.': 1.0986123085021973, 'I’m on the fence about this paper but am leaning towards acceptance.': 1.0986123085021973, 'Such detailed empirical explorations are difficult and time-consuming to construct yet can serve as important stepping stones for future work.': 1.098610520362854, 'I see the length of the paper as a strength since it allows for a very in-depth look into the effectiveness and transferability of different kinds of adversarial examples.': 1.0986080169677734, 'There are, however, some concerns:': 1.0944631099700928, '1) While the length of the paper is a strength in my mind, the key contributions should be made much more clear.': 1.0969911813735962, 'As evidenced by my comment earlier, I got confused at some point between the ensemble/non-ensemble method, and about the contribution of the Clarifai evaluation and what I should be focusing on where.': 1.098607063293457, 'I’d strongly suggest a radical revision which more clearly focuses the story:': 1.0986123085021973, 'First, we demonstrate that non-targeted attacks are easy while targeted attacks are hard (evidenced by a key experiment comparing the two; we refer to appendix or later sections for the extensive exploration of e.g., current Section 3)': 1.098610520362854, 'Thus, we propose an ensemble method that is able to handle targeted attacks much better (evidenced by experiments focusing on the comparison between ensemble and non-ensemble method, both in a controlled setting and on Clarifai)': 1.0986123085021973, 'Also, here are all the other details and explorations.': 1.0986123085021973, ""2) Instead of using ResNet-152, Res-Net-101 and ResNet-50 as three of the five models, it would've been better to use one ResNet architecture and the other two, say, AlexNet and Network-in-Network."": 1.0985718965530396, 'This would make the ensemble results a lot more compelling.': 1.0986123085021973, 'This paper present an experimental study of the robustness of state-of-the-art CNNs to different types of ""attacks"" in the context of image classication.': 0.7339497208595276, 'Specifically, an attack aims to fool the classification system with a specially corrupted image, i.e. making it misclassify the image as (1) any wrong class (non-targeted attack) or (2) a target class, chosen in advance by the attacker (targeted attack).': 0.48417574167251587, 'For instance, the attacker could corrupt an image of an ostrich in such a way that it would be classified as a megalith.': 0.9255464673042297, ""Even though the attacker's agenda is not so clear in this example, it is still interesting to study the weaknesses of current systems in view of (1) improving them in general and (2) actual risks with e.g. autonomous vehicles."": 1.0974926948547363, 'The paper is mostly experimental.': 1.0986123085021973, 'In short, it compares different strategies (already published in previous papers) for all popular networks  (VGG, GoogLeNet, ResNet-50/101/152) and the two aforementionned types of attacks.': 1.0681099891662598, 'The experiments are well conducted and clearly exposed.': 0.6619710922241211, 'A convincing point is that attacks are also conducted on ""clarifai.com"" which is a black-box classification system.': 0.5208039879798889, 'Some analysis and insightful explanations are also provided to help understanding why CNNs are prone to such attacks (Section 6).': 0.8158441185951233, 'To sum up, the main findings are that non-targeted attacks are easy to perform, even on a black-box system.': 0.4900338053703308, ""Non-targeted attacks are more difficult to realize with existing schemes, but the authors propose a new approach for that that vastly improves over existing attacks (even though it's still far from perfect: ~20% success rate on clarifai.com versus 2% with previous schemes)."": 0.5460087060928345, 'Arguably, The paper still has some weaknesses:': 1.0986123085021973, '- The authors are treating the 3 ResNet-based networks as different, yet they are obviously clearly correlated. See Table 7 for instance. This is naturally expected because their architecture is similar (only their depth varies). Hence, it does not sound very fair to state that ""One interesting finding is that [...] the first misclassified label (non-targeted) is the same for all models except VGG-16 and GoogLeNet."", i.e., the three ResNet-based networks.': 1.0791702270507812, '- A subjective measure is employed to evaluate the effectiveness of the attacks on the black box system. While this is for a good reason (clarifai.com returns image labels that are different from ImageNet), it is not certain that the reported numbers are fair (even though the qualitative results look convincing).': 1.095268964767456, '- The novelty of the proposed approach (optimizing an ensemble of network instead of a single network) is limited. However, this was not really the point of the paper, and it is effective, so it seems ok overall.': 1.0986077785491943, '- The paper is quite long. This is expected because it is an extensive evaluation study, but still. I suggest the authors prune some near-duplicate content (e.g. Section 2.3 has a high overlap with Section 1, etc.).': 1.0962055921554565, ""- The paper would benefit from additional discussions with the recent and related work of Fawzi et al (NIPS'16) in Section 6. Indeed the work of Fawzi et al. is mostly theoretical and well aligned with the experimental findings and observations (in particular in Section 6)."": 1.0934436321258545, 'To conclude, I think that this paper is somewhat useful for the community and could help to further improve existing architectures, as well as better assess their flaws and weaknesses.': 0.7278491854667664, 'I reviewed the manuscript as of December 7th.': 1.0868308544158936, 'Summary:': 1.0986123085021973, 'The authors investigate the transferability of adversarial examples in deep networks.': 0.509682834148407, 'The authors confirm that transferability exists even in large models but demonstrate that it is difficult to manipulate the network to adversarially perturb an image into a specifically desired label.': 1.0985442399978638, 'The authors additionally demonstrate real world attacks on a vision web service and explore the geometric properties of adversarial examples.': 0.7238905429840088, 'Major Comments:': 1.0986123085021973, '1. The paper contains a list of many results and it is not clear what single message this paper provides. As mentioned in the comments, this paper is effectively 15 pages and 9 page of results in the Appendix heavily discussed throughout the main body of the paper. Although there is no strict page limit for this conference, I do feel this pushes the spirit of a conference publication. I do not rule out this paper for acceptance based on the length but I do hold it as a negative because clarity of presentation is an important quality. If this paper is ultimately accepted, I would suggest that the authors make some effort to cut down the length even further beyond the 13 pages posted elsewhere. I have marked some sections to highlight areas that may be trimmed.': 1.0984292030334473, ""2. The section of geometric understanding is similar to results of 'Adversarial Perturbations of Deep Neural Networks' in Warde-Farley and Goodfellow (2015). See Figure 1.2. I am not clear what the authors show above-and-beyond these results. If there are additional findings, the authors should emphasize them."": 1.0984272956848145, '3. The authors expand on observations by Goodfellow et al (2014) and Szegedy et al (2013) demonstrating that large-scale models are susceptible to adversarial perturbations (see also Kurakin et al (2016)). The authors additionally demonstrate that attempting to perform adversarial manipulation to convert an image to a particular, desired label is more difficult.': 1.0984615087509155, '4. The authors demonstrate that they can target a real-world vision API. These results are compelling but it is not clear what these results demonstrate above-and-beyond Papernot et al (2016).': 1.050916075706482, 'As far I can understand, I think that the most interesting result from this paper not previously described in the literature is to note about the unique difficulty about performing adversarial manipulation to convert an image to a particular, desired label.': 1.0659486055374146, 'The rest of the results appear to expand on other results that have already appeared in the literature and the authors need to better explain what these makes these results unique above-and-beyond previous work.': 0.6402590274810791, 'Areas to Trim the Paper:': 1.0547232627868652, 'Table 1 is not necessary.': 1.08201265335083, 'Just cite other results or write the Top-1 numbers in the text.': 1.0976923704147339, 'Condense Section 2.2.1 and cite heavily.': 1.0986037254333496, 'Figure 2 panels may be overlaid to highlight a comparison.': 1.0984219312667847}"
366,https://openreview.net/forum?id=SywUHFcge,"{'Under my point of view, the robustness of a classifier against adversarial noise it is interesting if we find any relationship between that robustness and generalization to new unseen test samples.': 0.707828164100647, 'I guess that this relationship is direct in most of the problems but perhaps classifier C1 could be more robust than C2 against adv.': 0.8829195499420166, 'noise but not better for new unseen samples from the task in consideration.': 1.2482680082321167, 'Best results on new unseen samples are normally related to robustness against the common distortions of the data, e.g. invariance to scale, rotation… than robustness to adv. noise.': 0.7611371874809265, 'I can not see any direct conclusion from table 5 results.': 1.324388861656189, 'Essentially i am not convinced about the necessity to measure the robustness against adversarial noise.': 1.3862227201461792, 'This paper aims at making three contributions:': 1.3862943649291992, 'Charecterizing robustness to adversarials in a topological manner.': 1.3844541311264038, 'Connecting the topological characterization to more quantitative measurements and evaluating deep networks.': 1.3143939971923828, 'Using Siamese network training to create models robust to adversarial in a practical manner and evaluate their properties.': 0.9503391981124878, 'In my opinion the paper would improve greatly if the first, topological analysis attempt would be removed from the paper altogether.': 1.386215329170227, 'A central notion of the paper is the abstract characterization of robustness.': 0.9638347625732422, 'The main weakness is the notion of strong robustness itself, which is an extremely rigid notion.': 1.386255145072937, 'It requires the partitioning of the predictor function by class to match the exact partitioning of the oracle.': 0.9236247539520264, 'This robustness is almost never the case in real life: it requires that the predictor is almost perfect.': 1.3685840368270874, 'The main flaw however is that the output space is assumed to have discrete topology and continuity is assumed for the classifier.': 0.7104062438011169, 'Continuity of the classifier wrt.': 1.3862943649291992, 'a discrete output is also never really satisfied.': 1.3862943649291992, 'However, if the output space is assumed to be continues values with an interesting topology (like probabilities), then the notion of strong robustness becomes so constrained and strict, that it has even less practical sense and relevance.': 1.3862943649291992, 'Based on those definition, several uninteresting, trivial consequences follow.': 1.3180930614471436, 'They seem to be true, with inelegant proofs, but that matters little as they seem irrelevant for any practical purposes.': 1.3171062469482422, 'The second part is a well executed experiment by training a Siamese architecture with an explicit additional robustness constraint.': 1.3862943649291992, 'The approach seems to be working very well, but is compared only to a baseline (stability training) which performs worse than the original model without any trainings for adversarials.': 1.3862943649291992, 'This is strange as adversarial examples have been studied extensively in the past year and several methods claimed improvements over the original model not trained for robustness.': 1.3862943649291992, 'The experimental section and approach would look interesting, if it were compared with a stronger baseline, however the empty theoretical definitions and analysis attempts make the paper in its current form unappealing.': 1.3862943649291992, 'This paper theoretically analyzes the adversarial phenomenon by modeling the topological relationship between the feature space of the trained and the oracle discriminate function.': 1.386290192604065, 'In particular, the (complicated) discriminant function (f) is decomposed into a feature extractor (g) and a classifier (c), where the feature extractor (g) defines the feature space.': 1.3862932920455933, 'The main contribution of this paper is to propose abstract understanding and analysis for adversarial phenomenon, which is interesting and important.': 1.3354276418685913, 'However, this paper also has the following problems.': 0.8040412664413452, '1) It is not clear how the classifier c can affect the overall robustness to adversarial noises.': 0.8765729665756226, 'The classifier c seems absent from the analysis, which somehow indicates that the classifier does not matter.': 1.3862066268920898, '(Please correct me if it is not true)': 1.3512436151504517, 'This is counter-intuitive.': 1.3862943649291992, 'For example, if we always take the input space as the feature space and the entire f as the classifier c, the strong robustness can always hold.': 0.6934502720832825, 'I am also wondering if the metric d has anything to do with the classifier c.': 1.3862943649291992, '2) A very relevant problem is how to decompose f into g and c.': 1.3862943649291992, 'For examples, one can take any intermediate layer or the input space as the feature space for a neural network.': 0.70404052734375, 'Will this affect the analysis of the adversarial robustness?': 1.3862943649291992, '3)': 1.3858872652053833, 'The oracle is a good concept.': 0.8185147047042847, 'However, it is hard to explicitly define it.': 1.3679910898208618, 'In this paper, the feature space of the oracle is just the input image space, and the inf-norm is used as the metric.': 0.9811681509017944, 'This implementation makes the algorithm in Section 4 quite similar to existing methods (though there are some detailed differences as mentioned in the discussion).': 1.172959327697754, 'Due to the above problems, I feel that some aspects of the paper are not ready.': 0.887214183807373, 'If the problems are resolved or better clarified, I believe a higher rating can be assigned to this paper.': 1.3861973285675049, 'In addition, the main text of this paper is somehow too long, the arguments can be more focused if the main paper become more concise.': 1.385675311088562, 'A promising attempt to quantify the difference between human annotator and DNN.': 0.49242207407951355, 'The contribution of defining topological equivalence of two metric spaces is significant to understand the origin of adversarial examples.': 0.6032970547676086, 'Appreciate the examples showed in Figure2 and Figure 3, straightforward and intuitively helpful.': 1.2044811248779297, 'More interestingly, a useful measure is provided for quantify the robustness of DNN model.': 1.3383699655532837, 'Researchers can use this measure to compare different DNN models.': 1.3862943649291992, 'Some critiques:': 1.3862943649291992, 'It might be nicer to think more about the detailed mechanics of human vision.': 1.3862943649291992, 'In the early state of primary visual cortex, feature space might be linear, L2 norm might be sufficient to capture the key difference between DNN and human metrics.': 1.3862940073013306, 'However, when the information flow goes deep, single one metric might not be sufficient when the Oracle feature space is highly curved.': 1.386293888092041, 'Tracking and reducing the metric distance in cognitive level would be extremely hard.': 1.3862943649291992, 'This paper offers a pioneering advance to tackle this problem.': 1.3862943649291992, 'Hope more work can be done to further our understanding towards the limitation of DNN.': 1.3862943649291992}"
367,https://openreview.net/forum?id=Sywh5KYex,"{'This paper proposes a network called Gated Residual Networks layer design that adds gating to shortcut connections with a scalar to regulate the gate.': 1.0922740697860718, 'The authors claim that this approach will improve the training Residual Networks.': 1.0986121892929077, 'It seems the authors could get competitive performance on CIFAR-10 to state of art models with only Wide Res Nets.': 1.0985499620437622, 'Wide Gated ResNet requires much more parameters than DenseNet (and other Res Net variants) for obtaining a little improvement over Dense Net.': 0.46892622113227844, 'More importantly, the authors state that they obtained the best results on CIFAR-10 and CIFAR-100 but the updated version of DenseNet (Huang et al. (2016b)) has new results for a version called DenseNet-BC which outperforms all of the results that authors reported (3.46 for CIFAR-10 and 17.18 for CIFAR-100 with 25.6M parameters, DenseNet-BC still outperforms with 15.3M parameters which is much less that 36.5M).': 1.0296978950500488, 'The Res Net variants papers with state of art results report result for Image Net.': 1.0986119508743286, 'Therefore the empirical results need also the Image Net to demonstrate that improvement claimed is achieved.': 1.0985935926437378, 'The proposed trick adopts Highway Neural Networks and Residual Networks with an intuitive motivation.': 1.0986071825027466, 'It is not sufficiently novel and the empirical results do not prove sufficient effectiveness of this incremental approach.': 1.0986123085021973, 'This paper proposes to learn a single scalar gating parameter instead of a full gating tensor in highway networks.': 1.0986123085021973, 'The claim is that such gating is easier to learn and allows a network to flexibly utilize computation.': 1.0986123085021973, 'The basic idea of the paper is simple and is clearly presented.': 1.067864179611206, 'It is a natural simplification of highway networks to allow easily ""shutting off"" layers while keeping number of additional parameters low.': 1.0986123085021973, 'However, in this regard the paper leaves out a few key points.': 0.8254002332687378, 'Firstly, it does not mention that the gates in highway networks are data-dependent which is potentially more powerful than learning a fixed gate for all units and independent of data.': 1.0986123085021973, 'Secondly, it does not do a fair comparison with highway networks to show that this simpler formulation is indeed easier to learn.': 1.0986120700836182, 'Did the authors try their original design of u = g(k)f(x) + (1 - g(k))x where f(x) is a plain layer instead of a residual layer?': 1.098574161529541, 'Based on the arguments made in the paper, this should work fine.': 1.0985321998596191, ""Why wasn't it tested?"": 1.0986123085021973, ""If it doesn't work, are the arguments incorrect or incomplete?"": 1.0985970497131348, 'For the MNIST experiments, since the hyperparameters are fixed, the plots are misleading if any dependence on hyperparameters exists for the different models.': 1.0986123085021973, 'This experiment appears to be based on Srivastava et al (2015).': 0.3607327342033386, 'If it is indeed designed to test optimization at aggressive depths, then apart from doing a hyperparameter search, the authors should not use regularization such as dropout or batch norm, which do not appear in the theoretical arguments for the architecture.': 0.9421082735061646, 'For CIFAR experiments, the obtained improvements compared to the baseline (wide resnets) are very small and therefore it is important to report the standard deviations (or all results) in both cases.': 1.0986121892929077, ""It's not clear that the differences are significant."": 1.0659104585647583, 'Some questions regarding g(): Was g() always ReLU?': 1.0986123085021973, ""Doesn't this have potential problems with g(k) becoming 0 and never recovering?"": 1.0812543630599976, 'Does this also mean that for the wide resnet in Fig 7, most residual blocks are zeroed out since k < 0?': 1.0986123085021973, 'The paper presents a layer architecture where a single parameter is used to  gate the output response of layer to amplify or suppress it.': 1.0986123085021973, 'It is shown that such an architecture can ease optimization of a deep network as it is easy to learn identity mappings in layers helping in better gradient propagation to lower layers (better supervision).': 1.0986123085021973, 'Using an introduced SDI metric it shown that gated residual networks can most easily learn identity mappings compared to other architectures.': 1.0986123085021973, 'Although good theoretical reasoning is presented the observed experimental evidence of learned k values does not seem to strongly support the theory given that learned  k values are mostly very small and not varying much across layers.': 1.0986123085021973, 'Also, experimental validation of the approach is not quite strong in terms of reported performances and number of large scale experiments.': 1.0986123085021973}"
368,https://openreview.net/forum?id=SyxeqhP9ll,"{'This paper addresses one of the major shortcomings of generative adversarial networks - their lack of mechanism for evaluating held-out data.': 1.0986114740371704, 'While other work such as BiGANs/ALI address this by learning a separate inference network, here the authors propose to change the GAN objective function such that the optimal discriminator is also an energy function, rather than becoming uninformative at the optimal solution.': 1.0985838174819946, 'Training this new objective requires gradients of the entropy of the generated data, which are difficult to approximate, and the authors propose two methods to do so, one based on nearest neighbors and one based on a variational lower bound.': 1.0984584093093872, 'The results presented show that on toy data the learned discriminator/energy function closely approximates the log probability of the data, and on more complex data the discriminator give a good measure of quality for held out data.': 1.0986123085021973, 'I would say the largest shortcomings of the paper are practical issues around the scalability of the nearest neighbors approximation and accuracy of the variational approximation, which the authors acknowledge.': 0.4054655134677887, 'Also, since entropy estimation and density estimation are such closely linked problems, I wonder if any practical method for EGANs will end up being equivalent to some form of approximate density estimation, exactly the problem GANs were designed to circumvent.': 1.0984731912612915, 'Nonetheless, the elegant mathematical exposition alone makes the paper a worthwhile contribution to the literature.': 1.0986123085021973, 'Also, some quibbles about the writing - it seems that something is missing in the sentence at the top of pg.': 1.098611831665039, '5 ""Finally, let\'s whose discriminative power"".': 1.0986123085021973, ""I'm not sure what the authors mean to say here."": 1.0986123085021973, 'And the title undersells the paper - it makes it sound like they are making a small improvement to training an existing model rather than deriving an alternative training framework.': 0.4263572096824646, 'The authors present a method for changing the objective of generative adversarial networks such that the discriminator accurately recovers density information about the underlying data distribution.': 1.0986123085021973, 'In the course of deriving the changed objective they prove that stability of the discriminator is not guaranteed in the standard GAN setup but can be recovered via an additional entropy regularization term.': 1.0986123085021973, 'The paper is clearly written, including the theoretical derivation.': 1.0967233180999756, 'The derivation of the additional regularization term seems valid and is well explained.': 1.0986123085021973, 'The experiments also empirically seem to support the claim that the proposed changed objective results in a ""better"" discriminator.': 1.0982619524002075, 'There are only a few issues with the paper in its current form:': 1.0985937118530273, 'The presentation albeit fairly clear in the details following the initial exposition in 3.1 and the beginning of 3.2 fails to accurately convey the difference between the energy based view of training GANs and the standard GAN.': 1.0986123085021973, ""As a result it took me several passes through the paper to understand why the results don't hold for a standard GAN."": 1.0986123085021973, 'I think it would be clearer if you state the connections up-front in 3.1 (perhaps without the additional f-gan perspective) and perhaps add some additional explanation as to how c() is implemented right there or in the experiments (you may want to just add these details in the Appendix, see also comment below).': 1.0986123085021973, 'The proposed procedure will by construction only result in an improved generator and unless I misunderstand something does not result in improved stability of GAN training.': 1.0985870361328125, ""You also don't make such a claim but an uninformed reader might get this wrong impression, especially since you mention improved performance compared to Salimans et al. in the Inception score experiment."": 1.0984898805618286, 'It might be worth-while mentioning this early in the paper.': 1.0951772928237915, 'The experiments, although well designed, mainly convey qualitative results with the exception of the table in the appendix for the toy datasets.': 0.9489356875419617, 'I know that evaluating GANs is in itself not an easy task but I wonder whether additional more quantitative experiments could be performed to evaluate the discriminator performance.': 1.098297119140625, 'For example: one could evaluate how well the final discriminator does separate real from fake examples, how robust its classification is to injected noise (e.g. how classification accuracy changes for noised training data).': 1.098610520362854, 'Further one might wonder whether the last layer features learned by a discriminator using the changed objective are better suited for use in auxiliary tasks (e.g. classifying objects into categories).': 1.0986121892929077, 'Main complaint: It is completely unclear what the generator and discriminators look like for the experiments.': 1.0986123085021973, 'You mention that code will be available soon but I feel like a short description at least of the form of the energy used should also appear in the paper somewhere (perhaps in the appendix).': 1.0986123085021973, 'The submission explores several alternatives to provide the generator function in generative adversarial training with additional gradient information.': 1.0986123085021973, 'The exposition starts by describing a general formulation about how this additional gradient information (termed K(p_gen) could be added to the generative adversarial training objective function (Equation 1).': 1.0986123085021973, 'Next, the authors prove that the shape of the optimal discriminator does indeed depend on the added gradient information (Proposition 3.1), which is unsurprising.': 1.0986123085021973, 'Finally, the authors propose three particular alternatives to construct K(p_gen): the negative entropy of the generator distribution, the L2 norm of the generator distribution, and a constant function (which resembles the EBGAN objective of Zhao et al, 2016).': 1.0985928773880005, 'The exposition moves then to an experimental evaluation of the method, which sets K(p_gen) to be the approximate entropy of the generator distribution.': 1.0986123085021973, 'At this point, my intuition is that the objective function under study is the vanilla GAN objective, plus a regularization term that encourages diversity (high entropy) in the generator distribution.': 1.0986123085021973, 'The hope of the authors is that this regularization will transform the discriminator into an estimate of the energy landscape of the data distribution.': 1.0986073017120361, 'The experimental evaluation proceeds by 1) showing the contour plots of the obtained generator distribution for a 2D problem, 2) studying the generation diversity in MNIST digits, and 3) showing some samples for CIFAR-10 and CelebA.': 1.0980947017669678, 'The 2D problem results are convincing, since one can clearly observe that the discriminator scores translate into unnormalized values of the density function.': 1.0986108779907227, 'The MNIST results offer good intuition also: the more prototypical digits are assigned larger scores (unnormalized densities) by the discriminator, and the less prototypical digits are assigned smaller scores.': 1.0986101627349854, 'The sample experiments from Section 5.3 are less convincing, since no samples from baseline models are provided for comparison.': 1.0986123085021973, 'To this end, I would recommend the authors to clarify three aspects.': 1.098607063293457, 'First, we have seen that entropy regularization leads to a discriminator that estimates the energy landscape of the data distribution.': 1.080870270729065, 'But, how does this regularization reshape the generator function?': 1.0986123085021973, 'It would be nice to see the mean MNIST digit according to the generator, and some other statistics if possible.': 1.098610520362854, 'Second, how do the samples produced by the proposed methods compare (visually speaking) to the state-of-the art?': 1.098601222038269, 'Third, what are the *shortcomings* of this method versus vanilla GAN?': 1.0985854864120483, 'Too much computational overhead?': 1.0986123085021973, 'What are the qualitative and quantitative differences between the two entropy estimators proposed in the manuscript?': 1.0986121892929077, 'Overall, a clearly written paper.': 1.0986123085021973, 'I vote for acceptance.': 1.0986123085021973, 'As an open question to the authors: What breakthroughs should we pursue to derive a GAN objective where the discriminator is an estimate of the data density function, after training?': 0.937064528465271}"
369,https://openreview.net/forum?id=r10FA8Kxg,"{'Description.': 0.6931471824645996, 'This paper describes experiments testing whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy.': 0.6931471824645996, 'The experiments are performed on he CIFAR 10 dataset where deep convolutional teacher networks are used to train shallow student networks using L2 regression on logit outputs.': 0.6931471824645996, 'The results show that similar accuracy on the same parameter budget can be only obtained when multiple layers of convolution are used.': 0.6931471824645996, 'Strong  points.': 0.6931471824645996, 'The experiments are carefully done with thorough selection of hyperparameters.': 0.6931471824645996, 'The paper shows interesting results that go partially against conclusions from the previous work in this area (Ba and Caruana 2014).': 0.6931471824645996, 'The paper is well and clearly written.': 0.6931471824645996, 'Weak points:': 0.6931471824645996, 'CIFAR is still somewhat toy dataset with only 10 classes.': 0.6931471824645996, 'It would be interesting to see some results on a more challenging problem such as ImageNet.': 0.6931471824645996, 'Would the results for a large number of classes be similar?': 0.6931471824645996, 'Originality:': 0.6931471824645996, 'This is mainly an experimental paper, but the question it asks is interesting and worth investigation.': 0.6931471824645996, 'The experimental results are solid and provide new insights.': 0.6931471824645996, 'Quality:': 0.6931471824645996, 'The experiments are well done.': 0.6931471824645996, 'Clarity:': 0.6931471824645996, 'The paper is well written and clear.': 0.6931471824645996, 'Significance:': 0.6931471824645996, 'The results go against some of the conclusions from previous work, so should be published and discussed.': 0.6931198835372925, 'Overall:': 0.459017813205719, 'Experimental paper with interesting results.': 0.6931471824645996, 'Well written.': 0.6931471824645996, 'Solid experiments.': 0.693146288394928, 'This paper aims to investigate the question if shallow non-convolutional networks can be as affective as deep convolutional ones for image classification, given that both architectures use the same number of parameters.': 0.6931471228599548, 'To this end the authors conducted a series of experiments on the CIFAR10 dataset.': 0.6909050345420837, 'They find that there is a significant performance gap between the two approaches, in favour of deep CNNs.': 0.6931460499763489, 'The experiments are well designed and involve a distillation training approach, and the results are presented in a comprehensive manner.': 0.6930913329124451, 'They also observe (as others have before) that student models can be shallower than the teacher model from which they are trained for comparable performance.': 0.6931471824645996, 'My take on these results is that they suggest that using (deep) conv nets is more effective, since this model class encodes a form of a-prori or domain knowledge that images exhibit a certain degree of translation invariance in the way they should be processed for high-level recognition tasks.': 0.6931471824645996, 'The results are therefore perhaps not quite surprising, but not completely obvious either.': 0.693088710308075, 'An interesting point on which the authors comment only very briefly is that among the non-convolutional architectures the ones using 2 or 3 hidden layers outperform those with 1, 4 or 5 hidden layers.': 0.6931448578834534, 'Do you have an interpretation / hypothesis of why this is the case?': 0.6931471824645996, 'It  would be interesting to discuss the point a bit more in the paper.': 0.6931471824645996, 'It was not quite clear to me why were the experiments were limited to use  30M parameters at most.': 0.6931471824645996, 'None of the experiments in Figure 1 seem to be saturated.': 0.6931471824645996, 'Although the performance gap between CNN and MLP is large, I think it would be worthwhile to push the experiment further for the final version of the paper.': 0.6931471824645996, 'The authors state in the last paragraph that they expect shallow nets to be relatively worse in an ImageNet classification experiment.': 0.6931471824645996, 'Could the authors argue why they think this to be the case?': 0.6931471824645996, 'One could argue that the much larger training dataset size could compensate for shallow and/or non-convolutional choices of the architecture.': 0.6931471824645996, 'Since MLPs are universal function approximators, one could understand architecture choices as expressions of certain priors over the function space, and in a large-data regimes such priors could be expected to be of lesser importance.': 0.6931471824645996, 'This issue could for example be examined on ImageNet when varying the amount of training data.': 0.6931471824645996, 'Also, the much higher resolution of ImageNet images might have a non-trivial impact on the CNN-MLP comparison as compared to the results established on the CIFAR10 dataset.': 0.6931471824645996, 'Experiments on a second data set would also help to corroborate the findings, demonstrating to what extent such findings are variable across datasets.': 0.6931471824645996}"
370,https://openreview.net/forum?id=r17RD2oxe,"{'The paper presents a simple method for constructing a visual hierarchy of ImageNet classes based on a CNN trained on discriminate between the classes.': 1.0986031293869019, 'It investigates two metrics for measuring inter-class similarity: (1) softmax probability outputs, i.e., the class confusion matrix, and (2) L2 distance between fc7 features, along with three methods for constructing the hierarchy given the distance matrix: (1) approximation central point, (2) minimal spanning tree, and (3) multidimensional scaling of Borg&Groenen 2005.': 1.0986120700836182, 'There are two claimed contributions: (1) Constructs a biology evolutionary tree, and (2) Gives insight into the representations produced by deep networks.': 1.0986123085021973, 'Regarding (1), while the motivation of the work is grounded in biology, in practice the method is based only on visual similarity.': 1.0974477529525757, 'The constructed trees thus can’t be expected to reflect the evolutionary hierarchy, and in fact there are no quantitative experiments that demonstrate that they do.': 1.09528386592865, 'Regarding (2), the technical depth of the exploration is not sufficient for ICLR.': 1.0980770587921143, 'I’m not sure what we can conclude from the paper beyond the fact that CNNs are able to group categories together based on visual similarities, and deeper networks are able to do this better than more shallow networks (Fig 2).': 1.0982296466827393, 'In summary, this paper is unfortunately not ready for publication at this time.': 0.615757942199707, ""This paper introduces a hierarchical clustering method using learned CNN features to build 'the tree of life'."": 1.0986123085021973, 'The assumption is that the feature similarity indicates the distance in the tree.': 1.0986123085021973, 'The authors tried three different ways to construct the tree: 1) approximation central point 2) minimum spanning tree and 3) multidimensional scaling based method.': 1.0986121892929077, 'Out of them, MDS works the best.': 1.0986123085021973, 'It is a nice application of using deep features.': 1.0986123085021973, 'However, I lean toward rejecting the paper because the following reasons:': 1.0986123085021973, '1) All experiments are conducted in very small scale.': 1.0986123085021973, 'The experiments include 6 fish species, 11 canine species, 8 vehicle classes.': 1.0986123085021973, 'There are no quantitative results, only by visualizing the generated tree versus the wordNet tree.': 1.0986123085021973, 'Moreover, the assumption of using wordNet is not quite valid.': 1.0986123085021973, 'WordNet is not designed for biology purpose and it might not reflect the true evolutionary relationship between species.': 1.0986123085021973, '2) Limited technical novelty.': 1.0986123085021973, 'Most parts of the pipeline are standard, e.g. use pretrained model for feature extraction, use previous methods to construct hierarchical clustering.': 1.0986123085021973, 'I think the technical contribution of this paper is very limited.': 1.0986123085021973, 'I like this paper in that it is a creative application of computer vision to Biology.': 1.0986123085021973, 'Or, at least, that would be a good narrative but I\'m not confident biologists would actually care about the ""Tree of Life"" built from this method.': 1.0986123085021973, ""There's not really any biology in this paper, either in methodology or evaluation."": 1.0986123085021973, 'It boils down to a hierarchical clustering of visual categories with ground truth assumed to be the WordNet hierarchy (which may or may not be the biological ground truth inheritance relationships between species, if that is even possible to define': 1.0986123085021973, ""it probably isn't for dog species which interbreed and it definitely isn't for vehicles) or the actual biological inheritance tree or what humans would do in the same task."": 1.0986123085021973, ""If we're just worried about visual relationships and not inheritance relationships then a graph is the right structure, not a tree."": 1.0986123085021973, 'A tree is needlessly lossy and imposes weird relationships (e.g. ImageNet has a photo of a ""toy rabbit"" and by tree distance it is maximally distant from ""rabbit"" because the toy is in the devices top level hierarchy and the real rabbit is in the animal branch.': 1.0986123085021973, 'Are those two images really as semantically unrelated as is possible?).': 1.0986123085021973, 'Our visual world is not a hierarchy.': 1.0986123085021973, 'Our biological world can reasonably be defined as one.': 1.0986123085021973, 'One could define the task of trying to recover the biological inheritance tree from visual inputs, although we know that would be tough to do because of situations like convergent evolution.': 1.0986123085021973, 'Still, one could evaluate how well various visual features can recover the hierarchical relationship of biological organisms.': 1.0986123085021973, ""This paper doesn't quite do that."": 1.0986123085021973, 'And even if it did, it would still feel like a bit of a solution in search of a problem.': 1.0986123085021973, ""The paper says that this type of exercise can help us understand deep features, but I'm not sure sure how much it reveals."": 1.0986123085021973, ""I guess it's a fair question to ask if a particular feature produces meaningful class-to-class distances, but it's not clear that the biological tree of life or the wordnet hierarchy is the right ground truth for that (I'd argue it's not)."": 1.0986123085021973, ""Finally, the paper mentions human baselines in a few places but I'm not really seeing it."": 1.0986123085021973, '""Experiments show that the proposed method using deep representation is very competitive to human beings in building the tree of life based on the visual similarity of the species.""': 1.0986123085021973, 'and then later ""The reconstructed quality is as good as what human beings could reconstruct based on the visual similarity.""': 1.0986123085021973, ""That's the extent of the experiment?"": 1.0986123085021973, ""A qualitative result and the declaration that it's as good as humans could do?"": 1.0986123085021973}"
371,https://openreview.net/forum?id=r1Aab85gg,"{'This paper discusses aligning word vectors across language when those embeddings have been learned independently in monolingual settings.': 1.0986123085021973, 'There are reasonable scenarios in which such a strategy could come in helpful, so I feel this paper addresses an interesting problem.': 1.0986123085021973, 'The paper is mostly well executed but somewhat lacks in evaluation.': 1.0986123085021973, 'It would have been nice if a stronger downstream task had been attempted.': 1.0986123085021973, 'The inverted Softmax idea is very nice.': 1.0986123085021973, 'A few minor issues that ought to be addressed in a published version of this paper:': 1.0986123085021973, '1) There is no mention of Haghighi et al (2008)': 1.0986123085021973, '""Learning Bilingual Lexicons from Monolingual Corpora."", which strikes me as a key piece of prior work regarding the use of CCA in learning bilingual alignment.': 1.0986123085021973, 'This paper and links to the work here ought to be discussed.': 1.0986123085021973, '2)': 1.0986123085021973, 'Likewise, Hermann & Blunsom (2013) ""Multilingual distributed representations without word alignment.""': 1.0986123085021973, 'is probably the correct paper to cite for learning multilingual word embeddings from multilingual aligned data.': 1.0986123085021973, '3) It would have been nicer if experiments had been performed with more divergent language pairs rather than just European/Romance languages': 1.0986123085021973, '4)': 1.0986123085021973, 'A lot of the argumentation around the orthogonality requirements feels related to the idea of using a Mahalanobis distance / covar matrix to learn such mappings.': 1.0986123085021973, 'This might be worth including in the discussion': 1.0986123085021973, '5) I don\'t have a better suggestion, but is there an alternative to using the term ""translation (performance/etc.)"" when discussing word alignment across languages?': 1.0682833194732666, 'Translation implies something more complex than this in my mind.': 1.0986123085021973, '6)': 1.0986123085021973, 'The Mikolov citation in the abstract is messed up': 1.0986123085021973, 'The paper focuses on bilingual word representation learning with the following setting:': 0.6738648414611816, '1. Bilingual representation is learnt in an offline manner i.e., we already have monolingual representations for the source and target language and we are learning a common mapping for these two representations.': 0.46584194898605347, '2. There is no direct word to word alignments available between the source and target language.': 1.0985966920852661, 'This is a practically useful setting to consider and authors have done a good job of unifying the existing solutions for this problem by providing theoretical justifications.': 1.098607063293457, 'Even though the authors do not propose a new method for offline bilingual representation learning, the paper is significant for the following contributions:': 0.6406623125076294, '1. Theory for offline bilingual representation learning.': 1.0986117124557495, '2. Inverted softmax.': 1.0986123085021973, '3. Using cognate words for languages that share similar scripts.': 1.0986123085021973, '4. Showing that this method also works at sentence level (to some extent).': 1.0986123085021973, 'Authors have addressed all my pre-review questions and I am ok with their response.': 0.9968047142028809, 'I have few more comments:': 1.0986121892929077, '1. Header for table 3 which says “word frequency” is misleading. “word frequency” could mean that rare words occur in row-1 while I guess authors meant to say that rare words occur in row-5.': 0.6132953763008118, '2. I see that authors have removed precision @5 and @10 from table-6. Is it because of the space constraints or the results have different trend? I would like to see these results in the appendix.': 1.0986121892929077, '3. In table-6 what is the difference between row-3 and row-4? Is the only difference NN vs. inverted softmax? Or there are other differences? Please elaborate.': 1.0986123085021973, '4. Another suggestion is to try running an additional experiment where one can use both expert dictionary and cognate dictionary. Comparing all 3 methods in this setting should give more valuable insights about the usefulness of cognate dictionary.': 1.0986123085021973, 'This paper extends preceding works to create a mapping between the word embedding space of two languages.': 1.0986123085021973, 'The word embeddings had been independently trained on monolingual data only, and various forms of bilingual information is used to learn the mapping.': 1.0986123085021973, 'This mapping is then used to measure the precision of translations.': 1.0986123085021973, 'In this paper, the authors propose two changes: ""CCA"" and ""inverted softmax"".': 1.0986123085021973, 'Looking at Table 1, CCA is only better than Dina et al in 1 out of 6 cases (It/En @1).': 1.0986120700836182, 'Most of the improvements are in fact obtained by the introduction of the inverted softmax normalization.': 1.0986123085021973, 'Overall, I wonder which aspect of this paper is really new.': 1.0986123085021973, 'You mention:': 1.0986123085021973, '- Faruqui & Dyer 2014 already used CCA and dimensionality reduction': 1.0986123085021973, ""- Xing et al 2015 argued already that Mikolov's linear matrix should be orthogonal"": 0.5819092988967896, 'Could you make clear in what aspect your work is different from Faruqui & Dyer 2014 (other the fact that you applied the method to measure translation precision) ?': 1.0986123085021973, 'Using cognates instead of a bilingual directory is a nice trick.': 1.0986123085021973, 'Please explain how you obtained this list of cognates ?': 1.0986123085021973, 'Obviously, this only works for languages with the same alphabet (for instance Greek and Russian are excluded)': 1.0986123085021973, 'Also, it seems to me that in linguistics the term ""cognate"" refers to words which have a common etymological origin - they don\'t necessarily have the same written form (e.g. night, nuit, noche, Nacht).': 1.0986121892929077, 'Maybe, you should use a different term ?': 1.0986123085021973, 'Those words are probably proper names in news texts.': 1.0986123085021973}"
372,https://openreview.net/forum?id=r1BJLw9ex,"{'This paper proposes new initialization for particular architectures and a correction trick to batch normalization to correct variance introduced by dropout.': 1.0986011028289795, 'While authors state interesting observations, the claims are not supported with convincing results.': 1.0986123085021973, 'I guess Figure 1 is only for mnist and for only two values of p with one particular network architecture, the dataset and empirical setup is not clear.': 0.42018991708755493, 'The convergence is demonstrated only for three dropout values in Figure 2 which may cause an unfair comparison.': 1.0986123085021973, 'For instance how does the convergence compare for the best dropout rate after cross-validation (three figures each figure has three results for one method with different dropouts [bests cv result for each one])?': 1.098586916923523, 'Also how is the corresponding validation error and test iterations?': 1.0986123085021973, 'Also only mnist does not have to generalize to other benchmarks.': 1.0986123085021973, 'Figure 3 gives closer results for Adam optimizer, learning rate is not selected with random search or bayesian optimization, learning decay iterations fixed and regularization coefficient is set to a small value without tuning.': 0.543670117855072, 'A slightly better tuning of parameters may close the current gap.': 1.0986123085021973, 'Also Nesterov based competitor gives unreasonably worse accuracy compared to recent results which may indicate that this experiment should not be taken into account.': 1.0985697507858276, 'In Table 2, there is no significant improvement on CIFAR10.': 1.0986123085021973, 'The CIFAR100 difference is not significant without including batch normalization variance re-estimation.': 1.0986123085021973, ""However there is no result for 'original with BN update' therefore it is not clear whether the BN update helps in general or not."": 1.0986123085021973, 'SVHN also does not have result for original with BN update.': 1.0986123085021973, 'There should be baselines with batch normalizations for Figure 1,2 3 to support the claims convincingly.': 1.0986123085021973, 'The main criticism about batch normalization is additional computational cost by giving (Mishkin et al, 2016 )': 1.0986114740371704, 'as reference however this should not be a reason to not to compare the initialization to batch-normalization.': 1.0986123085021973, 'In fact, (Mishkin et al, 2016) performs comparison to batch normalization and also with and without data augmentation with recent state of art architectures.': 1.0458906888961792, 'None of the empirical results have data augmentation.': 1.0986123085021973, 'It is not clear if the initialization or  batch normalization update will help or make it worse for that case.': 1.0986123085021973, 'Recent state of art methods methods like Res Net variant and Dense Net scale to many depths and report result for ImageNet.': 1.0868756771087646, 'Although the authors claim that this can be extended to residual network variants, it is not clear if there is going to be any empirical gain for that architectures.': 1.0986108779907227, 'This work requires a comprehensive and fair comparison.': 1.09861159324646, 'Otherwise the contribution is not significant.': 0.8388537168502808, 'The main observation made in the paper is that the use of dropout increases the variance of neurons.': 1.0985047817230225, 'Correcting for this increase in variance, in the parameter initialization, and in the test-time statistics of batch normalization, improves performance, as is shown reasonably convincingly in the experiments.': 1.0986119508743286, 'This observation is important, as it applies to many of the models used in the literature.': 1.0973201990127563, ""It's not extremely novel (it's been observed in the literature before that our simple dropout approximations at test time do not achieve the accuracy obtained by full Monte Carlo dropout)"": 1.0986123085021973, 'The paper could use more experimental validation.': 1.0986121892929077, 'Specifically:': 0.7483749985694885, ""I'm guessing the correction for dropout variance at test time is not only specific to batch normalization: Standard dropout, in networks without batch normalization, corrects only for the mean at test time (by dividing activations by one minus the dropout probability)."": 1.0982587337493896, 'This work suggests it would be beneficial to also correct for the variance.': 1.0894949436187744, 'Has this been tested?': 1.0986123085021973, 'How does the dropout variance correction compare to using Monte Carlo dropout at test time?': 1.0986123085021973, '(i.e. just averaging over a large number of random dropout masks)': 1.0986123085021973, 'The paper presents an approach for compensating the input/activation variance introduced by dropout in a network.': 1.0986123085021973, 'Additionally, a practical inference trick of re-estimating the batch normalization parameters with dropout turned off before testing.': 1.0986123085021973, 'The authors very well show how dropout influences the input/activation variance and then scale the initial weights accordingly to achieve unit variance which helps in avoiding activation outputs exploding or vanishing.': 1.0986053943634033, 'It is shown that the presented approach serves as a good initialization technique for deep networks and results in performances op par or slightly better than the existing approaches.': 0.4238525629043579, 'The limited experimental validation and only small difference in accuracies compared to existing methods makes it difficult to judge the effectiveness of presented approach.': 1.0935710668563843, 'Perhaps observing the statistics of output activations and gradients over training epochs in multiple experiments can better support the argument of stability of the network using proposed approach.': 1.0986123085021973, 'Authors might consider adding some validation for considering the backpropagation variance.': 1.0986123085021973, 'On multiple occasions comparison is drawn against batch normalization which I believe does much more than a weight initialization technique.': 1.0986123085021973, 'The presented approach is a good initialization technique just not sure if its better than existing ones.': 1.0986123085021973}"
373,https://openreview.net/forum?id=r1Bjj8qge,"{'This paper tries to solve the problem of interpretable representations with focus on Sum Product Networks.': 1.0986123085021973, 'The authors argue that SPNs are a powerful linear models that are able to learn parts and their combinations, however, their representations havent been fully exploited by generating embeddings.': 1.0986117124557495, 'Pros:': 1.0986123085021973, 'The idea is interesting and interpretable models/representations is an important topic.': 1.0986123085021973, 'Generating embeddings to interpret SPNs is a novel idea.': 1.0986120700836182, 'The experiments are interesting but could be extended.': 1.0986123085021973, 'Cons:': 1.0986123085021973, ""The author's contribution isn't fully clear and there are multiple claims that need support."": 1.0986123085021973, 'For example, SPNs are indeed interpretable as is, since the bottom-up propagation of information from the visible inputs could be visualized at every stage, and the top-down parse could be also visualized as it has been done before (Amer & Todorovic, 2015).': 1.0986123085021973, 'Another example, Proposition one claims that MPNs are perfect encoder decoders since the max nodes always have one max value, however, what if it was uniformally distributed node, or there are two equal values?': 1.0986123085021973, 'Did the authors run into such cases?': 1.0986123085021973, 'Did they address all edge cases?': 1.0986123085021973, 'A good comparison could have been against Generative Adversarial Networks (GANs), Generative Stochastic Networks (GSNs) and Variational Autoencoders too since they are the state-of-the-art generative models, rather than comparing with RBMs and Nade.': 1.0986123085021973, 'I would suggest that the authors take sometime to evaluate their approach against the suggested methods, and make sure to clarify their contributions and eliminate over claiming statements.': 1.0986123085021973, 'I agree with the other comments raised by Anon-Reviewer1.': 1.0986123085021973, ""The paper's aim is - as argued in the paper and the responses to other reviewers comments - that SPN and MPN can be interpreted as encoders and decoders of RL."": 1.0985878705978394, 'Well - this is an interesting perspective and could be (potentially) worth a paper.': 1.0986123085021973, 'However': 1.0986123085021973, 'the current draft is far from being convincing in that respect - and I am talking about the updated / improved version as of now.': 1.0986123085021973, 'The paper does not require minor revisions to make this point apparent - but a significant and major rewrite which seems beyond what the authors have done so far.': 1.0986123085021973, 'the experiments are (as also pointed out by other reviewers) rather unstructured and difficult to see much of an insight.': 1.0986123085021973, 'I should probably also list (as the other other reviewers) flaws and issues with the experiments - but given the detailed comments by the other reviewers there seems to be little additional value in doing so': 1.0979976654052734, 'So in essence the paper simply does not deliver at this point on its promise (as far as I am concerned) and in that sense I suggest a very clear reject for this conference.': 1.0986095666885376, 'As for the dataset employed: MNIST should be considered a toy-dataset for pretty much all purposes these days - and in that sense the dataset choice is not helping me (and many other people that you might want to convince) to safe this paper.': 0.5848822593688965, ""The authors propose and evaluate using SPN's to generate embeddings of input and output variables, and using MPN to decode output embeddings to output variables."": 1.0986123085021973, 'The advantage of predicting label embeddings is to decouple dependencies in the predicted space.': 1.0986123085021973, ""The authors show experimentally that using SPN based embeddings is better than those produced by RBM's."": 1.0986123085021973, 'This paper is fairly dense and a bit hard to read.': 1.0986123085021973, 'After the discussion, the main contributions of the authors are:': 1.0986123085021973, ""1. They propose the scheme of learning SPN's over Y and then using MPN's to decode the output, or just SPNs to embed X."": 1.0986123085021973, ""2. They propose how to decode MPN's with partial data."": 1.0986123085021973, '3. They perform some analysis of when their scheme will lead to perfect encoding/decodings.': 1.0986123085021973, '4. They run many, many experiments comparing various ways of using their proposed method to make predictions on multi-label classification datasets.': 0.4147586524486542, 'My main concerns with this paper are as follows:': 1.0986123085021973, 'The point of this paper is about using generative models for representation learning.': 1.0986028909683228, ""In their experiments, the main task is discriminative; e.g. predict multiple Y from X. The only discriminative baseline is a L2 regularized logistic regression, which does not have any structure on the output; it'd be nice to see how a discriminative structured prediction method would do, such as CRF or belief propagation."": 1.098611831665039, 'The many experiments suggest that their encoder/decoder scheme is working better than the alternatives; can you please give more details on the relative computation complexity of each method?': 1.0986043214797974, ""One thing I'm still having trouble understanding is *why* this method works better than MADE and the other alternatives."": 1.0986123085021973, 'Is it learning a better model of the distribution of Y?': 1.0986123085021973, 'Is it better at separating out correlations in the output into individual nodes?': 1.0986123085021973, 'Does it have larger representations?': 1.0986123085021973, 'I think the experiments are overkill and if anything, they way they are presented detract from the paper.': 1.0986123085021973, ""There's already far too many numbers and graphs presented to be easy to understand."": 1.0986119508743286, 'If I have to dig through hundreds of numbers to figure out if your claim is correct, the paper is not clear enough.': 1.0970758199691772, 'And, I said this before in my comments, please do not refer to Q1, Q2, etc.': 1.0986065864562988, 'these shortcuts let you make the paper more dense with fewer words but at the cost of readability.': 0.9100856184959412, 'I *think*': 0.42679718136787415, 'I convinced myself that your method works...I would love to see a table that shows, for each condition: (A) a baseline X->Y, (B) one *average* result across datasets for your method, and (C) one *average* result from a reasonable best competitor method.': 1.0985684394836426, 'Please show for both the exact match and hamming losses, as that will demonstrate the gap between independent linear prediction and structured prediction.': 1.0962305068969727, 'That would still be plenty of numbers but would make it much easier for the reader to verify your claims and you can put everything else in the Appendix.': 1.0777063369750977, 'E.g. something like:': 1.098587989807129, 'Input | Predicted Output | Decoder | Hamming | Exact Match': 0.614611804485321, 'X | P(Y)': 0.4200207591056824, '| CRF | xx.xx | xx.xx   (this is your baseline)': 0.6543477177619934, 'SPN E_X | P(Y)': 1.085558533668518, '| n/a | xx.xx | xx.xx': 0.5667102932929993, 'X | SPN E_Y | MPN | xx.xx | xx.xx  (given X, predict E_Y, then decode it with an MPN)': 1.0966451168060303, 'Does a presentation like that make sense?': 1.0986123085021973, ""It's just really hard and time-consuming for me as a reviewer to verify your results, the way you've laid them out currently."": 1.0985995531082153}"
374,https://openreview.net/forum?id=r1Chut9xl,"{""The paper claims improved inference for density estimation of sparse data (here text documents) using deep generative Gaussian models (variational auto-encoders), and a method for deriving word embeddings from the model's generative parameters that allows for a degree of interpretability similar to that of Bayesian generative topic models."": 1.3862943649291992, 'To discuss the contributions I will quickly review the generative story in the paper: first a K-dimensional latent representation is sampled from a multivariate Gaussian, then an MLP (with parameters \\theta) predicts unnormalised potentials over a vocabulary of V words, the potentials are exponentiated and normalised to make the parameters of a multinomial from where word observations are repeatedly sampled to make a document.': 1.3862943649291992, 'Here intractable inference is replaced by the VAE formulation where an inference network (with parameters \\phi) independently predicts for each document the mean and variance of a normal distribution (amenable to reparameterised gradient computation).': 1.3862943649291992, 'The first, and rather trivial, contribution is to use tf-idf features to inject first order statistics (a global information) into local observations.': 1.3246184587478638, 'The authors claim that this is particularly helpful in the case of sparse data such as text.': 1.3862756490707397, 'The second contribution is more interesting.': 1.3862943649291992, 'In optimising generative parameters (\\theta) and variational parameters (\\phi), the authors turn to a treatment which is reminiscent of the original SVI procedure.': 1.3862943649291992, 'That is, they see the variational parameters \\phi as *global* variational parameters, and the predicted mean \\mu(x) and covariance \\Sigma(x) of each observation x are treated as *local* variational parameters.': 1.3862943649291992, 'In the original VAE, local parameters are not directly optimised, instead they are indirectly optimised via optimisation of the global parameters utilised in their prediction (shared MLP parameters).': 1.3862943649291992, 'Here, local parameters are optimised holding generative parameters fixed (line 3 of Algorithm 1).': 1.3862943649291992, 'The optimised local parameters are then used in the gradient step of the generative parameters (line 4 of Algorithm 1).': 1.3860821723937988, 'Finally, global variational parameters are also updated (line 5).': 1.3796294927597046, 'Whereas indeed other authors have proposed to optimise local parameters, I think that deriving this procedure from the more familiar SVI makes the contribution less of a trick and easier to relate to.': 1.367832064628601, ""Some things aren't entirely clear to me."": 1.3833965063095093, 'I think it would have been nice if the authors had shown the functional form of the gradient used in step 3 of Algorithm 1.': 0.9299350380897522, 'The gradient step for global variational parameters (line 5 of Algorithm 1) uses the very first prediction of local parameters (thus ignoring the optimisation in step 3), this is unclear to me.': 1.3562148809432983, 'Perhaps I am missing a fundamental reason why that has to be the case (either way, please clarify).': 1.3518599271774292, 'The authors argue that this optimisation turns out helpful to modelling sparse data because there is evidence that the generative model p_\\theta(x|z) suffers from poor initialisation.': 1.1609677076339722, 'Please, discuss why you expect the initialisation problem to be worse in the case of sparse data.': 1.345200538635254, 'The final contribution is a neat procedure to derive word embeddings from the generative model parameters.': 0.6946684718132019, 'These embeddings are then used to interpret what the model has learnt.': 1.186254858970642, 'Interestingly, these word embeddings are context-sensitive once that the latent variable models an entire document.': 0.6240947246551514, 'About Figures 2a and 2b: the caption says that solid lines indicate validation perplexity for M=1 (no optimisation of local parameters) and dashed lines indicate M=100 (100 iterations of optimisation of local parameters), but the legends of the figures suggest a different reading.': 1.3831706047058105, 'If I interpret the figures based on the caption, then it seems that indeed deeper networks exposed to more data benefit from optimisation of local parameters.': 0.732544481754303, 'Are the authors pretty sure that in Figure 2b models with M=1 have reached a plateau (so that longer training would not allow them to catch up with M=100 curves)?': 1.2232047319412231, 'As the authors explain in the caption, x-axis is not comparable on running time, thus the question.': 1.0700154304504395, 'The analysis of singular values seems like an interesting way to investigate how the model is using its capacity.': 1.2257508039474487, 'However, I can barely interpret Figures 2c and 2d, I think the authors could have walked readers through them.': 1.365058183670044, 'As for the word embedding I am missing an evaluation on a predictive task.': 0.6331113576889038, 'Also, while illustrative, Table 2b is barely reproducible.': 1.3480896949768066, 'The text reads ""we create a document comprising a subset of words in the the context’s Wikipedia page."" which is rather vague.': 1.3339297771453857, 'I wonder whether this construct needs to be carefully designed in order to get Table 2b.': 1.321531057357788, 'In sum, I have a feeling that the inference technique and the embedding technique are both useful, but perhaps they should have been presented separately so that each could have been explored in greater depth.': 1.2788009643554688, 'First I would like to apologize for the delay in reviewing.': 1.3330644369125366, 'Summary : In this paper a variational inference is adapted to deep generative models, showing improvement for non-negative sparse dataset. The authors offer as well a method to interpret the data through the model parameters.': 1.3862919807434082, 'The writing is generally clear.': 0.9576890468597412, 'The methods seem correct.': 1.3862714767456055, 'The introspection approach appears to be original.': 1.3789185285568237, 'I found very interesting the experiment on the polysemic word embedding.': 0.7109191417694092, 'I would however have like to see how the obtained embedding would perform with respect to other more common embeddings in solving a supervised task.': 1.3353430032730103, 'Minor :': 1.3000737428665161, 'Eq. 2: too many closing parentheses': 1.2938082218170166, 'This paper introduces three tricks for training deep latent variable models on sparse discrete data:': 1.3862943649291992, '1) tf-idf weighting': 1.3862943649291992, '2) Iteratively optimizing variational parameters after initializing them with an inference network': 1.3862943649291992, '3) A technique for improving the interpretability of the deep model': 1.3862943649291992, 'The first idea is sensible but rather trivial as a contribution.': 1.3862943649291992, 'The second idea is also sensible, but is conceptually not novel.': 1.3862943649291992, 'What is new is the finding that it works well for the dataset used in this paper.': 1.3862943649291992, 'The third idea is interesting, and seems to give qualitatively reasonable results.': 1.3862943649291992, 'The quantitative semantic similarity results don’t seem that convincing, but I am not very familiar with the relevant literature and therefore cannot make a confident judgement on this issue.': 1.3857474327087402, 'This paper presents a small trick to improve the model quality of variational autoencoders (further optimizing the ELBO while initializing it from the predictions of the q network, instead of just using those directly) and the idea of using Jacobian vectors to replace simple embeddings when interpreting variational autoencoders.': 1.3862943649291992, 'The idea of the Jacobian as a natural replacement for embeddings is interesting, as it does seem to cleanly generalize the notion of embeddings from linear models.': 1.3862943649291992, ""It'd be interesting to see comparisons with other work seeking to provide context-specific embeddings, either by clustering or by smarter techniques (like Neelakantan et al, Efficient non-parametric estimation of multiple embeddings per word in vector space, or Chen et al A Unified Model for Word Sense Representation and Disambiguation)."": 1.3862943649291992, ""With the evidence provided in the experimental section of the paper it's hard to be convinced that the Jacobian of VAE-generated embeddings is substantially better at being context-sensitive than prior work."": 1.3862943649291992, 'Similarly, the idea of further optimizing the ELBO is interesting but not fully explored.': 1.3862943649291992, ""It's unclear, for example, what is the tradeoff between the complexity of the q network and steps further optimizing the ELBO, in terms of compute versus accuracy."": 1.3862943649291992, 'Overall the ideas in this paper are good': 1.3862943649291992, ""but I'd like to see them a little more fleshed out."": 1.3862943649291992}"
375,https://openreview.net/forum?id=r1G4z8cge,"{'The paper shows the relation between stochastically perturbing the parameter of a model at training time, and considering a mollified objective function for optimization.': 1.0986123085021973, 'Aside from Eqs. 4-7 where I found hard to understand what the weak gradient g exactly represents, Eq. 8 is intuitive and the subsequent Section 2.3 clearly establishes for a given class of mollifiers the equivalence between minimizing the mollified loss and training under Gaussian parameter noise.': 1.0986123085021973, 'The authors then introduce generalized mollifiers to achieve a more sophisticated annealing effect applicable to state-of-the-art neural network architectures (e.g. deep ReLU nets and LSTM recurrent networks).': 1.0986123085021973, 'The resulting annealing effect can be counterintuitive:': 1.0986123085021973, 'In Section 4, the Binomial (Bernoulli?)': 1.0979801416397095, 'parameter grows from 0 (deterministic identity layers) to 1 (deterministic ReLU layers), meaning that the network goes initially through a phase of adding noise.': 0.41953250765800476, 'This might effectively have the reverse effect of annealing.': 1.080775499343872, 'Annealing schemes used in practice seem very engineered (e.g. Algorithm 1 that determines how units are activated at a given layer consists of 9 successive steps).': 1.0986123085021973, 'Due to the more conceptual nature of the authors contribution (various annealing schemes have been proposed, but the application of the mollifying framework is original), it could have been useful to reserve a portion of the paper to analyze simpler models with more basic (non-generalized) mollifiers.': 1.0986123085021973, 'For example, I would have liked to see simple cases, where the perturbation schemes derived from the mollifier framework would be demonstrably more suitable for optimization than a standard heuristically defined perturbation scheme.': 1.0986123085021973, 'This paper first discusses a general framework for improving optimization of a complicated function using a series of approximations.': 1.0985336303710938, 'If the series of approximations are well-behaved compared to the original function, the optimization can in principle be sped up.': 1.0986119508743286, 'This is then connected to a particular formulation in which a neural network can behave as a simpler network at high noise levels but regain full capacity as training proceeds and noise lowers.': 1.0986123085021973, 'The idea and motivation of this paper are interesting and sound.': 1.0862430334091187, 'As mentioned in my pre-review question, I was wondering about the relationship with shaping methods in RL.': 1.0986032485961914, 'I agree with the authors that this paper differs from how shaping typically works (by modifying the problem itself) because in their implementation the architecture is what is ""shaped"".': 1.0986123085021973, 'Nevertheless, the central idea in both cases is to solve a series of optimization problems of increasing difficulty.': 1.0985571146011353, ""Therefore, I strongly suggest including a discussion of the differences between shaping, curriculum learning (I'm also not sure how this is different from shaping), and the present approach."": 1.0986123085021973, 'The presentation of the method for neural networks lacks clarity in presentation.': 0.7366898059844971, 'Improving this presentation will make this paper much easier to digest.': 1.0984609127044678, 'In particular:': 1.0985790491104126, 'Alg. 1 can not be understood at the point that it is referenced.': 1.0986075401306152, 'Please explain the steps to Eq. 25 more clearly and connect to steps 1-6 in Alg. 1.': 1.0986123085021973, 'Define u(x) clearly before defining u*(x)': 1.0986123085021973, 'There are several concerns with the experimental evaluations.': 1.0558087825775146, ""There should be a discussion about why doesn't the method work for solving much more challenging network training problems, such as thin and deep networks."": 1.0885910987854004, 'Some specific concerns:': 1.0986123085021973, 'The MLPs trained (Parity and Pentomino) are not very deep at all.': 1.0986121892929077, 'An experiment of training thin networks with systematically increasing depth would be a better fit to test this method.': 1.0980639457702637, 'Network depth is well known to pose optimization challenges.': 1.001408338546753, 'Instead, it is stated without reference that ""Learning the mapping from sequences of characters to the word-embeddings is a difficult problem.""': 0.9952476024627686, 'For cases where the gain is primarily due to the regularization effect, this method should be compared to other weight noise regularization methods.': 1.0986123085021973, 'I also suggest comparing to highway networks, since there are thematic similarities in Eq. 22, and it is possible that they can automatically anneal their behavior from simple to complex nets during training, considering that they are typically initialized with a bias towards copying behavior.': 1.09861159324646, 'For CIFAR-10 experiment, does the mollified model also use Residual connections?': 1.0728662014007568, 'If so, why?': 1.0984599590301514, 'In either case, why does the mollified net actually train slower than the residual and stochastic depth networks?': 1.0394834280014038, 'This is inconsistent with the MLP results.': 0.8751354217529297, 'Overall, the ideas and developments in this paper are promising, but it needs more work to be a clear accept for me.': 0.7617309093475342, 'The authors show that the idea of smoothing a highly non-convex loss function can make deep neural networks easier to train.': 1.0986123085021973, 'The paper is well-written, the idea is carefully analyzed, and the experiments are convincing, so we recommend acceptance.': 1.0986123085021973, 'For a stronger recommendation, it would be valuable to perform more experiments.': 1.0986123085021973, 'In particular, how does your smoothing technique compare to inserting probes in various layers of the network?': 1.0986123085021973, 'Another interesting question would be how it performs on hard-to-optimize tasks such as algorithm learning.': 1.0986123085021973, 'For example, in the ""Neural GPU Learns Algorithms"" paper the authors had to relax the weights of different layers of their RNN to make it optimize': 1.0986123085021973, 'could this be avoided with your smoothing technique?': 1.0986123085021973}"
376,https://openreview.net/forum?id=r1GKzP5xx,"{'The authors show how the hidden states of an LSTM can be normalised in order to preserve means and variances.': 1.0986123085021973, 'The method’s gradient behaviour is analysed.': 1.0986123085021973, 'Experimental results seem to indicate that the method compares well with similar approaches.': 1.0630627870559692, 'Points': 1.0986123085021973, '1)': 1.0986123085021973, 'The writing is sloppy in parts.': 1.0986123085021973, 'See at the end of the review for a non-exhaustive list.': 0.40617257356643677, '2) The experimental results show marginal improvements, of which the the statistical significance is impossible to asses.': 1.0984423160552979, '(Not completely the author’s fault for PTB, as they partially rely on results published by others.)': 1.0960336923599243, 'Weight normalisation seems to be a viable alternative in the: the performance and runtime are similar.': 0.5328389406204224, 'The implementation complexity of weight norm is, however, arguably much lower.': 1.0978353023529053, 'More effort could have been put in by the authors to clear that up.': 1.097886562347412, 'In the current state, practitioners as well as researchers will have to put in more effort to judge whether the proposed method is really worth it for them to replicate.': 1.0986098051071167, '3) Section 4 is nice, and I applaud the authors for doing such an analysis.': 1.098586082458496, 'List of typos etc.': 1.0986123085021973, 'maintain -> maintain': 1.0986123085021973, 'requisits -> requisites': 1.0986123085021973, 'a LSTM -> an LSTM': 1.0986123085021973, '""The gradients of ot and ft are equivalent to equation 25.”': 1.0986123085021973, 'Gradients cannot be equivalent to an equation.': 1.0986123085021973, '“beacause""-> because': 1.0986123085021973, 'One of the γx > γh at the end of page 5 is wrong.': 0.7597167491912842, 'I think this build upon previous works, in the attempt of doing something similar to batch norm specific for RNNs.': 1.0986123085021973, 'To me the experiments are not yet very convincing, I think is not clear this works better than e.g. Layer Norm or not significantly so.': 1.0986123085021973, ""I'm not convinced on how significant the speed up is either, I can appreciate is faster, but it doesn't feel like order of magnitude faster."": 1.0986123085021973, ""The theoretical analysis also doesn't provide any new insights."": 1.0986123085021973, 'All in all I think is good incremental work, but maybe is not yet significant enough for ICLR.': 1.0986123085021973, 'The paper proposes an extension of weight normalization / normalization propagation to recurrent neural networks.': 1.08454430103302, 'Simple experiments suggest it works well.': 1.0986123085021973, 'The contribution is potentially useful to a lot of people, as LSTMs are one of the basic building blocks in our field.': 1.0529590845108032, 'The contribution is not extremely novel: the change with respect to weight normalization is minor.': 1.0986099243164062, 'The experiments are also not very convincing: Layer normalization is reported to have higher test error as it overfits on their example, but in terms of optimization it seems to work better.': 1.0986123085021973, ""Also the authors don't seem to use the data dependent parameter init for weight normalization as proposed in that paper."": 1.0983220338821411}"
377,https://openreview.net/forum?id=r1IRctqxg,"{'The paper proposes a new criterion (sample importance) to study the impact of samples during the training of deep neural networks.': 1.098611831665039, 'This criterion is not clearly defined (the term \\phi^t_{i,j} is never defined, only \\phi^t_i is defined; Despite the unclear definition, it is understood that sample importance is the squared l2 norm of the gradient for a sample i and at time t strangely scaled by the squared learning rate (the learning rate should have nothing to do with the importance of a sample in this context).': 1.0986119508743286, 'The paper presents experiments on the well known MNIST and CIFAR datasets with correspondingly appropriate network architectures and choice of hyper-parameters and initialisations.': 1.0986123085021973, 'The size of the hidden layers is a bit small for Mnist and very small for CIFAR (this could explain the very poor performance in figure 6: 50% error on CIFAR)': 1.0986123085021973, 'The study of the evolution of sample importance during training depending on layers seems to lead to trivial conclusions': 1.0986123085021973, '- “the overall sample importance is different under different epochs” => yes the norm of the gradient is expected to vary': 1.044694423675537, '- “Output layer always has the largest average sample importance per parameter, and its contribution reaches the maximum in the early training stage and then drops” => 1. yes since the gradient flows backwards, the gradient is expected to be stronger for the output layer and it is expected to become more diffuse as it propagates to lower layers which are not stable. As learning progresses one would expect the output layer to have progressively smaller gradients. 2. the norm of the gradient depends on the scaling of the variables': 0.9449796676635742, 'The question of Figure 4 is absurd “Is Sample Importance the same as Negative log-likelihood of a sample?”.': 1.098591685295105, 'Of course not.': 1.0986123085021973, 'The results are very bad on CIFAR which discredits the applicability of those results.': 1.0979952812194824, 'On Mnist performance is not readable (figure 7): Error rate should only be presented between 0 and 10 or 20%': 1.0985593795776367, 'Despite these important issues (there are others), the paper manages to raises some interesting things: the so-called easy samples and hard samples do seem to correspond (although the study is very preliminary in this regard) to what would intuitively be considered easy (the most representative/canonical samples) and hard (edge cases) samples.': 1.098611831665039, 'Also the experiments are very well presented.': 0.8421331644058228, 'This paper examines the so called ""Sample Importance"" of each sample of a training data set, and its effect to the overall learning process.': 1.0986120700836182, 'The paper shows empirical results that shows different training cases induces bigger gradients at different stages of learning and different layers.': 1.0986123085021973, 'The paper shows some interesting results contrary to the common curriculum learning ideas of using easy training samples first.': 1.0986123085021973, 'However, it is unclear how one should define ""Easy"" training cases.': 1.0986123085021973, 'In addition, the experiments demonstrating ordering either NLL or SI is worse than mixed or random batch construction to be insightful.': 1.0986123085021973, 'Possible Improvements:': 1.0986123085021973, 'It would be nice to factor out the magnitudes of the gradients to the contribution of ""sample importance"".': 1.0986123085021973, 'Higher gradient (as a function of a particular weight vector) can be affected weight/initialization, thereby introducing noise to the model.': 1.0986123085021973, 'It would also be interesting if improvements based on ""Sample importance"" could be made to the batch selection algorithm to beat the baseline of random batch selection.': 1.0986123085021973, 'Overall this paper is a good paper with various experiments examining how various samples in SGD influences the various aspect of training.': 1.0986123085021973, '(paper summary)': 1.0986123085021973, 'The authors introduce the notion of “sample importance”, meant to measure the influence of a particular training example on the training of a deep neural network.': 1.0986123085021973, 'This quantity is closely related to the squared L2 norm of the gradient, where the summation is performed over (i) parameters of a given layer or (ii) across all parameters.': 1.0986123085021973, 'Summing this quantity across time gives the “overall importance”, used to tease apart easy from hard examples.': 1.0986123085021973, 'From this quantity, the authors illustrate the impact of [easy,hard] example during training, and their impact on layer depth.': 1.0986123085021973, '(detailed review)': 1.0986120700836182, 'I have several objections to this paper.': 1.0985654592514038, 'First and foremost, I am not convinced of the “sample importance” as a meaningful metric.': 1.0986123085021973, 'As previously mentioned, the magnitude of gradients will change significantly during learning, and I am not sure what conclusions one can draw from \\sum_t g_i^t vs \\sum_t g_j^t.': 1.0986123085021973, 'For example, gradients tend to have higher norms early in training than at convergence, in which case weighting each gradient equally seems problematic.': 1.0986123085021973, 'I tried illustrating the above with a small thought experiment during the question period: “if” the learning rate were too high, training may not even converge in which case sample importance would be ill-defined.': 1.0986123085021973, 'Having a measure which depends on the learning rate seems problematic to me, as does the use of the L2 norm.': 1.098587155342102, 'The “input Fisher” norm, \\mathbb{E} \\frac{\\partial \\log p} {\\partial x} (for a given time-step) may be better suited, as it speaks directly to the sensitivity of the classifier to the input x (and is insensitive to changes in the mean gradient norm).': 1.0982364416122437, 'But again summing Fisher norms across time may not be meaningful.': 1.0861799716949463, 'The experimental analysis also seems problematic.': 1.0986123085021973, 'The authors claim from Fig. 2 that output layers are primarily learnt in the early stage of training.': 1.0986108779907227, 'However, this is definitely not the case for CIFAR-10 and is debatable for MNIST: sample importance remains high for all layers during training, despite a small spike early on the output layer.': 1.0985592603683472, 'Fig 2. (lower, middle) and Fig. 6 also seems to highlight an issue with the SI measure: the SI is dominated by the input layer which has the most parameters, and can thus more readily impact the gradient norm.': 0.7988135814666748, 'Different model architectures may have yielded different conclusions.': 1.012970209121704, 'Had the authors managed to use the SI to craft a better curriculum, this would have given significant weight to the measure.': 1.0986119508743286, 'Unfortunately, these results are negative.': 1.0981512069702148, 'PROS:': 1.0982211828231812, '+ extensive experiments': 1.0986123085021973, 'CONS:': 1.0986123085021973, 'sample importance is a heuristic, not entirely well justified': 1.0986123085021973, 'SI yields limited insight into training of neural nets': 1.0986123085021973, 'SI does not inform curriculum learning': 1.0986123085021973}"
378,https://openreview.net/forum?id=r1LXit5ee,"{'The paper presents a learning algorithm for micromanagement of battle scenarios in real-time strategy games.': 1.0986123085021973, 'It focuses on a complex sub-problem of the full RTS problem.': 1.0986123085021973, 'The assumptions and restrictions made (greedy MDP, distance-based action encoding, etc.) are clear and make sense for this problem.': 1.0986123085021973, 'The main contribution of this paper is the zero-order optimization algorithm and how it is used for structured exploration.': 1.0986123085021973, 'This is a nice new application of zero-order optimization meets deep learning for RL, quite well-motivated using similar arguments as DPG.': 1.0986123085021973, 'The results show clear wins over vanilla Q-learning and REINFORCE, which is not hard to believe.': 1.0986123085021973, 'Although RTS is a very interesting and challenging domain (certainly worthy as a domain of focused research!), it would have been nice to see results on other domains, mainly because it seems that this algorithm could be more generally applicable than just RTS games.': 1.0986123085021973, 'Also, evaluation on such a complex domain makes it difficult to predict what other kinds of domains would benefit from this zero-order approach.': 1.0986123085021973, 'Maybe the authors could add some text to clarify/motivate this.': 1.0986123085021973, 'There are a few seemingly arbitrary choices that are justified only by ""it worked in practice"".': 1.0986123085021973, 'For example, using only the sign of w / Psi_{theta}(s^k, a^k).': 1.0986123085021973, 'Again later: ""Also we neglected the argmax operation that chooses the actions"".': 1.0854995250701904, 'I suppose this and dividing by t could keep things nicely within or close to [-1,1] ?': 1.0986123085021973, 'It might make sense to try truncating/normalizing w/Psi; it seems that much information must be lost when only taking the sign.': 1.0981018543243408, 'Also lines such as ""We did not extensively experiment with the structure of the network, but we found the maxpooling and tanh nonlinearity to be particularly important"" and claiming the importance of adagrad over RMSprop without elaboration or providing any details feels somewhat unsatisfactory and leaves the reader wondering why.. e.g. could these only be true in the RTS setup in this paper?': 1.092054009437561, 'The presentation of the paper can be improved, as some ideas are presented without any context making it unnecessarily confusing.': 1.0986121892929077, 'For example, when defining f(\\tilde{s}, c) at the top of page 5, the w vector is not explained at all, so the reader is left wondering where it comes from or what its use is.': 1.098417043685913, 'This is explained later, of course, but one sentence on its role here would help contextualize its purpose (maybe refer later to the section where it is described fully).': 1.041633129119873, 'Also page 7: ""because we neglected that a single u is sampled for an entire episode""; actually, no, you did mention this in the text above and it\'s clear from the pseudo-code too.': 1.020907998085022, '""perturbated"" -> ""perturbed""': 1.0985865592956543, 'After response period:': 1.0985960960388184, 'No rebuttal entered, therefore review remains unchanged.': 1.0986123085021973, 'This is a very interesting and timely paper, with multiple contributions.': 1.0978091955184937, 'it proposes a setup for dealing with combinatorial perception and action-spaces that generalizes to an arbitrary number of units and opponent units,': 1.0827538967132568, 'it establishes some deep RL baseline results on a collection of Starcraft subdomains,': 1.0986123085021973, 'it proposes a new algorithm that is a hybrid between black-box optimization REINFORCE, and which facilitates consistent exploration.': 1.0837311744689941, 'As mentioned in an earlier comment, I don’t see why the “gradient of the average cumulative reward” is a reasonable choice, as compared to just the average reward?': 1.0986063480377197, 'This over-weights late rewards at the expense of early ones, so the updates are not matching the measured objective.': 1.0986120700836182, 'The authors state that they “did not observe a large difference in preliminary experiments”': 1.0986123085021973, 'so if that is the case, then why not choose the correct objective?': 1.0982357263565063, 'DPQ is characterized incorrectly: despite its name, it does not “collect traces by following deterministic policies”, instead it follows a stochastic behavior policy and learns off-policy about the deterministic policy.': 1.0986123085021973, 'Please revise this.': 1.0986123085021973, 'Gradient-free optimization is also characterized incorrectly (“it only scales to few parameters”), recent work has shown that this can be overcome (e.g. the TORCS paper by Koutnik et al, 2013).': 1.0986123085021973, 'This also suggests that your “preliminary experiments with direct exploration in the parameter space” may not have followed best practices in neuroevolution?': 1.0986123085021973, 'Did you try out some of the recent variants of NEAT for example, which have been applied to similar domains in the past?': 1.0986123085021973, 'On the specific results, I’m wondering about the DQN transfer from m15v16 to m5v5, obtaining the best win rate of 96% in transfer, despite only reaching 13% (the worst) on the training domain?': 1.0986123085021973, 'Is this a typo, or how can you explain that?': 1.0986123085021973, 'This work introduces some StarCraft micro-management tasks (controlling individual units during a battle).': 1.0986123085021973, 'These tasks are difficult for recent DeepRL methods due to high-dimensional, variable action spaces (the action space is the task of each unit, the number of units may vary).': 1.0986123085021973, 'In such large action spaces, simple exploration strategies (such as epsilon-greedy) perform poorly.': 1.0986123085021973, 'They introduce a novel algorithm ZO to tackle this problem.': 1.0986123085021973, 'This algorithm combines ideas from policy gradient, deep networks trained with backpropagation for state embedding and gradient free optimization.': 1.0986123085021973, 'The algorithm is well explained and is compared to some existing baselines.': 1.0986123085021973, 'Due to the gradient free optimization providing for much better structured exploration, it performs far better.': 1.0986121892929077, 'This is a well-written paper and a novel algorithm which is applied to a very relevant problem.': 1.0986120700836182, 'After the success of DeepRL approaches at learning in large state spaces such as visual environment, there is significant interest in applying RL to more structured state and action spaces.': 1.0986120700836182, 'The tasks introduced here are interesting environments for these sorts of problems.': 1.0985362529754639, 'It would be helpful if the authors were able to share the source code / specifications for their tasks, to allow other groups to compare against this work.': 1.0986123085021973, 'I found section 5 (the details of the raw inputs and feature encodings) somewhat difficult to understand.': 1.0986123085021973, 'In addition to clarifying, the authors might wish to consider whether they could provide the source code to their algorithm or at least the encoder to allow careful comparisons by other work.': 1.0986090898513794, 'Although discussed, there is no baseline comparison with valued based approaches with attempt to do better exploration by modeling uncertainty (such as Bootstrapped DQN).': 1.0944775342941284, 'It would useful to understand how such approaches, which also promise better exploration, compare.': 1.0910567045211792, 'It would also be interesting to discuss whether action embedding models such as energy-based approaches (e.g. https://arxiv.org/pdf/1512.07679v2.pdf, http://www.jmlr.org/papers/volume5/sallans04a/sallans04a.pdf ) or continuous action embeddings (https://arxiv.org/pdf/1512.07679v2.pdf ) would provide an alternative approach for structured exploration in these action spaces.': 1.0986082553863525}"
379,https://openreview.net/forum?id=r1PRvK9el,"{'In this paper, the authors proposed an implicit ResoNet model for knowledge base completion.': 1.0986123085021973, 'The proposed model performs inference implicitly by a search controller and shared memory.': 1.0976225137710571, 'The proposed approach demonstrates promising results on FB15k benchmark dataset.': 1.0986123085021973, 'Pros:': 1.0986123085021973, 'The proposed approach demonstrates strong performance on FB15k dataset.': 1.0986123085021973, 'The idea of using shared memory for knowledge base completion is new and interesting.': 1.0986123085021973, 'The proposed approach is general and can be applied in various tasks.': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'There is no qualitative analysis on the results, and it is hard to see why the proposed approach works on the knowledge-base completion task.': 1.0546085834503174, 'The introduction section can be improved.': 1.0986123085021973, 'Specifically, the authors should motivate ""shared memory"" more in the introduction and how it different from existing methods that using ""unshared memory"" for knowledge base completion.': 1.0986117124557495, 'Similarly, the function of search controller is unclear in the introduction section as it is unclear what does search mean in the content of knowledge base completion.': 0.5733901858329773, 'The concept of shared memory and search controller only make sense to me after reading through section 2.': 0.6576195359230042, 'This paper proposes a method for link prediction on Knowledge Bases.': 1.0986123085021973, 'The method contains 2 main innovations: (1) an iterative inference process that allows the model to refine its predictions and (2) a shared memory component.': 1.0986123085021973, 'Thanks to these 2 elements, the model introduced in the paper achieved remarkable results on two benchmarks.': 1.0986123085021973, 'The paper is fairly written.': 1.0986123085021973, 'The model is interesting and the experimental results are strikingly good.': 1.0986123085021973, 'Still, I only rate for a weak accept for the following reasons.': 1.0986123085021973, '* The main problem with this paper is that there is little explanation of how and why the two new elements aforementioned are leading to such better results.': 1.0986123085021973, 'For instance:': 1.0986123085021973, '- What are the performance without the shared memory? And when its size is grown?': 1.0986123085021973, '- How does the performance is impacted when one varies Tmax from 1 to 5 (which the chosen value for the experiments I assume)? This gives an indications of how often the termination gate works.': 1.0986123085021973, '- It would also be interesting to give the proportion of examples for which the inference is terminated before hitting Tmax.': 1.0986123085021973, '- What is the proportion of examples for which the prediction changed along several inference iterations?': 1.0986123085021973, '*': 1.0986123085021973, 'A value of \\lambda set to 10 (Section 2) seems to indicate a low temperature for the softmax.': 1.098202109336853, 'Is the attention finally attending mostly at a single cell?': 1.0986123085021973, 'How do the softmax activations change with the type of relationships?': 1.0986123085021973, 'the entity type?': 1.0986123085021973, '* FB15k and WN18 are quite old overused benchmarks now.': 1.0986123085021973, 'It would be interesting to test on larger conditions.': 1.0986123085021973, '[Summary]': 1.0986123085021973, 'This paper proposes a new way for knowledge base completion which highlights: 1) adopting an implicit shared memory, which makes no assumption about its structure and is completely learned during training; 2) modeling a multi-step search process that can decide when to terminate.': 1.0986123085021973, 'The experimental results on WN18 and FB15k seem pretty good.': 1.096203088760376, 'The authors also perform an analysis on a shortest path synthetic task, and demonstrate that this model is better than standard seq2seq.': 1.0986123085021973, 'The paper is well-written and it is easy to follow.': 1.0964484214782715, '[Major comments]': 1.0986123085021973, 'I actually do like the idea and am also impressed that this model can work well.': 1.0986123085021973, 'The main concern is that this paper presents too little analysis about how it works and whether it is sensitive to the hyper-parameters, besides that only reporting a final model on WN18 and FB15k.': 1.0986123085021973, 'One key hyper-parameter I believe is the size of shared memory (using 64 for the experiments).': 1.0986117124557495, 'I don’t think that this number should be fixed for all tasks, at least it should depend on the KB scale.': 1.0974974632263184, 'Could you verify this in your experiments?': 0.9246150255203247, 'Would it be even possible to make a memory structure with dynamic size?': 1.0986123085021973, 'The RL setting (stochastic search process) is also one highlight of the paper, but could you demonstrate that how much it does really help?': 1.0986123085021973, 'I think it is necessary to compare to the following: remove the termination gate and fix the number of inference steps and see how well the model does?': 1.0986123085021973, 'Also show how the performance varies on # of steps?': 1.0986123085021973, 'I appreciate your attempts on the shortest path synthetic task.': 1.0986123085021973, 'However, I think it would be much better if you can demonstrate that under a real KB setting.': 1.0986123085021973, 'You can still perform the shortest path analysis, but using KB  (e.g., Freebase) entities and relations.': 1.0986123085021973, '[Minor comments]': 1.0986123085021973, 'I am afraid that the output gate illustrated in Figure 1 is a bit confusing.': 1.0986123085021973, 'There should be only one output, depending on when the search process is terminated.': 1.0986123085021973}"
380,https://openreview.net/forum?id=r1R5Z19le,"{'The authors propose a semi-supervised technique for neural networks which includes two objectives: (1) the neural net embeddings of two samples with identical labels is constrained to be closer than the embeddings of samples with different labels (2) the embedding of an unlabeled example is constrained to be close to the embeddings of the closest labeled sample (and far away from the other ones).': 0.6912814974784851, 'While the authors list a number of previous works, they do not relate them very well with their approach, and actual differences appear unclear.': 0.6931366324424744, 'In particular, the approach seems rather incremental with respect to (Hadsell et al. 2006), the way the neighbors are chosen being the main difference; in that respect, (Weston et al, 2008 or 2012) (which could be viewed as an application of DrLim from Hadsell et al, applied to semi-supervised learning) is even closer to the approach proposed here.': 0.693146824836731, 'There is also no balancing constraint in the proposed approach, which was known to be crucial in all these models from the 2000s.': 0.6931108832359314, 'Concerning the experimental results, reported performance are very good; given the approach is very close to previous work, it is hard to know if good performance come from better neural net architectures or something else.': 0.6931363344192505, 'This should be clarified.': 0.2926446795463562, 'In summary, the novelty is not clearly defined in this paper; differences with existing literature should be highlighted.': 0.6931471228599548, 'Experimental results are very good, but it is hard to know where comes the difference in performance with previous (very related) work.': 0.5060504674911499, 'This work presents an embedding approach for semi-supervised learning with neural nets, in the presence of little labeled data.': 0.6931471824645996, 'The intuition is to learn a metric embedding that forms “clusters” with the following desiderata: two labeled examples from the class should have a smaller distance in this embedding compared to any example from another class & a given unlabeled example embedding will be closer to all of the embeddings of *some* label (i.e. that a given unlabeled example will be “matched” to one cluster).': 0.6931467652320862, 'The paper formulates these intuitions as two differentiable losses and does gradient descent on their sum.': 0.6931471824645996, 'It’s unclear to me how different is this work from the sum of Hoffer & Ailon (2015) (which is eq. 3) and Grandvalet & Bengio (2004) (seems to be related to eq. 4).': 0.6931471824645996, 'Would be nice if the authors not only cited the previous work but summarized the actual differences.': 0.6931471824645996, 'In Section 5.1, the authors say that Szegedy et al. (2015) use random noise in the targets': 0.6931471824645996, 'is that actually true?': 0.6931471824645996, 'I think only soft targets are used (which are not noisy).': 0.6931471824645996, 'Does the choice of \\lambda_{1,2} make a difference?': 0.6931471824645996, 'How is k for k-NN actually chosen?': 0.6931471824645996, 'Is there a validation set?': 0.6931471824645996, 'Figure 1 would benefit from showing where the labeled examples were at the beginning of training (relative to each other / rest of the data).': 0.6931471824645996, 'The submission seems overall OK, but somewhat light on actual data-driven or theoretical insights.': 0.6931471824645996, 'I would’ve liked experiments showing the influence of data set sizes at the very least, and ablation experiments that showed the influence of each of the corresponding losses.': 0.6931471824645996}"
381,https://openreview.net/forum?id=r1S083cgx,"{'This paper has no machine learning algorithmic contribution: it just uses the the same combination  of LSTM and bivariate mixture density network as Graves, and the detailed explanation in the appendix even misses one key essential point: how are the Gaussian parameters obtained as a transformation of the output of the LSTM.': 1.0986123085021973, 'There are also no numerical evaluation suggesting that the algorithm is some form of improvement over the state-of-the-art.': 1.0986123085021973, 'So I do not think such a paper is appropriate for a conference like ICLR.': 1.0986123085021973, 'The part describing the handwriting tasks and the data transformation is well written and interesting to read, it could be valuable work for a conference more focused on handwriting recognition, but I am no expert in the field.': 1.0986123085021973, 'The paper presents a method for sequence generation with a known method applied to feature extracted from another existing method.': 1.0986123085021973, 'The paper is heavily oriented towards to chosen technologies and lacks in literature on sequence generation.': 1.0979257822036743, 'In principle, rich literature on motion prediction for various applications could be relevant here.': 0.5480174422264099, 'Recent models exist for sequence prediction (from primed inputs) for various applications, e.g. for skeleton data.': 1.0986123085021973, 'These models learn complex motion w/o any pre-processing.': 1.0986123085021973, 'Evaluation is a big concern.': 1.0952996015548706, 'There is no quantitative evaluation.': 1.0986123085021973, 'There is no comparision with other methods.': 0.422996461391449, 'I still wonder whether the intermediate representation (developed by Plamondon et al.) is useful in this context of a fully trained sequence generation model and whether the model could pick up the necessary transformations itself.': 1.0986123085021973, 'This should be evaluated.': 0.8495208024978638, 'Details:': 1.097342610359192, 'There are several typos and word omissions, which can be found by carefully rereading the paper.': 0.7791828513145447, 'At the beginning of section 3, it is still unclear what the application is.': 1.09311842918396, 'Prediction of dynamic parameters?': 0.39151695370674133, 'What for?': 1.0986123085021973, 'Section 3 should give a better motivation of the work.': 1.0068565607070923, 'Concerning the following paragraph': 1.0806443691253662, '""While such methods are superior for handwriting analysis and biometric purposes, we opt for a less precise method (Berio & Leymarie, 2015) that is less sensitive to sampling quality and is aimed at generating virtual target sequences that remain perceptually similar to the original trace.': 0.7244499921798706, '""': 1.0986123085021973, 'This method has not been explained.': 1.0986063480377197, 'A paper should be self-contained.': 1.0986121892929077, 'The authors mentioned that the ""V2V-model is conditioned on (...)""; but not enough details are given.': 1.0983692407608032, 'Generally speaking, more efforts could be made to make the paper more self-contained.': 0.4120651185512543, 'This paper takes a model based on that of Graves and retrofits it with a representation derived from the work of Plamondon.': 1.0986123085021973, 'part of the goal of deep learning has been to avoid the use of hand-crafted features and have the network learn from raw feature representations, so this paper is somewhat against the grain.': 1.0745694637298584, ""The paper relies on some qualitative examples as demonstration of the system, and doesn't seem to provide a strong motivation for there being any progress here."": 1.0986123085021973, ""The paper does not provide true text-conditional handwriting synthesis as shown in Graves' original work."": 1.0986123085021973, 'Be more consistent about your bibliography (e.g. variants of Plamondon\'s own name, use of ""et al.""': 1.0986123085021973, 'in the bibliography etc.)': 1.0986123085021973}"
382,https://openreview.net/forum?id=r1Ue8Hcxg,"{'This paper explores an important part of our field, that of automating architecture search.': 1.0986123085021973, 'While the technique is currently computationally intensive, this trade-off will likely become better in the near future as technology continues to improve.': 1.0986084938049316, 'The paper covers both standard vision and text tasks and tackle many benchmark datasets, showing there are gains to be made by exploring beyond the standard RNN and CNN search space.': 1.0986123085021973, 'While one would always want to see the technique applied to more datasets, this is already far more sufficient to show the technique is not only competitive with human architectural intuition but may even surpass it.': 1.0986123085021973, 'This also suggests an approach to tailor the architecture to specific datasets without resulting in hand engineering at each stage.': 1.0986123085021973, 'This is a well written paper on an interesting topic with strong results.': 0.993811845779419, 'I recommend it be accepted.': 1.0579946041107178, 'This paper presents search for optimal neural-net architectures based on actor-critic framework.': 1.0986121892929077, 'The method treats DNN as a variable length sequence, and uses RL to find the target architecture, which acts as an actor.': 1.0986062288284302, 'The node selection is an action in the RL context, and evaluation error of the outcome architecture corresponds to reward.': 1.0985107421875, 'A auto-regressive two-layer LSTM is used as a controller and critic.': 1.098611831665039, 'The method is evaluated on two different problems, and each compared with number of other human-created architectures.': 1.0982630252838135, 'This is very exciting paper!': 1.0986123085021973, 'Hand selecting architectures is difficult, and it is hard to know how far from optimal results the hand-designed networks are.': 1.098584771156311, 'The presented method is  novel.': 0.8853020071983337, 'The authors do an excellent job of describing it in detail, with all the improvements that needed to be done.': 1.0949459075927734, 'The tested data represents well the capability of the method.': 1.0878912210464478, 'It is very interesting to see the differences between the generated architectures and human generated ones.': 1.092110514640808, 'The paper is written very clearly, and is very accessible.': 1.0985277891159058, 'The coverage and contrast with the related literature is done well.': 1.0759683847427368, 'It would be interesting to see the data about the time needed for training, and correlation between time/resources needed to train and the quality of the model.': 1.073878526687622, 'It would also be interesting to see how human bootstrapped models perform and involve.': 0.8254112601280212, 'Overall, an excellent and interesting paper.': 1.0986120700836182, 'This paper proposed to use RL and RNN to design the architecture of networks for specific tasks.': 1.0986123085021973, 'The idea of the paper is quite promising and the experimental results on two datasets show that method is solid.': 1.0986123085021973, 'The pros of the paper are:': 1.0986123085021973, '1. The idea of using RNN to produce the description of the network and using RL to train the RNN is interesting and promising.': 0.9692386984825134, '2. The generated architecture looks similar to what human designed, which shows that the human expertise and the generated network architectures are compatible.': 1.021946668624878, 'The cons of the paper are:': 1.0986123085021973, '1. The training time of the network is long, even with a lot of computing resources.': 0.28464123606681824, '2. The experiments did not provide the generality of the generated architectures. It would be nice to see the performances of the generated architecture on other similar but different datasets, especially the generated sequential models.': 0.38003969192504883, 'Overall, I believe this is a nice paper.': 1.0954375267028809, 'But it need more experiments to show its potential advantage over the human designed models.': 0.9990134239196777}"
383,https://openreview.net/forum?id=r1Usiwcex,"{'The paper tackles the task of music generation.': 0.9428865909576416, 'They use an orderless NADE model for the task of ""fill in the notes"".': 1.0986123085021973, 'Given a roll of T timesteps of pitches, they randomly mask out some pitches, and the model is trained to predict the missing notes.': 1.0986123085021973, 'This follows how the orderless NADE model can be trained.': 1.0986123085021973, 'During sampling, one normally follows an ancestral sampling procedure.': 1.0986123085021973, 'For this, an ordering is defined over outputs, and one runs the model on the current input, samples one of the outputs according to the order, adds this output to the next input, and continues this procedure until all outputs have been sampled.': 1.0986123085021973, 'The key point of the paper is that this is a bad sampling strategy.': 1.0986051559448242, 'Instead, they suggest the strategy of Yao et al. 2014, which uses a blocked Gibbs sampling approach.': 1.0986123085021973, 'The blocked Gibbs strategy instead masks N inputs randomly and independently, samples them, and repeats this procedure.': 1.0986123085021973, 'The point of this strategy is the make sure the sampling chain mixes well, which will happen for large N. However, since the samples are independent, having a large N gives incoherent samples.': 1.0986120700836182, 'Thus, the authors follow an annealed schedule for N, making it smaller over time, which will eventually reduce to ancestral sampling (giving global structure to the sample).': 1.0986123085021973, 'They conduct a variety of experiments involving both normal metrics and human evaluations, and find that this blocked Gibbs sampling outperforms other sampling procedures.': 1.0986123085021973, 'This is a well written paper - great job.': 1.0986123085021973, ""My main problem with the paper is that having read Uria and Yao, I don't know how much I have learned from this work in the context of this being an ICLR submission."": 1.0986078977584839, 'If this was submitted to some computational music / art conference, this paper would be a clear accept.': 1.0986123085021973, ""However, for ICLR, I don't see enough novelty compared with previous works this builds upon."": 1.0986123085021973, 'Orderless NADE is an established model.': 1.0986123085021973, 'The blocked Gibbs sampling and annealing scheme are basically the exact same one used in Yao.': 1.0986096858978271, ""Thus, the main novelty of this paper is its application to the music domain, and finding that Yao's method works better for sampling music."": 1.0986123085021973, 'This is a good contribution, but more tailored to those working in the music domain.': 1.0986117124557495, 'If the authors found that these results also hold for other domains like images (e.g. on CIFAR / tiny Imagenet) and text (e.g. document generation), then I would change my mind and accept this paper for ICLR.': 1.0985091924667358, 'Even just trying musical domains other than Bach chorales would be useful.': 1.0986123085021973, 'However, as it stands, the experiments are not convincing enough.': 1.0986089706420898, 'The paper presents a way to model the distribution of four-part Bach chorales using Convolutional Neural Networks.': 1.0982728004455566, 'Furthermore it addresses the task of artificial music generation by sampling from the model using blocked Gibbs sampling and shows': 0.5652620792388916, 'The CNN model for the distribution seems very appropriate for the data at hand.': 0.8106726408004761, 'Also the analysis of the proposed sampling schemes with the analogy between Gibbs sampling and human music composition are very interesting.': 1.086351752281189, 'I am not too sure about the evaluation though.': 0.8648085594177246, 'Since the reported likelihoods are not directly comparable to previous work, I have difficulties judging the quality of the quantitative results.': 1.0985318422317505, 'For the human evaluation I would like to see the data for the direct comparisons between the models.': 0.47902190685272217, 'E.g. How did NADE vs. Bach perform.': 1.098611831665039, 'Also I find the question: ‘what piece of music do you prefer’ a stronger test than the question ‘what piece is more musical to you’ because I don’t really know what ‘musical’ means to the AMT workers.': 1.0985159873962402, 'Finally, while I think the Bach Chorales are interesting musical pieces that deserve to be subject of the analysis but I find it hard to judge how well this modelling approach will transfer to other types of music which might have a very different data distribution.': 1.0986121892929077, 'Nevertheless, in conclusion, I believe this is an exciting model for an interesting task that produces non-trivial musical data.': 1.0986114740371704, 'This paper proposed COCONET, which is a neural autoregressive model with convolution, to do music composition task.': 1.098611831665039, 'This paper also proposed to use blocked Gibbs sampling instead of the ancestral sampling of the original NADE model to generate better pieces of music.': 1.0983725786209106, 'The experimental results showed that the NLL of COCONET is better than the other baselines and the human evaluation task by Amazon’s Mechanical Turk illustrated that the model can generate compelling music.': 0.6250394582748413, 'In general, I think the paper is good.': 1.0852699279785156, 'Using NADE based model with convolution operations on music generation tasks and using blocked Gibbs sampling contains some kind of novelty.': 1.0986119508743286, 'However, the novelty of the paper is incremental, since the blocked Gibbs sampling for NADE model is already proposed by Yao et al., (2014) and the using NADE based model for music modeling has also been proposed by Boulanger-Lewandowski  et al., (2012).': 1.0983484983444214}"
384,https://openreview.net/forum?id=r1VGvBcxl,"{'The paper introduces GA3C, a GPU-based implementation of the A3C algorithm, which was originally designed for multi-core CPUs.': 1.0986123085021973, 'The main innovation is the introduction of a system of queues.': 1.0986123085021973, 'The queues are used for batching data for prediction and training in order to achieve high GPU occupancy.': 1.0986123085021973, ""The system is compared to the authors' own implementation of A3C as well as to published reference scores."": 1.0986123085021973, 'The paper introduces a very natural architecture for implementing A3C on GPUs.': 1.0986123085021973, 'Batching requests for predictions and learning steps for multiple actors to maximize GPU occupancy seems like the right thing to do assuming that latency is not an issue.': 1.0986003875732422, 'The automatic performance tuning strategy is also really nice to see.': 1.0986123085021973, 'I appreciate the response showing that the throughput of GA3C is 20% higher than what is reported in the original A3C paper.': 1.0986123085021973, 'What is still missing is a demonstration that the learning speed/data efficiency is in the right ballpark.': 1.0986123085021973, 'Figure 3 of your paper is comparing scores under very different evaluation protocols.': 1.0986123085021973, 'These numbers are just not comparable.': 1.0986123085021973, 'The most convincing way to show that the learning speed is comparable would be time vs score plots or data vs score plots that show similar or improved speed to A3C. For example, this open source implementation seems to match the performance on Breakout: https://github.com/muupan/async-rl': 1.0986123085021973, 'One or two plots like that would complete this paper very nicely.': 1.0986123085021973, 'I appreciate the additional experiments included in the revised version of the paper.': 1.0986123085021973, 'The learning speed comparison makes the paper more complete and I’m slightly revising my score to reflect that.': 1.0986123085021973, 'Having said that, there is still no clear demonstration that the higher throughput of GA3C leads to consistently faster learning.': 1.0986123085021973, 'With the exception of Pong, the training curves in Figure 6 seem to significantly underperform the original A3C results or even the open source implementation of A3C on Breakout and Space Invaders (https://github.com/muupan/async-rl).': 0.7160738706588745, 'This paper introduce a variant of A3C model where while agents run on multiple cores on CPU the model computations which is the computationally intensive part is passed to the GPU.': 1.0334980487823486, 'And they perform various analysis to show the gained speedup.': 1.0986123085021973, 'Thanks the authors for the replying to the questions and adjusting the paper to make it more clear.': 1.0986123085021973, ""It's an interesting modification the the original algorithm."": 1.0986123085021973, 'And section 5 does a through analysis of gpu utilization on different configurations.': 1.0986123085021973, 'The main weakness of the paper is lack of more extensive experiments in more atari domains and non atari domains, also multiple plots for multiple runs for observing the instabilities.': 1.0308228731155396, 'Stability is a very important issue in RL, and also the most successful algorithms should be able to achieve good results in various domains.': 1.0986123085021973, 'I do understand the computational resource limitation, especially in academia if in fact this work was done outside Nvidia.': 1.0986123085021973, ""This paper introduces a 'GPU-friendly' variant of A3C which relaxes some synchronicity constraints in the original A3C algorithm to make it more friendly to a high-throughput GPU device."": 1.0986123085021973, 'The analysis of the effects of this added latency is thorough.': 1.0986123085021973, 'The systems analysis of the algorithm is extensive.': 1.0986123085021973, 'One caveat is that the performance figures in Table 3 are hard to compare since the protocols vary so much.': 0.7502068281173706, ""I understand that DeepMind didn't provide reproducible code for A3C, but I gather from the comment that the authors have re-implemented vanilla A3C as well, in which case it would be good to show what this reimplementation of A3C achieves in the same setting used by DM, and in the setting of the experiment conducted using GA3C (1 day)."": 0.9473896026611328, ""It would be good to clarify in the text that the experimental protocol differed (top 5 out of 50 vs single run), and clarify why the discrepancy, even if the answer is that the authors didn't have time / resources to reproduce the same protocol."": 1.0986123085021973, 'A bit more care would go a long way to establishing that indeed, there is no price to pay for the approximations that were made.': 1.0986123085021973, 'I applaud the authors for open-sourcing the code, especially since there is a relative shortage of properly tested open-source implementations in that general area, and getting these algorithms right is non-trivial.': 1.0986123085021973, 'A disclaimer: having never implemented A3C myself, I have a low confidence in my ability to appropriately assess of the algorithmic aspects of the work.': 1.0986123085021973}"
385,https://openreview.net/forum?id=r1VdcHcxx,"{'The paper shows that BN, which does not work out of the box for RNNs, can be used with LSTM when the operator is applied to the hidden-to-hidden and the input-to-hidden contribution separately.': 1.0880054235458374, 'Experiments are conducted to show that it leads to improved generalisation error and faster convergence.': 1.0986123085021973, 'The paper is well written and the idea well presented.': 1.0986123085021973, 'i)': 1.0986123085021973, 'The data sets and consequently the statistical assumptions used are limited (e.g. no continuous data, only autoregressive generative modelling).': 1.0986123085021973, 'ii)': 1.0986123085021973, 'The hyper parameters are nearly constant over the experiments.': 1.0986123085021973, 'It is ruled out that they have not been picked in favor of one of the methods.': 1.0986123085021973, 'E.g. just judging from the text, a different learning rate could have lead to equally fast convergence for vanilla LSTM.': 1.0986123085021973, 'Concluding, the experiments are flawed and do not sufficiently support the claim.': 1.0986123085021973, 'An exhaustive search of the hyper parameter space could rule that out.': 1.0986123085021973, 'This paper extends batch normalization successfully to RNNs where batch normalization has previously failed or done poorly.': 1.0986123085021973, 'The experiments and datasets tackled show definitively the improvement that batch norm LSTMs provide over standard LSTMs.': 1.0986123085021973, 'They also cover a variety of examples, including character level (PTB and Text8), word level (CNN question-answering task), and pixel level (MNIST and pMNIST).': 1.0986123085021973, 'The supplied training curves also quite clearly show the potential improvements in training time which is an important metric for consideration.': 1.0986123085021973, 'The experiment on pMNIST also solidly shows the advantage of batch norm in the recurrent setting for establishing long term dependencies.': 1.0986123085021973, 'I additionally also appreciated the gradient flow insight, specifically the impact of unit variance on tanh derivatives.': 1.0986123085021973, 'Showing it not just for batch normalization but additionally the ""toy task"" (Figure 1b) was hugely useful.': 1.0946184396743774, 'Overall I find this paper a useful additional contribution to the usage of batch normalization and would be necessary information for successfully employing it in a recurrent setting.': 1.0986121892929077, 'Contributions': 1.0986123085021973, 'The paper presents an adaptation of batch normalization for RNNs in the case of LSTMs, along the horizontal depth.': 1.0985865592956543, 'Contrary to previous work from (Laurent 2015; Amodei 2016), the work demonstrates that batch-normalizing the hidden states of RNNs can improve optimization, and argues with quantitative experiments that the key factor to making this work is proper initialization of parameters, in particular gamma.': 1.095914602279663, 'Experiments show some gain in performance over vanilla LSTMs on Sequential MNIST, PTB, Text8 and CNN Question-Answering.': 1.027329921722412, 'Novelty+Significance': 1.0985820293426514, 'Batch normalization has been key for training deeper and deeper networks (e.g. ResNets) and it seems natural that we would want to extend it to RNNs.': 1.0951937437057495, 'The paper shows that it is possible to do so with proper initialization of parameters, contrary to previous work from (Laurent 2015;': 1.0986084938049316, 'Amodei 2016).': 1.0899673700332642, 'Novelty comes from where to batch norm (i.e. not in the cell update) and in the per-time step statistics.': 0.9949138760566711, 'Adding batch normalization to LSTMs incurs additional computational cost and bookkeeping; for training speed comparisons (e.g. Figure 2) the paper only compares LSTM and BN-LSTM by iteration count; given the additional complexity of the BN-LSTM I would have also liked to see a wall-clock comparison.': 0.743180513381958, 'As RNNs are used across many tasks, this work is of interest to many.': 1.0983154773712158, 'However, the results gains are generally minor and require several tricks to work in practice.': 0.41557660698890686, 'Also, this work doesn’t address a question about batch normalization that it seems natural that it helps with faster training, but why would it also improve generalization?': 0.8399534225463867, 'Clarity': 1.0986123085021973, 'The paper is overall very clear and well-motivated.': 1.0986123085021973, 'The model is well described and easy to understand, and the plots illustrate the points clearly.': 1.0986123085021973, 'Summary': 1.0986123085021973, 'Interesting though relatively incremental adaptation, but shows batch normalization to work for RNNs where previous works have not succeeded.': 1.0986123085021973, 'Comprehensive set of experiments though it is questionable if the empirical gains are significant enough to justify the increased model complexity as well as computational overhead.': 1.0986123085021973, 'Pros': 1.0986123085021973, 'Shows batch normalization to work for RNNs where previous works have not succeeded': 1.0986123085021973, 'Good empirical analysis of hyper-parameter choices and of the activations': 1.0986123085021973, 'Experiments on multiple tasks': 1.0986123085021973, 'Cons': 1.0986123085021973, 'Relatively incremental': 1.0986123085021973, 'Several ‘hacks’ for the method (per-time step statistics, adding noise for exploding variance, sequence-wise normalization)': 1.0986123085021973, 'No mention of computational overhead': 1.0986123085021973, 'Only character or pixel-level tasks, what about word-level?': 1.0986123085021973}"
386,https://openreview.net/forum?id=r1WUqIceg,"{'The paper introduced an extension of Adam optimizer that automatically adjust learning rate by comparing the subsequent values of the cost function during training.': 1.0986075401306152, 'The authors empirically demonstrated the benefit of the Eve optimizer on CIFAR convnets, logistic regression and RNN problems.': 1.0985782146453857, 'I have the following concerns about the paper': 0.5652379989624023, 'The proposed method is VARIANT to arbitrary shifts and scaling to the cost function.': 1.0980191230773926, 'A more fair comparison with other baseline methods would be using additional exponential decay learning scheduling between the lower and upper threshold of d_t.': 1.0986123085021973, 'I suspect 1/d_t': 1.0986123085021973, 'just shrinks as an exponential decay from Figure 2.': 1.0986121892929077, 'Three additional hyper-parameters: k, K, \\beta_3.': 1.0981541872024536, 'Overall, I think the method has its fundamental flew and the paper offers very limited novelty.': 1.0986117124557495, 'There is no theoretical justification on the modification, and it would be good for the authors to discuss the potential failure mode of the proposed method.': 1.0986123085021973, 'Furthermore, it is hard for me to follow Section 3.2.': 1.098431944847107, 'The writing quality and clarity of the method section can be further improved.': 1.0986123085021973, 'The paper demonstrates a semi-automatic learning rate schedule for the Adam optimizer, called Eve.': 1.0986123085021973, 'Originality is somehow limited but the method appears to have a positive effect on neural network training.': 1.0986123085021973, 'The paper is well written and illustrations are appropriate.': 1.0986117124557495, 'Pros:': 1.0986056327819824, 'probably a more sophisticated scheduling technique than a simple decay term': 0.8360121250152588, 'reasonable results on the CIFAR dataset (although with comparably small neural network)': 1.0982544422149658, 'Cons:': 1.082027554512024, 'effect of momentum term would be of interest': 1.0986123085021973, ""the Adam reference doesn't point to the conference publications but only to arxiv"": 1.0960803031921387, 'comparison to Adam not entirely conclusive': 1.0986123085021973, 'As you noted for Figure 5 Left, sometimes it seems sufficient to tune learning rates.': 1.09861159324646, 'I see your argument for Figure 6 Right,': 1.0986123085021973, 'but': 1.0968916416168213, '1) not for all good learning rates make Adam fail, I guess you selected the one where it did (note that Adam was several times faster than Eve in the beginning)': 1.0186711549758911, '2) I don\'t buy ""Eve always converges"" because you show it only for 0.1 and since Eve is not Adam, 0.1 of Adam is not 0.1 of Eve because of d_t.': 1.097348928451538, 'To my understanding, you define d_t over time with 3 hyperparameters.': 1.0986109972000122, 'Similarly, one can define d_t directly.': 1.098580241203308, 'The behaviour of d_t that you show is not extraordinary and can be parameterized.': 1.0986106395721436, 'If Eve is better than Adam, then looking at d_t we can directly see whether we underestimated or overestimated learning rates.': 1.098572850227356, 'You could argue that Eve does it automatically but you do tune learning rates for each problem individually anyway.': 1.0986123085021973}"
387,https://openreview.net/forum?id=r1X3g2_xl,"{'The authors propose to apply virtual adversarial training to semi-supervised classification.': 1.0975435972213745, 'It is quite hard to assess the novelty on the algorithmic side at this stage: there is a huge available literature on semi-supervised learning (especially SVM-related literature, but some work were applied to neural networks too); unfortunately the authors do not mention it, nor relate their approach to it, and stick to the adversarial world.': 1.0843952894210815, 'In terms of novelty on the adversarial side, the authors propose to add perturbations at the level of words embeddings, rather than the input itself (having in mind applications to NLP).': 1.0986123085021973, 'Concerning the experimental section, authors focus on text classification methods.': 1.0986069440841675, 'Again, comparison with the existing SVM-related literature is important to assess the viability of the proposed approach; for example (Wang et al, 2012) report 8.8% on IMBD with a very simple linear SVM (without transductive setup).': 1.0986121892929077, 'Overall, the paper reads well and propose a semi-supervised learning algorithm which is shown to work in practice.': 1.0986082553863525, 'Theoretical and experimental comparison with past work is missing.': 1.095786690711975, 'This paper applies the idea of the adversarial training and virtual adversarial training to the LSTM-based model in the text context.': 1.0985695123672485, 'The paper is in general well written and easy to follow.': 0.6917915940284729, 'Extending the idea of the adversarial training to the text tasks is simple but non-trivial.': 1.0984786748886108, 'Overall the paper is worth to publish.': 1.0986123085021973, 'I only have a minor comment: it is also interesting to see how much adversarial training can help in the performance of RNN, which is a simpler model and may be easier to analyze.': 1.0985347032546997, '*** Paper Summary ***': 1.0400313138961792, 'This paper applies adversarial and virtual adversarial training to LSTM for text classification.': 1.0986123085021973, 'Since text inputs are discrete adversarial perturbation are applied to the (normalized) word embeddings.': 1.0986123085021973, 'Extensive experiments are reported and demonstrate the advantage of these methods.': 1.0986123085021973, '*** Review Summary ***': 1.0986123085021973, 'The paper reads well and has sufficent references.': 1.0986123085021973, 'The application of adversarial training to text data is a simple but not trivial extension.': 1.0986123085021973, 'The experimental section presents extensive experiments with comparison to alternative strategies.': 1.0986123085021973, 'The proposed method is simple and effective and can be easily be applied after reading the paper.': 1.0986123085021973, '*** Detailed Review ***': 1.0986123085021973, 'The paper reads well.': 1.0986123085021973, 'I have only a few comments regarding experiments and link to prior resarch:': 1.0986123085021973, 'Experiments:': 1.0986123085021973, 'In Table 2 (and for other datasets as well), could you include an SVM baseline?': 1.0986123085021973, 'e.g. S Wang and C Manning 2012?': 0.4170311689376831, 'As another baseline, did you consider dropping words, i.e. masking noise?': 1.0986123085021973, 'It is generally better than dropout/gaussian noise for text application (e.g. denoising autoencoders)?': 1.0267983675003052, 'I am not sure I understand why virtual adversarial is worse than the baseline in Table 5.': 1.0986121892929077, 'If you tune epsilon, in the worse case you would get the same performance as the baseline?': 1.0986123085021973, 'Was it that validation was unreliable?': 1.0986024141311646, 'Related Work:': 1.0986123085021973, 'I think it would be interesting to point at SVM, transductive SVM who achieve something similar to adversarial training.': 1.0986121892929077, 'When maximizing the margin in a (transductive) SVM, it is equivalent to move the example toward the decision boundary, i.e. moving them in the direction of increase of the loss gradient.': 1.098209023475647, 'Also it would be interesting to draw a parallel between adversarial training and contrastive divergence.': 1.0467969179153442, 'The adversarial samples are very close in nature to the one step Markov Chain samples from CD.': 1.0962519645690918, 'See Bengio 2009.': 1.0312241315841675, 'Related to this technique are also approaches that try to explicitely cancel the Jacobian at data points, e.g. Rifai et al 2011.': 1.0986119508743286, '*** References ***': 1.0984580516815186, 'Marginalized Denoising Autoencoders for Domain Adaptation.': 0.9261959791183472, 'Minmin Chen, K Weinberger.': 1.0533815622329712, 'Stacked Denoising Autoencoders.': 1.0611927509307861, 'Pascal Vincent.': 1.08711838722229, 'JMLR 2011.': 1.098154902458191, 'Learning invariant features through local space contraction, Salah Rifai, Xavier Muller, Xavier Glorot, Gregoire Mesnil, Yoshua Bengio and Pascal Vincent, 2011.': 1.0986123085021973, 'Learning Deep Architectures for AI, Yoshua Bengio 2009': 1.0986123085021973, 'Large Scale Transductive SVMs.': 1.0986123085021973, 'Ronan Collobert et al 2006': 1.0986123085021973, 'Optimization for Transductive SVM.': 1.0986123085021973, 'O Chapelle, V Sindhwani, SS Keerthi JMLR 2008': 1.0986123085021973}"
388,https://openreview.net/forum?id=r1YNw6sxg,"{'The paper proposes a novel approach for learning visual servoing based on Q-iteration.': 1.0985400676727295, 'The main contributions of the paper are:': 1.0986123085021973, '1. Bilinear dynamics model for predicting next frame (features) based on action and current frame': 1.0986121892929077, '2. Formulation of servoing with a Q-function that learns weights for different feature channels': 1.064313530921936, '3. An elegant method for optimizing the Bellman error to learn the Q-function': 0.42176830768585205, 'Pros:': 1.0984573364257812, '+': 0.8129371404647827, 'The paper does a good job of exploring different ways to connect the action (u_t) and frame representation (y_t) to predict next frame features (y_{t+1}).': 1.0986123085021973, 'They argue in favour of a locally connected bilinear model which strikes the balance between computation and expressive ability.': 1.0986123085021973, 'Cons:': 1.0975570678710938, 'While, sec. 4 makes good arguments for different choices, I would have liked to see more experimental results comparing the 3 approaches: fully connected, convolutional and locally connected dynamics.': 1.0986123085021973, 'The idea of weighting different channels to capture the importance of obejcts in different channels seems more effective than treating errors across all channels equally.': 1.0986123085021973, 'This is also validated experimentally, where unweighted performance suffers consistently.': 1.0986123085021973, '+ Solving the Bellman error is a difficult problem in Q-learning approaches.': 1.0986069440841675, 'The current paper presents a solid optimization scheme based on the key-observation that scaling Q-function parameters does not affect the best policy chosen.': 1.0986123085021973, 'This enables a more elegant FQI approach as opposed to typical optimization schemes which (c_t + \\gamma min_u Q_{t+1}) fixed.': 0.8681582808494568, 'However, I would have liked to see the difference between FQI and such an iterative approach which holds the second term in Eq. 5 fixed.': 0.776824414730072, 'Experimental results:': 1.0986123085021973, 'Overall, I find the experimental results unsatisfying given the small scale and toy simulations.': 0.9846839904785156, 'However, the lack of benchmarks in this domain needs to be recognized.': 1.0985980033874512, 'Also, as pointed out in pre-review section, the idea of modifying the VGG needs to be experimentally validated.': 0.8453686833381653, 'In its current form, it is not clear whether the modified VGG would perform better than the original version.': 1.0975008010864258, 'Overall, the contribution of the paper is solid in terms of technical novelty and problem formulations.': 1.0986123085021973, 'However, the paper could use stronger experiments as suggested to earlier to bolster its claims.': 1.0985931158065796, 'This paper investigates the benefits of visual servoing using a learned': 1.0986123085021973, 'visual representation.': 1.0986123085021973, 'The authors  propose to first learn an action-conditional': 1.0986123085021973, 'bilinear model of the visual features (obtained from a pre-trained VGG net) from': 1.0986123085021973, 'which a policy can be derived using a linearization of the dynamics.': 1.0986123085021973, 'A multi-scale,': 1.0986123085021973, 'multi-channel and locally-connected variant of the bilinear model is presented.': 1.0985697507858276, 'Since the bilinear model only predicts the dynamics one step ahead, the paper': 0.613537609577179, 'proposes a weighted objective which incorporates the long-term values of the': 1.0986123085021973, 'current policy.': 0.4843606948852539, 'The evaluation problem is addressed using a fitted-value approach.': 0.37065941095352173, 'The paper is well written, mathematically solid, and conceptually exhaustive.': 0.909100353717804, 'The experiments also demonstrate the benefits of using a value-weighted objective': 1.0046650171279907, 'and is an important contribution of this paper.': 0.9005210399627686, 'This paper also seems to be the': 0.4288994073867798, 'first to outline a trust-region fitted-q iteration algorithm.': 0.4059802293777466, 'The use of': 1.0981676578521729, 'pre-trained visual features is also shown to help, empirically, for generalization.': 1.0722932815551758, 'Overall, I recommend this paper as it would benefit many researchers in robotics.': 0.4120228886604309, 'However, in the context of this conference, I find the contribution specifically on': 1.009811282157898, 'the ""representation"" problem to be limited.': 1.0986123085021973, 'It shows that a pre-trained VGG': 1.0719462633132935, 'representation is useful, but does not consider learning it end-to-end.': 0.9527977705001831, 'This is not': 0.6210013628005981, 'to say that it should be end-to-end, but proportionally speaking, the paper': 1.0986121892929077, 'spends more time on the control problem than the representation learning one.': 1.060247540473938, 'Also, the policy representation is fixed and the values are approximated': 0.49567973613739014, 'in linear form using problem-specific features.': 1.0986123085021973, ""This doesn't make the paper"": 0.9822550415992737, 'less valuable, but perhaps less aligned with what I think ICLR should be about.': 0.834427535533905, '1) Summary': 1.0986123085021973, 'This paper proposes to tackle visual servoing (specifically target following) using spatial feature maps from convolutional networks pre-trained on general image classification tasks.': 1.0986123085021973, 'The authors combine bilinear models of one-step dynamics of visual feature maps at multiple scales with a reinforcement learning algorithm to learn a servoing policy.': 1.0986123085021973, 'This policy is learned by minimizing a regularized weighted average of distances to features predicted by the aforementioned model of visual dynamics.': 1.0986119508743286, '2) Contributions': 1.0986123085021973, '+ Controlled experiments in simulation quantifying the usefulness of pre-trained deep features for visual servoing.': 1.0986123085021973, '+ Clear performance benefits with respect to many sensible baselines, including ones using ground truth bounding boxes.': 1.0984687805175781, '+ Principled learning of multi-scale visual feature weights with an efficient trust-region fitted Q-iteration algorithm to handle the problem of distractors.': 0.9716740846633911, '+ Good sample efficiency thanks to the choice of Q-function approximator and the model-based one-step visual feature dynamics.': 1.0986123085021973, '+ Open source virtual city environment to benchmark visual servoing.': 1.0986123085021973, '3) Suggestions for improvement': 0.5876571536064148, 'More complex benchmark:': 1.098601222038269, 'Although the environment is not just a toy synthetic one, the experiments would benefit greatly from more complex visual conditions (clutter, distractors, appearance and motion variety, environment richness and diversity, etc).': 1.098599910736084, 'At least, the realism and diversity of object appearances could be vastly improved by using a larger number of 3D car models, including more realistic and diverse ones that can be obtained from Google SketchUp for instance, and populating the environment with more distractor cars (in traffic or parked).': 1.0986123085021973, 'This is important as the main desired quality of the approach is robustness to visual variations.': 0.9717828035354614, 'End-to-end and representation learning:': 0.6964008212089539, 'Although the improvements are already significant in the current synthetic experiments, it would be interesting to measure the impact of end-to-end training (i.e. also fine-tuning the convnet), as it is possibly needed for better generalization in more challenging visual conditions.': 1.0986121892929077, 'It would also allow to measure the benefit of deep representation learning for visual servoing, which would be relevant to ICLR (there is no representation learning so far, although the method can be straightforwardly adapted as the authors mention briefly).': 1.0955655574798584, 'Reproducibility:': 1.0986123085021973, 'The formalism and algorithms are clearly explained, but there is a slightly overwhelming mass of practical tricks and implementation details described with varying levels of details throughout the paper and appendix.': 1.094481348991394, 'Grouping, simplifying, or reorganizing the exposition of these implementation details would help, but a better way would probably consist in only summarizing the most important ones in section and link to an open source implementation of the method for completeness.': 1.084149718284607, 'Typos:': 1.098341464996338, 'p.2: ""learning is a relative[ly] recent addition""': 1.058170199394226, 'p.2: ""be applied [to] directly learn""': 1.0882786512374878, '4) Conclusion': 0.5220202207565308, 'In spite of the aforementioned limits of the experiments, this paper is interesting and solid, in part thanks to the excellent reply to the pre-review questions and the subsequent improved revision.': 1.0986123085021973, 'This leads me to believe the authors are more than capable of following to a significant extent the aforementioned suggestions for improvement, thus leading to an even better paper.': 1.0986123085021973}"
389,https://openreview.net/forum?id=r1aGWUqgg,"{'This paper is about learning unsupervised state representations using multi-task reinforcement learning.': 1.0986123085021973, 'The authors propose a novel approach combining gated neural networks with multitask learning with robotics priors.': 1.0986123085021973, 'They evaluated their approach on two simulated datasets and showed promising results.': 1.0986123085021973, 'The paper is clearly written and is theoretically sound.': 1.0986123085021973, 'Positives:': 1.0986123085021973, '+': 1.0986123085021973, 'Gating to enable learning a joint representation': 1.0986123085021973, '+ Multi-task learning extended from a single task in prior work': 1.0986123085021973, '+ Combining multiple types of losses to learn a strong representation (Coherence, Proportionality, Causality, Repeatability, Consistency and Separation)': 1.0986123085021973, 'Negatives:': 1.0986123085021973, 'Parameters choice is arbitrary (w parameters)': 1.0986123085021973, 'Limiting the multi-task learning to be different to individual tasks rather than sharing and transferring knowledge between tasks': 1.0986123085021973, 'The experiments could have been conducted using a standardized simulation tool such as OpenAI Gym to make it easy to compare.': 1.0986123085021973, 'I would recommend that the authors consider a more standardized way of picking the model parameters and evaluate on a more standard and high-dimensional datasets.': 1.0986123085021973, 'This paper builds upon the method of Jonschkowski & Brock to learn state representations for multiple tasks, rather than a single task.': 1.0986123085021973, 'The research direction of learning representations for multiple tasks is an interesting one, and largely unexplored.': 1.0986123085021973, 'The approach in the paper is to learn a different representation for each task, and a different policy for each task, where the task is detected automatically and built into the neural network.': 1.0695250034332275, 'The authors state that the proposed method is orthogonal to multi-task learning, though the end goal of learning to solve multiple tasks is the same.': 1.0738550424575806, 'It would be interesting and helpful to see more discussion on this point in the paper, as discussed in the pre-review question phase.': 0.4326128363609314, 'References to other multi-task learning works, e.g. policy distillation and actor-mimic (both ICLR ’16), may be appropriate as well.': 0.8896085619926453, 'The method proposes to jointly learn a task classifier with a state representation learner, by using a differentiable gating mechanism to control the flow of information.': 0.5561445355415344, 'The paper proposes a task coherence prior for this gating mechanism to ensure that the learned task classifier is temporally coherent.': 0.8222423791885376, 'Introducing this structure is what enables the method to improve performance over the standard, non-multitask approach.': 0.8486583828926086, 'The evaluation involves two toy experimental scenarios.': 0.4086557924747467, 'The first involves controlling one of two cars to drive around a track.': 1.0981370210647583, 'In this task, detecting the “task” is very easy, and the learned state representation is linear in the observation.': 0.3686217963695526, 'The paper evaluates the performance of the policies learned with the proposed approach, and shows sufficient comparisons to demonstrate the usefulness of the approach over a standard non-multitask set-up.': 1.0971318483352661, 'In the second navigation scenario, only the state representation is qualitatively shown, not the resulting control policy nor any other learned state representations for comparison.': 1.098409652709961, 'Since the multi-task state representation learning approach is only useful if you can also learn control better, the paper should also evaluate on control, with the same comparisons as in the first experiment.': 1.0985584259033203, 'Without this evaluation, the experiment is incomplete.': 1.0657727718353271, 'Lastly, to be on par with publications at a venue like ICLR, the method should be evaluated more thoroughly, on a wider range of set-ups, to demonstrate the generality of the approach and show that the method applies to more complex tasks.': 0.4306817054748535, 'While in theory, the method should scale, the experiments do not demonstrate that it can handle more realistic scenarios, such as scaling beyond MNIST-level images, to 3D or real images, or higher-dimensional control tasks.': 0.7300023436546326, 'Evaluating the method in this more complex scenario is important, because unexpected issues can come up when trying to scale.': 1.0986120700836182, 'If scaling-up is straight-forward, then running this experiment (and including it in the paper) should be straight-forward.': 0.636446475982666, 'In summary, here are the pros and cons of this paper:': 1.0252336263656616, 'Cons': 1.0986123085021973, 'The approach does not necessarily share information across tasks for better learning, and requires learning a different policy for each task': 0.6668695211410522, 'Only one experimental set-up that evaluates learned policy with multi-task state representation': 1.093828558921814, 'No experiments on more realistic scenarios, such 3D environments or high-dimensional control problems': 1.0466865301132202, 'Pros:': 1.0986123085021973, 'This approach enables using the same network for multiple tasks, which is often not true for transfer and multi-task learning approaches': 0.9723296165466309, 'Novel way to learn a single policy for multiple tasks, including a task coherence prior which ensures that the task classification is meaningful': 1.0983242988586426, 'Experimentally validated on two toy tasks.': 1.0986123085021973, 'One task shows improvement over baseline approaches': 1.098561406135559, 'Thus, my rating would be higher if the paper included an evaluation of the control policy for navigation and included another more challenging and compelling scenario.': 1.0950757265090942, 'Lastly, here are some minor comments/questions on how I think the paper could be improved, but are not as important as the above:': 1.0985872745513916, 'Approach:': 1.0986123085021973, 'Could this approach be combined with other state representation learning approaches?': 1.0986123085021973, 'e.g. approaches that use an autoencoder.': 1.0986123085021973, 'Experiments:': 1.0986123085021973, 'One additional useful comparison would be to evaluate performance in the single-task setting (e.g. only controlling the red car), as an upper bound on how well the policy should be able to perform.': 1.0986123085021973, 'Does the learned multi-task policy reach the same level of performance?': 1.0986123085021973, 'This upper bound will be tighter than the “known car position” baseline (which is also useful in its own right).': 1.0986123085021973, 'Does the “observations” baseline eventually reach the performance of the LRP approach?': 1.0986123085021973, 'It would be useful to know if this approach simply speeds up learning (significantly) or if it enables better performance.': 1.0986123085021973, 'If there are aliasing issues with the images, why not just use higher resolution images?': 1.0986123085021973, 'The paper presents a method to learn a low-dimensional state representations from raw obervation for multi-task setting.': 1.09842848777771, 'In contrast to classic multi-task learning setting where a joint representation is usually learned by exploring the transferable information among different tasks, the method aims to identify individual task and solve them separately.': 1.0986123085021973, 'To this end, the authors extend the learning with robotic priors approach by extending the loss function with additional term for task coherence, i.e., a task only changes representation between training episodes.': 1.0986123085021973, 'The method has been evaluated on two tasks, multi-task slot-car racing and mobile navigation to prove its efficacy.': 1.0986123085021973, 'there were several unclear issues:': 0.3612041473388672, ""1. The first question is that if the method is only appealing on the scenario like the slot-car racing, otherwise it should be benchmarked with mutli-task learning. While the author made the argument in the related work, the proposed method is orthogonal to multi-task learning they did admit both explore shared knowledge between tasks. What's the advantage and disadvantage for the proposed method for general mutiple task setting, in particular over the multi-task learning?"": 0.9790138006210327, 'The reply of the authors was not fully satisfactory.': 1.0986123085021973, 'The argument did not support the lack of comparison to multi-task joint-learning.': 1.0986121892929077, ""It seems they don't plan to include any comparison neither."": 1.0986123085021973, ""I think it's important for the fundamental motivation for the work, without such comparison, the method seems to be purely an alternative to multi-task joint-learning without any(or much) practical advantage."": 1.096380352973938, '2.Following up to the previous question, please clarify the results on the mobile navigation scenario.': 1.0986123085021973, ""It's not clear how the plot on the right indicates MT-LRP identifies all tasks as the author claimed and and seems very weak to support the method, in particular compared to the multi-slot car-racing driving experiment, there is too little results to make sound argument (almost no comparison to alternative methods, i.e. no baseline method, is that true for the problem)."": 1.0727360248565674, 'The explanation of the authors did provide more details and more explicit information.': 1.0986123085021973, '3. The proposed gated neural network architecture seems to be a soft gated structure(correct me if I am wrong), a possible baseline would be a hard gated unit, how would this affect the conclusion. This is particularly interesting as the authors reflect on the constraint that the representation should stay consistent during the training.': 1.0986123085021973, ""The author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it's probably less an issue compared to Question 1."": 1.0986123085021973, 'In summary, while there are remaining concerns about lacking comparisons, the is a weak tendency towards accepting the submission.': 1.0986123085021973}"
390,https://openreview.net/forum?id=r1aPbsFle,"{'This work offers a theoretical justification for reusing the input word embedding in the output projection layer.': 1.0984946489334106, 'It does by proposing an additional loss that is designed to minimize the distance between the predictive distribution and an estimate of the true data distribution.': 1.0986123085021973, 'This is a nice setup since it can effectively smooth over the labels given as input.': 1.0986123085021973, 'However, the construction of the estimate of the true data distribution seems engineered to provide the weight tying justification in Eqs. 3.6 and 3.7.': 1.0986123085021973, ""It is not obvious why the projection matrix L in Eq 3.6 (let's rename it to L') should be the same as that in Eq. 2.1."": 1.0986117124557495, ""For example, L' could be obtained through word2vec embeddings trained on a large dataset or it could be learned as an additional set of parameters."": 1.0986120700836182, ""In the case that L' is a new learned matrix, it seems the result in Eq 4.5 is to use an independent matrix for the output projection layer, as is usually done."": 1.0986123085021973, 'The experimental results are good and provide support for the approximate derivation done in section 4, particularly the distance plots in figure 1.': 1.0984787940979004, 'Minor comments:': 0.990950882434845, 'Third line in abstract: where model -> where the model': 1.0986123085021973, 'Second line in section 7: into space -> into the space': 1.0986123085021973, ""Shouldn't the RHS in Eq 3.5 be \\sum \\tilde{y_{t,i}}(\\frac{\\hat{y}_t}{\\tilde{y_{t,i}}} - e_i) ?"": 0.8955247402191162, 'This paper provides a theoretical framework for tying parameters between input word embeddings and output word representations in the softmax.': 1.098610758781433, 'Experiments on PTB shows significant improvement.': 1.0974628925323486, 'The idea of sharing or tying weights between input and output word embeddings is not new (as noted by others in this thread), which I see as the main negative side of the paper.': 1.0986123085021973, 'The proposed justification appears new to me though, and certainly interesting.': 1.0974684953689575, 'I was concerned that results are only given on one dataset, PTB, which is now kind of old in that literature.': 1.0986123085021973, ""I'm glad the authors tried at least one more dataset, and I think it would be nice to find a way to include these results in the paper if accepted."": 1.0986121892929077, 'Have you considered using character or sub-word units in that context?': 1.0986123085021973, 'This paper gives a theoretical motivation for tieing the word embedding and output projection matrices in RNN LMs.': 1.0978074073791504, 'The argument uses an augmented loss function which spreads the output probability mass among words with close word-embedding.': 1.0986123085021973, 'I see two main drawbacks from this framework:': 1.0986123085021973, 'The augmented loss function has no trainable parameters and is used for only for regularization.': 1.0986123085021973, 'This is not expected to give gains with large enough datasets.': 1.0986123085021973, 'The augmented loss is heavily “engineered” to produce the desired result of parameter tying.': 1.0986123085021973, 'It’s not clear what happens if you try to relax it a bit, by adding parameters, or estimating y~ in a different way.': 1.0986121892929077, 'Nevertheless the argument is very interesting, and clearly written.': 1.0980902910232544, 'The simulated results indeed validate the argument, and the PTB results seem promising.': 1.0985785722732544, 'Section 3:': 1.0984249114990234, 'Can you clarify if y~ is conditioned on the t example or on the entire history.': 1.0986123085021973, 'Eq. 3.5: i is enumerated over V (not |V|)': 0.8718162178993225}"
391,https://openreview.net/forum?id=r1br_2Kge,"{'Summary: This paper presents a novel sketching method in solving sparse polynomial function of a sparse binary vector within a single-layer neural network.': 1.0986123085021973, 'Pros:1.': 1.0986123085021973, 'The paper is written clearly.': 1.0986123085021973, '2. I like the idea of using improper learning to reduce the dimensionality.': 1.0986123085021973, ""3. It's novel to use neural network to do sketching decoding."": 1.0986123085021973, 'Cons:1.': 1.0986123085021973, 'The experiments part is kind of weak in comparison with other sketching/projection methods.': 1.0986123085021973, '2. Any theoretical analysis to show why this approach needs fewer paramerters than feature hash, etc. My other concern is that since t and m are tunable, how to set these parameter for different problems?': 1.0986123085021973, ""3. The authors claim they have faster training. However it's not shown in experiments."": 1.0986123085021973, 'Comments: It may be interesting to compare with sparse feed-forward neural networks.': 1.0986123085021973, 'Summary of the paper:': 1.0986123085021973, 'The paper introduces sketches that approximates linear and sparse polynomials on binary data.': 1.0986123085021973, 'The paper shows that such sketches can be represented as a one layer neural network.': 1.0986123085021973, 'Experiments are conducted on some language processing tasks.': 1.0986123085021973, 'Novelty:': 1.0986123085021973, 'Approximation for linear functions and polynomial have been studied in previous work, hashing here is learned as a neural network which is novel.': 1.0986123085021973, 'Although I have some reserves on the connexion between the theory presented and the learning part in the paper (Please see comments below).': 1.0986123085021973, 'The main concern is that the  single layer neural network that is  learned  is not restricted to the class of  hash functions studied, hence it is hard to know if we are really testing the hashing class or just a one layer neural network.': 1.0986123085021973, 'Clarity:': 1.0986123085021973, 'The paper is  written in a clear way.': 1.0986123085021973, 'Comments :': 1.0986123085021973, ""The link between neural network and the hashing stems from the implementation of the 'and operation' with a relu and a binary weight V and a bias 1-t. V encodes the support of the linear weight for instance in the linear model."": 1.0986123085021973, 'When Learning V it is regularized with l_1 norm to encourage sparsity, nevertheless the weights are not restricted to be positive to be faithful to the theory introduced in the paper.': 1.0986123085021973, 'When learning V by back propagation when can try projected gradient to restrict V values to be in [0,1].': 1.0986123085021973, 'On a synthetic example where the support of w is known it would be interesting to see if V recovers the support of w.': 1.0986123085021973, 'Similarly  for polynomial seems group sparsity is a good regularizer, difficulty being here that the groups are unknown.': 1.0986123085021973, 'Many works have tackled that issue see for instance fbach/IEEE TSP shervashidze bach.pdf"" target=""_blank""': 1.0986123085021973, 'rel=""nofollow"">http://www.di.ens.fr/ fbach/IEEE TSP shervashidze bach.pdf': 1.0986123085021973, 'The experiments are limited to small sets.': 1.0986123085021973, 'Sketching is a powerful tool that has found application in a number of classical machine learning methods like least squares, kernel learning and tensor methods.': 1.0986123085021973, 'It is very exciting that such application can be extended to deep neural networks, which represent the state-of-the-art for numerous learning tasks.': 1.0986123085021973, 'However, I have doubts on some aspects of the experimental and analytical settings adopted in this paper, which (in my opinion) might limit its impact in both theoretical understanding and practical applicability.': 1.0986123085021973, '1. On the theoretical side, I found the sparse linear/polynomial classifiers assumption very troublesome. If the classifier that one wishes to learn is indeed sparse linear or sparse polynomial, why should neural networks be a good option compared to, say, Lasso or sparse logistic regression models, which are theoretically sound and even optimal for the particular function class to be learnt?': 1.091641902923584, 'I think a more relevant setting is where one wants to learn a feedforward neural network, with perhaps sparse weights on each of its layers.': 1.0985373258590698, 'It is an interesting question of whether dimensionality reduction preserves the richness of such sparse weight networks, and how aggressively one can reduce the dimension into (i.e., t in the paper), which leads to much smaller parameter space.': 1.0986123085021973, 'Also, from a learning theoretical perspective, the existence of a small neural network that approximates a sparse linear/polynomial classifier does not mean such a network can be effectively learnt.': 1.0984336137771606, 'It would be good that some VC dimension /generalization aspects are analyzed as well.': 1.0986123085021973, 'For example, intuitively network that operates on sketched inputs should have smaller model complexity.': 1.0986098051071167, 'Does it lead to improved generalization, and how such improvement should be traded off for the richness of the network (which desires *larger* sketch length)?': 1.0985939502716064, '2. On the practical side, it seems the most relevant application would be in NLP where inputs are naturally sparse high-dimensional vectors. Reducing the input dimension indeed reduces the size of the model and at least save storage cost. However, in practice the high dimensionality is typically dealt with using a LookupTable structure (encoding), which is not difficult to train at all, because the gradient for each data point only involves one entry in the LookupTable. I think the experiments should include comparison with such traditional approaches and comment on both accuracy and training/testing time. Also, if the argument is that the vocabulary size is so large that LookupTable cannot be stored in GPU memory, such an example should be demonstrated and show how sketching helps.': 1.0983799695968628}"
392,https://openreview.net/forum?id=r1fYuytex,"{'Experimental results look reasonable, validated on 3 tasks.': 1.0986123085021973, 'References could be improved, for example I would rather see': 1.0986050367355347, ""Rumelhart's paper cited for back-propagation than the Deep Learning book."": 1.0976924896240234, 'The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks.': 1.0986119508743286, 'The paper removes some of the connections in the fully connected layers and shows performance and computational efficiency increase in networks on three different datasets.': 1.0986123085021973, 'It is also a good addition that the authors combine their method with binary and ternary connect studies and show further improvements.': 1.0986123085021973, 'The paper was hard for me to understand because of this misleading statement: In this paper, we propose sparsely-connected networks by reducing the number of connections of fully-connected networks using linear-feedback shift registers (LFSRs).': 1.0986123085021973, 'It led me to think that LFSRs reduced the connections by keeping some of the information in the registers.': 1.0985198020935059, 'However, LFSR is only used as a random binary generator.': 1.098551869392395, 'Any random generator could be used but LFSR is chosen for the convenience in VLSI implementation.': 1.098576545715332, 'This explanation would be clearer to me: In this paper, we propose sparsely-connected networks by randomly removing some of the connections in fully-connected networks.': 1.0986120700836182, 'Random connection masks are generated by LFSR, which is also used in the VLSI implementation to disable the connections.': 1.0753895044326782, 'Algorithm 1 is basically training a network with back-propogation where each layer has a binary mask that disables some of the connections.': 1.0986123085021973, 'This explanation can be added to the text.': 1.098604440689087, 'Using random connections is not a new idea in CNNs.': 1.0872188806533813, 'It was used between CNN layers in a 1998 paper by Yann LeCun and others: http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf It was not used in fully connected layer before.': 0.6655720472335815, 'The sparsity in fully connected layer decreases the computational burden but it is difficult to speed up.': 1.0936208963394165, ""Also the author's VLSI implementation does not speed up the network inference."": 1.0544801950454712, 'How are the results of this work compared to Network in Network (NiN)?': 1.0986123085021973, 'https://arxiv.org/abs/1312.4400': 1.0986123085021973, 'In NiN, the authors removed the fully connected layers completely and used a cheap pooling operation and also got improved performance.': 1.0986123085021973, 'Are the results presented here better?': 1.0986123085021973, 'It would be more convincing to see this method tested on ImageNet, which actually uses a big fully connected layer.': 1.0986123085021973, 'Increased my rating from 5-6 after rebuttal.': 1.0986123085021973, 'From my original comments:': 1.0986123085021973, 'The results looks good but the baselines proposed are quite bad.': 1.0986123085021973, 'For instance in the table 2 ""Misclassification rate for a 784-1024-1024-1024-10 "" the result for the FC with floating point is 1.33%.': 1.0986123085021973, 'Well far from what we can obtain from this topology, near to 0.8%.': 1.0986123085021973, 'I would like to see ""significant"" compression levels on state of the art results or good baselines.': 1.0986123085021973, 'I can get 0,6% with two FC hidden layers...': 1.0986123085021973, 'In CIFAR-10 experiments, i do not understand  why ""Sparsely-Connected 90% + Single-Precision Floating-Point"" is worse than ""Sparsely-Connected 90% + BinaryConnect"".': 1.0986123085021973, 'So it is better to use binary than float.': 1.0986123085021973, 'Again i think that in the experiments the authors are not using all the techniques that can be easily applied to float but not to binary (gaussian noise or other regularizations).': 1.0986123085021973, 'Therefore under my point of view the comparison between float and binary is not fair.': 1.0986123085021973, 'This is a critic also for the original papers about binary and ternary precision.': 1.0986123085021973, 'In fact with this convolutional network, floating (standard) precision we can get lower that 9% of error rate.': 1.0986123085021973, 'Again bad baselines.': 1.0986123085021973, 'The authors reply still does not convince me.': 1.0986123085021973, 'I still think that the same technique should be applied on more challenging scenarios.': 1.0986123085021973}"
393,https://openreview.net/forum?id=r1kGbydxg,"{'The paper is straightforward, easy to read, and has clear results.': 1.0986123085021973, ""Since all these parameterisations end up outputting torques, it seems like there shouldn't be much difference between them."": 1.0986123085021973, 'There is a known function that convert from one representation to another (or at least to torques).': 1.0986123085021973, 'Is it not possible that the only reason proportional control is a little better is that the tracking cost is a function of positions?': 1.0986123085021973, 'Would we get the same result if there was no reference-pose cost, only a locomotion cost?': 1.0986123085021973, 'Would we get the same result if the task was to spin a top?': 1.0986123085021973, 'My guess is no.': 1.0986123085021973, 'This work is interesting, but not likely to generalise to other scenarios, and in that sense is rather limited.': 1.0986123085021973, 'The video is nice.': 1.0986123085021973, 'Paper studies deep reinforcement learning paradigm for controlling high dimensional characters.': 1.0986123085021973, 'Experiments compare the effect different control parameterizations (torques, muscle-activations, PD control with target joint positions and target joint velocities) have on the performance of reinforcement learning and optimized control policies.': 1.0986120700836182, 'Evaluated are different planer gate cycle trajectories.': 1.0986123085021973, 'It is illustrated that more abstract parameterizations are in fact better and result in more robust and higher quality policies.': 0.6550329327583313, '> Significance & Originality:': 1.0986123085021973, 'The explored parameterizations are relatively standard in humanoid control.': 1.0986123085021973, 'The real novelty is systematic evaluation of the various parameterizations.': 1.0986123085021973, 'I think this type of study is important and insightful.': 1.0986123085021973, 'However, the findings are very specific to the problem and specific tested architecture.': 1.0986123085021973, 'Its not clear that findings will transferable to other networks on other control problems/domains.': 1.0985918045043945, 'As such for the ICLR community, this may have limited breadth and perhaps would have broader appeal in robotics / graphics community.': 1.0967059135437012, '> Clarity:': 1.0986123085021973, 'The paper is well written and is pretty easy to understand for someone who has some background with constrained multi-body simulation and control.': 1.0248572826385498, '> Experiments:': 1.0986123085021973, 'Experimental validation is lacking somewhat in my opinion.': 1.0986123085021973, 'Given that this is a fundamentally experimental paper, I would have liked to see more analysis of sensitivity to various parameters and analysis of variance of performance when policy is optimized multiple times.': 1.0972446203231812, 'This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.': 1.0986123085021973, ""My biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments."": 1.098611831665039, 'The authors only consider a single neural network architecture and a single reward function.': 1.0986123085021973, 'For example, is the torque controller limited by the policy network?': 1.0986123085021973, 'My suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data).': 0.4469960331916809, 'In the paper\'s current form, the term ""DeepRL"" seems arbitrary.': 1.0986123085021973, 'On the positive side, the paper is well-structured and easy to read.': 1.0986123085021973, 'The experiments are sound, clear and easy to interpret.': 1.0092127323150635, ""It's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work."": 1.0986123085021973}"
394,https://openreview.net/forum?id=r1kQkVFgl,"{'This paper presents an improved neural language models designed for selected long-term dependency, i.e., to predict more accurately the next identifier for the dynamic programming language such as Python.': 1.0986123085021973, 'The improvements are obtained by:': 1.098597764968872, '1) replacing the fixed-widow attention with a pointer network, in which the memory only consists of context representation of the previous K identifies introduced for the entire history.': 1.0764402151107788, '2) a conventional neural LSTM-based language model is combined with such a sparse pointer network with a controller, which linearly combines the prediction of both components using a dynamic weights, decided by the input, hidden state, and the context representations at the time stamp.': 0.10583412647247314, 'Such a model avoids the the need of large window size of the attention to predict next identifier, which usually requires a long-term dependency in the programming language.': 0.8776806592941284, 'This is partly validated by the python codebase (which is another contribution of this paper) experiments in the paper.': 1.0985852479934692, 'While the paper still misses some critical information that I would like to see, including how the sparse pointer network performance chances with different size of K, and how computationally efficient it is for both training and inference time compared to LSTM w/ attention of various window size, and ablation experiments about how much (1) and (2) contribute respectively, it might be of interest to the ICLR community to see it accepted.': 1.0986123085021973, 'This paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically-typed languages.': 1.0986123085021973, 'Code suggestion seems an area where attention and/or pointers truly show an advantage in capturing long term dependencies.': 1.0986123085021973, 'The sparse pointer method does seem to provide better results than attention for similar window sizes - specifically, comparing a window size of 20 for the attention and sparse pointer method shows the sparse pointer winning fairly definitively across the board.': 1.0986123085021973, 'Given a major advantage of the pointer method is being able to use a large window size well thanks to the supervision the pointer provides, it was unfortunate (though understandable due to potential memory issues) not to see larger window sizes.': 1.0986123085021973, 'Having a different batch size for the sparse pointer and attention models is unfortunate given it complicates an otherwise straight comparison between the two models.': 1.098541259765625, 'The construction and filtering of the Python corpus sounds promising but as of now it is still inaccessible (listed in the paper as TODO).': 1.0985934734344482, 'Given that code suggestion seems an interesting area for future long term dependency work, it may be promising as an avenue for future task exploration.': 1.0986121892929077, 'Overall this paper and the dataset are likely an interesting contribution even though there are a few potential issues.': 1.0986123085021973, 'This paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy that tracks the use of certain token types, like identifiers.': 1.0986123085021973, 'Additionally they release a Python open source dataset.': 1.0956504344940186, 'As expected this augmentation, the fixed attention policy, improves the perplexity of the model.': 1.0986123085021973, 'It seems important to dig a bit deeper into these results and show the contribution of different token types to the achieve perplexity.': 1.0986123085021973, 'This is alluded to in the text, but a more thorough comparison would be welcome.': 1.0986121892929077, 'The idea of an attention policy that takes advantage of expert knowledge is a nice contribution, but perhaps if limited novelty': 1.0986123085021973, 'for example the Maddison and Tarlow 2014 paper, which the authors cite, has scoping rules that track previously used identifiers in scope.': 1.0986123085021973}"
395,https://openreview.net/forum?id=r1nTpv9eg,"{'This paper addresses the question of how to utilize physical interactions to answer questions about physical outcomes.': 1.3846821784973145, 'This question falls into a popular stream in ML community': 1.3851816654205322, 'understanding physics.': 1.3862943649291992, 'The paper moved a step further and worked on experimental setups where there is no prior about the physical properties/rules and it uses a deep reinforcement learning (DRL) technique to address the problem.': 1.2913880348205566, 'My overall opinion about this paper is: an interesting attempt and idea, yet without a clear contribution.': 1.2875722646713257, 'The experimental setups are quite interesting.': 1.3862942457199097, 'The goal is to figure out which blocks are heavier or which blocks are glued together': 1.3421846628189087, 'only by pushing and pulling objects around without any prior.': 0.6963530778884888, 'The paper also shows reasonable performances on each task with detailed scenarios.': 0.6990233659744263, 'While these experiments and results are interesting, the contribution is unclear.': 1.3518034219741821, 'My main question is: does this result bring us any new insight?': 1.2411468029022217, 'While the scenarios are interesting and focused on physical experiments, this is not any more different (potentially easier) than learning from playing games (e.g. Atari).': 0.949346661567688, 'In other words, are the tasks really different from other typical popular DRL tasks?': 1.3862943649291992, 'To this end, I would have been more excited if authors showed some more new insights or experiments on learned representations and etc.': 1.3862943649291992, 'Currently, the paper only discusses the factual outcome.': 1.3862943649291992, 'For example, it describes the experimental setup and how much performances an agent could achieve.': 1.3862943649291992, 'The authors could probably dissect the learned representations further, or discuss how the experimental results are linked to the human behavior or physical properties/laws.': 1.3862943649291992, 'I am very in-between for my overall rating.': 1.3862943649291992, 'I think the paper could have a deeper analysis.': 0.8687455654144287, 'I however recommend the acceptance because of its merit of the idea.': 1.031503677368164, 'The followings are some detailed questions (not directly impacting my overall rating):': 1.3862943649291992, '(1) Page 2 ""we assume that the agent has no prior knowledge about the physical properties of objects, or the laws of physics, and hence must interact with the objects in order to learn to answer questions about these properties."": why does one ""must"" interact with objects in order to learn about the properties?': 1.386255145072937, ""Can't we also learn through observation?"": 1.3862943649291992, '(2) Figure 1right is missing a Y-axis label.': 1.3835495710372925, '(3) Page 3: A relating to bandit is interesting, but the formal approach is all based on DRL.': 1.288936734199524, '(4) Page 5 ""which makes distinguishing between the two heaviest blocks very difficult"": I am a bit confused why having a small mass gap makes the task harder (unless it\'s really close to 0).': 1.3862942457199097, ""Shouldn't a machine be possible to distinguish even a pixel difference of speed?"": 1.3773014545440674, ""If not, isn't this just because of the network architecture?"": 1.195099115371704, '(5) Page 5 ""Since the agents exhibit similar performance using pixels and features we conduct the remaining experiments in this section using feature observations, since these agents are substantially faster to train."": How about at least showing a correlation of performances at the instance level (rather than average performances)?': 1.386273741722107, 'Even so, I think this is a bit of big conclusion.': 1.322829246520996, '(6) Throughout the papers, I felt that many conclusions (e.g. difficulty and etc) are based on a particularly chosen training distribution.': 1.3822247982025146, 'For example, how does an agent really know when the instance is any more difficult?': 1.3862580060958862, ""Doesn't this really depend on the empirically learned distribution of training samples (i.e. P(m_3 | m_1, m_2), where m_i indicates masses of object 1, 2, and 3)?"": 1.3862943649291992, ""In other words, does what's hard/easy matter much unless this is more thoroughly tested over various types of distributions?"": 1.3862943649291992, '(7) Any baseline approach?': 1.3862943649291992, 'This paper investigates the question of gathering information (answering question)': 1.3862943649291992, 'through direct interaction with the environment.': 1.3862943649291992, 'In that sense, it is closely': 1.3862943649291992, 'related to ""active learning"" in supervised learning, or to the fundamental': 1.3862943649291992, 'problem of exploration-exploitation in RL.': 1.3862943649291992, 'The authors consider a specific': 1.3862943649291992, 'instance of this problem in a physics domain and learn': 1.3862943649291992, 'information-seeking policies using recent deep RL methods.': 1.3862943649291992, 'The paper is mostly empirical and explores the effect of changing the': 1.3862943649291992, 'cost of information (via the discount factor) on the structure of the learned': 1.3862943649291992, 'policies.': 1.3862943649291992, 'It also shows that general-purpose deep policy gradient methods are': 1.3862943649291992, 'sufficient powerful to learn such tasks.': 1.3862943649291992, 'The proposed environment is, to my knowledge,': 1.3862943649291992, 'novel as well the task formulation in section 2.': 1.3862943649291992, '(And it would be very valuable to the': 1.3862943649291992, 'the community if the environment would be open-sourced)': 1.3862943649291992, 'The expression ""latent structure/dynamics"" is used throughout the text and the connection': 1.3862943649291992, 'with bandits is mentioned in section 4.': 1.3862943649291992, 'It therefore seems that authors aspire': 1.3862943649291992, ""for more generality with their approach but the paper doesn't quite fully ground"": 1.3862943649291992, 'the proposed approach formally in any existing framework nor does it provide a': 1.3862943649291992, 'new one completely.': 1.3862943649291992, 'For example: how does your approach formalize the concept of ""questions"" and ""answers"" ?': 1.3862943649291992, 'What makes a question ""difficult"" ?': 1.3862943649291992, 'How do you quantify ""difficulty"" ?': 1.3862943649291992, 'How do you define the ""cost of information""?': 1.3862943649291992, 'What are its units (bits, scalar reward), its semantics ?': 1.3862943649291992, 'Do you you have an MDP or a POMDP ?': 1.3862943649291992, 'What kind of MDP do you consider ?': 1.3862943649291992, 'How do you define your discounted MDP ?': 1.3862943649291992, 'What is the state and action spaces ?': 1.3862943649291992, 'Some important problem structure under the ""interaction/labeling/reward""': 1.3862943649291992, 'paragraph of section 2 would be worth expressing directly in your definition': 1.3862943649291992, 'of the MDP: labeling actions can only occur during the ""labeling phase"" and that the transition': 1.3862943649291992, 'and reward functions have a specific structure (positive/negative, lead to absorbing state).': 1.3862943649291992, 'The notion of ""phase"" could perhaps be implemented by considering an augmented state space :': 1.3862943649291992, 'This paper purports to investigate the ability of RL agents to perform ‘physics experiments’ in an environment, to infer physical properties about the objects in that environment.': 1.3862943649291992, 'The problem is very well motivated; indeed, inferring the physical properties of objects is a crucial skill for intelligent agents, and there has been relatively little work in this direction, particularly in deep RL.': 1.3862943649291992, 'The paper is also well-written.': 1.3862943649291992, 'As there are no architectural or theoretical contributions of the paper (and none are claimed), the main novelty comes in the task application – using a recurrent A3C model for two tasks that simulate an agent interacting with an environment to infer physical properties of objects.': 1.3862943649291992, 'More specifically, two tasks are considered – moving blocks to determine their mass, and poking towers such that they fall to determine the number of rigid bodies they are composed of.': 1.3299224376678467, 'These of course represent a very limited cross-section of the prerequisite abilities for an agent to understand physics.': 1.3862411975860596, 'This in itself is not a bad thing, but since there is no comparison of different (simpler) RL agents on the tasks, it is difficult to determine if the tasks selected are challenging.': 1.1234432458877563, 'As mentioned in the pre-review question, the ‘Which is Heavier’ task seems quite easy due to the actuator set-up, and the fact that the model simply must learn to take the difference between successive block positions (which are directly encoded as features in most experiments).': 1.3714512586593628, 'Thus, it is not particularly surprising that the RL agent can solve the proposed tasks.': 1.3862943649291992, 'The main claim beyond solving two proposed tasks related to physics simulation is that “the agents learn different strategies for these tasks that balance the cost of gathering information against the cost of making mistakes”.': 0.9058483242988586, 'The ‘cost of gathering information’ is implemented by multiplying the reward with a value of gamma < 1.': 0.7199872136116028, 'This is somewhat interesting behaviour, but is hardly surprising given the problem setup.': 1.312876582145691, 'One item the authors highlight is that their approach of learning about physical object properties through interaction is different from many previous approaches, which use visual cues.': 1.3661805391311646, 'However, the authors also note that this in itself is not novel, and has been explored in other work (e.g. Agrawal et al. (2016)).': 1.3851752281188965, 'I think it’s crucial for the authors to discuss these approaches in more detail (potentially along with removing some other, less relevant information from the related work section), and specifically highlight why the proposed tasks in this paper are interesting compared to, for example, learning to move objects towards certain end positions by poking them.': 1.3693182468414307, 'To discern the level of contribution of the paper, one must ask the following questions:': 1.352748990058899, '1) how much do these two tasks contribute (above previous work) to the goal of having agents learn the properties of objects by interaction; and': 0.7355810403823853, '2) how much do the results of the RL agent on these tasks contribute to our understanding of agents that interact with their environment to learn physical properties of objects?': 0.9901756048202515, 'It is difficult to know exactly, but due to the concerns outlined above, I am not convinced that the answers to (1) or (2) are “to a significant extent”.': 1.3779566287994385, 'In particular, for (1), since the proposed agent is able to essentially solve both tasks, it is not clear that the tasks can be used to benchmark more advanced agents (e.g. it can’t be used as a set of bAbI-like tasks).': 1.382980227470398, 'Another possible concern, as pointed out by Reviewer 3, is that the description of the model is extremely concise.': 1.3754644393920898, 'It would be nice to have, for example, a diagram illustrating the inputs and outputs to the model at each time step, to ease replication.': 1.381009578704834, 'Overall, it is important to make progress towards agents that can learn to discover physical properties of their environment, and the paper contributes in this direction.': 1.3856180906295776, 'However, the technical contributions of this paper are rather limited – thus, it is not clear to what extent the paper pushes forward research in this direction beyond previous work that is mentioned.': 1.3862824440002441, 'It would be nice, for example, to have some discussion about the future of agents that learn physics from interaction (speculation on more difficult versions of the tasks in this paper), and how the proposed approach fits into that picture.': 1.3862943649291992, 'EDIT: score updated, see comments below': 1.3862943649291992, 'This paper presents interesting experimental findings that state-of-the-art deep reinforcement learning methods enable agent learning of latent (physical) properties in its environment.': 1.3862943649291992, 'The paper formulates the problem of an agent labeling environmental properties after interacting with the environment based on its actions, and applies the deep reinforcement learning model to evaluate whether such learning is possible.': 1.3862943649291992, 'The approach jointly learns the convolutional layers for pixel-based perception and its later layers for learning actions based on reinforcement signals.': 1.3862943649291992, 'We have a mixed opinion about this paper.': 1.3862943649291992, 'The paper is written clearly and presents interesting experimental findings.': 1.3862943649291992, 'It introduces and formulates a problem potentially important for many robotics applications.': 0.7707316875457764, 'Simultaneously, the paper suffers from lacking algorithmic contributions and missing (some of) crucial experiments to confirm its true benefits.': 1.3862943649291992, 'Pros:': 1.3862943649291992, '+': 1.3862943649291992, ""This paper introduces a new problem of learning latent properties in the agent's environment."": 1.384826421737671, 'The paper presents a framework to appropriately combine existing tools to address the formulated problem.': 0.8501140475273132, 'The paper tries reinforcement learning with image inputs and fist-like actuator actions.': 1.3705894947052002, 'This will lead to its direct application to robots.': 1.3862241506576538, 'Cons:': 1.3862943649291992, 'Lacking algorithmic contribution: this paper applies existing tools/methods to solve the problem rather than developing something new or extending them.': 1.230431079864502, 'The approach essentially is training LSTMs with convolutional layers using the previous Asynchronous Advantage Actor Critic.': 0.4866948127746582, 'In the Towers experiment, the results of probably the most important setting, ""Fist Pixels"", are missing.': 1.3862786293029785, 'This setting receiving pixel inputs and using the Fist actuator in a continuous space is the setting closest to real-world robots, and thus is very important to confirm whether the proposed approach will be directly applicable to real-world robots.': 1.3862941265106201, 'However, Figure 5 is missing the results with this setting.': 0.851434588432312, 'Is there any reason behind this?': 1.3793175220489502, 'The paper lacks its comparison to any baseline methods.': 1.3833099603652954, 'Without explicit baselines, it is difficult to see what the agent is really learning and what aspect of the proposed approach is benefitting the task.': 1.3861784934997559, ""For instance, in the Towers task, how would an agent randomly pushing/hitting the tower (using 'Fist') a number of times and then passively observing its consequence to produce a label perform compared to this approach?"": 1.3862940073013306, 'That is, how would an approach with a fixed action policy (but with everything else) perform compared to the full deep reinforcement learning version?': 1.3862943649291992}"
396,https://openreview.net/forum?id=r1osyr_xg,"{'This paper proposes a method for estimating the context sensitivity of paraphrases and uses that to inform a word embedding learning model.': 1.0985726118087769, 'The main idea and model are presented convincingly and seem plausible.': 1.0986055135726929, 'The main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration.': 1.0266104936599731, 'The evaluation does not convincingly determine whether the model is a significant improvement over simpler methods (particularly those that do not require the paraphrase database!).': 1.0986123085021973, 'Likewise, the model section did not convince me that this was the most obvious model formulation to try.': 1.0986106395721436, 'The paper would be stronger if model choices were explained more convincingly or - better yet - alternatives were explored.': 1.0986123085021973, 'On balance I lean towards rejecting the paper and encouraging the authors to submit a revised and improved version at a near point in the future.': 1.0986123085021973, 'Detailed/minor points below:': 1.0982224941253662, '1) While the paper is grammatically mostly correct, it would benefit from revision with the help of a native English speaker.': 0.9528149962425232, 'In its current form long sections are very difficult to understand due to the unconventional sentence structure.': 1.0986121892929077, '2)': 1.0986123085021973, 'The tables need better and more descriptive labels.': 0.3499142527580261, '3)': 1.0986123085021973, 'The results are somewhat inconclusive.': 0.48253804445266724, 'Particularly in the analogy task in Table 4 it is surprising that CBOW does better on the semantic aspect of the task than your embeddings which are specifically tailored to be good at this?': 1.0986123085021973, '4) Why was ""Enriched CBOW"" not included in the analogy task?': 0.7083394527435303, ""5) In the related work section several papers are mentioned that learn embeddings from a combination of lexica and corpora, yet it is repeatedly said that this was the first work of such a kind / that there hasn't been enough work on this."": 1.0247834920883179, 'That feels a little misleading.': 1.018571376800537, 'This paper tries to leverage an external lexicon / knowledge base to improve corpus-based word representations by determining (in a fuzzy way) which potential paraphrase is the most appropriate in a particular context.': 1.0985674858093262, 'I think this paper is a bit lost in translation.': 0.7758684158325195, 'The grammatical and storytelling styles made it really difficult for me to concentrate, and even unintelligible at times.': 1.0973275899887085, ""One of the most important criteria in a conference paper is to communicate one's ideas clearly; unfortunately, I do not feel that this paper meets that standard."": 1.0986114740371704, 'In addition, the evaluation is rather lacking.': 1.098611831665039, ""There are many ways to evaluate word representations, and Google's analogy dataset has many issues (see, for example, Linzen's paper from RepEval 2016, as well as Drozd et al., COLING 2016)."": 1.0986121892929077, 'Finally, this work does not provide any qualitative result or motivation.': 1.0986123085021973, 'Why does this method work better?': 1.0986123085021973, 'Where does it fail?': 1.0986123085021973, 'What have we learned about word representations / lexicons / corpus-based methods in general?': 1.0986123085021973, 'This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology.': 1.0986026525497437, 'Sometimes polysemy is context-dependent, but prior approaches have neglected this fact when incorporating external paraphrase information during learning.': 1.0986123085021973, 'The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates, down-weighting those candidates that depend strongly on context.': 1.0986123085021973, 'This function is inferred from bilingual translation agreement.': 1.0969825983047485, 'The main argumentation leading to the model selection is intuitive, and I believe that the inclusion of good paraphrases and the elimination of bad paraphrases during training should in principle improve word representation quality.': 1.0986120700836182, 'However, the main questions are how well the proposed method achieves this goal, and, even if it achieves it well, whether it makes much difference in practical terms.': 1.0980186462402344, 'Regarding the first question, I am not entirely convinced that the parameterization of the control function f(x_ij) is optimal.': 1.0986123085021973, 'It would have been nice to see some experiments investigating different choices, in particular some baselines where the effect of f is diminished (so that it reduces to f=1 in the limit) would have been interesting.': 1.0973297357559204, 'I also feel like there would be a lot to gain from having f be a function of the nearby word embeddings, though this would obvious incur a significant slowdown.': 1.0986120700836182, ""(See for example 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should probably be cited.)"": 1.0447951555252075, 'As it stands, the experimental results do not clearly distinguish the fuzzy paraphrase approach from prior work, i.e. tables 3 and 4 do not show major trends one way or the other.': 1.0742508172988892, 'Regarding the second question, it is hard to draw many conclusions from analogy tasks alone, especially when effects unrelated to good/bad paraphrasing such as corpus size/content, window size, vocabulary size, etc., can have an outsize effect on performance.': 1.0985920429229736, 'Overall, I think this is a good paper presenting a sensible idea, but I am not convinced by the experiments that the specific approach is achieving its goal.': 1.0985931158065796, 'With some improved experiments and analysis, I would wholeheartedly recommend this paper for acceptance; as it stands, I am on the fence.': 0.9351873397827148}"
397,https://openreview.net/forum?id=r1rhWnZkg,"{'Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.': 1.0986123085021973, 'Strengths:': 1.0986123085021973, '1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work.': 0.42392221093177795, '2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).': 0.7943353652954102, '3. The various design choices made in model development have been experimentally verified.': 0.5196300148963928, 'Weaknesses/Suggestions:': 1.0986123085021973, '1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).': 1.0986120700836182, '2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data?': 0.8155365586280823, '3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.': 0.5623907446861267, '4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.': 0.43795424699783325, '5. In the caption for Table 1, fix the following: “have not” -> “have no”': 1.01233971118927, 'Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper.': 1.0986123085021973, 'However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling.': 1.0986123085021973, 'It does lead to reduction in number of parameters but it is not clear how much that helps experimentally.': 1.0986123085021973, 'So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling.': 1.0986123085021973, 'Results on the VQA task are good for this simple model, the ablation study of table 1 gives some insights as to what is important.': 1.0986123085021973, 'Missing are some explanations about the language embedding and the importance in deciding embedding dimension and final output dimension, equivalent to deciding the projected dimension in the compact bilinear model.': 1.0986123085021973, 'Since the main contribution of the': 1.0986123085021973, 'paper seems to be slightly better performance with fairly large reduction in parameters vs. compact bilinear something should be said about choice of those hyper parameters.': 1.0986123085021973, 'If you increase embedded and output dimensions to equalize parameters to the compact bilinear model are further gains possible?': 1.0986123085021973, 'How is the question encoded?': 1.0986123085021973, 'Is word order preserved in this encoding, the compact bilinear model compared to in table 1 mentions glove, the proposed model is using this as well?': 1.0986123085021973, 'The meaning of visual attention in this model along with the number of glimpses should be tied to the sentence embedding, so now we are looking at particular spatial components when that part of the sentence is encoded, then we stack according to your equation 9?': 1.0986123085021973, 'This work proposes to approximate the bilinear pooling (outer product) with a formulation which uses the Hadamard Product (element-wise product).': 1.0986123085021973, 'This formulation is evaluated on the visual question answering (VQA) task together with several other model variants.': 1.0986123085021973, 'Strength:': 1.0986123085021973, '1. The paper discusses how the Hadamard product can be used to approximate the full outer product.': 0.8613249063491821, '2. The paper provides an extensive experimental evaluation of other model aspect for VQA.': 0.9590092301368713, '3. The full model archives a slight improvement over prior state-of-the-art on the challenging and large scale VQA challenge.': 0.6511443853378296, 'Weaknesses:': 0.8970028162002563, '1. Novelty: The paper presents only a new “interpretation” of the Hadamard product which has previously been widely used for pooling, including for VQA.': 0.5927903056144714, '2. Experimental evaluation:': 0.9573931097984314, '2.1. An experimental direct comparison with MCB missing. Although the evaluated model is similar to Fukui et al. several other changes have been made, including question encoding (GRU vs. LSTM), normalization (tanh vs. L2 vs. none). The small difference in performance (0.44% om Table 1) could easily be attributed to these differences.': 0.9001628756523132, '2.2. An experimental comparison to the full outer product (e.g. for a lower dimension) is missing. It remains unclear how good the proposed approximation for the full outer product is. While a comparison to MCB is presented this seems insufficient as MCB is a very different model.': 0.7896835803985596, '2.3. One of the most important hyper parameters for the Hadamard Product seems to be the dimension of the lower dimensional embedding d. What effect does changing this have?': 0.7356408834457397, '2.4. Comparison with other pooling strategies, e.g. elementwise sum instead of elementwise product.': 1.0982911586761475, '3. No theoretical analysis or properties of the approximation are presented.': 0.5473747849464417, '4. The paper seems to be general at the beginning, but the claim of the benefit of the Hadamard product is only shown experimentally on the VQA dataset.': 0.8722445368766785, '5. Related work: The comparison to the related works in the appendix should at least be mentioned in the main paper, even if the details are the supplemental.': 0.8636268377304077, 'Minor': 1.0986123085021973, 'It is not clear why the Lu et al, 2015 is cited rather than the published paper from Antol et al.': 1.0986121892929077, 'Sect 2, first sentence: “every pairs” -> “every pair”': 0.4336843192577362, 'Summary:': 1.0953394174575806, 'While the paper provides a new best performance and an interesting interpretation of Hadamard product, to be a strong paper, either a more theoretical analysis of the properties of this approximation is required or a corresponding experimental evaluation.': 1.0986114740371704, 'It is a bit unfortunate that most of the experimental evaluation is not about the main claim of the paper (the Hadamard product) but of unrelated aspects which are important to achieve high performance in the VQA challenge.': 0.4362803101539612, 'To be more convincing I would like to see the following experiments': 1.0986123085021973, 'Comparison with Outer product in the identical model': 1.0986123085021973, 'Comparison with MCB in the identical model': 1.0986123085021973, 'Comparison with elementwise sum instead of elementwise product': 1.0986123085021973, 'One of the most important hyper parameters for the Hadamard Product seems to be the dimension of the lower dimensional embedding d.': 1.0986123085021973, 'What effect does changing this have?': 1.0986123085021973}"
398,https://openreview.net/forum?id=r1rz6U5lg,"{'This work builds on top of STOKE (Schkufza et al., 2013), which is a superoptimization engine for program binaries.': 1.0986123085021973, 'It works by starting with an existing program, and proposing modifications to it according to a proposal distribution.': 1.0986123085021973, 'Proposals are accepted according to the Metropolis-Hastings criteria.': 1.0986123085021973, 'The acceptance criteria takes into account the correctness of the program, and performance of the new program.': 1.0986123085021973, 'Thus, the MCMC process is likely to converge to correct programs with high performance.': 1.0986123085021973, 'Typically, the proposal distribution is fixed.': 1.0986123085021973, 'The contribution of this work is to learn the proposal distribution as a function of the features of the program (bag of words of all the opcodes in the program).': 1.0986123085021973, 'The experiments compare with the baselines of uniform proposal distribution, and a baseline where one just learns the weights of the proposal distribution but without conditioning on the features of the program.': 1.0986123085021973, 'The evaluation shows that the proposed method has slightly better performance than the compared baselines.': 1.0986123085021973, 'The significance of this work at ICLR seems to be quite low., both because this is not a progress in learning representations, but a straightforward application of neural networks and REINFORCE to yet another task which has non-differentiable components.': 1.0986123085021973, 'The task itself (superoptimization) is not of significant interest to ICLR readers/attendees.': 1.0986123085021973, 'A conference like AAAI/UAI seem a better fit for this work.': 1.0986123085021973, 'The proposed method is seemingly novel.': 1.0986123085021973, 'Typical MCMC-based synthesis methods are lacking due to their being no learning components in them.': 1.0986123085021973, 'However, to make this work compelling, the authors should consider demonstrating the proposed method in other synthesis tasks, or even more generally, other tasks where MH-MCMC is used, and a learnt proposal distribution can be beneficial.': 1.0986123085021973, 'Superoptimization alone (esp with small improvements over baselines) is not compelling enough.': 1.0986123085021973, 'It is also not clear if there is any significant representation learning is going on.': 1.0986123085021973, 'Since a BoW feature is used to represent the programs, the neural network cannot possibly learn anything more than just correlations between presence of opcodes and good moves.': 1.0986123085021973, 'Such a model cannot possibly understand the program semantics in any way.': 1.0986123085021973, 'It would have been a more interesting contribution if the authors had used a model (such as Tree-LSTM) which attempts to learn the semantics the program.': 1.0986123085021973, 'The quite naive method of learning makes this paper not a favorable candidate for acceptance.': 1.0986123085021973, 'This is an interesting and pleasant paper on superoptimization, that extends the  problem approached by the stochastic search STOKE to a learned stochastic search, where the STOKE proposals are the output of a neural network which takes some program embedding as an input.': 1.0986123085021973, 'The authors then use REINFORCE to learn an MCMC scheme with the objective of minimizing the final program cost.': 1.0986123085021973, 'The writing is clear and results highlight the efficacy of the method.': 1.0986123085021973, 'comments / questions:': 1.0986123085021973, 'Am I correct in understanding that of the entire stochastic computation graph, only the features->proposal part is learned.': 1.0986123085021973, 'The rest is still effectively the stoke MCMC scheme?': 1.0986123085021973, ""Does that imply that the 'uniform' model is effectively Stoke and is your baseline (this should probably be made explicit )"": 1.0986123085021973, 'Did the authors consider learning the features instead of using out of the box features (could be difficult given the relatively small amount of data - the feature extractor might not generalize).': 1.0986123085021973, ""In a different context, 'Markov Chain Monte Carlo and Variational Inference:Bridging the Gap' by Salimans et al. suggests considering a MCMC scheme as a stochastic computation graph and optimizing using a variational (i.e. RL) criterion."": 1.0986123085021973, ""The problem is different, it uses HMC instead of MCMC, but it might be worth citing as a similar approach to 'meta-optimized' MCMC algorithms."": 1.0986123085021973, 'Two things I really liked about this paper:': 1.0986123085021973, ""1. The whole idea of having a data-dependent proposal distribution for MCMC. I wasn't familiar with this, although it apparently was previously published. I went back: the (Zhu, 2000) paper was unreadable. The (Jampani, 2014) paper on informed sampling was good. So, perhaps this isn't a good reason for accepting to ICLR."": 1.0968170166015625, '2. The results are quite impressive. The rough rule-of-thumb is that optimization can help you speed up code by 10%. The standard MCMC results presented on the paper on randomly-generated programs roughly matches this (15%). The fact that the proposed algorithm get ~33% speedup is quite surprising, and worth publishing.': 1.0942864418029785, ""The argument against accepting this paper is that it doesn't match the goals of ICLR."": 1.0986119508743286, ""I don't go to ICLR to hear about generic machine learning papers (we have NIPS and ICML for that)."": 1.0985456705093384, 'Instead, I go to learn about how to automatically represent data and models.': 1.0646440982818604, 'Now, maybe this paper talks about how to represent (generated) programs, so it tangentially lives under the umbrella of ICLR.': 1.0853779315948486, 'But it will compete against more relevant papers in the conference': 1.0984548330307007, 'it may just be a poster.': 1.098565697669983, 'Sending this to a programming language conference may have more eventual impact.': 0.42772501707077026, 'Nonetheless, I give this paper an ""accept"", because I learned something valuable and the results are very good.': 1.0977280139923096}"
399,https://openreview.net/forum?id=r1tHvHKge,"{'This paper presents a heuristic for avoiding large negative rewards which have already been experienced by distilling such events into a ""danger model"".': 1.0986123085021973, 'The paper is well written including some rather poetic language': 1.093803882598877, '[*].': 1.0986123085021973, 'The heuristic is evaluated in two toy domains.': 1.095353364944458, 'I would think that in order to properly evaluate this one would use a well known benchmark e.g. Atari.': 1.0550860166549683, 'Atari seems particularly apt since those games are full of catastrophes (i.e. sudden death).': 1.0265672206878662, ""[*] this reviewer's favourite quotes:"": 1.0986123085021973, '""Imagine a self-driving car that had to periodically hit a few pedestrians in order to remember that it’s undesirable.""': 0.9044549465179443, '""The child can learn to adjust its behaviour without actually having to stab someone.""': 1.0984914302825928, '""... the catastrophe lurking just past the optimal shave.""': 1.0986123085021973, '- The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature. After the pre-review comments, authors do mention that they compared against expected SARSA but I would really like to see these and other extensive baselines before accepting this paper.': 0.4378652274608612, 'There is also an increasing amount of literature of using reward replay buffers in deep RL agents (c.f. Jaderberg, Max, et al.': 1.0985996723175049, '""Reinforcement learning with unsupervised auxiliary tasks.': 1.0985745191574097, '"", Blundell, Charles, et al.': 1.098610281944275, '""Model-free episodic control.""': 1.0986123085021973, ', Narasimhan et al.': 1.0985320806503296, '""Language understanding for text-based games using deep reinforcement learning""), which could perhaps reinforce the agent to avoid revisiting catastrophic states.': 1.085209608078003, 'Overall, the approach presented is not very principled.': 1.0775988101959229, ""For instance, why isn't catastrophe directly provided as a signal to the learner instead of a separate model?"": 1.0986106395721436, 'This paper addresses an important and timely topic in a creative way.': 0.9265865683555603, 'I consider it to have three flaws (and one good idea).': 1.0986123085021973, '1) insufficient context of what is known and had been studied before (in shallow RL), for example within the field of “robust RL”.': 0.8297996520996094, 'A good place to start might be with the work of Shie Mannor.': 0.46824076771736145, '2) an ill-defined general problem setup.': 1.0792921781539917, 'Does it make sense to do post-hoc labeling of certain actions as “catastrophic” if the agent is not informed about that metric during learning?': 1.0986099243164062, 'Training a system to do one thing (maximize reward), but then evaluating it with a different metric is misleading.': 1.0986120700836182, 'On the training metric, it could even be that the baseline outperforms the new algorithm?': 1.0981922149658203, 'So I’d want to see plots for “average reward” in fig 3 as well.': 1.0985924005508423, 'Also, what would the baseline learn if it was given large negative rewards for entering these otherwise invisible “danger states”?': 1.098606824874878, '3) a somewhat ad-hoc solution, that introduces new domain-specific hyperparameters (k_r, k_lambda and lambda) a second deep network and and two additional replay memories.': 0.6666679382324219, 'In terms of results, I’m also unsure whether I can trust the results, given the long-standing track-record of cart-pole being fully solved by many methods: is DQN an outlier here?': 1.0986123085021973, 'Or is the convnet not an appropriate function-approximator?': 1.0986111164093018, 'Actually: which exact variant “state-of-the-art” variant of DQN are you using?': 1.0874689817428589, 'The good idea that I encourage the authors to pursue further is D_d, this set of rare but dangerous states, that should be kept around in some form.': 1.0978467464447021, 'I see it as an ingredient for continual learning that most typical methods lack': 1.0970685482025146, 'it is also one of the big differences between RL and supervised learning, where such states would generally be discarded as outliers.': 1.098392128944397, 'Given my comment a couple of weeks ago, and the prompt response (“we implemented expected SARSA”), I would have expected that the paper had been revised with the new results by now?': 0.910749614238739, 'In any case, I’m open to discussing all these points and revising my opinion based on an updated version of the paper.': 1.074479341506958, 'Minor comment: the bibliography is done sloppily, with missing years, conference venues and missing/misspelled author lists, e.g. “Sergey et al. Levine”. I also think it is good form to cite the actual conference publications instead of arXiv where applicable.': 0.8005498051643372}"
400,https://openreview.net/forum?id=r1te3Fqel,"{'SYNOPSIS:': 1.0986123085021973, 'The paper proposes a new neural network-based model for reading comprehension (reading a passage of text and answering questions based on the passage).': 1.0986123085021973, 'It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities.': 1.0986120700836182, 'The authors compare their model on the Stanford Question Answering Dataset (SQuAD), and show improvements over the baselines, while apparently lagging quite far behind the current state of the art reported on the SQuAD leaderboard.': 1.0986123085021973, 'THOUGHTS:': 1.0986123085021973, 'The main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question.': 1.0986123085021973, 'However, both approaches considered': 1.0986123085021973, 'using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N': 1.0986123085021973, 'seem somewhat orthogonal to the idea of ""learning end-to-end "" an answer chunk extraction model.': 1.0910786390304565, 'Furthermore, as other reviews have pointed out, it seems that the linguistic features actually contribute a lot to the final accuracy (Table 3).': 1.0983872413635254, 'One could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an ""end-to-end trained"" system.': 1.0986123085021973, 'The paper is generally well written, but there are several crucial sections in parts describing the model where it was really hard for me to follow the descriptions.': 1.0986123085021973, 'In particular, the attention mechanism seems fairly standard to me in a seq2seq sense (i.e. there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader).': 1.0986120700836182, ""I may be missing something, but even after the clarification round I still don't understand how it is novel compared to standard attention used in for instance seq2seq models."": 1.0986123085021973, 'Finally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard (https://rajpurkar.github.io/SQuAD-explorer/).': 1.0979351997375488, 'Of course, it may be that further training and hyperparameter optimizations may improve these results.': 1.086491584777832, ""Therefore, given the lack of model novelty (based on my understanding), and the lack of strong results (based on the leaderboard), I don't feel the paper is ready in its current form to be accepted to the conference."": 1.0986123085021973, 'Note: The GRU citation should be (Cho et al., 2014), not (Bengio et al., 2015).': 1.0986123085021973, 'The paper proposed an end-to-end machine learning model called dynamic reader for the machine reading comprehension task.': 1.0986123085021973, 'Compared to earlier systems, the proposed model is able to extract and rank a set of answer candidates from a given document.': 1.0986123085021973, 'There are many recent models focusing on building good question answering systems by extracting phrases from a given article.': 1.0986123085021973, 'It seems that there are two different aspects that are unique in this work:': 1.0986123085021973, '1. The use of convolution model, and': 1.0986123085021973, '2. Dynamic chunking': 1.0986123085021973, 'Convolution network is often only used for modeling character-based word embeddings so I am curious about its effectiveness on representing phrases.': 1.0986123085021973, 'Therefore, I wish there could be more analysis on how effective it is, as the authors do not compare the convolution framework to other alternative approaches such as LSTM.': 1.09859037399292, 'The comparisons are important, as the authors uses uni-gram, bi-gram and tri-gram information in the convolution network, and it is not clear to me that if tri-gram information is still needed for LSTM models.': 0.6867111921310425, 'The dynamic chunking is a good idea, and a very similar idea is proposed in some of the recent papers such as [Kenton et al, 16], which also targets at the same dataset.': 0.6913315653800964, 'However, I would like to see more analysis on the dynamic chunking.': 1.0986123085021973, 'Why this approach is a good approach for representing answer chunks?': 1.0986123085021973, 'Given the representation of the chunk is constructed by the first and the end word representations generated by a convolution network, I am not sure about the ability of this representation to capture the long answer phrases.': 0.9507797360420227, 'The authors do not use character base embedding but use some of the previous trained NLP models.': 1.0986123085021973, 'It would be interesting if the authors could show what are the advantages and disadvantages of using linguistic features compared to character embeddings.': 1.0986123085021973, 'In short, there are several good ideas proposed in the paper, but the lack of proper analysis make it difficult to judge how important the proposed techniques are.': 1.0985912084579468, 'SUMMARY.': 1.0986123085021973, 'The paper propose a reading-comprehension question answering system for the recent QA task where answers of a question can be either single tokens or spans in the given text passage.': 1.0986016988754272, 'The model first encodes the passage and the query using a recurrent neural network.': 0.7739844918251038, 'With an attention mechanism the model calculates the importance of each word on the passage with respect to each word in the question.': 1.0906273126602173, 'The encoded words in the passage are concatenated with the attention; the resulting vector is re-encoded with a further RNN.': 1.0939427614212036, 'Three convolutional neural networks with different filter size (1,2,3-gram) are used to further capture local features.': 1.094382643699646, 'Candidate answers are selected either matching POS patterns of answers in the training set or choosing all possible text span until a certain length.': 1.0598423480987549, 'Each candidate answer has three representations, one for each n-gram representation.': 1.0980957746505737, 'The compatibility of these representation with the question representation is then calculated.': 1.0985949039459229, 'The scores are combined linearly and used for calculating the probability of the candidate answer being the right answer for the question.': 0.9421936273574829, 'The method is tested on the SQUAD dataset and outperforms the proposed baselines.': 1.0904510021209717, 'OVERALL JUDGMENT': 1.0986123085021973, 'The method presented in this paper is interesting but not very motivated in some points.': 0.41292572021484375, 'For example, it is not explained why in the attention mechanism it is beneficial to concatenate the original passage encoding with the attention-weighted ones.': 1.0985381603240967, 'The contributions of the paper are moderately novel proposing mainly the attention mechanism and the convolutional re-encoding.': 1.0986123085021973, 'In fact, combining questions and passages and score their compatibility has became a fairly standard procedure in all QA models.': 1.0986123085021973, 'DETAILED COMMENTS': 1.0986123085021973, 'Equation (13) i should be s, not s^l.': 1.0986123085021973, 'I still do not understand the sentence "" the best function is to concatenate the hidden stat of the fist word in a chunk in forward RNN and that of the last word in backward RNN"".': 1.0986123085021973, 'The RNN is over what all the words in the chunk?': 1.0986123085021973, 'in the passage?': 1.0986123085021973, 'The answer the authors gave in the response does not clarify this point.': 1.0986123085021973}"
401,https://openreview.net/forum?id=r1w7Jdqxl,"{'The authors proposed to learn embeddings of users and items by using deep neural network for a recommendation task.': 1.0986123085021973, 'The resulting method has only minor differences from the previous CDL, in which neural networks were also used for recommendation tasks.': 1.0986123085021973, 'In the experiments, since the proposed method, DualNets have use more item features than WMF and CDL, the comparisons are unfair.': 1.0765022039413452, 'This paper provides a minor improvement paper of DeepRS.': 1.0986123085021973, 'The major improvement comes from the coupling of user-item factors in prediction.': 1.0986123085021973, 'While the motivation is clear, the improvement of the model architecture is minor.': 1.0985370874404907, 'I think the author should improve the paper to discuss more on the impact of introduction of coupling, which might make this paper stronger.': 1.0986015796661377, 'Specifically, conduct isolate experiment to change loss, architecture gradually, from a non-coupled network to a final proposed coupled network to demonstrate the importance of coupling.': 1.0986123085021973, 'Another important missing part of the paper seems to be time complexity.': 1.0986123085021973, 'Since coupled net would be much more costly to generate recommendations, a discussion on how it would impact real world usages should be added.': 1.0986119508743286, 'Overall, I think this is a paper that should be improved before accepted.': 1.0986123085021973, 'The responses to the pre-review questions are not strong; especially w.r.t.': 1.0986123085021973, 'the question about dataset density and why the dataset had to be subsampled, the authors responded that subsampling is common in recommender systems work, including the papers cited.': 0.7731360793113708, 'This isn\'t a particularly strong justification of why subsampling is a good idea, and in particular doesn\'t answer the question of ""how would the results look without subsampling,"" which I think is a question that could easily have been answered directly.': 1.0986123085021973, 'Especially given that the goal of dealing with the cold-start issue is so heavily emphasized in the paper, in seems odd to sample the data to reduce sparsity.': 1.0986123085021973, 'Other than that, the pre-review questions seem to have been answered satisfactorily.': 1.0985698699951172, 'The contribution of the paper is to propose user and item embedding methods, as a means of learning complex non-linear interactions between users and items.': 1.0986086130142212, 'This is fairly similar to recent work on deep RS, though the network formulation has some differences.': 1.098140835762024, 'Overall this is an reasonably put together paper that makes a contribution in an important area, though there are still some shortcomings that should be addressed, namely:': 1.0986123085021973, '1) The evaluation is unusual.': 0.6009765863418579, 'Recall@M is the only result reported, though this is not usually an evaluation seen in recommender systems research.': 0.5658895969390869, 'At the very least other performance measures (rmse or AUC) should be reported for completeness, even if the results are not strong': 1.098611831665039, '2) Given that the contribution is fairly simple (i.e., the ""standard"" recommender systems task, but with a new model)': 0.9386589527130127, ""it's a shame that unusual data samples have to be taken."": 0.6882111430168152, ""This should be a case where it's possible to report results against competing methods using *exactly* the same data they used, and exactly the same error measure, for the fairest comparison possible."": 1.0986096858978271, ""Without the above it's hard to tell how much the performance improvements are really due to the method being better, versus the choice of datasets and the choice of loss functions."": 1.0986002683639526}"
402,https://openreview.net/forum?id=r1xUYDYgg,"{'Validity:': 1.0986123085021973, 'The presented work seems technically valid.': 1.0986123085021973, 'Code for matrix library sushi2 and DL library sukiyaki2 are on github, including live demos that run in your browser.': 1.0986123085021973, 'https://mil-tokyo.github.io/sukiyaki2/examples/mnist/ was fun, but seemed very slow (5 mnist images per second).': 1.0986123085021973, 'The demo page would be more interesting if it showed what model was being trained, which implementation was being used (pure js or webcl?), which hardware was being used for the computation, and how that compared with other people who logged into the page.': 1.0986123085021973, 'As it is, the demo is kind of unclear as to what is happening.': 1.0986123085021973, 'Relevance:': 1.0986123085021973, 'The grand vision of a DLTraining@Home is exciting.': 1.0986123085021973, 'While much work remains, having a solid WebCL foundation seems valuable.': 1.0986123085021973, 'The big advantage of javascript is that it runs everywhere, especially on idle desktops and laptops around the world.': 1.0986123085021973, 'However, these sorts of computers do not (with probability 1) have K80 or S9120 video cards.': 1.0986123085021973, ""Instead, they have a wide variety of every consumer-grade card ever sold, which call for different blocking, tiling, and looping strategies in the computational kernels that underpin deep learning inference and training algorithms (hence, autotuning), which isn't discussed."": 1.0986123085021973, 'Sushi2 and Sukiyaki2 seem relatively young as projects.': 1.0986123085021973, 'They are not widely followed on github, there is no tutorial-style documentation for Sukiyaki2, and the implementations of e.g. convolution do not seem to have seen much engineering work.': 1.0986123085021973, 'Speed of evaluation seems to be one of the main focal points of the paper, but it’s not a major selling point to the ICLR audience because it seems about ¼ as fast as e.g. cuDNN on standard (e.g. AWS nodes)': 1.0986123085021973, 'NVidia hardware.': 1.0986123085021973, ""The performance of sukiyaki2 vs AMD's Caffe port is impressive."": 1.0986123085021973, 'Benchmarking on high-end compute server hardware is an interesting point of reference, but the questions that come to mind for me when reading this paper are': 1.0986123085021973, '(1) How would this fit into a live-video processing application on a mobile device': 1.0986123085021973, '(2) What kind of a “cluster” would this present to someone trying to do distributed deep learning in the wild by drawing on idle graphics cards: how much memory do they have, how might we handle data for training on such computers, what is the compute speed vs. communication latency and bandwidth.': 1.0986123085021973, 'Answers to these questions are out of scope for this paper, but it would have been interesting to see at least some preliminary discussion.': 1.0986123085021973, 'Novelty:': 1.0986123085021973, 'I’m not aware of a more mature WebCL-based HPC library.': 1.0986123085021973, 'Presentation:': 1.0986123085021973, 'Table 1 is hard to read because it is actually two tables with different formatting, and the numbers (speeds?) aren’t labeled with units.': 1.0986123085021973, 'While it is interesting that this can be done, and it will be useful for some, it does seem like the audience is not really the mainstream ICLR audience, who will not be afraid to use a conventional ML toolkit.': 1.0986123085021973, 'There is no new algorithm here, nor is there any UI/meta-design improvement to make it easier for non-experts to design and train neural network systems.': 1.0986123085021973, ""I think there will be relatively little interest at ICLR in such a paper that doesn't really advance the state of the art."": 1.0986123085021973, 'I have no significant objection to the presentation or methodology of the paper.': 1.0986123085021973, 'This paper presents a JavaScript framework including WebCL components for training and deploying deep neural networks.': 1.0986123085021973, 'The authors show that it is possible to reach competitive speeds with this technology, even higher speed than a compiled application with ViennaCL on AMD GPUs.': 1.0986123085021973, 'While remaining a little more than factor three slower than compiled high performance software on NVIDIA GPUs, it offers compelling possibilities for easily deployable training and application settings for deep learning.': 1.0986123085021973, 'My main points of criticism are:': 0.690719485282898, '1. In Tab. 4 different batch sizes are used. Even if this is due to technical limits for the Javascript library, it would only be fair to use the smaller batch sizes for the other frameworks as well (on the GPUs probably in favor of the presented framework).': 0.8156241178512573, '2. In Fig. 6, why not include more information in the graphs? Especially, as stated in the question, why not include the node.js values? While I do see the possible application with one server and many ""low performance"" clients, the setting of having a few dedicated high performance servers is quite likely. Even if not, these are good values to compare with. For the sake of consistency, please include in both subfigures Firefox, Chrome, node.js.': 0.6612603068351746, 'Apart from these points, well-written, understandable and conclusive.': 0.5721563696861267}"
403,https://openreview.net/forum?id=r1y1aawlg,"{'This paper proposes a method for iteratively improving the output of an existing machine translation by identifying potential mistakes and proposing a substitution, in this case using an attention-based model.': 1.3862943649291992, 'It is motivated by the method in which (it is assumed) human translators operate.': 1.3862943649291992, 'The paper is interesting and imaginative.': 1.132958173751831, 'However, in general terms, I am somewhat sceptical of this kind of approach': 1.3862943649291992, 'whereby a machine learning method is used to identify and correct the predictions of another method, or itself': 1.3536391258239746, 'because in the first case, if the new method is better, why not use it from the outset in place of the other method?': 1.3862910270690918, 'And in the second case, since the method has no new information compared to previously, why is it more likely to identify more past mistakes and correct them, than identify past correct terms and turn them into new errors?': 1.3862910270690918, 'That is unless there is a specific reason that an iterative approach can be shown to converge to a better solution when run over several epochs.': 1.3862923383712769, 'This paper does not convince me on these points.': 1.378653883934021, 'Indeed, unsurprisingly, the authors note that ""the probability of correctly labelling a word as a mistake remains low (62%)"" - this admittedly beats a random-chance baseline, but is not compared to something more meaningful, such as simply contrasting the existing system with a more powerful convolutional model and labelling all discrepancies as mistakes.': 1.3862844705581665, 'The oracle experiments are rather meaningless - they just serve to confirm that improving a translation is very easy when the existing mistakes have been identified, but much harder when they are not.': 1.3752254247665405, 'Although I do like the paper on the whole, to really convince me that main objective': 1.2595266103744507, 'ie. that **iterative** improvement is beneficial': 1.3862941265106201, 'has been satifactorily demonstrated it would be necessary to include stronger baselines - and in particular, to show that an iterative refinement scheme can really improve over a system closely matched to the attention-based model, both when used in isolation and when used in system combination with a PBMT system, and to demonstrate that the PBMT system is not simply acting as a regulariser for the attention-based model.': 1.1480255126953125, 'Minor comments:': 1.1520031690597534, 'I find the notation excessively fiddly at times - eg F^i = (F^{i,1}, F^{i,|F^i|}) - why use |F^i| here when F is a matrix, so surely the length of the slice is not dependent on i?': 0.901993453502655, 'In the discussion in section 4 - it seems that this still creates a mismatch between the training and test conditions - could anything be done about this?': 1.3862428665161133, 'This paper proposes a model for iteratively refining translation hypotheses.': 1.3860549926757812, 'This has several benefits, including enabling the translation model to condition not only on “left context”, but also on “right context”, and potentially enabling more rapid and/or accurate decoding.': 1.3861855268478394, 'The motivation given is that often translators (and text generators generally) use a process of refinement in generating outputs.': 1.3862942457199097, 'This is an important idea that is not currently playing much of a role in neural net models, so this paper is a welcome contribution.': 1.386293888092041, 'However, while I think this is an important first step, I do feel that the lack of in depth analysis suggests this paper is not quite ready for a final publication version.': 1.3710745573043823, 'For example, there are many possible connections to prior work in NLP, MT, and other parts of ML that could better contextualize this work (see specifics below).': 1.3272132873535156, 'More substantively, the model in Section 3 could be interpreted as a globally normalized, undirected (~CRF) translation model trained using a pseudo-likelihood objective.': 1.386293888092041, 'In this analysis, the model squarely back in the context of traditional discriminative translation models which used “undirected” features, and the decoding algorithm then looks more like a standard greedy hill-climbing algorithm (albeit with an extra heuristic model for selecting which variable to update), which is also nothing unfamiliar.': 1.386276364326477, 'My second criticism the limitations of the model are not well discussed.': 1.3862943649291992, 'For example, the proposed editing procedure cannot obviously remove or insert a word from a translation.': 1.386143445968628, 'While I think this is a reasonable assumption than can be made for the sake of tractability, it is very unfortunate since missing or extra words (esp. function words) are a common problem in the baseline models that are being used.': 1.386293888092041, 'Second, the standard objections to absolute positional models (vs. relative positional models) seem particularly crucial to bring up in this work, especially since they might make some of the design decisions a bit more justifiable.': 1.3787298202514648, 'Overall, this is an initial step in an interesting direction, but it needs more thorough analysis to demonstrate its value.': 1.3862943649291992, 'A more thorough analysis will also likely suggest some important model variants (for example: is a global translation model really the goal?': 1.3862802982330322, 'or is a post-editing model that fixes outputs with more complex operations more ideal?)': 1.3862943649291992, 'Related work:': 1.3814079761505127, 'I think that more could be done to put this work in the context of what has come before and what is currently going on in other parts of ML.': 0.7693477272987366, 'The idea of iterative refinement has been proposed in other problems that have complex output spaces, for example the DRAW model of Gregor et al. and the conditional adversarial network models used to refine images proposed recently by Isola et al.': 0.8051354885101318, 'In NLP, there have been several (stochastic) hill climbing approaches that have been proposed, such as the work on parsing by Zhang and Lei et al. (2014) who use random initial guesses and then do greedy hill climbing using a series of local refinements, the structured prediction cascades of Weiss and Taskar (2009) (not to mention general coarse-to-fine modeling strategies).': 1.2873799800872803, 'Finally, in MT, Arun et al. (2009) who use a Gibbs sampler to refine an initial guess to do decoding with a more complex model.': 1.1638052463531494, 'The use of an explicit error model is rather novel in the context of correction, but I would point out that although the proposed architecture is different, the discriminative word lexicon models of Mauser et al. (2009) and the neural version of the same by Ha et al. (2014) are similar in spirit.': 1.3858201503753662, 'There have also been a number of papers on “automatic post editing”, including the shared task at WMT2016, and there are not only standard test sets and baselines, but also datasets that could actually be used to train a post editing model with human-generated data.': 0.7791457176208496, 'Minimally using the techniques they described could be a useful foil for the models presented in this paper.': 1.3630281686782837, '“the target sentence is also embedded in distributional space via a lookup table” I think “distributional space” is a bit unclear.': 1.386289119720459, 'Maybe “the target sentence is represented in terms of distributed word representations via a lookup table” or something like that.': 1.384476661682129, '“distributional” suggests that the representations are derived from how the words are distributed in the corpus, whereas you are learning these representations on this task which isn’t modeling their distribution except only very indirectly.': 1.384637475013733, 'Section 3 Model: In Section 3, the model computes the distribution over target word types at an absolute position i in the output sentence, given the target language context and the source language context.': 1.3862943649291992, 'It is introduced as the model that is used to refine an existing hypothesis, but it is not immediately clear that the training data for this model (at least in this section) are the gold standard translations- “training set” could be interpreted in variety of ways.': 0.9363098740577698, 'This becomes clearer when reading later in the paper, but it’s a bit less clear when reading from the beginning for the first time.': 1.386283040046692, 'The use of a fixed sized window for representing the target word in context also seems to make something like a model 1 assumption since only the lexical features (and not any “alignment” or “positional” features) determine the attention.': 1.067914366722107, 'This should be clarified since it will make the assumptions of the model more transparent (and also suggest possible refinements to the model, e.g., including (representations) of i and j as components of S^j and T^i, which would allow model 2/3-like responses to be learned- although by leaving them out, the model might behave a bit more like a relative positional model than an absolute positional model, which is probably attractive).': 1.3848912715911865, 'Finally, some discussion for why a fixed window is used to represent the target sentence is worth including (since a global context is apparently used to represent the source sentence).': 1.383783221244812, 'The relationship between this training objective and pseudo likelihood (PL; Besag, 1975) might be worth mentioning.': 0.9787017703056335, 'Since I believe this is just a PL objective for a certain global model, this suggests alternative decoding algorithms, or certainly a different analysis of the proposed decoding objective.': 1.2330210208892822, 'The section 4 model conditions on the true context of a position in the true target, the current target guess, and the source.': 1.3785070180892944, 'I don’t completely understand the rationale for this model since at test time only two of these variables are available, and the replacement of y_ref with y_g seems hard to justify.': 1.3862943649291992, 'Disclosure: I am not an expert in machine translation algorithms.': 1.3862943649291992, 'Summary: A human translator does not come up with the final translation right': 1.3862943649291992, 'away.': 1.3862943649291992, 'Instead, (s)he uses an iterative process, starting with a rough draft': 1.3862943649291992, 'which is corrected little by little.': 1.3862943649291992, 'The idea behind this paper is to': 1.3862943649291992, 'implement a similar framework for an automated system.': 1.3862943649291992, 'This paper is generally well written.': 1.3862943649291992, 'It is my opinion however that drawings illustrating the architectures would help': 1.3862943649291992, 'understanding how the different algorithms relate to one another.': 1.3862943649291992, 'I like a lot that you report on a preliminary experiment to give an': 1.3862943649291992, 'intuition of how difficult the task is.': 1.3862943649291992, 'You should highlight the links': 1.3862943649291992, 'between the task of finding the errors in a guess translation and the task': 1.3862943649291992, 'of iterative refinement.': 1.3862943649291992, 'Could you use post-edited text to have a more': 1.3862943649291992, 'solid ground-truth?': 1.3862943649291992, 'My main concern with this paper is that in the experimental section the': 1.3862943649291992, 'iterative approach tries to improve upon only one type of machine translation.': 1.3862943649291992, 'Which immediately prompts these questions:': 1.3862943649291992, 'why did they choose that approach to improve on?': 1.3862943649291992, 'what is the part of the improvement that comes from the choice of the': 1.3862943649291992, 'initial draft (maybe it was a very bad draft)?': 1.3862943649291992, 'Here are some minor typos:': 1.3862943649291992, 'p.2: ... a lookup table that replace*S* each word... ?': 1.3862943649291992, 'p.3: I might be mistanken but it seems to me that j is used for two': 1.3862943649291992, 'different things.': 1.3862943649291992, 'It is confusing.': 1.3862943649291992, 'p.3: ...takes as input these representation*S* and outputs... ?': 1.3862943649291992, 'This work proposes to iteratively improve a sentence that has been generated from another MT system (in this case, a phrase-based system).': 1.3862943649291992, 'The authors use a neural net that takes in the source sentence and a window of (gold) words around the current target word, and predicts the current target word.': 1.3862943649291992, 'During testing, the gold words are replaced with the generated words.': 1.3862943649291992, 'While this is an interesting area of research, I am not convinced by the proposed approach, and experimental evidence is lacking.': 0.9632349014282227, 'Under the current framework, it is all but impossible for the model to do anything more than a rudimentary word replacement (e.g. it cannot change ""I went to the fridge even though I was not hungry"" to ""Although I was not hungry, I went to the fridge"").': 1.0466970205307007, 'The fact that only 0.6 words are edited on average supports this.': 1.3828096389770508, 'Specific comments:': 1.3862943649291992, 'It would be interesting to see what the improvements are if the baseline model is a neural system.': 0.6214243173599243, 'It seems strange (to me at least) that T^i and L(y^{-i|k}) only look at a window of 2k words.': 1.2812575101852417, 'It means that when making the decision to change the i-th word, the model does not know what was generated outside of the window?': 1.3814500570297241, 'Relatedly, the idea of changing individual words based on local (i.e. word-level) scores seems counterintuitive.': 1.3862922191619873, ""Given that we have the full generated sentence, don't we want a global score?"": 1.3862926959991455, 'Scoring at the sentence-level could also make room for non-greedy search strategies, which could potentially facilitate richer edits.': 1.3862578868865967, 'How does the approach compare to a model that simply re-ranks the k-best output?': 1.3862870931625366, 'Instead of editing, did you consider learning an encoder-decoder that takes in x, y_g, and generates y_ref?': 1.3862943649291992, 'When decoding you can attend to both x and y_g.': 1.3862943649291992, 'Iteratively improving a generated text was also explored in https://arxiv.org/pdf/1510.09202v1.pdf from a reinforcement learning angle.': 1.3862943649291992, ""I don't understand footnote 1."": 1.3862943649291992}"
404,https://openreview.net/forum?id=r1yjkAtxe,"{'The paper starts by pointing out the need for methods that perform both state and temporal representation learning for RL and which allow gaining insight into what is being learned (perhaps in order to allow a human operator to intervene if necessary).': 0.9343675374984741, 'This is a very important goal from a practical point of view, and it is great to see research in this direction.': 1.0940537452697754, 'For this reason, I would like to encourage the authors to pursue this further.': 0.982875406742096, 'However, I am not at all convinced that the current incarnation of this work is the right answer.': 0.41846323013305664, 'Part of the issues are more related to presentation, part may require rethinking.': 0.463519424200058, 'In order to get the ""interpretability"", the authors opt for some fairly specific ways of performing abstraction.': 1.0986120700836182, 'For example, their skills always start In a single skill initiation state, and likewise end in one state.': 1.0986123085021973, 'This seems unnecessarily restrictive, and it is not clear why this restriction is needed (other than convenience).': 1.0986123085021973, 'Similarly, clustering is the basis for forming the higher level states, and there is a specific kind of clustering used here.': 1.0986123085021973, 'Again, it is not clear why this has to be done via clustering as opposed to other methods.': 1.0986123085021973, 'Ensuring temporal coherence in the particular form employed also seems restrictive.': 0.5882448554039001, 'There is a reference to supplementary material where some of these choices are explained, but I could not find this in the posted version of the paper.': 1.0986052751541138, 'The authors should either explain clearly why these specific choices are necessary, or (even better) try to think if they can be relaxed while still keeping interpretability.': 1.0986123085021973, 'From a presentation point of view, the paper would benefit from formal definitions of AMDP and SAMDP, as well as from formal descriptions of the algorithms employed in constructing these representations (eg Bellman equations for the models, and update rules for the algorithms learning them).': 1.0910420417785645, 'While intuitions are given, the math is not precisely stated.': 1.0966883897781372, 'The overhead of constructing an SAMDP (computation time and space) should be clarified as well.': 1.098567247390747, 'The experiments are well carried out and it is nice to have both gridworld experiments, where visualization are easy to perform and understand, as well as Atari games (gridworld still have their place despite what other reviewers might say).': 1.0912848711013794, 'The results are positive, but because the proposed approach has many moving parts which rely on specific choices, significance and general ease of use are unclear at this point.': 0.45952528715133667, 'Perhaps having the complete supplementary would have helped in this respect.': 0.03930184245109558, 'Small comment: The two lines after Eq 2 contain typos in the notation and a wrong sign in the equation.': 0.6269392967224121, 'The framework of semi-Markov decision processes (SDMPs) has been long used to model skill learning and temporal abstraction in reinforcement learning.': 1.0986123085021973, 'This paper proposes a variant of such a model called a semi-aggregated MDP model.': 1.0985718965530396, 'The formalism of SAMDP is not defined clearly enough to merit serious attention.': 0.690871000289917, 'The approach is quasi heuristic and explained through examples rather than clear definition.': 1.0873984098434448, 'The work also lacks sufficient theoretical rigor.': 1.072838306427002, 'Simple experiments are proposed using 2D grid worlds to demonstrate skills.': 1.0986123085021973, 'Grid worlds have served their purpose long enough in reinforcement learning, and it is time to retire them.': 1.0986099243164062, 'More realistic domains are now routinely used and should be used in this paper as well.': 1.0986123085021973, 'The paper presents a method for visualization and analysis of policies from observed trajectories that the policies produce.': 1.0986119508743286, 'The method infers higher level skills and clusters states.': 1.0986123085021973, 'The result is a simplified, discrete higher-order state and action transition matrix.': 1.0986123085021973, 'This model can be used for analysis, modeling, and interpretation.': 1.0570409297943115, 'To construct semi-aggregated MDP the authors propose combining ideas for creating semi-MPDs and agregrated-MDPs.': 1.0986121892929077, 'The method consists of choosing features, state clustering, skill inference, reward and skill length inference, and model selection.': 1.0986123085021973, 'The method was demonstrated on a small grid-world problem, and DQN-trained agent for playing Atari games.': 1.0986123085021973, 'The authors correctly identify that tools and means for interpretibility of RL methods are important for analysis, and deployment of such methods for real-world applications.': 1.0984055995941162, 'This is particularly true in robotics and high-consequence systems.': 1.0986123085021973, 'The end-result of the presented method is a high-level transition matrix.': 0.7347908020019531, 'There is a big body of literature looking into hierarchical RL methods where lower level skills are combined with higher level policies.': 0.5079658627510071, 'The presented method has the similar result, but the advantage of the presented method is that it comes up with a structure and analyzes already trained agent, which is very interesting.': 1.0827820301055908, 'The paper would benefit from emphasizing this difference, and contrasting with the broader body of literature.': 1.0919078588485718, 'To build the model, the authors propose combining the ideas from two existing ideas, semi-MPDs and agregrated-MDPs with using modified k-means for state clustering.': 0.6304349899291992, 'It appears that the novelty of the presented method is limited.': 1.0986123085021973, 'The paper would have been stronger if the authors explicitly stated the contributions over combining existing methods, and better highlighted the practical utility of the method.': 1.0986123085021973, 'The evaluation section would be made stronger with more analytical results and precise evaluation, showing full strength of the method.': 1.0986123085021973, 'The paper is difficult to read.': 1.0986123085021973, 'To improve readability:': 1.0986123085021973, 'The Semi-Aggregated MDP section should include more precise description of the methods.': 1.0986123085021973, 'The narrative that builds intuition is welcome.': 1.0986123085021973, 'In addition to the existing narrative, algorithms and formulas where applicable should be included as well.': 1.0986123085021973, 'The paper should be self-contained.': 1.0986123085021973, 'For example, more background on Occams Razor principle should be included.': 1.0986123085021973, 'Reduce the number of acronyms, in particular similarly sounding acronyms.': 1.0986123085021973, 'Define acronyms before using.': 1.0986123085021973, 'Be more clear on the contributions, contrast with relevant literature, and the specific benefits of the presented method.': 1.0986123085021973, 'Fix typos, formatting mistakes etc., as they can be distracting for reading.': 1.0986123085021973, 'The approach of reverse engineering the hierarchy, and learning high-level transition matrix is very interesting and promising.': 1.0986123085021973, 'Perhaps the method can be used to outperform single network approach by using the model as an input to more specialized hierarchical trainers and learn complex behaviors more optimally then possible with one large network approach.': 1.0986123085021973, 'Unfortunately, the paper falls short in the novelty, precision, and clarity.': 1.0986123085021973}"
405,https://openreview.net/forum?id=rJ0-tY5xe,"{'This paper proposed an integration of memory network with reinforcement learning.': 1.0985872745513916, 'The experimental data is simple, but the model is very interesting and relatively novel.': 1.0986123085021973, 'There are some questions about the model:': 1.0906009674072266, '1. how does the model extend to the case with multiple variables in a single sentence?': 1.0824687480926514, '2. If the answer is out of vocabulary, how would the model handle it?': 0.38923025131225586, '3. I hope the authors can provide more analysis about the curriculum learning part, since it is very important for the RL model training.': 0.4856935143470764, '4. In the training, in each iteration, how the data samples were selected, by random or from simple one depth to multiple depth?': 0.47844937443733215, 'This paper introduces a nice dataset/data generator that creates bAbI like tasks, except where the questioning answering agent is required to clarify the values of some variables in order to succeed.': 1.0986123085021973, 'I think the baselines the authors use to test the tasks are appropriate.': 1.098610758781433, 'I am a bit worried that the tasks may be too easy (as the bAbI tasks have been), but still, I think locally these will be useful.': 1.098610281944275, 'If the generation code is well written, and the tasks are extensible, they may be useful for some time.': 1.0986123085021973, 'This paper investigates a set of tasks that augment the basic bAbI problems.': 1.0986117124557495, 'In particular, some of the people and objects in the scenarios are replaced with unknown variables.': 1.0986123085021973, 'Some of these variables must be known to solve the question, thus the agent must learn to query for the values of these variables.': 1.0986123085021973, 'Interestingly, one can now measure both the performance of the agent in correctly answering the question, and its efficiency in asking for the values of the correct unknown variables (and not variables that are unnecessary to answer the question).': 1.0071665048599243, 'This inferring of unknown variables goes beyond what is required for the vanilla version of the bAbI tasks, which are now more or less solved.': 1.0986071825027466, 'The paper is well-written, and the contributions are clear.': 1.0986123085021973, 'Due to the very limited vocabulary and structure of the bAbI problems in general, I think these tasks (and variants on them) should be viewed more as basic reasoning tasks than natural language understanding.': 1.0984001159667969, 'I’m not convinced by the claim of the paper that this really tests the ‘interaction’ capabilities of agents – while the task is phrased as a kind of interaction, I think it’s more aptly described by simply ‘inferring important unknown variables’, which (while important) is more related to reasoning.': 0.8570190668106079, 'I’m not sure whether the connection of this ability to ‘interaction’ is more a superficial one.': 1.0986089706420898, 'That being said, it is certainly true that conversational agents will need basic reasoning abilities to converse meaningfully with humans.': 1.0973280668258667, 'I sympathise with the general goal of the bAbI tasks, which is to test these reasoning abilities in synthetic environments, that are just complicated enough (but not more) to drive the construction of interesting models.': 1.0965880155563354, 'I am convinced by the authors that their extension to these tasks are interesting and worthy of future investigation, and thus I recommend the acceptance of the paper.': 1.098232626914978}"
406,https://openreview.net/forum?id=rJ0JwFcex,"{'The paper presents a method to synthesize string manipulation programs based on a set of input output pairs.': 0.9827058911323547, 'The paper focuses on a restricted class of programs based on a simple context free grammar sufficient to solve string manipulation tasks from the FlashFill benchmark.': 0.3652869462966919, ""A probabilistic generative model called Recursive-Reverse-Recursive Neural Network (R3NN) is presented that assigns a probability to each program's parse tree after a bottom-up and a top-down pass."": 0.8687703013420105, 'Results are presented on a synthetic dataset and a Microsoft Excel benchmark called FlashFill.': 1.049237847328186, 'The problem of program synthesis is important with a lot of recent interest from the deep learning community.': 1.0676127672195435, 'The approach taken in the paper based on parse trees and recursive neural networks seems interesting and promising.': 1.0985791683197021, 'However, the model seems too complicated and unclear at several places (details below).': 1.0310709476470947, 'On the negative side, the experiments are particularly weak, and the paper does not seem ready for publication based on its experimental results.': 1.0986043214797974, 'I was positive about the paper until I realized that the method obtains an accuracy of 38% on FlashFill benchmark when presented with only 5 input-output examples but the performance degrades to 29% when 10 input-output examples are used.': 1.078924298286438, 'This was surprising to the authors too, and they came up with some hypothesis to explain this phenomenon.': 0.6437395811080933, 'To me, this is a big problem indicating either a bug in the code or a severe shortcoming of the model.': 0.9065394401550293, 'Any model useful for program synthesis needs to be applicable to many input-output examples because most complicated programs require many examples to disambiguate the details of the program.': 1.098285436630249, 'Given the shortcoming of the experiments, I am not convinced that the paper is ready for publication.': 1.0611543655395508, 'Thus, I recommend weak reject.': 1.0986123085021973, 'I encourage the authors to address the comments below and resubmit as the general idea seems promising.': 1.0986090898513794, 'More comments:': 1.0986123085021973, 'I am unclear about the model at several places:': 1.098610520362854, 'How is the probability distribution normalized?': 1.0986123085021973, 'Given the nature of bottom-up top-down evaluation of the potentials, should one enumerate over different completions of a program and the compare their exponentiated potentials?': 1.0660901069641113, 'If so, does this restrict the applicability of the model to long programs as the enumeration of the completions gets prohibitively slow?': 0.40040910243988037, 'What if you only use 1 input-output pair for each program instead of 5?': 1.097489833831787, 'Do the results get better?': 1.0986123085021973, 'Section 5.1.2 is not clear to me.': 0.5459498763084412, 'Can you elaborate by potentially including some examples?': 0.9835421442985535, 'Does your input-output representation pre-supposes a fixed number of input-output examples across tasks (e.g. 5 or 10 for all of the tasks)?': 1.0951675176620483, 'Regarding the experiments,': 1.0986123085021973, 'Could you present some baseline results on FlashFill benchmark based on previous work?': 0.4743672013282776, 'Is your method only applicable to short programs?': 1.082924485206604, '(based on the choice of 13 for the number of instructions)': 0.5940458178520203, 'Does a program considered correct when it is identical to a test program, or is it considered correct when it succeeds on a set of held-out input-output pairs?': 1.0896962881088257, 'When using 100 or more program samples, do you report the accuracy of the best program out of 100 (i.e. recall) or do you first filter the programs based on training input-output pairs and then evaluate a program that is selected?': 0.6198669075965881, 'Your paper is well beyond the recommended limit of 8 pages.': 1.0986120700836182, 'please consider making it shorter.': 1.0986123085021973, 'This paper sets out to tackle the program synthesis problem: given a set of input/output pairs discover the program that generated them.': 1.0986123085021973, 'The authors propose a bipartite model, with one component that is a generative model of tree-structured programs and the other component an input/output pair encoder for conditioning.': 1.0986123085021973, 'They consider applying many variants of this basic model to a FlashFill DSL.': 1.0986123085021973, 'The experiments explore a practical dataset and achieve fine numbers.': 1.0986123085021973, 'The range of models considered, carefulness of the exposition, and basic experimental setup make this a valuable paper for an important area of research.': 1.0986123085021973, ""I have a few questions, which I think would strengthen the paper, but think it's worth accepting as is."": 1.0986123085021973, 'Questions/Comments:': 1.0986123085021973, 'The dataset is a good choice, because it is simple and easy to understand.': 1.0986123085021973, 'What is the effect of the ""rule based strategy"" for computing well formed input strings?': 1.0986123085021973, 'Clarify what ""backtracking search"" is?': 1.0986123085021973, 'I assume it is the same as trying to generate the latent function?': 1.0986123085021973, 'In general describing the accuracy as you increase the sample size could be summarize simply by reporting the log-probability of the latent function.': 1.0986123085021973, ""Perhaps it's worth reporting that?"": 1.0986123085021973, 'Not sure if I missed something.': 0.39318546652793884, 'This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel.': 1.0767791271209717, 'The approach is to model successive “extensions” of a program tree conditioned on some embedding of the input/output pairs.': 0.9724952578544617, 'Extension probabilities are computed as a function of leaf and production rule embeddings — one of the main contributions is the so-called “Recursive-Reverse-Recursive Neural Net” which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way).': 1.0985848903656006, 'There are many strong points about this paper.': 1.0961658954620361, 'In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future.': 1.0986123085021973, 'The R3NN idea is a good one and the authors motivate it quite well.': 0.5806238055229187, 'Moreover, the authors have explored many variants of this model to understand what works well and what does not.': 1.0947513580322266, 'Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read.': 1.0986123085021973, 'Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer.': 1.0986123085021973, 'And it’s unclear why the authors did not simply train on longer programs…  It also seems that the number of I/O pairs is fixed?': 1.0986123085021973, 'So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt…).': 1.0986123085021973, 'Overall however, I would certainly like to see this paper accepted at ICLR.': 1.0986123085021973, 'Other miscellaneous comments:': 1.0986123085021973, '* Too many e’s in the expansion probability expression — might be better just to write “Softmax”.': 1.0986123085021973, '* There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see).': 1.0986123085021973, '*': 1.0986123085021973, 'The authors claim that using hyperbolic tangent activation functions is important — I’d be interested in some more discussion on this and why something like ReLU would not be good.': 1.0986123085021973, '* It’s unclear to me how batching was done in this setting since each program has a different tree topology.': 1.0986123085021973, 'More discussion on this would be appreciated.': 1.0883643627166748, 'Related to this, it would be good to add details on optimization algorithm (SGD?': 1.0985805988311768, 'Adagrad?': 1.0985912084579468, 'Adam?), learning rate schedules and how weights were initialized.': 0.8335516452789307, 'At the moment, the results are not particularly reproducible.': 1.066334843635559, 'In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs?': 1.085591197013855, 'Or was it some other reason?)': 1.0939133167266846, '* There is a missing related work by Piech et al': 1.0986123085021973, '(Learning Program Embeddings…) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs).': 1.098164677619934}"
407,https://openreview.net/forum?id=rJ6DhP5xe,"{'Paper Summary': 1.0986123085021973, 'This paper evaluates the ability of two unsupervised learning models to learn a': 1.0986123085021973, 'generalizable physical intuition governing the stability of a tower of blocks.': 1.0986123085021973, 'The two models are (1)': 1.0986123085021973, 'A model that predicts the final state of the tower given': 1.0986123085021973, 'the initial state, and (2) A model that predicts the sequence of states of this': 1.0986123085021973, 'tower over time given the initial state.': 1.0986123085021973, 'Generalizability is evaluated by': 1.0986123085021973, 'training a model on towers made of a certain number of blocks but testing on': 1.0986123085021973, 'towers made of a different number of blocks.': 1.0986123085021973, 'Strengths': 1.0986123085021973, 'This paper explores an interesting way to evaluate representations in terms of': 1.0986123085021973, 'their generalizability to out-of-domain data, as opposed to more standard': 1.0986123085021973, 'methods which use train and test data drawn from the same distribution.': 1.0986119508743286, 'Experiments show that the predictions of deep unsupervised learning models on': 1.0986123085021973, 'such out-of-domain data do seem to help, even though the models were not': 1.0986123085021973, 'trained explicitly to help in this way.': 1.0986123085021973, 'Weaknesses': 1.0986123085021973, 'Based on Fig 4, it seems that the models trained on 3 blocks (3CD, 3CLD)': 1.0986123085021973, '``generalize"" to 4 and 5 blocks.': 1.0986123085021973, 'However, it is plausible that these models': 1.0986123085021973, 'only pay attention to the bottom 3 blocks of the 4 or 5 block towers in order to': 1.0986123085021973, 'determine their stability.': 1.0986123085021973, 'This would work correctly a significant fraction of': 1.0986123085021973, 'the time.': 1.0986123085021973, 'Therefore, the models might actually be overfitting to 3 block towers': 1.0986123085021973, 'and not really generalizing the physics of these blocks.': 1.0986123085021973, 'Is this a possibility ?': 1.0986123085021973, 'I think more careful controls are needed to make the claim that the features': 1.0986123085021973, 'actually generalize.': 1.0986123085021973, 'For example, test the 3 block model on a 5 block test set': 0.9044345617294312, 'but only make the 4th or 5th block unstable.': 0.6278451681137085, 'If the model still works well, then': 1.0985699892044067, 'we could argue that it is actually generalizing.': 0.479709267616272, 'The experimental analysis seems somewhat preliminary and can be improved.': 1.0640066862106323, 'In': 1.0986123085021973, 'particular, it would help to see visualizations of what the final state looks': 0.36272528767585754, 'like for models trained on 3 blocks but test on 5 (and vice-versa).': 0.9070860147476196, 'That would': 1.0986098051071167, 'help understand if the generalization is really working.': 0.42230933904647827, 'The discriminative': 0.9659993648529053, 'objective gives some indication of this, but might obfuscate some aspects of': 0.8739361763000488, 'physical realism that we would really want to test.': 0.5025569796562195, 'In Figure 1 and 2, it is': 0.9679492712020874, 'not mentioned whether these models are being tested on the same number of blocks': 0.47545918822288513, 'they were trained for.': 1.0982441902160645, 'It seems that the task of the predicting the final state is really a binary': 0.8728146553039551, 'task - whether or not to remove the blocks and replace them with gray': 0.4866871237754822, 'background.': 1.0968621969223022, 'The places where the blocks land in case of a fall is probably quite': 0.529195249080658, 'hard to predict, even for a human, because small perturbations can have a big': 0.6784629225730896, 'impact on the final state.': 0.9778547286987305, 'It seems that in order to get a generalizable': 0.8084172606468201, 'physics model, it could help to have a high frame rate sequence prediction task.': 0.4954576790332794, 'Currently, the video is subsampled to only 5 time steps.': 0.29722392559051514, 'Quality': 1.0986121892929077, 'A more detailed analysis and careful choices of testing conditions can increase': 1.0959489345550537, 'the quality of this paper and strengthen the conclusions that can be drawn from': 0.7565893530845642, 'this work.': 1.0296432971954346, 'Clarity': 0.45476606488227844, 'The paper is well written and easy to follow.': 1.0981172323226929, 'Originality': 1.0980448722839355, 'The particular setting explored in this paper is novel.': 1.077370524406433, 'Significance': 1.0986123085021973, 'This paper provides a valuable addition to the growing work on': 1.0069091320037842, 'transferability/generalizability as an evaluation method for unsupervised': 0.5916973948478699, 'learning.': 1.007598638534546, 'However, more detailed experiments and analysis are needed to make': 0.7832736968994141, 'this paper significant enough for an ICLR paper.': 1.0321124792099, 'Minor comments and suggestions': 1.0939513444900513, 'The acronym IPE is used without mentioning its expansion anywhere in the text.': 1.0986100435256958, 'There seems to be a strong dependence on data augmentation.': 0.9549028277397156, 'But given that': 1.0978267192840576, 'this is a synthetic dataset, it is not clear why more data was not generated': 0.9582279324531555, 'in the first place.': 1.0842431783676147, 'Table 3 : It might be better to draw this as a 9 x 3 grid : 9 rows corresponding to the': 1.0884820222854614, 'models and 3 columns corresponding to the test sets.': 0.28483784198760986, 'Mentioning the train set is': 0.5887367725372314, 'redundant since it is already captured in the model name.': 0.16564972698688507, 'That might make it': 0.8012482523918152, 'easier to read.': 0.9995415806770325, 'Overall': 1.0986123085021973, 'This is an excellent direction to work and preliminary results look great.': 1.091125726699829, 'However, more controls and detailed analysis are needed to make strong': 0.31816694140434265, 'conclusions from these experiments.': 1.0964473485946655, 'Summary': 0.5139899253845215, '===': 1.0985952615737915, 'This paper trains models to predict whether block towers will fall down': 1.0986123085021973, 'or not.': 0.40583279728889465, 'It shows that an additional model of how blocks fall down': 1.0986123085021973, '(predicting a sequence of frames via unsupervised learning) helps the original': 0.41730374097824097, 'supervised task to generalize better.': 1.0986078977584839, 'This work constructs a synthetic dataset of block towers containing': 1.0986123085021973, '3 to 5 blocks places in more or less precarious positions.': 0.4272201359272003, 'It includes both': 1.0986123085021973, ""labels (the tower falls or not) and video frame sequences of the tower's"": 0.8592966794967651, 'evolution according to a physics engine.': 1.0983800888061523, 'Three kinds of models are trained.': 1.0984623432159424, 'The first (S) simply takes an image of a': 1.0985890626907349, ""tower's starting state and predicts whether it will fall or not."": 0.9925095438957214, 'The': 1.0958503484725952, 'other two types (CD and CLD) take both the start state and the final state of the': 0.7496429085731506, 'tower (after it has or has not fallen) and predict whether it has fallen or not,': 1.0985064506530762, 'they only differ in how the final state is provided.': 0.5924776792526245, 'One model (ConvDeconv, CD)': 0.7594910264015198, 'predicts the final frame using only the start frame and the other': 0.8131005764007568, '(ConvLSTMDeconv) predicts a series of intermediate frames before coming': 0.904268205165863, 'to the final frame.': 0.704362154006958, 'Both CD and CLD are unsupervised.': 0.48757204413414, 'Each model is trained on towers of a particular heigh and tested on': 1.0981669425964355, 'towers with an unseen height.': 0.600793719291687, 'When the height of the train towers': 1.098604440689087, 'is the same as the test tower height, all models perform roughly the same': 0.7073153257369995, '(with in a few percentage points).': 0.120817631483078, 'However, when the test height is': 0.9735585451126099, 'greater than the train height it is extremely helpful to explicitly': 0.8595402240753174, 'model the final state of the block tower before deciding whether it has': 0.8842365145683289, 'fallen or not (via CD and CLD models).': 1.0775392055511475, 'Pros': 1.0986123085021973, '* There are very clear (large) gains in accuracy from adding an unsupervised': 0.5621756911277771, 'final frame predictor.': 1.079298973083496, 'Because the generalization problem is also particularly': 0.38102102279663086, 'clear (train and test with different numbers of blocks), this makes for': 0.37797942757606506, 'a very nice toy example where unsupervised learning provides a clear benefit.': 0.6755442023277283, '*': 1.098611831665039, 'The writing is clear.': 1.0986123085021973, 'Cons': 1.0986123085021973, 'My one major concern is a lack of more detailed analysis.': 1.0427467823028564, 'The paper': 1.098610758781433, 'establishes a base result, but does not explore the idea to the extent': 0.7624091506004333, 'to which I think an ICLR paper should.': 0.4561980366706848, 'Two general directions for potential': 1.097776174545288, 'analysis follow:': 0.4801986813545227, '* Is this a limitation of the particular way the block towers are rendered?': 0.9665250778198242, 'The LSTM model could be limited by the sub-sampling strategy.': 1.0985360145568848, 'It looks': 1.0985509157180786, 'like the sampling may be too coarse from the provided examples.': 0.4535166323184967, 'For the': 1.0930941104888916, 'two towers in figure 2 that fall, they have fallen after only 1 or 2': 0.7548578977584839, 'time steps.': 0.6311091184616089, 'How quickly do most towers fall?': 1.0986123085021973, 'What happens if the LSTM': 1.0738686323165894, 'is trained at a higher frame rate?': 0.15323086082935333, 'What is the frame-by-frame video': 0.5190954208374023, 'prediction accuracy of the LSTM?': 0.6192584037780762, '(Is that quantity meaningful?)': 0.797375500202179, 'How much does performance improve if the LSTM is provided ground truth': 0.8492648601531982, 'for only the first k frames?': 1.0391864776611328, '* Why is generalization to different block heights limited?': 0.9219180345535278, 'Is it limited by model capacity or architecture design?': 1.0985515117645264, 'What would happen if the S-type models were made wider/deeper with the CD/CLD': 1.096590518951416, 'fall predictor capacity fixed?': 0.36097344756126404, 'Is it limited by the precise task specification?': 0.9373759627342224, 'What would happen if networks were trained with towers of multiple heights': 1.0986121892929077, '(apparently this experiment is in the works)?': 0.49804264307022095, 'I appreciate that one experiment in this direction was provided.': 1.0985416173934937, 'Is it limited by training procedure?': 1.0314180850982666, 'What if the CD/CLD models were trained': 0.5599236488342285, 'in an end-to-end manner?': 1.0986123085021973, 'What if the double frame fall predictor were trained': 1.0986123085021973, 'with ground truth final frames instead of generated final frames?': 1.0973557233810425, 'Minor concerns:': 1.097184419631958, '* It may be asking too much to re-implement Zhang et.': 0.8376855254173279, 'al. 2016 and PhysNet': 1.0825690031051636, 'for the newly proposed dataset, but it would help the paper to have baselines': 0.962881326675415, 'which are directly comparable to the proposed results.': 1.0984240770339966, 'I do not think this': 1.005387306213379, 'is a major concern because the point of the paper is about the role of': 0.7771197557449341, 'unsupervised learning rather than creating the best fall prediction network.': 0.18391618132591248, 'The auxiliary experiment provided is motivated as follows:': 1.0961599349975586, '""One solution could be to train these models to predict how many blocks have': 0.926314115524292, 'fallen instead of a binary stability label.""': 0.8647950291633606, 'Is there a clear intuition for why this might make the task easier?': 0.9272274971008301, '* Will the dataset, or code to generate it, be released?': 0.5931692123413086, 'Overall Evaluation': 1.0985878705978394, 'The writing, presentation, and experiments are clear and of high enough': 1.0986095666885376, 'quality for ICLR.': 1.0712004899978638, 'However the experiments provide limited analysis past': 1.0986120700836182, 'the main result (see comments above).': 1.094887137413025, 'The idea is a clear extension of ideas behind unsupervised': 0.850190281867981, 'learning (video prediction) and recent results in intuitive physics from': 0.7213930487632751, 'Lerer et.': 1.0978970527648926, 'al. 2016 and Zhang et.': 0.40310174226760864, 'al. 2016, so there is only moderate novelty.': 0.4648517668247223, 'However, these results would provide a valuable addition to the literation,': 0.03971850872039795, 'especially if more analysis was provided.': 0.002909194678068161, '*** Paper Summary ***': 0.6113696098327637, 'The paper proposes to learn a predictive model (aka predict the next video frames given an input image) and uses the prediction from this model to improve a supervised classifier.': 1.0986120700836182, 'The effectiveness of the approach is illustrated on a tower stability dataset.': 0.40583211183547974, '*** Review Summary ***': 0.9932081699371338, 'This work seems rather preliminary in terms of experimentation and using forward modeling as pretraining has already been proposed and applied to video and text classification tasks.': 1.0533223152160645, 'Discussion on related work is insufficient.': 1.0986123085021973, 'The end task choice (will there be motion?) might not be the best to advocate for unsupervised training.': 1.0986123085021973, '*** Detailed Review ***': 0.5420097708702087, 'This work seems rather preliminary.': 1.0735043287277222, 'There is no comparison with alternative semi-supervised strategies.': 0.939121425151825, 'Any approach that consider the next frames as latent variables (or privileged information) can be considered.': 1.0986056327819824, 'Also I am not sure if the supervised stability prediction model is actually needed once the next frame is predicted.': 1.0986123085021973, 'Basically the task can be reduced to predict whether there will be motion in the video following the current frame or not (for instance comparing the first frame and last prediction or the density of gray in the top part of the video might work just as well).': 1.0986123085021973, 'Also training a model to predict the presence of motion from the unsupervised data only would probably do very well.': 1.0986120700836182, 'I would suggest to stir away from task where the label can be inferred trivially from the unsupervised data, meaning that unlabeled videos can be considered labeled frames in that case.': 1.0986123085021973, 'The related work section misses a discussion on previous work on learning unsupervised features from video (through predictive models, dimensionality reduction...) for helping classification of still images or videos [Fathi et al 2008; Mabahi et al 2009; Srivastava et al 2015].': 1.0986087322235107, 'More recently, Wang and Gupta (2015) have obtained excellent ImageNet results from features pre trained on unlabeled videos.': 1.0985701084136963, 'Vondrick et al (2016) have shown that generative models of video can help initialize models for video classification tasks.': 1.097716212272644, 'Also in the field of text classification, pre training of classifier with a language model is a form predictive modeling, e.g. Dai & Le 2015.': 1.0985993146896362, 'I would also suggest to report test results on the dataset from Lerrer et al 2016 (I understand that you need your own videos to pre train the predictive model) but stability prediction only require still images.': 1.0985944271087646, 'Overall, I feel the experimental section is too preliminary.': 0.8910129070281982, 'It would be better to focus on a task where solving the unsupervised task does not necessarily imply that the supervised task is trivially solved (or conversely that a simple rule can turn the unlabeled data into label data).': 1.0985057353973389, '*** Reference ***': 0.9835155010223389, 'Fathi, Alireza, and Greg Mori.': 0.5162194967269897, '""Action recognition by learning mid-level motion features.""': 1.0980805158615112, 'Computer Vision and Pattern Recognition, 2008.': 1.0359715223312378, 'CVPR 2008.': 1.0507735013961792, 'IEEE Conference on.': 1.0986123085021973, 'IEEE, 2008.': 1.098592758178711, 'Mobahi, Hossein, Ronan Collobert, and Jason Weston.': 0.530709981918335, '""Deep learning from temporal coherence in video.""': 0.7449408769607544, 'Proceedings of the 26th Annual International Conference on Machine Learning.': 0.36221081018447876, 'ACM, 2009.': 1.0986123085021973, 'Srivastava, Nitish, Elman Mansimov, and Ruslan Salakhutdinov.': 0.742124617099762, '""Unsupervised learning of video representations using lstms.""': 0.5137863159179688, 'CoRR, abs/1502.04681 2 (2015).': 1.0907444953918457, 'A. Dai, Q.V. Le, Semi-supervised Sequence Learning, NIPS, 2015': 1.0986123085021973, 'Unsupervised learning of visual representations using videos, X Wang, A Gupta, ICCV 2015': 1.0897780656814575, 'Generating videos with scene dynamics, C Vondrick, H Pirsiavash, A Torralba, NIPS 16': 0.9736596345901489}"
408,https://openreview.net/forum?id=rJ8Je4clg,"{'In this paper, the authors proposed a extension to the DQN algorithm by introducing both an upper and lower bound to the optimal Q function.': 0.6671148538589478, 'The authors show experimentally that this approach improves the data efficiency quite dramatically such that they can achieve or even supersede the performance of DQN that is trained in 8 days.': 1.098585605621338, 'The idea is novel to the best of my knowledge and the improvement over DQN seems very significant.': 1.098605990409851, 'Recently, Remi et al have introduced the Retrace algorithm which can make use of multi-step returns to estimate Q values.': 1.0985304117202759, 'As I suspect, some of the improvements that comes from the bounds is due to the fact that multi-step returns is used effectively.': 1.0986123085021973, 'Therefore, I was wondering whether the authors have tried any approach like Retrace or Tree backup by Precup et al. and if so how do these methods stack up against the proposed method.': 1.0985904932022095, 'The author have very impressive results and the paper proposes a very promising direction for future research and as a result I would like to make a few suggestions:': 1.0832403898239136, 'First, it would be great if the authors could include a discussion about deterministic vs stochastic MDPs.': 0.7059066891670227, 'Second, it would be great if the authors could include some kind of theoretically analysis about the approach.': 1.0986088514328003, 'Finally, I would like to apologize for the late review.': 1.0986123085021973, 'In this paper, a Q-Learning variant is proposed that aims at ""propagating"" rewards faster by adding extra costs corresponding to bounds on the Q function, that are based on both past and future rewards.': 1.0986123085021973, 'This leads to faster convergence, as shown on the Atari Learning Environment benchmark.': 0.8459527492523193, 'The paper is well written and easy to follow.': 1.0986123085021973, 'The core idea of using relaxed inequality bounds in the optimization problem is original to the best of my knowledge, and results seem promising.': 1.0715065002441406, 'This submission however has a number of important shortcomings that prevent me from recommending it for publication at ICLR:': 1.0986123085021973, '1. The theoretical justification and analysis is very limited. As far as I can tell the bounds as defined require a deterministic reward to hold, which is rarely the case in practice. There is also the fact that the bounds are computed using the so-called ""target network"" with different parameters theta-, which is another source of discrepancy. And even before that, the bounds hold for Q* but are applied on Q for which they may not be valid until Q gets close enough to Q*. It also looks weird to take the max over k in (1, ..., K) when the definition of L_j,k makes it look like the max has to be L_j,1 (or even L_j,0, but I am not sure why that one is not considered), since L*_j,0 >= L*_j,1 >= ... >= L*_j,K. Neither of these issues are discussed in the paper, and there is no theoretical analysis of the convergence properties of the proposed method.': 1.0074785947799683, '[Update: some of these concerns were addressed in OpenReview comments]': 0.48747408390045166, '2. The empirical evaluation does not compensate, in my opinion, for the lack of theory. First, since there are two bounds introduced, I would have expected ""ablative"" experiments showing the improvement brought by each one independently. It is also unfortunate that the authors did not have time to let their algorithm run longer, since as shown in Fig. 1 there remain a significant amount of games where it performs worse compared to DQN. In addition, comparisons are limited to vanilla DQN and DDQN: I believe it would have been important to compare to other ways of incorporating longer-term rewards, like n-step Q-Learning or actor-critic. Finally, there is no experiment demonstrating that the proposed algorithm can indeed improve other existing DQN variants: I agree with the author when they say ""We believe that our method can be readily combined with other techniques developed for DQN"", however providing actual results showing this would have made the paper much stronger.': 1.0434157848358154, 'In conclusion, I do believe this line of research is worth pursuing, but also that additional work is required to really prove and understand its benefits.': 1.0986123085021973, 'Minor comments:': 1.0986123085021973, 'Instead of citing the arxiv Wang et al (2015), it would be best to cite the 2016 ICML paper': 1.0986123085021973, 'The description of Q-Learning in section 3 says ""The estimated future reward is computed based on the current state s or a series of past states s_t if available.""': 1.0986123085021973, 'I am not sure what you mean by ""a series of past states"", since Q is defined as Q(s, a) and thus can only take the current state s as input, when defined this way.': 1.0986123085021973, 'The introduction of R_j in Alg. 1 is confusing since its use is only explained later in the text (in section 5 ""In addition, we also incorporate the discounted return R_j in the lower bound calculation to further stabilize the training"")': 1.0986123085021973, 'In Fig.': 1.0986123085021973, 'S1 the legend should not say ""10M"" since the plot is from 1M to 10M': 1.0986123085021973, 'This paper proposes an improvement to the q-learning/DQN algorithm using constraint bounds on the q-function, which are implemented using quadratic penalties in practice.': 1.0986123085021973, 'The proposed change is simple to implement and remarkably effective, enabling both significantly faster learning and better performance on the suite of Atari games.': 1.0986123085021973, 'I have a few suggestions for improving the paper:': 1.0986123085021973, 'The paper could be improved by including qualitative observations of the learning process with and without the proposed penalties, to better understand the scenarios in which this method is most useful, and to develop a better understanding of its empirical performance.': 1.0986123085021973, 'It would also be nice to include zoomed-out versions of the learned curves in Figure 3, as the DQN has yet to converge.': 1.0986123085021973, 'Error bars would also be helpful to judge stability over different random seeds.': 1.0986123085021973, 'As mentioned in the paper, this method could be combined with D-DQN.': 1.0986123085021973, 'It would be interesting to see this combination, to see if the two are complementary.': 1.0986123085021973, 'Do you plan to do this in the final version?': 1.0986123085021973, 'Also, a couple questions:': 1.0986123085021973, 'Do you think the performance of this method would continue to improve after 10M frames?': 1.0986123085021973, 'Could the ideas in this paper be extended to methods for continuous control like DDPG or NAF?': 1.0986123085021973}"
409,https://openreview.net/forum?id=rJ8uNptgl,"{'This paper proposes a network quantization method for compressing the parameters of neural networks, therefore, compressing the amount of storage needed for the parameters.': 1.0986123085021973, 'The authors assume that the network is already pruned and aim for compressing the non-pruned parameters.': 1.0986123085021973, 'The problem of network compression is a well-motivated problem and of interest to the ICLR community.': 1.0986123085021973, 'The main drawback of the paper is its novelty.': 1.0986123085021973, 'The paper is heavily built on the results of Han 2015 and only marginally extends Han 2015 to overcome its drawbacks.': 1.0986123085021973, 'It should be noted that the proposed method in this paper has not been proposed before.': 1.0986123085021973, 'The paper is well-structured and easy to follow.': 1.0986123085021973, 'Although it heavily builds on Han 2015, it is still much longer than Han 2015.': 1.0986123085021973, 'I believe that there is still some redundancy in the paper.': 1.0986123085021973, 'The experiments section starts on Page 12': 1.0986123085021973, 'whereas for Han 2015 the experiments start on page 5.': 1.0986123085021973, 'Therefore, I believe much of the introductory text is redundant and can be efficiently cut.': 1.0986123085021973, 'Experimental results in the paper show good compression performance compared to Han 2015 while losing very little accuracy.': 1.0986123085021973, 'Can the authors mention why there is no comparison with Hang 2015 on ResNet in Table 1?': 1.0986123085021973, 'Some comments:': 1.0986123085021973, '1) It is not clear whether the procedure depicted in figure 1 is the authors’ contribution or has been in the literature.': 1.0986114740371704, '2) In section 4.1 the authors approximate the hessian matrix with a diagonal matrix.': 1.0986123085021973, 'Can the authors please explain how this approximation affects the final compression?': 1.0986123085021973, 'Also how much does one lose by making such an approximation?': 1.0986123085021973, 'minor typos (These are for the revised version of the paper):': 1.0986123085021973, '1) Page 2, Parag 3, 3rd line from the end: fined-tuned -> fine-tuned': 1.0986123085021973, '2) Page 2, one para to the end, last line: assigned for -> assigned to': 1.0986123085021973, '3) Page 5, line 2, same as above': 1.0986123085021973, '4) Page 8, Section 5, Line 3: explore -> explored': 1.0986123085021973, 'This paper proposes a novel neural network compression technique.': 1.0986123085021973, 'The goal is to compress maximally the network specification via parameter quantisation with a minimum impact on the expected loss.': 1.0986123085021973, 'It assumes pruning of the network parameters has already been performed, and only considers the quantisation of the individual scalar parameters of the network.': 1.0986123085021973, 'In contrast to previous work (Han et al. 2015a, Gong et al. 2014)': 1.0986123085021973, 'the proposed approach takes into account the effect of the weight quantisation on the loss function that is used to train the network, and also takes into account the effect on a variable-length binary encoding of the cluster centers used for the quantisation.': 1.093955397605896, 'Unfortunately, the submitted paper is 20 pages, rather than the 8 recommended.': 1.0986123085021973, 'The length of the paper seems unjustified to me, since the first three sections (first five pages) are very generic and redundant can be largely compressed or skipped (including figures 1 and 2).': 1.098611831665039, 'Although not a strict requirement by the submission guidelines, I would suggest the authors to compress their paper to 8 pages, this will improve the readability of the paper.': 1.0986123085021973, 'To take into account the impact on the network’s loss the authors propose to use a second order approximation of the cost function of the loss.': 1.0986071825027466, 'In the case of weights that originally constitute a local minimum of the loss, this leads to a formulation of the impact of the weight quantization on the loss in terms of a weighted k-means clustering objective, where the weights are derived from the hessian of the loss function at the original weights.': 1.098305344581604, 'The hessian can be computed efficiently using a back-propagation algorithm similar to that used to compute the gradient, as shown in cited work from the literature.': 1.0986123085021973, 'The authors also propose to alternatively use a second-order moment term used by the Adam optimisation algorithm, since it can be loosely interpreted as an approximate Hessian.': 1.0986123085021973, 'In section 4.5 the authors argue that with their approach it is more natural to quantise weights across all layers together, due to the hessian weighting which takes into account the variable impact across layers of quantisation errors on the network performance.': 1.0986123085021973, 'The last statement in this section, however, was not clear to me:': 1.0948171615600586, '“In such deep neural networks, quantising network parameters of all layers together is more efficient since optimizing layer-by-layer clustering jointly across all layers requires exponential time complexity with respect to the number of layers.”': 1.0986074209213257, 'Perhaps the authors could elaborate a bit more on this point?': 1.077183485031128, 'In section 5 the authors develop methods to take into account the code length of the weight quantisation in the clustering process.': 1.0924102067947388, 'The first method described by the authors (based on previous work), is uniform quantisation of the weight space, which is then further optimised by their hessian-weighted clustering procedure from section 4.': 0.936262845993042, 'For the case of nonuniform codeword lengths to encode the cluster indices, the authors develop a modification of the Hessian weighted k-means algorithm in which the code length of each cluster is also taken into account, weighted by a factor lambda.': 1.0984238386154175, 'Different values of lambda give rise to different compression-accuracy trade-offs, and the authors propose to cluster weights for a variety of lambda values and then pick the most accurate solution obtained, given a certain compression budget.': 1.092186689376831, 'In section 6 the authors report a number of experimental results that were obtained with the proposed methods, and compare these results to those obtained by the layer-wise compression technique of Han et al 2015, and to the uncompressed models.': 1.057296872138977, 'For these experiments the authors used three datasets, MNIST, CIFAR10 and ImageNet, with data-set specific architectures taken from the literature.': 1.0454649925231934, 'These results suggest a consistent and significant advantage of the proposed method over the work of Han et al.': 0.523701548576355, 'Comparison to the work of Gong et al 2014 is not made.': 1.0986123085021973, 'The results illustrate the advantage of the hessian weighted k-means clustering criterion, and the advantages of the variable bitrate cluster encoding.': 0.716560423374176, 'In conclusion I would say that this is quite interesting work, although the technical novelty seems limited (but I’m not a quantisation expert).': 1.0985441207885742, 'Interestingly, the proposed techniques do not seem specific to deep conv nets, but rather generically applicable to quantisation of parameters of any model with an associated cost function for which a locally quadratic approximation can be formulated.': 0.837236762046814, 'It would be useful if the authors would discuss this point in their paper.': 1.0958306789398193, 'The paper has two main contributions:': 1.0986123085021973, '1) Shows that uniform quantization works well with variable length (Huffman) coding': 1.0986123085021973, '2) Improves fixed-length quantization by proposing the Hessian-weighted k-means, as opposed to standardly used vanilla k-means.': 1.0986123085021973, 'The Hessian weighting is well motivated, and it is also explained how to use an efficient approximation ""for free"" when using the Adam optimizer, which is quite neat.': 1.098609209060669, 'As opposed to vanilla k-means, one of the main benefits of this approach (apart from improved performance) is that no tuning on per-layer compression rates is required, as this is achieved for free.': 1.0986123085021973, ""To conclude, I like the paper: (1) is not really novel but it doesn't seem other papers have done this before"": 0.818455696105957, ""so it's nice to know it works well, and (2) is quite neat and also works well."": 1.0946240425109863, 'The paper is easy to follow, results are good.': 1.093316674232483, ""My only complaint is that it's a bit too long."": 0.8993618488311768, 'Minor note - I still don\'t understand the parts about storing ""additional bits for each binary codeword for layer indication"" when doing layer-by-layer quantization. What\'s the problem of just having an array of quantized weight values for each layer, i.e. q[0][:] would store all quantized weights for layer 0, q[1][:] for layer 1 etc, and for each layer you would have the codebook. So the only overhead over joint quantization is storing the codebook for each layer, which is insignificant. I don\'t understand the ""additional bit"" part. But anyway, this is really not a important as I don\'t think it affects the paper at all, just authors might want to additionally clarify this point (maybe I\'m missing something obvious, but if I am then it\'s likely some other people will as well).': 1.0985535383224487}"
410,https://openreview.net/forum?id=rJEgeXFex,"{'This is a well written, organized, and presented paper that I enjoyed reading.': 1.0985660552978516, 'I commend the authors on their attention to the narrative and the explanations.': 1.0985100269317627, 'While it did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using, given the record of billing codes.': 1.0985699892044067, 'The dataset they use is impressive and useful and, frankly, more interesting than the typical toy datasets in machine learning.': 0.9947570562362671, 'That said, the investigation of those results was not as deep as I thought it should have been in an empirical/applications paper.': 1.0986123085021973, 'Despite their focus on the application, I was encouraged to see the authors use cutting edge choices (eg Keras, adadelta, etc) in their architecture.': 1.0986123085021973, 'A few points of criticism:': 0.713149905204773, 'The numerical results are in my view too brief.': 1.0986123085021973, 'Fig 4 is anecdotal, Fig 5 is essentially a negative result (tSNE is only in some places interpretable), so that leaves Table 1.': 1.097211480140686, 'I recognize there is only one dataset, but this does not offer a vast amount of empirical evidence and analysis that one might expect out of a paper with no major algorithmic/theoretical advances.': 1.0986120700836182, ""To be clear I don't think this is disqualifying or deeply concerning; I simply found it a bit underwhelming."": 1.0985993146896362, 'To be constructive, re the results I would recommend removing Fig 5 and replacing that with some more meaningful analysis of performance.': 1.0986123085021973, 'I found Fig 5 to be mostly uninformative, other than as a negative result, which I think can be stated in a sentence rather than in a large figure.': 1.0983186960220337, 'There is a bit of jargon used and expertise required that may not be familiar to the typical ICLR reader.': 1.0931528806686401, 'I saw that another reviewer suggested perhaps ICLR is not the right venue for this work.': 0.45587053894996643, ""While I certainly see the reviewer's point that a medical or healthcare venue may be more suitable, I do want to cast my vote of keeping this paper here... our community benefits from more thoughtful and in depth applications."": 1.0986037254333496, 'Instead I think this can be addressed by tightening up those points of jargon and making the results more easy to evaluate by an ICLR reader (that is, as it stands now researchers without medical experience have to take your results after Table 1 on faith, rather than getting to apply their well-trained quantitative eye).': 1.098484754562378, 'Overall, a nice paper.': 0.6441004276275635, 'In light of the detailed author responses and further updates to the manuscript, I am raising my score to an 8 and reiterating my support for this paper.': 1.0986123085021973, 'I think it will be among the strongest non-traditional applied deep learning work at ICLR and will receive a great deal of interest and attention from attendees.': 1.0986123085021973, 'This paper describes modern deep learning approach to the problem of predicting the medications taken by a patient during a period of time based solely upon the sequence of ICD-9 codes assigned to the patient during that same time period.': 1.0986123085021973, 'This problem is formulated as a multilabel sequence classification (in contrast to language modeling, which is multiclass classification).': 1.0976276397705078, 'They propose to use standard LSTM and GRU architectures with embedding layers to handle the sparse categorical inputs, similar to that described in related work by Choi, et al.': 1.0986121892929077, 'In experiments using a cohort of ~610K patient records, they find that RNN models outperform strong baselines including an MLP and a random forest, as well as a common sense baseline.': 1.0971513986587524, 'The differences in performance between the recurrent models and the MLP appear to be large enough to be significant, given the size of the test set.': 1.0986123085021973, 'Strengths:': 1.0986123085021973, 'Very important problem.': 1.0986123085021973, 'As the authors point out, two the value propositions of EHRs': 1.0986123085021973, 'which have been widely adopted throughout the US due to a combination of legislation and billions of dollars in incentives from the federal government': 1.028609275817871, 'included more accurate records and fewer medication mistakes.': 1.0982791185379028, 'These two benefits have largely failed to materialize.': 1.0986123085021973, 'This seems like a major opportunity for data mining and machine learning.': 1.0879381895065308, 'Paper is well-written with lucid introduction and motivation, thorough discussion of related work, clear description of experiments and metrics, and interesting qualitative analysis of results.': 1.0970395803451538, 'Empirical results are solid with a strong win for RNNs over convincing baselines.': 1.0986121892929077, 'This is in contrast to some recent related papers, including Lipton & Kale et al, ICLR 2016, where the gap between the RNN and MLP was relatively small, and Choi et al, MLHC 2016, which omitted many obvious baselines.': 1.0515044927597046, 'Discussion is thorough and thoughtful.': 1.0986121892929077, 'The authors are right about the kidney code embedding results: this is a very promising result.': 0.7143107056617737, 'Weaknesses:': 1.0986123085021973, 'The authors make several unintuitive decisions related to data preprocessing and experimental design, foremost among them the choice NOT to use full patient sequences but instead only truncated patient sequences that each ends at randomly chosen time point.': 1.0986106395721436, ""This does not necessarily invalidate their results, but it is somewhat unnatural and the explanation is difficult to follow, reducing the paper's potential impact."": 0.5988250970840454, ""It is also reduces the RNN's potential advantage."": 1.098576545715332, 'The chosen metrics seem appropriate, but non-experts may have trouble interpreting the absolute and relative performances (beyond the superficial, e.g., RNN score 0.01 more than NN!).': 1.0146420001983643, 'The authors should invest some space in explaining (1) what level of performance': 0.40542227029800415, 'for each metric': 1.097633719444275, 'would be necessary for the model to be useful in a real clinical setting and (2) whether the gaps between the various models are ""significant"" (even in an informal sense).': 1.058543086051941, 'The paper proposes nothing novel in terms of methods, which is a serious weakness for a methods conference like ICLR.': 1.0966544151306152, 'I think it is strong enough empirically (and sufficiently interesting in application) to warrant acceptance regardless, but there may be things the authors can do to make it more competitive.': 1.093546986579895, 'For example, one potential hypothesis is that higher capacity models are more prone to overfitting noisy targets.': 1.0983854532241821, 'Is there some way to investigate this, perhaps by looking at the kinds of errors each model makes?': 0.92906254529953, 'I have a final comment: as a piece of clinical work, the paper has a huge weakness: the lack of ground truth labels for missing medications.': 1.0985524654388428, 'Models are both trained and tested on data with noisy labels.': 0.680842936038971, ""For training, the authors are right that this shouldn't be a huge problem, provided the label noise is random (even class conditional isn't too big of a problem)."": 1.0986037254333496, 'For testing, though, this seems like it could skew metrics.': 0.9263907670974731, 'Further, the assumption that the label noise is not systemic seems very unlikely given that these data are recorded by human clinicians.': 1.0976970195770264, 'The cases shown in Appendix C lend some credence to this assertion: for Case 1, 7/26 actual medications received probabilities < 0.5.': 1.0978800058364868, 'My hunch is that clinical reviewers would view the paper with great skepticism.': 0.41873207688331604, 'The authors will need to get creative about evaluation': 1.0986123085021973, 'or invest a lot of time/money in labeling data': 1.0986123085021973, 'to really prove that this works.': 1.0986123085021973, 'For what it is worth, I hope that this paper is accepted as I think it will be of great interest to the ICLR community.': 1.0984582901000977, ""However, I am borderline about whether I'd be willing to fight for its acceptance."": 1.0986123085021973, ""If the authors can address the reviewers' critiques"": 1.0986123085021973, 'and in particular, dive into the question of overfitting the imperfect labels and provide some insights': 1.0986123085021973, 'I might be willing to raise my score and lobby for acceptance.': 1.0986123085021973, 'This is a well-conducted and well-written study on the prediction of medication from diagnostic codes.': 1.0986123085021973, 'The authors compared GRUs, LSTMs, feed-forward networks and random forests (making a case for why random forests should be used, instead of SVMs) and analysed the predictions and embeddings.': 1.0986123085021973, 'The authors also did address the questions of the reviewers.': 1.0986123085021973, 'My only negative point is that this work might be more relevant for a data science or medical venue rather than at ICLR.': 1.0986123085021973}"
411,https://openreview.net/forum?id=rJJ3YU5ge,"{'This paper introduces a large-scale multi-model product classification system.': 1.098567247390747, 'The model consists of three modules, Image CNN (VGG 16 architecture), text CNN (Kim 2014) and decision-level fusion policies.': 1.0986123085021973, 'The authors have tried several fusion methods: including policies taking inputs from text and image CNN probabilities; choose either CNN; average the predictions; end-to-end training.': 1.0986123085021973, 'Experimental results show that text CNN alone works better than image CNN and multi-model fusion can improve the accuracy by a small margin.': 1.0986123085021973, 'It is a little bit surprising that end-to-end feature level fusion works worse than text CNN alone.': 1.0986089706420898, 'The writing is clear and there are a lot of useful practical experiences of learning large-scale model.': 1.0986123085021973, 'However, I lean toward rejecting the paper because the following:': 0.39964234828948975, '1)': 1.0986121892929077, 'No other dataset reported.': 1.0984817743301392, ""The authors haven't mentioned releasing the walmart dataset and it is going to be really hard to reproduce the results without the dataset."": 1.0986114740371704, '2) Technical novelty is limited.': 0.777409017086029, 'All the decision-level fusion policies have been investigated by some previous methods before.': 1.0986123085021973, '3) Performance gain is also limited.': 0.4273495376110077, 'This paper tackles the problem of multi-modal classification of text and images.': 0.831620991230011, 'Pros:': 1.075435996055603, 'Interesting dataset and application.': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'The results are rather lacklustre, showing a very mild improvement compared to the oracle improvement.': 1.0986113548278809, 'But perhaps some insights as to whether the incorrect decisions are humanly possible would help with significance of the results.': 1.09803307056427, 'Could have explored some intermediate architectures such as feature fusion + class probabilities with/without finetuning.': 1.0984855890274048, 'There are no feature fusion results reported.': 1.0841604471206665, 'No evaluation on standard datasets or comparison to previous works.': 1.0985769033432007, 'What is the policy learnt for CP-1?': 1.0986123085021973, 'Given 2 input class probabilities, how does the network perform better than max or mean?': 1.0986123085021973, 'This paper presents a system approach to combine multiple modalities to perform classification in a practical scenario (e-commerce).': 1.0986123085021973, 'In general, I find the proposed approach in the paper sound and solid, but do not see novelty in the paper: feature fusion and decision time fusion are both standard practices in multi-modal analysis, and the rest of the paper offers no surprise in implementing such approaches.': 1.0986123085021973, 'This seems to be a better fit for venues that focus more on production systems, and seems to be a bad fit for ICLR where the focus is more on research of novel algorithms and theories.': 1.0986042022705078}"
412,https://openreview.net/forum?id=rJJRDvcex,"{'Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.': 1.385421872138977, 'Paper summary: this work proposes to use RNNs inside a convolutional network architecture as a complementary mechanism to propagate spatial information across the image.': 1.3862943649291992, 'Promising results on classification and semantic labeling are reported.': 1.3862764835357666, 'Review summary:': 1.379964828491211, 'The text is clear, the idea well describe, the experiments seem well constructed and do not overclaim.': 1.3862943649291992, 'Overall it is not a earth shattering paper, but a good piece of incremental science.': 1.3861814737319946, 'Pros:': 1.33987557888031, '* Clear description': 1.3847694396972656, '* Well built experiments': 1.3862943649291992, '* Simple yet effective idea': 1.386278510093689, '*': 1.3862943649291992, 'No overclaiming': 1.3862943649291992, '* Detailed comparison with related work architectures': 1.3862924575805664, 'Cons:': 1.382736086845398, '* Idea somewhat incremental (e.g. can be seen as derivative from Bell 2016)': 1.3830792903900146, '* Results are good, but do not improve over state of the art': 1.3862943649291992, 'Quality: the ideas are sound, experiments well built and analysed.': 1.3862453699111938, 'Clarity: easy to read, and mostly clear (but some relevant details left out, see comments below)': 1.3862942457199097, 'Originality: minor, this is a different combination of ideas well known.': 1.3860721588134766, 'Significance: seems like a good step forward in our quest to learn good practices to build neural networks for task X (here semantic labelling and classification).': 1.3862935304641724, 'Specific comments:': 1.3862943649291992, '* Section 2.2 “we introduction more nonlinearities (through the convolutional layers and ...”.': 1.3441243171691895, 'Convolutional layers are linear operators.': 1.3857131004333496, '* Section 2.2, why exactly RNN cannot have pooling operators ?': 1.3862940073013306, 'I do not see what would impede it.': 0.9581480026245117, '* Section 3 “into the computational block”, which block ?': 1.3132778406143188, 'Seems like a typo, please rephrase.': 1.1454938650131226, '* Figure 2b and 2c not present ?': 1.3851606845855713, 'Please fix figure or references to it.': 1.3859251737594604, '* Maybe add a short description of GRU in the appendix, for completeness ?': 1.3862807750701904, '* Section 5.1, last sentence.': 1.3862943649291992, 'Not sure what is meant.': 1.3862943649291992, 'The convolutions + relu and pooling in ResNet do provide non-linearities “between layers” too.': 1.3630551099777222, 'Please clarify': 1.3839648962020874, '* Section 5.2.1 (and appendix A), how is the learning rate increased and decreased ?': 1.3862943649291992, 'Manually ?': 1.3862943649291992, 'This is an important detail that should be made explicit.': 0.8757171034812927, 'Is the learning rate schedule the same in all experiments of each table ?': 0.7030960321426392, 'If there is a human in the loop, what is the variance in results between “two human schedulers” ?': 1.3836901187896729, '* Section 5.2.1, last sentence; “we certainly have  a strong baseline”; the Pascal VOC12 for competition 6 reports 85.4 mIoU as best known results.': 1.3855082988739014, 'So no, 64.4 is not “certainly strong”.': 1.2393205165863037, 'Please tune down the statement.': 0.7597312331199646, '* Section 5.2.3 Modules -> modules': 1.3862943649291992, 'The results ignore any mention of increased memory usage or computation cost.': 1.3862943649291992, 'This is not a small detail.': 1.3862943649291992, 'Please add a discussion on the topic.': 1.3862943649291992, '* Section 6 “adding multi-scale spatial” -> “adding spatial” (there is nothing inherently “multi” in the RNN)': 1.3862943649291992, '* Section 6 Furthermoe -> Furthermore': 1.3862943649291992, '* Appendix C, redundant with Figure 5 ?': 1.3862943649291992, 'The paper proposes a method of integrating recurrent layers within larger, potentially pre-trained, convolutional networks.': 1.3862943649291992, 'The objective is to combine the feature extraction abilities of CNNs with the ability of RNNs to gather global context information.': 1.3862943649291992, 'The authors validate their idea on two tasks, image classification (on CIFAR-10) and semantic segmentation (on PASCAL VOC12).': 1.3862943649291992, 'On the positive side, the paper is clear and well-written (apart from some occasional typos), the proposed idea is simple and could be adopted by other works, and can be deployed as a beneficial perturbation of existing systems, which is practically important if one wants to increase the performance of a system without retraining it from scratch.': 1.017052173614502, 'The evaluation is also systematic, providing a clear ablation study.': 0.7350383996963501, 'On the negative side, the novelty of the work is relatively limited, while the validation is lacking a bit.': 1.203037142753601, 'Regarding novelty, the idea of combining a recurrent layer with a CNN, something practically very similar was proposed in Bell et al (2016).': 0.8607901930809021, 'There are a few technical differences (e.g. cascading versus applying in parallel the recurrent layers), but in my understanding these are minor changes.': 1.3620810508728027, 'The idea of initializing the recurrent network with the CNN is reasonable but is at the level of improving one wrong choice in the original work of Bell, rather than really proposing something novel.': 1.3794004917144775, 'This contribution ("" we use RNNs within layers"") is repeatedly mentioned in the paper (including intro &  conclusion), but in my understanding was part of Bell et al, modulo minor changes.': 1.3581408262252808, 'Regarding the evaluation, experiments on CIFAR are interesting, but only as proof of concept.': 1.217976450920105, 'Furthermore, as noted in my early question, Wide Residual Networks (Sergey Zagoruyko, Nikos Komodakis, BMVC16)': 1.381463885307312, 'report  better results on CIFAR-10 (4% error), while not using any recurrent layers (rather using instead a wide, VGG-type, ResNet variant).': 1.3860657215118408, 'So.': 1.3862943649291992, 'The authors answer: ""Wide Residual Networks use the depth of the network to spread the receptive field across the entire image (DenseNet (Huang et al., 2016) similarly uses depth).': 1.332587718963623, 'Thus there is no need for recurrence within layers to capture contextual information.': 1.3862943649291992, 'In contrast, we show that a shallow CNN, where the receptive field would be limited, can capture contextual information within the whole image if a L-RNN is used.""': 1.3862911462783813, 'So, we agree that WRN do not need recurrence - and can still do better.': 1.3862943649291992, 'The point of my question has practically been whether using a recurrent layer is really necessary; I can understand the answer as being ""yes, if you want to keep your network shallow"".': 1.3862935304641724, ""I do not necessarily see why one would want to keep one's network shallow."": 1.3862943649291992, 'Probably an evaluation on imagenet would bring some more insight about the merit of this layer.': 1.3862943649291992, 'Regarding semantic segmentation, one of my questions has been:': 1.3862943649291992, '""Is the boost you are obtaining due to something special to the recurrent layer, or is simply because one is adding extra parameters on top of a pre-trained network?': 1.3306376934051514, '(I admit I may have missed some details of your experimental evaluation)""': 1.3862674236297607, 'The answer was:': 1.3862943649291992, '""...For PASCAL segmentation, we add the L-RNN into a pre-trained network (this adds recurrence parameters), and again show that this boosts performance - more so than adding the same number of parameters as extra CNN layers - as it is able to model long-range dependences""': 0.695978045463562, ""I could not find one such experiment in the paper ('more so than adding the same number of parameters as extra CNN layers'); I understand that you have 2048 x 2048 connections for the recurrence, it would be interesting to see what you get by spreading them over (non-recurrent) residual layers."": 0.3764343857765198, 'Clearly, this is not going to be my criterion for rejection/acceptance, since one can easily make it fail - but I was mostly asking for some sanity check': 1.370874047279358, 'Furthermore, it is a bit misleading to put in Table 3 FCN-8s and FCN8s-LRNN, since this gives the impression that the LRNN gives a  boost by 10%.': 0.7293087244033813, 'In practice the ""FCN8s"" prefix of ""FCN8s-LRNN"" is that of the authors, and not of Long et al (as indicated in Table 2, 8s original is quite worse than 8s here).': 0.8039665818214417, 'Another thing that is not clear to me is where the boost comes from in Table 2; the authors mention that ""when inserting the L-RNN after pool 3 and pool4 in FCN-8s, the L-RNN is able to learn contextual information over a much larger range than the receptive field of pure local convolutions. ""': 1.2002280950546265, 'This is potentially true, but I do not see why this was not also the case for FCN-32s (this is more a property of the recurrence rather than the 8/32 factor, right?)': 0.512378454208374, 'A few additional points:': 1.3862923383712769, 'It seems like Fig 2b and Fig2c never made it into the pdf.': 0.811015784740448, 'Figure 4 is unstructured and throws some 30 boxes to the reader - I would be surprised if anyone is able to get some information out of this (why not have a table?)': 0.5172984600067139, 'Appendix A: this is very mysterious.': 1.3862943649291992, 'Did you try other learning rate schedules?': 1.3862943649291992, '(e.g. polynomial)': 1.3862943649291992, 'What is the performance if you apply a standard training schedule?': 1.3862943649291992, '(e.g. step).': 1.3862943649291992, 'Appendix C: ""maps .. is"" -> ""maps ... are""': 1.3862943649291992, 'This paper proposes a cascade of paired (left/right, up/down) 1D RNNs as a module in CNNs in order to quickly add global context information to features without the need for stacking many convolutional layers.': 1.3862943649291992, 'Experimental results are presented on image classification and semantic segmentation tasks.': 1.3862943649291992, 'The paper is very clear and easy to read.': 1.3862943649291992, 'Enough details are given that the paper can likely be reproduced with or without source code.': 1.3862943649291992, 'Using 1D RNNs inside CNNs is a topic that deserves more experimental exploration than what exists in the literature.': 1.2309365272521973, 'Cons (elaborated on below):': 1.3862943649291992, '(1) Contributions relative to, e.g. Bell et al., are minor.': 1.3654770851135254, ""(2) Disappointed in the actual use of the proposed L-RNN module versus how it's sold in the intro."": 0.8924237489700317, '(3) Classification experiments are not convincing.': 1.381021499633789, '(1,2): The introduction states w.r.t.': 1.3862942457199097, 'Bell et al.': 1.3681718111038208, '""more substantial differences are two fold: first, we treat the L-RNN module as a general block, that can be inserted into any layer of a modern architecture, such as into a residual module.': 1.2178207635879517, 'Second, we show (section 4) that the': 1.2831823825836182, 'L-RNN can be formulated to be inserted into a pre-trained FCN (by initializing with zero recurrence': 1.1411018371582031, 'matrices), and that the entire network can then be fine-tuned end-to-end.""': 0.7113322615623474, 'I felt positive about these contributions after reading the intro, but then much less so after reading the experimental sections.': 1.182562232017517, 'Based on the first contribution (""general block that can be inserted into any layer""), I strongly expected to see the L-RNN block integrated throughout the CNN starting from near the input.': 1.3810614347457886, 'However, the architectures for classification and segmentation only place the module towards the very end of the network.': 0.8308693766593933, 'While not exactly the same as Bell et al.': 1.339557409286499, '(there are many technical details that differ), it is close.': 1.3846880197525024, 'The paper does not compare to the design from Bell et al.': 1.2717646360397339, 'Is there any advantage to the proposed design?': 1.206360936164856, 'Or is it a variation that performs similarly?': 1.3165868520736694, 'What happens if L-RNN is integrated earlier in the network, as suggested by the introduction?': 1.366917371749878, ""The second difference is a bit more solid, but still does not rise to a 'substantive difference' in my view."": 1.1103473901748657, 'Note that Bell et al. also integrate 1D RNNs into an ImageNet pretrained VGG-16 model.': 1.1220710277557373, 'I do, however, think that the method of integration proposed in this paper (zero initialization) may be more elegant and does not require two-stage training by first freezing the lower layers and then later unfreezing them.': 0.8287164568901062, '(3) I am generally skeptical of the utility of classification experiments on CIFAR-10 when presented in isolation (e.g., no results on ImageNet too).': 0.7007327079772949, 'The issue is that CIFAR-10 is not interesting as a task unto itself *and* methods that work well on CIFAR-10 do not necessarily generalize to other tasks.': 0.762711763381958, 'ImageNet has been useful because, thus far, it produces features that generalize well to other tasks.': 1.047189474105835, 'Showing good results on ImageNet is much more likely to demonstrate a model that learns generalizable features.': 0.9944712519645691, 'However, that is not even necessarily true, and ideally I would like to see that that a model that does well on ImageNet in fact transfers its benefit to at least one other ask (e.g., detection).': 0.6944600343704224, 'One additional issue with the CIFAR experiments is that I expect to see a direct comparison of models A-F with and without L-RNN.': 1.0496457815170288, 'It is hard to understand from the presented results if L-RNN actually adds much.': 1.32893967628479, 'In sum, I have a hard time taking away any valuable information from the CIFAR experiments.': 1.234920620918274, 'Minor suggestion:': 1.3784568309783936, 'Figure 4 is hard to read.': 1.3862943649291992, 'The pixelated rounded corners on the yellow boxes are distracting.': 1.3862943649291992, 'The authors propose the use of a vertical and horizontal one-dimensional RNN (denoted as L-RNN module) to capture long-range dependencies and summarize convolutional feature maps.': 1.3862916231155396, 'L-RNN modules are an alternative to deeper or wider networks, 2D RNNs, dilated (Atrous) convolutional layers, and a simple flatten or global pooling layer when applied to the last convolutional layer for classification.': 1.381180763244629, 'L-RNN modules are faster than 2D RNNs, since rows and columns can be processed in parallel, are easy to implemented, and can be inserted in existing convolutional networks.': 0.701336681842804, 'The authors demonstrate improvements for classification and semantic segmentation.': 1.3862942457199097, 'However, further evaluations are required that show for which use cases L-RNNs are superior to alternatives for summarizing convolutional feature maps:': 1.1737325191497803, '1. I suggest to use a fixed CNN with as certain number of layers, and summarize the last feature map by a) a flatten layer, b) global average pooling, c) a 2D RNN, d) and dilated convolutional layers for segmentation. The authors should report both the run-time and number of parameters for these variants in addition to prediction performances. For segmentation, the number of dilated convolutional layers should be chosen such that the number of parameters is similar to a single L-RNN module.': 0.2389068603515625, '2. The authors compare classification performances only on 32x32 CIFAR-10 images. For higher resolution images, the benefit of L-RNN modules to capture long-range dependencies might be more pronounced. I therefore suggest evaluating classification performances on one additional dataset with higher resolution images, e.g. ImageNet or the CUB bird dataset.': 0.49587535858154297, 'Additionally, I have the following minor comments:': 1.3862942457199097, '3. The authors use vanilla RNNs. It might be worth investigating LSTMs or GRUs instead.': 1.3265652656555176, '4. For classification, the authors summarize hidden states of the final vertical recurrent layer by global max pooling. Is this different from more common global average pooling or concatenating the final forward and backward recurrent states?': 0.9402671456336975, '5. Table 3 is hard to understand since it mingles datasets (Pascal P and COCO C) and methods (CRF post-processing). I suggest, e.g., using an additional column with CRF ‘yes’ or ‘no’. I further suggest listing the number of parameters and runtime if possible.': 1.3862943649291992, '6. Section 3 does not clearly describe in which order batch-normalization is applied in residual blocks. Figure 2 suggest that the newer BN-ReLU-Conv order described in He et al. (2016) is used. This should be mentioned in the text.': 1.3862943649291992, 'Finally, the text needs to be revised to reach publication level quality.': 1.3862943649291992, 'Specifically, I have the following comments:': 1.3862943649291992, '7. Equation (1) is the update of a vanilla RNN, which should be stated more clearly. I suggest to first describe (bidirectional) RNNs, to reference GRUs and LSTMs, and then describe how they are applied here to images. Figure 1 should also be referenced in the text.': 1.3862943649291992, '8. In section 2.2, I suggest to describe Bell at al. more clearly. Why are they using eight instead of four RNNs?': 1.3862943649291992, '9. Section 4 starts with a verbose description about transfer learning, which can be compressed into a single reference or skipped entirely.': 1.3862943649291992, '10. Equation (6) seems to be missing an index i.': 1.3862943649291992, '11.In particular section 5 and 6 contain a lot of clutter and slang, which should be avoided:': 1.3862943649291992, '11.1 page 8: ‘As can be seen’, ‘we turn to that case next’': 1.3862943649291992, '11.2 page 9: ‘to the very high value’, ‘as noted earlier’,  ‘less context to contribute here’': 1.3862943649291992, '11.3 page 10: ‘In fact’, ‘far deeper’, ‘a simple matter of’, ‘there is much left to investigate.': 1.3862943649291992}"
413,https://openreview.net/forum?id=rJLS7qKel,"{'Deep RL (using deep neural networks for function approximators in RL algorithms) have had a number of successes solving RL in large state spaces.': 1.0985774993896484, 'This empirically driven work builds on these approaches.': 1.0986123085021973, 'It introduces a new algorithm which performs better in novel 3D environments from raw sensory data and allows better generalization across goals and environments.': 1.0986123085021973, 'Notably, this algorithm was the winner of the Visual Doom AI competition.': 1.0986123085021973, 'The key idea of their algorithm is to use additional low-dimensional observations (such as ammo or health which is provided by the game engine) as a supervised target for prediction.': 1.0985840559005737, 'Importantly, this prediction is conditioned on a goal vector (which is given, not learned) and the current action.': 1.0986123085021973, 'Once trained the optimal action for the current state can be chosen as the action that maximises the predicted outcome according the goal.': 1.0986123085021973, 'Unlike in successor feature representations, learning is supervised and there is no TD relationship between the predictions of the current state and the next state.': 1.0986123085021973, 'There have been a number of prior works both in predicting future states as part of RL and goal driven function approximators which the authors review in section 2.': 1.0986123085021973, 'The key contributions of this work are the focus on Monte Carlo estimation (rather than TD), the use of low-dimensional ‘measurements’ for prediction, the parametrized goals and, perhaps most importantly, the empirical comparison to relevant prior work.': 0.7141435146331787, 'In addition to the comparison with Visual Doom AI, the authors show that their algorithm is able to learn generalizable policies which can respond, without further training, to limited changes in the goal.': 1.0986119508743286, 'The paper is well-communicated and the empirical results compelling and will be of significant interest.': 1.0986123085021973, 'Some minor potential improvements:': 1.0986123085021973, 'There is an approximation in the supervised training as it is making an on-policy assumption but it learns from a replay buffer (with the Monte Carlo regression the expectation of the remainder of the trajectory is assumed to follow the current policy, but is being sampled from episodes generated by prior versions of the policy).': 1.083196759223938, 'This should be discussed.': 1.0986123085021973, 'The algorithm uses additional metadata (the information about which parts of the sensory input are worth predicting) that the compared algorithms do not.': 1.0986123085021973, 'I think this, and the limitations of this approach (e.g. it may not work well in a sensory environment if such measurements are not provided) should be mentioned more clearly.': 1.0986123085021973, 'This paper presents an on-policy deep RL method with additional auxiliary intrinsic variables.': 1.0986123085021973, 'The method is a special case of an universal value function based approach and the authors do cite the correct references.': 1.0986123085021973, 'Maybe the biggest claimed technical contribution of this paper is to distill many of the existing ideas to solve 3D navigation problems.': 1.0986123085021973, 'I think the contributions should be more clearly stated in the abstract/intro': 1.0986123085021973, 'I would have liked to see failure modes of this approach.': 1.075216293334961, 'Under what circumstances does the model have problems generalizing to changing goals?': 1.0986123085021973, 'There are other conceptual problems': 1.0986123085021973, ""since this is an on-policy method, there will be catastrophic forgetting if the agent dosen't repeatedly train on goals from the distant past."": 0.562329113483429, 'Since the main contribution of this paper is to integrate several key ideas and show empirical advantage, I would have liked to see results on other domains like Atari (maybe using the ROM as intrinsic variables)': 1.0986121892929077, 'Overall, I think this paper does show clear empirical advantage of using the proposed underlying formulations and experimental insights from this paper might be valuable for future agents': 1.0986123085021973, 'The paper presents an on-policy method to predict future intrinsic measurements.': 1.0986123085021973, 'All the experiments are performed in the game of Doom (vizDoom to be exact), and instead of just predicting win/loss or the number of frags (score), the authors trained their model to predict (a sequence of) triplets of (health, ammunition, frags), weighted by (a sequence of) ""goal"" triplets that they provided as input.': 1.0986123085021973, 'Changing the weights of the goal triplet is a way to perform/guide exploration.': 1.0920612812042236, 'At test time, one can act by maximizing the long term goal only.': 1.0986123085021973, 'The results are impressive, as this model won the 2016 vizDoom competition.': 0.9396010041236877, 'The experimental section of the paper seems sound:': 1.0986123085021973, '- There are comparisons of DFP with A3C, DQN, and an attempt to compare with DSR (a recent similar approach from Kulkarni et al., 2016). DFP outperforms other approaches (or equal them when they reach a ceiling / optimum, as for A3C in scenario D1).': 1.0986121892929077, '- There is an ablation study that supports the thesis that all the ""added complexity"" of the paper\'s model is useful.': 1.0986123085021973, 'Predicting intrinsic motivation (Singh et al. 2004), auxiliary variables, and forward modelling, are well-studied domains of reinforcement learning.': 1.0986123085021973, 'The version that I read (December 4th revision) adequately references prior work, even if it is not completely exhaustive.': 1.0986123085021973, 'A few comments (nitpicks) on the form:': 1.0986123085021973, '- Doom is described as a 3D environment, whereas it is actually a 2D environment (the height is not a discriminative/actionable dimension) presented in (fake) 3D.': 1.0986123085021973, '- The use of ""P"" in (2) (and subsequently) may be misleading as it stands for prediction but not probability (as is normally the case for P).': 1.0986123085021973, '- The double use of ""j"" (admittedly, with different fonts) in (6) may be misleading.': 1.0986123085021973, '- Results tables could repeat the units of the measurements (in particular as they are heterogenous in Table 1).': 1.0986123085021973, 'I think that this paper is a clear accept.': 1.0986123085021973, 'One could argue that experiments could be conducted on different environments or that the novelty is limited, but I feel that ""correct"" (no-nonsense, experimentally sound on Doom, appendix providing details for reproducibility) and ""milestone"" (vizDoom winner) papers should get published.': 1.0986123085021973}"
414,https://openreview.net/forum?id=rJM69B5xx,"{'First I would like to apologize for the delay in reviewing.': 1.0986096858978271, 'summary : This work explores several experiments to transfer training a specific model of reading comprehension ( AS Reader), in an artificial and well populated dataset in order to perform in another target dataset.': 1.0942001342773438, 'Here is what I understand are their several experiments to transfer learning, but I am not 100% sure.': 1.0985571146011353, '1. The model is trained on the big artificial dataset and tested on the small target datasets (section 4.1)': 0.810723602771759, '2. The model is pre-trained on the big artificial dataset like before, then fine-tuned on a few examples from the target dataset and tested on the remaining target examples. Several such models are trained using different sub-sets of fine-tuning examples. The results are tested against the performance of randomly intialized then fine-tuned models (section 4.2).': 1.0892598628997803, '3. The model is pre-trained on the big artificial dataset like before. The model is made of an embedding component and an encoder  component. Alternatively, each component is reset to a random initialization, to test the importance of the pre-training in each component. Then the model is fine-tuned on a few examples from the target dataset and tested on the remaining target examples. (section 4.3)': 1.0268884897232056, 'I think what makes things difficult to follow is the fact that the test set is composed by several sub tasks, and sometimes what is reported is the mean performance across the tasks, sometimes the performance on a few tasks.': 1.0986123085021973, 'Sometimes what we see is the mean performance of several models?': 0.7049606442451477, 'You should report standard deviations also.': 1.0986123085021973, 'Could you better explain what you mean by best validation ?': 1.056396245956421, 'Interesting and unpretentious work.': 1.0986123085021973, 'The clarity of the presentation could be improved maybe by simplifying the experimental setup?': 1.0986123085021973, 'The interesting conclusion I think is reported at the end of the section 4.1, when the nuanced difference between the datasets are exposed.': 1.0986123085021973, 'Minor: unexplained acronyms: GRU, BT, CBT.': 1.0986123085021973, 'benfits p. 2': 1.0986123085021973, 'subsubset p. 6': 1.0986123085021973, 'This paper proposes a study of transfer learning in the context of QA from stories.': 1.0986123085021973, 'A system is presented with a a short story and has to answer a question about it.': 1.0986123085021973, 'This paper studies how a system trained to answer questions on a dataset can eventually be used to answer questions from another dataset.': 1.0986123085021973, 'The results are mostly negative: transfer seems almost non-existant.': 1.0986123085021973, 'This paper is centered around presenting negative results.': 1.0986123085021973, 'Indeed the main hypothesis of transferring between QA datasets with the attention sum reader turns out impossible and one needs a small portion of labeled data from the target dataset to get meaningful performance.': 1.0986123085021973, 'Having only negative results could be fine if the paper was bringing some value with a sharp analysis of the failure modes and of the reasons behind it.': 1.0986123085021973, 'Because this might indicate some research directions to follow.': 1.0986123085021973, 'However, there is not much of that.': 1.0986123085021973, 'The answers to the pre-review questions actually start to give some insights: typing seems to be transferred for instance.': 1.0986123085021973, 'How about the impact of syntax (very different between bAbI, Gutenberg books, and CNN news articles)?': 1.0986123085021973, 'And the word/entity/ngrams distributions overlap between the 3 datasets?': 1.0986123085021973, 'Unfortunately, there is not much to take-away from this paper.': 1.0986123085021973, 'This work investigates the performance of transfer learning from resource-rich setup (BookTest, CNN/Daily Mail corpora) to low-resource (bAbI, SQuAD benchmarks) settings.': 1.0986123085021973, 'Experiments show poor improvements in 0-shot learning.': 1.0986123085021973, 'However, when the model is exposed to few training instances some improvements are observed.': 1.0986123085021973, 'The claims made here require a more comprehensive analysis.': 1.0986123085021973, 'I criticize the use of bAbI as a low-resource real-world scenario.': 1.0986123085021973, 'bAbI is designed as a unit test and is far from representing many natural language phenomena.': 1.0986123085021973, 'Thus, the claims related to bAbI can only be weak evidence for questioning transfer learning high-resource to low-resource in real-world scenarios.': 1.0986123085021973, 'I highly recommend using recently proposed real-world scenarios [1,2].': 1.0986123085021973, 'More importantly, the work does not explain why and how do we get improvement using transfer learning.': 1.0986123085021973, 'They remotely address this by hypothesizing the knowledge of transfer is not just encoded in embeddings but also in the model.': 1.0986123085021973, 'Considering the related work [3], these claims bring a marginal novelty and still ""how and why"" should be central in this work.': 1.0986123085021973, '[1] http://www.msmarco.org/dataset.aspx': 1.0986123085021973, '[2] https://datasets.maluuba.com/NewsQA': 1.0986123085021973, '[3] http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.8551&rep=rep1&type=pdf': 1.0986123085021973}"
415,https://openreview.net/forum?id=rJPcZ3txx,"{'The paper details an implementation of sparse-full convolutions and a model to work out the potential speed-up of various sparsity levels for CNNs.': 1.0986123085021973, 'The first contribution is more about engineering, but the authors make the source code available which is greatly appreciated.': 1.098610520362854, 'The second contribution is perhaps more interesting, as so far pruning methods have focused on saving memory, with very modest speed gains.': 1.0986121892929077, 'Imbuing knowledge of running speed into a pruning algorithm seems like the proper way to tackle this problem.': 1.0984926223754883, 'The authors are very methodical in how they build the model and evaluate it very thoroughly.': 1.0986119508743286, 'It seems that the same idea could be used not just for pruning existing models, but also when building new architectures: selecting layers and their parameters as to achieve an optimal throughput rate.': 1.0986123085021973, 'This could make for a nice direction for future work.': 1.088465690612793, 'One point that is missing is some discussion of how transferable the performance model is to GPUs.': 1.0985685586929321, 'This would make the technique easier to adopt broadly.': 1.0982739925384521, 'Other areas for improvement: The points in Figure 4 are hard to distinguish (e.g. small red circle vs. small red square), and overall the figure could be made bigger; specifying whether the ""base learning rate"" in Section 3 is the start or end rate of the annealing schedule; typos: ""punning"" (p.4), ""spares"" (p.5).': 1.0986123085021973, 'The authors provide a well engineered solution to exploiting sparsity in convolutional layers of a deep network by recasting it as sparse matrix-vector multiplication.': 1.0986123085021973, 'This leads to very nice speedups and the analysis of when this is possible is also useful for practitioners.': 1.0986120700836182, 'My main concern with this paper is that the ""research"" aspect of it seems rather minimal, and it\'s mostly about performance engineering and comparisons.': 1.0986123085021973, 'It is upto the area chairs to decide how well such a paper fits in at ICLR.': 1.0986123085021973, 'This paper tackles the problem of compressing trained convnets with the goal of reducing memory overhead and speeding up the forward pass.': 1.0986123085021973, 'As I understand it, the main contribution of this work is to develop fast convolution routines for sparse conv weights int he case of general sparsity (as compared with structured sparsity).': 1.0986082553863525, 'They evaluate their method on both AlexNet and GoogLeNet as well as on various platforms.': 0.9098890423774719, 'The authors make code available online.': 1.0986123085021973, 'The paper is well written and does a good job of putting this work in the context of past model reduction techniques.': 1.0986064672470093, 'My main request of the authors would be to provide a concise summary of the speedup/memory gains achievable with this new work compared with previously published work.': 1.0986123085021973, 'The authors do show the various sparsity level obtained with various methods of pruning but it is unclear to me how to translate the information given in the paper into an understanding of gains relative to other methods.': 1.0986123085021973}"
416,https://openreview.net/forum?id=rJQKYt5ll,"{'This paper presents a theoretical treatment of transformation groups applied to convnets, and presents some empirical results showing more efficient usage of network parameters.': 1.0986123085021973, 'The basic idea of steerability makes huge sense and seems like a very important idea to develop.': 1.0986123085021973, 'It is also a very old idea in image processing and goes back to Simoncelli, Freeman, Adelson, as well as Perona/Greenspan and others in the early 1990s.': 1.0986123085021973, 'This paper approaches it through a formal treatment of group theory.': 1.0986123085021973, 'But at the end of the day the idea seems pretty simple - the feature representation of a transformed image should be equivalent to a transformed feature representation of the original image.': 1.0986123085021973, 'Given that the authors are limiting their analysis to discrete groups - for example rotations of 0, 90, 180, and 270 deg.': 1.0986123085021973, '- the formalities brought in from the group theoretic analysis seem a bit overkill.': 1.0986123085021973, ""I'm not sure what this buys us in the end."": 1.0986123085021973, 'it seems the real challenge lies in implementing continuous transformations, so if the theory could guide us in that direction it would be immensely helpful.': 1.0986123085021973, 'Also the description of the experiments is fairly opaque.': 1.0986123085021973, 'I would have a hard time replicating what exactly the authors did here in terms of implementing capsules or transformation groups.': 1.0986123085021973, 'The authors propose a parameterization of CNNs that guarantees equivariance wrt a large family of geometric transformations.': 1.0986123085021973, 'The mathematical analysis is rigorous and the material is very interesting and novel.': 1.0986123085021973, 'The paper overall reads well; there is a real effort to explain the math accessibly, though some small improvements could be made.': 1.0986123085021973, 'The theory is general enough to include continuous transformations, although the experiments are restricted to discrete ones.': 1.0986123085021973, 'While this could be seen as a negative point, it is justified by the experiments, which show that this set of transformations is powerful enough to yield very good results on CIFAR.': 1.0986123085021973, 'Another form of intertwiner has been studied recently by Lenc & Vedaldi': 1.0986123085021973, '[1]; they have studied equivariance empirically in CNNs, which offers an orthogonal view.': 1.0986123085021973, ""In addition to the recent references on scale/rotation deep networks suggested below, geometric equivariance has been studied extensively in the 2000's; mentioning at least one work would be appropriate."": 1.0986123085021973, 'The one that probably comes closest to the proposed method is the work by Reisert': 1.0986123085021973, '[2], who studied steerable filters for invariance and equivariance, using Lie group theory.': 1.0986123085021973, 'The difference, of course, is that the focus at the time was on kernel machines rather than CNNs, but many of the tools and theorems are relatable.': 1.0986123085021973, 'Some of the notation could be simplified, to make the formulas easier to grasp on a first read:': 0.9112995266914368, 'Working over a lattice Z^d is unnecessarily abstract': 0.8241652846336365, 'since the inputs are always images, Z^2 would make much of the later math easier to parse.': 1.098572015762329, ""Generalization is straightforward, so I don't think the results lose anything by it; and the authors go back to 2D latices later anyway."": 1.0357574224472046, ""It could be more natural to do away with the layer index l which appears throughout the paper, and have notation for current/next layer instead (e.g. pi and pi'; K and D instead of K_{l+1} and K_l)."": 1.0964915752410889, 'In any case I leave it up to the authors to decide whether to include these suggestions on notation, but I urge them to consider them (or other ways to unburden notation).': 1.0910183191299438, 'A few minor issues: Some statements would be better supported with an accompanying reference (e.g. ""Explicit formulas exist"" on page 5, the introduction of intertwiners on page 3).': 0.9950916171073914, 'Finally, there is a tiny mistake in the Balduzzi & Ghifary reference (some extra information was included as an author name).': 1.0919280052185059, '[1] Lenc & Vedaldi, ""Understanding image representations by measuring their equivariance and equivalence"", 2015': 1.0985790491104126, '[2] Reisert, ""Group integration techniques in pattern analysis: a kernel view"", 2008': 1.0985569953918457, 'This paper essentially presents a new inductive bias in the architecture of (convolutional) neural networks (CNN).': 1.0986123085021973, 'The mathematical motivations/derivations of the proposed architecture are detailed and rigorous.': 1.0986123085021973, 'The proposed architecture promises to produce equivariant representations with steerable features using fewer parameters than traditional CNNs, which is particularly useful in small data regimes.': 1.0986123085021973, 'Interesting and novel connections are presented between steerable filters and so called “steerable fibers”.': 1.0986123085021973, 'The architecture is strongly inspired by the author’s previous work, as well as that of “capsules” (Hinton, 2011).': 1.0986123085021973, 'The proposed architecture is compared on CIFAR10 against state-of-the-art inspired architectures (ResNets), and is shown to be superior particularly in the small data regime.': 1.0985939502716064, 'The lack of empirical comparison on large scale dataset, such as ImageNet or COCO makes this largely a theoretical contribution.': 1.0986123085021973, 'I would have also liked to see more empirical evaluation of the equivariance properties.': 1.0986123085021973, 'It is not intuitively clear exactly why this architecture performs better on CIFAR10 as it is not clear that capturing equivariances helps to classify different instances of object categories.': 1.0986123085021973, 'Wouldn’t action-recognition in videos, for example, not be a better illustrative dataset?': 1.0986123085021973}"
417,https://openreview.net/forum?id=rJRhzzKxl,"{'This paper studies the problem of transfer learning in the context of domain adaptation.': 1.0986123085021973, 'They propose to study it in the framework of knowledge distillation.': 1.081148624420166, 'Several settings are presented along with experiments on the Amazon Reviews dataset.': 1.0986123085021973, 'The paper is nicely written and the problem studied is very important towards progress in AI.': 1.0985944271087646, 'The results of the experiments could be improved but still justify the validity of applying distillation for transfer learning.': 1.0986123085021973, 'Of course, the experimental setting is rather limited but the benchmarks are competitive enough to be meaningful.': 1.0986123085021973, 'I had concerns regarding discussion of previous work but the extensive responses helped clarify this point (the authors should turn the arguments used in this thread into an appendix).': 1.0986123085021973, 'I think this paper would make an interesting ICLR paper.': 1.0918974876403809, 'The work extends knowledge distillation to domain adaptation scenario, the student model (for the target domain) is learned to mimic the prediction of the teacher model, learned on the source domain.': 1.0986123085021973, 'The authors extends the idea to multi-source domain settings, proposing to weight predictions of teacher model using several domain similarity measurements.': 1.0986123085021973, 'To improve the performance of proposed method when only a single source domain is available, the authors propose to use maximum cluster difference to inject pseudo-supervised examples labeled by the teacher model to train the student model.': 1.0986123085021973, 'The paper is well written and easy to follow.': 1.0985952615737915, 'The idea is straight-forward, albeit fairly heuristic in several cases.': 1.0985397100448608, 'It is not clear what is the advantage of the proposed method vs existing feature learning techniques for domain adaptation, which also does not require re-train source  models, and performs comparable to the proposed method.': 1.0986123085021973, 'Questions:': 1.0986123085021973, '1. Why did you choose to use different combination schemes in equation (3) and (5)? For example, in equation (5), what if minimizing H( (1-\\lambda) y_teacher + \\lambda P_t, P_s) instead?': 0.8257737159729004, '2. how will you extend the MCD definition to multi-class settings?': 1.0401207208633423, 'Paper describes technique for leveraging multiple teachers in teacher-student framework and performing unsupervised and semi-supervised domain adaptation.': 0.4051366448402405, 'The central idea relies on using similarity measure between source and target domains to weight the corresponding trustfulness of particular teachers, when using their prediction as soft targets in student training.': 1.0982818603515625, 'Authors provide an experimental validation on a single benchmark corpora for sentiment analysis.': 1.0986108779907227, 'What exactly constitutes the learned representation h used in MCD measure?': 1.0985790491104126, 'I assume those are the top level pre-softmax activations - is this the case?': 1.0986123085021973, 'Those tend to be typically more task related, would not the intermediate ones work better?': 0.30790477991104126, 'One not entirely clear aspect concerns types of distributions applicable to proposed learning - it assumes the vocabulary (or decision space) between tasks spans same categories, as otherwise one cannot derive the KL based objective, often used in TS framework.': 1.098611831665039, 'As such, approach is rater constrained in scope.': 1.0892252922058105, 'Authors shall refer in the related-work to similar ideas proposed in the related field of acoustic modelling (and adaptation of acoustic models), in particular, works of Yu et al.': 1.098611831665039, '[1] and the follow up work of Li et al.': 1.0985653400421143, '[2] which to an extent are a prior the work on knowledge distillation.': 1.0986123085021973, 'Reasonably related is also work on deep relationship networks [3], where MT generative approach is proposed to avoid negative transfers, something of central role in this paper.': 1.0986123085021973, 'Minors:': 1.0986123085021973, 'The student S similarly has an output probability -> models an output probability': 1.0986123085021973, '[1] KL-Divergence Regularized Deep Neural Network Adaptation For Improved Large Vocabulary Speech Recognition, Dong Yu, Kaisheng Yao, Hang Su, Gang Li, Frank Seide': 1.0986123085021973, '[2] Learning Small-Size DNN with Output-Distribution-Based Criteria, Jinyu Li, Rui Zhao, Jui-Ting Huang and Yifan Gong': 1.0986123085021973, '[3] Learning Multiple Tasks with Deep Relationship Networks, Mingsheng Long, Jianmin Wang': 1.0986123085021973}"
418,https://openreview.net/forum?id=rJTKKKqeg,"{'The paper proposed a multi-memory mechanism that memorizes different information into different components/entities.': 1.0986123085021973, 'It could be considered as a mixture model in RNN.': 1.0712430477142334, 'This is a very interesting model and result is convincing.': 1.0946334600448608, 'A limitation is that we do not know how to generalize to some unseen entities and how to visualize what entities the model learned.': 1.0986123085021973, 'This paper proposes a new memory augmented neural network (MANN) model called recurrent entity network (EntNet).': 1.0986013412475586, 'EntNet can be considered as a bank of RNNs with gating mechanism to update the hidden states of the RNNs and the hidden states act like the memory slots.': 1.0983130931854248, 'The model is very much relevant to NTM style architectures.': 1.0981230735778809, 'It is known that training the controller in NTM to read/write from memory slots is challenging.': 1.0983822345733643, 'EntNet cleverly pushes the complexity of the controller to individual memory slots.': 1.0986043214797974, 'It is as if each slot has a controller and they all act in a distributed manner.': 1.0986123085021973, 'Authors report strong results in bAbI tasks where the model achieves state of the art performance.': 1.0986076593399048, 'Synthetic world experiments justify that the model learns to capture the world dynamics.': 1.0986095666885376, 'However it is not clear if this will be scalable to complex real tasks.': 1.0983892679214478, 'EntNet also achieves reasonable performance with one-shot reading of the passage in CBT task.': 1.0984816551208496, 'I see EntNet as a generalization of RNNs and has some advantage over NTMs when it comes to training complexity.': 0.46177011728286743, 'This is definitely a good contribution to the conference.': 1.0986109972000122, 'I see that EntNet can have several other applications in future.': 0.4047797620296478, 'Authors have provided convincing answers to my pre-review questions.': 1.0985997915267944, 'Few more questions:': 1.0986123085021973, '1. Do you fix the size of the f vector set in equation (1)? If so, to what size in all the experiments?': 0.4064301550388336, '2. There are so many training details in the paper which makes it difficult to reproduce the results. Can the authors release the source code to reproduce all the results in the paper? I am willing to increase my rating if authors can release the code.': 0.38305002450942993, 'The work proposes a variant of a recurrent neural network that can selectively update a fixed number of multiple memory slots to update entity states.': 1.0986123085021973, 'The architecture is well motivated, especially with the motivating example, and the operation is shown to validate the intuition as shown in visualizations.': 1.0986123085021973, 'Experimental results, datasets and the baselines used are sufficient to quantitatively show the strength of the proposed architecture.': 1.0986123085021973, 'A limitation is failing to (explicitly) generalize to unseen entities, however this is not a trivial problem on its own and the authors have addressed to this issue and proposed several ideas as workarounds.': 1.0986117124557495, 'I consider the work as a good conference contribution.': 1.098610281944275}"
419,https://openreview.net/forum?id=rJXTf9Bxg,"{'Apologies for the late review.': 1.0986123085021973, 'This submission proposes method for class-conditional generative image modeling using auxiliary classifiers.': 1.0986123085021973, 'Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution.': 1.0882058143615723, 'The discriminator has two outputs and two corresponding objectives: determine whether a sample is real or generated, and independently to predict the (real or sampled) class label corresponding to the sample.': 1.070622444152832, 'Figure 2. nicely illustrates related methods - this particular method bears similarities to InfoGANs and Semi-supervised GANs.': 1.0986121892929077, 'Compared to infogans, this method also encourages correspondence between the latent c and the real class labels for the real examples (whereas infogans are presented as fully unsupervised).': 1.0985790491104126, 'The authors attempt at evaluating the method quantitatively by looking at the discriminability and diversity of samples.': 1.0986109972000122, 'It is found - not surprisingly - that higher resolution improves discriminability (because more information is present).': 0.6411932110786438, 'Discriminability: Figure 3 doesn’t have legends': 1.098090410232544, 'so it is a bit hard to understand what is going on.': 1.0986123085021973, 'Furthermore, my understanding is that when evaluating discriminability the authors downsample and then bicubically upsample the image, which is much more like a blurring, very different from retraining all the models to work on low resolution in the first place.': 1.0801297426223755, 'Diversity: The authors try to quantitatively evaluate diversity of samples by measuring the average MS-SSIM between randomly selected pairs of points within each class.': 1.0985788106918335, 'I think this method is significantly flawed and limited, for reasons mentioned in (Theis et al, 2015, A note on the evaluation…).': 1.0955095291137695, 'In its behaviour, MS-SSIM is not that dissimilar from Euclidean distance - although it is nonlinear and is bounded between -1 and 1.': 1.0985270738601685, 'Evaluating diversity/entropy of samples in high dimensions is very hard, especially if the distributions involved are non-trivial for example concentrated around manifolds.': 0.48514819145202637, 'Consider for example a generative model which randomly samples just two images.': 1.0986123085021973, 'Assuming that the MSSSIM between these two images is -1, this generative model can easily achieve an average MSSSIM score of 0, implying a conclusion that this model has more diversity than the training data itself.': 0.7066807150840759, 'Conversely, SSIM is designed not to be sensitive to contrast and average pixel intensity, so if a model is diverse in this sense, that will be ignored by this measure.': 0.9700875878334045, 'Overall, the paper proposes a new way to incorporate class labels into training GAN-type models.': 1.098008632659912, 'As far as I know the particular algorithm is novel, but I consider it incremental compared to what has been done before.': 1.0829637050628662, 'I think the proposed evaluation metrics are flawed, especially when evaluating the diversity of the samples for the aforementioned reasons.': 1.0978726148605347, 'This is a clear, easy to read, highly relevant paper that improves GAN training for images and explores evaluation criterion on GANs.': 1.0986123085021973, 'The main contributions are as follows:': 1.0986123085021973, 'Adding an auxiliary classifier head to a GAN discriminator and training a classification objective in addition to the real/fake objective improves performance.': 1.0986123085021973, 'Generator is conditioned on 1-hot encoding of class and is trained to generate the specified class.': 1.0986123085021973, 'Training different models on different subsets of imagenet classes improves performance.': 1.0986123085021973, 'They motivate evaluating GAN images by using a perceptual similarity metric (MS-SSIM) on pairs of samples to quantify diversity in the samples (and detect mode collapse)': 1.0986123085021973, 'They show this metric correlates with a discriminability metric (classification accuracy of pre-trained imagenet model on generated samples) .': 1.0986123085021973, 'The overall novelty of this approach is somewhat lacking in that previous methods have proposed training a classifier head on the discriminator and the discriminability metric proposed is simply the inception score of [1] except with class information.': 1.098466396331787, 'However, I think there is still a contribution to be made my putting these tricks together and successfully demonstrating image synthesis gains.': 1.0986123085021973, 'Questions for the authors:': 1.0986123085021973, '(1) Why do you think splitting the imagenet training into 100 different models improves performance?': 1.0986123085021973, 'Is the issue with the representation of the class?': 1.0986037254333496, 'In other words, if an encoding more meaningful that 1-hot vector was used do you still think 100 models would be needed.': 1.0986123085021973, 'Ideally we should hope that a generative model can leverage information from different classes to help with the generation of a particular class and also text-image synthesis models [2] have been quite successful when trained on diverse datasets (and these are conditioned on a semantically meaningful text encoding) which suggests to be that the issue is with the representation.': 1.0986120700836182, '(2) In section 3 the AC-GAN classification objective (omitting expectation for brevity) is given as L_S = log P(C=c|X_real) + log P(C=c|X_fake) and you say that both the discriminator and generator are trained to maximize this quantity.': 1.0985796451568604, 'Obviously the generator would want to maximize  log P(C=c|X_fake) for its given conditioning class c.': 1.0986119508743286, 'But can you explain why you would want the discriminator to also maximize the classification accuracy of generated samples?': 1.0986007452011108, 'Why not do something similar to the CatGAN paper': 1.091107726097107, '[3] and train the discriminator to be as uncertain as possible about the generated examples.': 0.9602858424186707, 'Seems counterintuitive to me to have both the generator and discriminator trying to optimize the same classification objective rather than not be adversarial wrt to this loss as well as the real/fake loss.': 1.079535722732544, 'Overall, this paper makes a clear contribution to GAN research both in terms of image quality and evaluation metrics and I would recommend it for acceptance.': 1.0986053943634033, '[1] Salimans et al.': 1.0986123085021973, 'Improved Techniques for Training GANs (https://arxiv.org/abs/1606.03498)': 1.0986123085021973, '[2] Reed et al.': 1.0986123085021973, 'Generative Adversarial Text to Image Synthesis (https://arxiv.org/abs/1605.05396)': 1.0986123085021973, '[3] Jost Tobias Springenberg.': 1.0986123085021973, 'Unsupervised and semi-supervised learning with categorical generative adversarial networks (https://arxiv.org/abs/1511.06390)': 1.0986123085021973, 'This paper introduces a class-conditional GAN as a generative model for images.': 1.0986123085021973, 'It introduces two main diagnostic tools for training GANs: one to assess whether a model is making full use of its output resolution and another to measure the diversity of generated samples.': 1.0986123085021973, 'Experiments are conducted on the CIFAR-10 and ImageNet datasets.': 1.0986123085021973, 'Pros:': 1.0986123085021973, '+': 1.0986123085021973, 'The paper is clear and well-written.': 1.0986121892929077, 'Experiments performed in the relatively under-explored 128 x 128 ImageNet setting.': 0.5623977780342102, 'The proposed MS-SSIM diversity metric appears to be a useful tool for detecting convergence issues in class-conditional GAN models.': 1.0986121892929077, 'Cons:': 1.0986123085021973, 'AC-GAN model itself is of limited novelty relative to other GAN approaches that condition on class.': 0.8351774215698242, 'Diversity metric is of limited use for training non class-conditional GANs.': 0.6626290678977966, 'No experimental comparison of AC-GAN to other class-conditional models.': 1.0986123085021973, 'To my knowledge training GANs on large, diverse images such as 128 x 128 ImageNet images is under-explored ([1] contains just a few samples in this setting).': 1.0986121892929077, 'Though the model is not very novel and a comparison to other class-conditional models is lacking, I feel the community will find the diagnostic tools and the thorough exploration of the ImageNet-trained model to be of interest.': 1.0986101627349854, '* Section 4.2: MS-SSIM is traditionally defined for grayscale images only.': 1.0918554067611694, 'How do you extend MS-SSIM to color images in your work?': 0.5683016180992126, 'Were they computed channel-wise across R,G, and B?': 1.0968321561813354, '* Section 4.4: It is difficult to tell whether a single AC-GAN was trained for all of CIFAR-10 or one for each group.': 0.6568763256072998, 'If single, why were the samples split into groups for computing Inception Score?': 0.4198875427246094, 'And if multiple, the comparison to Salimans et al. is not a direct one.': 1.097836971282959, 'Also it would be helpful to include the real data Inception score as a point of comparison.': 0.9584515690803528, '* Appendix D: The caption of Figure 9 states that the same number of training steps was taken for each model.': 0.40554356575012207, 'From this it seems possible that the models with more classes simply did not converge yet.': 1.0943595170974731, '[1] Salimans, Tim, et al.': 1.098610281944275, '""Improved techniques for training GANs.""': 1.0986123085021973, 'Advances in Neural Information Processing Systems.': 1.0986123085021973, '2016.': 1.0986123085021973}"
420,https://openreview.net/forum?id=rJY0-Kcll,"{'This work presents an LSTM based meta-learning framework to learn the optimization algorithm of a another learning algorithm (here a NN).': 1.0986123085021973, 'The paper is globally well written and the presentation of the main material is clear.': 1.0986096858978271, 'The crux of the paper: drawing the parallel between Robbins Monroe update rule and the LSTM update rule and exploit it to satisfy the two main desiderata of few shot learning (1- quick acquisition of new knowledge, 2- slower extraction of general transferable knowledge) is intriguing.': 1.0986123085021973, 'Several tricks re-used from (Andrychowicz et al. 2016)  such as parameter sharing and normalization, and novel design choices (specific implementation of batch normalization) are well  motivated.': 1.0986123085021973, 'The experiments are convincing.': 1.0986123085021973, 'This is a strong paper.': 1.0986123085021973, 'My only concerns/questions are the following:': 1.089233636856079, '1. Can it be redundant to use the loss, gradient and parameters as input to the meta-learner? Did you do ablative studies to make sure simpler combinations are not enough.': 0.2489543855190277, '2. It would be great if other architectural components of the network can be learned in a similar fashion (number of neurons, type of units, etc.). Do you have an opinion about this?': 0.14220353960990906, '3. The related work section (mainly focused on meta learning) is a bit shallow. Meta-learning is a rather old topic and similar approaches have been tried to solve the same problem even if they were not using LSTMs:': 0.20061786472797394, '- Samy Bengio PhD thesis (1989) is all about this ;-)': 0.5598434209823608, '- Use of genetic programming for the search of a new learning rule for neural networks (S. Bengio, Y. Bengio, and J. Cloutier. 1994)': 0.1736525595188141, '- I am convince Schmidhuber has done something, make sure you find it and update related work section.': 1.0980064868927002, 'Overall, I like the paper.': 1.0985820293426514, 'I believe the discussed material is relevant to a wide audience at ICLR.': 0.7543059587478638, 'This paper describes a new approach to meta learning by interpreting the SGD update rule as gated recurrent model with trainable parameters.': 1.0986123085021973, 'The idea is original and important for research related to transfer learning.': 1.0085259675979614, 'The paper has a clear structure, but clarity could be improved at some points.': 1.0986117124557495, 'Pros:': 1.0986123085021973, 'An interesting and feasible approach to meta-learning': 1.0753148794174194, 'Competitive results and proper comparison to state-of-the-art': 1.0986123085021973, 'Good recommendations for practical systems': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'The analogy would be closer to GRUs than LSTMs': 1.0978504419326782, 'The description of the data separation in meta sets is hard to follow and could be visualized': 1.0984370708465576, 'The experimental evaluation is only partly satisfying, especially the effect of the parameters of i_t and f_t would be of interest': 1.0986120700836182, ""Fig 2 doesn't have much value"": 1.0986114740371704, 'Remarks:': 1.0986123085021973, 'Small typo in 3.2: ""This means each coordinate has it"" -> its': 1.012137532234192, '>': 1.0985294580459595, 'We plan on releasing the code used in our evaluation experiments.': 1.0978726148605347, 'This would certainly be a major plus.': 1.0986123085021973, ""In light of the authors' responsiveness and the updates to the manuscript"": 1.098017692565918, 'in particular to clarify the meta-learning task': 1.096785068511963, 'I am updating my score to an 8.': 1.0986123085021973, 'This manuscript proposes to tackle few-shot learning with neural networks by leveraging meta-learning, a classic idea that has seen a renaissance in the last 12 months.': 1.0986123085021973, 'The authors formulate few-shot learning as a sequential meta-learning problem: each ""example"" includes a sequence of batches of ""training"" pairs, followed by a final ""test"" batch.': 1.0986123085021973, 'The inputs at each ""step"" include the outputs of a ""base learner"" (e.g., training loss and gradients), as well as the base learner\'s current state (parameters).': 1.0986123085021973, 'The paper applies an LSTM to this meta-learning problem, using the inner memory cells in the *second* layer to directly model the updated parameters of the base learner.': 1.0985777378082275, 'In doing this, they note similarities between the respective update rules of LSTM memory cells and gradient descent.': 1.0986055135726929, 'Updates to the LSTM meta-learner are computed based on the base learner\'s prediction loss for the final ""test"" batch.': 1.0986123085021973, 'The authors make several simplifying assumptions, such as sharing weights across all second layer cells (analogous to using the same learning rate for all parameters).': 1.098610520362854, 'The paper recreates the Mini-ImageNet data set proposed in Vinyals et al 2016, and shows that the meta-learner LSTM is competitive with the current state-of-the-art (Matchin Networks, Vinyals 2016) on 1- and 5-shot learning.': 1.0936256647109985, 'Strengths:': 1.098570466041565, 'It is intriguing': 1.092363953590393, 'and in hindsight, natural': 1.0986121892929077, 'to cast the few-shot learning problem as a sequential (meta-)learning problem.': 1.0971543788909912, 'While the authors did not originate the general idea of persisting learning across a series of learning problems, I think it is fair to say that they have advanced the state of the art, though I cannot confidently assert its novelty as I am not deeply familiar with recent work on meta-learning.': 1.0983132123947144, 'The proposed approach is competitive with and outperforms Vinyals 2016 in 1-shot and 5-shot Mini-ImageNet experiments.': 1.0842994451522827, 'The base learner in this setting (simple ConvNet classifier) is quite different from the nearest-neighbor-on-top-of-learned-embedding approach used in Vinyals 2016.': 1.0888056755065918, 'It is always exciting when state-of-the-art results can be reported using very different approaches, rather than incremental follow-up work.': 1.0985968112945557, 'As far as I know, the insight about the relationship between the memory cell and gradient descent updates is novel here.': 1.0874253511428833, 'It is interesting regardless.': 1.0729429721832275, 'The paper offers several practical insights about how to design and train an LSTM meta-learner, which should make it easier for others to replicate this work and apply these ideas to new problems.': 1.0931625366210938, 'These include proper initialization, weight sharing across coordinates, and the importance of normalizing/rescaling the loss, gradient, and parameter inputs.': 0.42019951343536377, 'Some of the insights have been previously described (the importance of simulating test conditions during meta-training; assuming independence between meta-learner and base learner parameters when taking gradients with respect to the meta-learner parameters), but the discussion here is useful nonetheless.': 1.0895864963531494, 'Weaknesses:': 1.0986123085021973, 'The writing is at times quite opaque.': 1.0986123085021973, 'While it describes very interesting work, I would not call the paper an enjoyable read.': 1.0980987548828125, 'It took me multiple passes (as well as consulting related work) to understand the general learning problem.': 0.5889986157417297, 'The task description in Section 2 (Page 2) is very abstract and uses notation and language that is not common outside of this sub-area.': 0.5673699975013733, 'The paper could benefit from a brief concrete example (based on MNIST is fine), perhaps paired with a diagram illustrating a sequence of few-shot learning tasks.': 1.0985373258590698, 'This would definitely make it accessible to a wider audience.': 1.0985949039459229, 'Following up on that note, the precise nature of the N-class, few-shot learning problem here is unclear to me.': 0.9188682436943054, 'Specifically, the Mini-ImageNet data set has 100 labels, of which 64/16/20 are used during meta-training/validation/testing.': 0.7839938402175903, 'Does this mean that only 64/100 classes are observed through meta-training?': 1.0458984375, 'Or does it mean that only 64/100 are observed in each batch, but on average all 100 are observed during meta-training?': 0.866575300693512, ""If it's the former, how many outputs does the softmax layer of the ConvNet base learner have during meta-training?"": 0.4784678518772125, '64 (only those observed in training) or 100 (of which 36 are never observed)?': 1.0985866785049438, 'Many other details like these are unclear (see question).': 1.0889110565185547, 'The plots in Figure 2 are pretty uninformative in and of themselves, and the discussion section offers very little insight around them.': 0.8843052983283997, 'This is an interesting paper with convincing results.': 1.0984718799591064, 'It seems like a fairly clear accept, but the presentation of the ideas and work therein could be improved.': 0.46053266525268555, 'I will definitely raise my score if the writing is improved.': 0.5297418832778931}"
421,https://openreview.net/forum?id=rJY3vK9eg,"{'This paper applies the pointer network architecture—wherein an attention mechanism is fashioned to point to elements of an input sequence, allowing a decoder to output said elements—in order to solve simple combinatorial optimization problems such as the well-known travelling salesman problem.': 1.0986123085021973, 'The network is trained by reinforcement learning using an actor-critic method, with the actor trained using the REINFORCE method, and the critic used to estimate the reward baseline within the REINFORCE objective.': 1.0980558395385742, 'The paper is well written and easy to understand.': 1.0986123085021973, 'Its use of a reinforcement learning and attention model framework to learn the structure of the space in which combinatorial problems of variable size can be tackled appears novel.': 1.0986119508743286, 'Importantly, it provides an interesting research avenue for revisiting classical neural-based solutions to some combinatorial optimization problems, using recently-developed sequence-to-sequence approaches.': 1.0956647396087646, 'As such, I think it merits consideration for the conference.': 1.0986123085021973, 'I have a few comments and some important reservations with the paper:': 1.0955076217651367, '1) I take exception to the conclusion that the pointer network approach can handle general types of combinatorial optimization problems.': 1.0986104011535645, 'The crux of combinatorial problems — for practical applications — lies in the complex constraints that define feasible solutions (e.g. simple generalizations of the TSP that involve time windows, or multiple salesmen).': 1.0986123085021973, 'For these problems, it is no longer so simple to exclude possible solutions from the enumeration of the solution by just « striking off » previously-visited instances; in fact, for many of these problems, finding a single feasible solution might in general be a challenge.': 1.098611831665039, 'It would be relevant to include a discussion of whether the Neural Combinatorial Optimization approach could scale to these important classes of problems, and if so, how.': 1.0986062288284302, 'My understanding is that this approach, as presented, would be mostly suitable for assignment problems with a very simple constraint structure.': 1.0985935926437378, '2)': 1.0986123085021973, 'The operations research literature is replete with a large number of benchmark problems that have become standard to compare solver quality.': 1.0984771251678467, 'For instance, TSPLIB contains a large number of TSP instances (http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/).': 0.989128589630127, 'Likewise, combinatorial optimization problems of various difficulties can be found here: http://www.mat.univie.ac.at/~neum/glopt/test.html.': 1.092800498008728, 'It would greatly add to the paper depth to compare the NCO solution quality on some of these problems.': 1.0986106395721436, '(The CPLEX solver famously evaluates its own progress on such a public library of problem instances.)': 0.5322245359420776, '3)': 1.0986108779907227, 'The paper should explain more clearly the structure of the critic network, and how it performs the mapping from a sequence of cities into a baseline prediction.': 1.0986076593399048, '4) Suggestions for Algorithm 1:': 1.091326117515564, 'Line 5, 6: the notation  is not clear; would  be what’s intended?': 1.098481297492981, 'Line 7: I’m assuming that this assignment must be done , as in lines 5,6?': 1.0470383167266846, 'Line 8:  is used in two different ways, on the LHS and RHS — slight abuse of notation (but we understand the intent).': 1.0697427988052368, '5) Suggestions for Algorithm 2:': 0.914366602897644, 'Line 13: same as for Algorithm 1': 1.0980790853500366, 'Line 15: use  instead of  to indicate multiplication': 1.0691585540771484, 'This paper is methodologically very interesting, and just based on the methodological contribution I would vote for acceptance.': 1.0986123085021973, ""However, the paper's sweeping claims of clearly beating existing baselines for TSP have been shown to not hold, with the local search method LK-H solving all the authors' instances to optimality"": 1.0986123085021973, ""in seconds on a CPU, compared to clearly suboptimal results by the authors' method in 25h on a GPU."": 1.0981327295303345, 'Seeing this clear dominance of the local search method LK-H, I find it irresponsible by the authors that they left Figure 1 as it is': 1.096807599067688, 'with the line for ""local search"" referring to an obviously poor implementation by Google rather than the LK-H local search method that everyone uses.': 1.0887998342514038, 'For example, at NIPS, I saw this Figure 1 being used in a talk (I am not sure anymore by whom, but I don\'t think it was by the authors), the narrative being ""RNNs now also clearly perform better than local search"".': 1.0986121892929077, 'Of course, people would use a figure like that for that purpose, and it is clearly up to the authors to avoid such misconceptions.': 1.098315715789795, 'The right course of action upon realizing the real strength of local search with LK-H would\'ve been to make ""local search"" the same line as ""Optimal"", showing that the authors\' method is still far worse than proper local search.': 1.0986123085021973, 'But the authors chose to leave the figure as it was, still suggesting that their method is far better than local search.': 1.0986121892929077, ""Probably the authors didn't even think about this, but this of course will mislead the many superficial readers."": 1.0986003875732422, 'To people outside of deep learning, this must look like a sensational yet obviously wrong claim.': 1.0986104011535645, 'I thus vote for rejection despite the interesting method.': 1.09861159324646, 'Update after rebuttal and changes:': 1.0986123085021973, ""I'm torn about this paper."": 0.6876441240310669, 'On the one hand, the paper is very well written': 1.0935676097869873, 'and I do think the method is very interesting and promising.': 0.48534250259399414, ""I'd even like to try it and improve it in the future."": 1.0985113382339478, 'So, from that point of view a clear accept.': 1.0952454805374146, ""On the other hand, the paper was using extremely poor baselines, making the authors' method appear sensationally strong in comparison, and over the course of many iterations of reviewer questions and anonymous feedback, this has come down to the authors' methods being far inferior to the state of the art."": 1.0941777229309082, ""That's fine (I expected that all along), but the problem is that the authors don't seem to want this to be true..."": 1.0714490413665771, 'E.g., they make statements, such as ""We find that both greedy approaches are time-efficient and just a few percents worse than optimality.""': 1.0985665321350098, 'That statement may be true, but it is very well known in the TSP community that it is typically quite trivial to get to a few percent worse than optimality.': 1.0773428678512573, ""What's hard and interesting is to push those last few percent."": 0.8780166506767273, ""(As a side note: the authors probably don't stop LK-H once it has found the optimal solution, like they do with their own method after finding a local optimum."": 0.5512790083885193, ""LK-H is an anytime algorithm, so even if it ran for a day that doesn't mean that it didn't find the optimal solution after milliseconds"": 0.7884023785591125, 'and a solution a few percent suboptimal even faster).': 0.6468427181243896, 'Nevertheless, since the claims have been toned down over the course of the many iterations, I was starting to feel more positive about this paper when just re-reading it.': 1.0626428127288818, 'That is, until I got to the section on Knapsack solving.': 1.098332166671753, 'The version of the paper I reviewed was not bad here, as it at least stated two simple heuristics that yield optimal solutions:': 1.0985941886901855, '""Two simple heuristics are ExpKnap, which employs brand-and-bound with Linear Programming bounds (Pisinger, 1995), and MinKnap, which employs dynamic programming with enumerative bounds (Pisinger, 1997).': 0.4258578419685364, 'Exact solutions can also be optained by quantizing the weights to high precisions and then performing dynamic programming with a pseudo-polynomial complexity (Bertsimas & Demir, 2002).""': 1.0984736680984497, 'That version then went on to show that these simple heuristics were already optimal, just like their own method.': 1.0972193479537964, 'In a revision between December 11 and 14, however, that paragraph, along with the optimal results of ExpKnap and MinKnap seems to have been dropped, and the authors instead introduced two new poor baseline methods (random search and greedy).': 1.0956145524978638, 'This was likely in an effort to find some methods that are not optimal on these very easy instances.': 1.098400354385376, 'I personally find it pointless to present results for random search here, as nobody would use that for TSP.': 1.0898853540420532, ""It's like comparing results on MNIST against a decision stump (yes, you'll do better than that, but that is not surprising)."": 1.0939382314682007, 'The results for greedy are interesting to see.': 0.8600173592567444, ""However, dropping the strong results of the simple heuristics ExpKnap and MinKnap (and their entire discussion) appears unresponsible, since the resulting table in the new version of the paper now suggests that the authors' method is better than all baselines."": 1.084592342376709, ""Of course, if all that one is after is a column of bold numbers for ones own approach that's what one can do, but I don't find it responsible to hide the better baselines."": 1.0986087322235107, ""Also, why don't the authors try at least the same OR-tools solver from Google that they tried for TSP?"": 1.0986121892929077, 'It seems to support Knapsack directly: https://developers.google.com/optimization/bin/knapsack': 1.0986123085021973, ""Really, all I'm after is a responsible (and if you wish, humble) presentation of what I believe to be great results that are really promising for the field."": 1.0981836318969727, 'The authors just need to make it crystal clear that, as of now, their method is still very far away from the state of the art.': 1.0986049175262451, ""And that's OK; you don't typically beat an entire field with one paper."": 1.0986123085021973, ""If the authors clearly stated that throughout, I would clearly argue for acceptance ... (Maybe that's not the norm in machine learning, but I don't think you have to beat everything quite yet if your approach is very different and promising"": 1.0955849885940552, 'see DL and ImageNet.)': 1.0986123085021973, 'Concretely, I would recommend that the authors do the following:': 1.0986123085021973, 'Put the original Figure 1 back in, but dropping the previous poor local search and labelling the line at 1.0 ""local search (LK-H) = exact (Concorde)"".': 1.0808035135269165, 'If the authors would like to, they could also in addition leave the previous poor local search in and label it ""Google OR tools (generic local search)""': 1.0985980033874512, 'Put the other baselines back into the Knapsack section': 1.0986123085021973, ""Make sure the wording clearly states throughout that the method is still quite far from the state of the art (it's perfectly fine to strongly state that the direction is very promising)."": 1.0985795259475708, 'Overall, this paper has to watch out for not becoming an example of promising too much (some would call it hype-generation) before having state-of-the-art results.': 1.0965090990066528, '(I believe the previous comparison against local search fell into that category, and now the current section on Knapsack does.)': 1.0986123085021973, ""I believe that would do a great disservice to our community since it builds up an expectation that the field cannot live up to (and that's a recipe for building up a bubble that has no other chance but burst)."": 1.0986119508743286, 'Having said all that, I think the paper can be saved if the authors embrace that they are still far away from the state of the art.': 1.098563313484192, ""I'll be optimistic and trust that they will come around and make the changes I suggested above."": 1.0986123085021973, ""Hoping for those changes, since I think the method is a solid step forward, I'm updating my score to a weak accept."": 1.0986123085021973, 'This paper proposes to use RNN and reinforcement learning for solving combinatorial optimization problems.': 1.0986123085021973, 'The use of pointer network is interesting as it enables generalization to arbitrary input size.': 1.0986123085021973, 'The proposed method also ""fintunes"" on test examples with active search to achieve better performance.': 1.0986123085021973, 'The proposed method is theoretically interesting as it shows that RNN and RL can be combined to solve combinatorial optimization problems and achieve comparable performance to traditional heuristic based algorithms.': 1.0986123085021973, 'However, the lack of complexity comparison against baselines make it impossible to tell whether the proposed method has any practical value.': 1.0986123085021973, 'The matter is further complicated by the fact that the proposed method runs on GPU while baselines run on CPU: it is hard to even come up with a meaningful unit of complexity.': 1.0986123085021973, 'Money spent on hardware and electricity per instance may be a viable option.': 1.0986123085021973, 'Further more, the performance comparisons should be taken with a grain of salt as traditional heuristic based algorithms can often give better performance if allowed more computation, which is not controlled across algorithms.': 1.0986123085021973}"
422,https://openreview.net/forum?id=rJbPBt9lg,"{'This paper considers the code completion problem: given partially written source code produce a distribution over the next token or sequence of tokens.': 1.3862943649291992, 'This is an interesting and important problem with relevance to industry and research.': 1.3862943649291992, 'The authors propose an LSTM model that sequentially generates a depth-first traversal over an AST.': 1.3862943649291992, 'Not surprisingly the results improve over previous approaches with more brittle conditioning mechanisms (Bielik et al. 2016).': 1.3862943649291992, 'Still, simply augmenting previous work with LSTM-based conditioning is not enough of a contribution to justify an entire paper.': 1.386289119720459, 'Some directions that would greatly improve the contribution include: considering distinct traversal orders, does this change the predictive accuracy?': 1.3862943649291992, 'Any other ways of dealing with UNK tokens?': 1.3862943649291992, 'The ultimate goal of this paper is to improve code completion, and it would be great to go beyond simply neurifying previous methods.': 1.3862943649291992, 'Comments:': 1.3862531185150146, 'Last two sentences of related work claim that other methods can only ""examine a limited subset of source code"".': 1.3862941265106201, ""Aside from being a vague statement, it isn't accurate."": 1.3821851015090942, 'The models described in Bielik et al. 2016 and Maddison & Tarlow 2014 can in principle condition on any part of the AST already generated.': 1.3860417604446411, ""The difference in this work is that the LSTM can learn to condition in a flexible way that doesn't increase the complexity of the computation."": 1.378753900527954, ""In the denying prediction experiments, the most interesting number is the Prediction Accuracy, which is P(accurate | model doesn't predict UNK)."": 1.3862930536270142, 'I think it would also be interesting to see P(accurate': 1.386293888092041, '| UNK is not ground truth).': 1.3823504447937012, 'Clearly the models trained to ignore UNK losses will do worse overall, but do they do worse on non-UNK tokens?': 1.3862627744674683, 'Pros:': 1.3862943649291992, 'using neural network on a new domain.': 1.3862943649291992, 'Cons:': 1.1442161798477173, 'It is not clear how it is guaranteed that the network generates syntactically correct code.': 1.3862943649291992, 'Questions, comments:': 1.3862943649291992, 'How is the NT2N+NTN2T top 5 accuracy is computed?': 1.3862147331237793, 'Maximizing the multiplied posterior probability of the two classifications?': 0.9743015170097351, 'Were all combinations of NT2N decision with all possible NTN2T considered?': 1.2442023754119873, 'Using UNK is obvious and should be included from the very beginning in all models, since the authors selected the size of the': 1.3862637281417847, 'lexicon, thus limited the possible predictions.': 1.3862943649291992, 'The question should then more likely be what is the optimal value of alpha for UNK.': 0.9940751194953918, 'See also my previous comment on estimating and using UNK.': 1.3862923383712769, 'Section 5.5, second paragraph, compares numbers which are not comparable.': 1.3862943649291992, 'This paper studies the problem of source code completion using neural network models.': 1.386293649673462, 'A variety of models are presented, all of which are simple variations on LSTMs, adapted to the peculiarities of the data representation chosen (code is represented as a sequence of (nonterminal, terminal) pairs with terminals being allowed to be EMPTY).': 1.3862888813018799, 'Another minor tweak is the option to ""deny prediction,"" which makes sense in the context of code completion in an IDE, as it\'s probably better to not make a prediction if the model is very unsure about what comes next.': 1.3862943649291992, 'Empirically, results show that performance is worse than previous work on predicting terminals but better at predicting nonterminals.': 1.2063571214675903, ""However, I find the split between terminals and nonterminals to be strange, and it's not clear to me what the takeaway is."": 1.3862943649291992, 'Surely a simple proxy for what we care about is how often the system is going to suggest the next token that actually appears in the code.': 1.3763039112091064, 'Why not compute this and report a single number to summarize the performance?': 1.3862943649291992, 'Overall the paper is OK, but it has a flavor of ""we ran LSTMs on an existing dataset"".': 1.386060118675232, 'The results are OK but not amazing.': 0.9984483122825623, 'There are also some issues with the writing that could be improved (see below).': 1.3862943649291992, ""In total, I don't think there is a big enough contribution to warrant publication at ICLR."": 1.3862943649291992, 'Detailed comments:': 1.3862943649291992, '* I find the NT2NT model strange, in that it predicts the nonterminal and the terminal independently conditional upon the hidden state.': 1.3862943649291992, '* The discussion of related work needs reworking.': 1.3862943649291992, 'For example, Bielik et al. does not generalize all of the works listed at the start of section 2, and the Maddison (2016) citation is wrong': 1.3862943649291992, 'While the overall direction is promising, there are several serious issues with the paper which affect the novelty and validity of the results:': 1.3834551572799683, '1. Incorrect claims about related work affecting novelty:': 1.3862943649291992, '- This work is not the first to explore a deep learning approach to automatic code completion: “Toward Deep Learning Software Repositories”, MSR’15 also uses deep learning for code completion, and is not cited.': 1.245827078819275, '- “Code Completion with Statistical Language Models”, PLDI’14 is cited incorrectly': 1.3862943649291992, 'it also does code completion with recurrent neural networks.': 0.6955109238624573, '- PHOG is independent of JavaScript': 1.3862943649291992, 'it does representation learning and has been applied to other languages (e.g., Python, see OOPSLA’16 below).': 1.0065680742263794, '- This submission is not the only one that “can automatically extract features”. Some high-precision (cited) baselines do it.': 1.3862943649291992, '- “Structured generative models of natural source code” is an incorrect citation. It is from ICML’14 and has more authors. It is also a log-linear model and conditions on more context than claimed in this submission.': 0.9539638161659241, '2. Uses a non-comparable prediction task for non-terminal symbols: The type of prediction made here is simpler than the one used in PHOG and state-of-the-art (see OOPSLA’16 paper below) and thus the claimed 11 point improvement is not substantiated. In particular, in JavaScript there are 44 types of nodes. However, a PHOG and OOPSLA’16 predictions considers not only these 44 types, but also whether there are right siblings and children of a node. This is necessary for predicting tree fragments instead of a sequence of nodes. It however makes the prediction harder than the one considered here (it leads to 150+ labels, a >3x increase).': 0.855401337146759, '3. Not comparing to state-of-the-art: the state-of-the-art however is not the basic PHOG cited here, but “Probabilistic Model for Code with Decision Trees”, (OOPSLA 2016) which appeared before the submission deadline for ICLR’17:': 0.5630583167076111, 'http://dl.acm.org/citation.cfm?id=2984041': 1.3858468532562256, 'On the same dataset, OOPSLA’16 has accuracy of 83.9%, and on the more difficult task than considered here (see above point).': 1.2781773805618286, 'Further, state-of-the-art (OOPSLA’16) has 2% higher accuracy than PHOG for tree values, which makes state of the art better than the current sub by 5%.': 0.4644940495491028, 'The work requires a deep revision pass w.r.t to novelty and precision claims which are currently incorrect.': 0.9914785027503967}"
423,https://openreview.net/forum?id=rJbbOLcex,"{'This paper introduces a model that blends ideas from generative topic models with those from recurrent neural network language models.': 0.6240279674530029, 'The authors evaluate the proposed approach on a document level classification benchmark as well as a language modeling benchmark and it seems to work well.': 1.0977203845977783, 'There is also some analysis as to topics learned by the model and its ability to generate text.': 0.30661001801490784, 'Overall the paper is clearly written and with the code promised by the authors others should be able to re-implement the approach.': 0.9787024855613708, 'I have 2 potentially major questions I would ask the authors to address:': 1.096165657043457, '1 - LDA topic models make an exchangability (bag of words) assumption.': 1.0986123085021973, 'The discussion of the generative story for TopicRNN should explicitly discuss whether this assumption is also made.': 0.4899962246417999, 'On the surface it appears it is since y_t is sampled using only the document topic vector and h_t': 1.0983726978302002, 'but we know that in practice h_t comes from a recurrent model that observes y_t-1.': 0.4352271258831024, 'Not clear how this clean exposition of the generative model relates to what is actually done.': 1.0985277891159058, 'In the Generating sequential text section it’s clear the topic model can’t generate words without using y_1 - t-1 but this seems inconsistent with the generative model specification.': 1.0986123085021973, 'This needs to be shown in the paper and made clear to have a complete paper.': 1.0986104011535645, '2 -  The topic model only allows for linear interactions of the topic vector theta.': 0.5613421201705933, 'It seems like this might be required to keep the generative model tractable but seems like a very poor assumption.': 1.09861159324646, 'We would expect the topic representation to have rich interactions with a language model to create nonlinear adjustments to word probabilities for a document.': 1.0985965728759766, 'Please add discussion as to why this modeling choice exists and if possible how future work could modify that assumption (or explain why it’s not such a bad assumption as one might imagine)': 1.0986053943634033, 'Figure 2 colors very difficult to distinguish.': 0.9709339737892151, 'This paper presents TopicRNN, a combination of LDA and RNN that augments traditional RNN with latent topics by having a switching variable that includes/excludes additive effects from latent topics when generating a word.': 1.0986117124557495, 'Experiments on two tasks are performed: language modeling on PTB, and sentiment analysis on IMBD.': 1.098595142364502, 'The authors show that TopicRNN outperforms vanilla RNN on PTB and achieves SOTA result on IMDB.': 0.9994220733642578, 'Some questions and comments:': 0.8432716727256775, 'In Table 2, how do you use LDA features for RNN (RNN LDA features)?': 0.6090180277824402, 'I would like to see results from LSTM included here, even though it is lower perplexity than TopicRNN.': 1.0986123085021973, ""I think it's still useful to see how much adding latent topics close the gap between RNN and LSTM."": 1.0986123085021973, 'The generated text in Table 3 are not meaningful to me.': 1.0966752767562866, 'What is this supposed to highlight?': 1.084366798400879, 'Is this generated text for topic ""trading""?': 0.6040970683097839, 'What about the IMDB one?': 1.0983384847640991, 'How scalable is the proposed method for large vocabulary size (>10K)?': 1.0986123085021973, 'What is the accuracy on IMDB if the extracted features is used directly to perform classification?': 1.0986123085021973, '(instead of being passed to a neural network with one hidden state).': 0.9295592308044434, 'I think this is a fairer comparison to BoW, LDA, and SVM methods presented as baselines.': 1.0802167654037476, 'This work combines a LDA-type topic model with a RNN and models this by having an additive effect on the predictive distribution via the topic parameters.': 0.46308696269989014, 'A variational auto-encoder is used to infer the topic distribution for a given piece of text and the RNN is trained as a RNNLM.': 1.0968048572540283, 'The last hidden state of the RNNLM and the topic parameters are then concatenated to use as a feature representation.': 1.0986018180847168, 'The paper is well written and easy to understand.': 1.0986123085021973, 'Using the topic as an additive effect on the vocabulary allows for easy inference but intuitively I would expect the topic to affect the dynamics too, e.g. the state of the RNN.': 0.9506864547729492, 'The results on using this model as a feature extractor for IMDB are quite strong.': 0.6964078545570374, 'Is the RNN fine-tuned on the labelled IMDB data?': 0.8570155501365662, 'However, the results for PTB are weaker.': 0.9452597498893738, 'From the original paper, an ensemble of 2 LSTMs is able to match the topicRNN score.': 1.0986120700836182, 'This method of jointly modelling topics and a language model seems effective and relatively easy to implement.': 1.09687340259552, 'Finally, the IMDB result is no longer state of the art since this result appeared in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification).': 0.7667382955551147, 'Some questions:': 1.0986067056655884, 'How important is the stop word modelling?': 0.6432574987411499, 'What do the results look like if l_t = 0.5 for all t?': 1.0937741994857788, 'It seems surprising that the RNN was more effective than the LSTM.': 1.0968862771987915, 'Was gradient clipping tried in the topicLSTM case?': 0.02063487097620964, 'Do GRUs also fail to work?': 1.0285011529922485, 'It is also unfortunate that the model requires a stop-word list.': 1.0383421182632446, 'Is the link in footnote 4 the one that is used in the experiments?': 1.0844908952713013, 'Does factoring out the topics in this way allow the RNN to scale better with more neurons?': 1.0850285291671753, 'How reasonable does the topic distribution look for individual documents?': 1.011973261833191, 'How peaked do they tend to be?': 0.3376058340072632, 'Can you show some examples of the inferred distribution?': 1.0779857635498047, ""The topics look odd for IMDB with the top word of two of the topics being the same: 'campbell'."": 1.0975033044815063, 'It would be interesting to compare these topics with those inferred by LDA on the same datasets.': 0.2565357983112335, 'Minor comments:': 0.7548295855522156, 'Below figure 2: GHz -> GB': 0.8448190689086914, '\\Gamma is not defined.': 1.0688908100128174}"
424,https://openreview.net/forum?id=rJe-Pr9le,"{'This paper proposes a model-based reinforcement learning approach focusing on predicting future rewards given a current state and future actions.': 1.0986123085021973, 'This is achieved with a ""residual recurrent neural network"", that outputs the expected reward increase at various time steps in the future.': 1.0986123085021973, 'To demonstrate the usefulness of this approach, experiments are conducted on Atari games, with a simple playing strategy that consists in evaluating random sequences of moves and picking the one with highest expected reward (and low enough chance of dying).': 1.0986123085021973, 'Interestingly, out of the 3 games tested, one of them exhibits better performance when the agent is trained in a multitask setting (i.e. learning all games simultaneously), hinting that transfer learning is occurring.': 1.0971516370773315, 'This submission is easy enough to read, and the reward prediction architecture looks like an original and sound idea.': 1.0986123085021973, 'There are however several points that I believe prevent this work from reaching the ICLR bar, as detailed below.': 1.0986123085021973, 'The first issue is the discrepancy between the algorithm proposed in Section 3 vs its actual implementation in Section 4 (experiments): in Section 3 the output is supposed to be the expected accumulated reward in future time steps (as a single scalar), while in experiments it is instead two numbers, one which is the probability of dying and another one which is the probability of having a higher score without dying.': 1.0986123085021973, 'This might work better, but it also means the idea as presented in the main body of the paper is not actually evaluated (and I guess it would not work well, as otherwise why implement it differently?)': 1.0986123085021973, 'In addition, the experimental results are quite limited: only on 3 games that were hand-picked to be easy enough, and no comparison to other RL techniques (DQN & friends).': 1.0986121892929077, 'I realize that the main focus of the paper is not about exhibiting state-of-the-art results, since the policy being used is only a simple heuristic to show that the model predictions can ne used to drive decisions.': 1.0981031656265259, 'That being said, I think experiments should have tried to demonstrate how to use this model to obtain better reinforcement learning algorithms: there is actually no reinforcement learning done here, since the model is a supervised algorithm, used in a manually-defined hardcoded policy.': 1.0985382795333862, 'Another question that could have been addressed (but was not) in the experiments is how good these predictions are (e.g. classification error on dying probability, MSE on future rewards, ...), compared to simpler baselines.': 1.0986121892929077, 'Finally, the paper\'s ""previous work"" section is too limited, focusing only on DQN and in particular saying very little on the topic of model-based RL.': 1.0956059694290161, 'I think a paper like for instance ""Action-Conditional Video Prediction using Deep Networks in Atari Games"" should have been an obvious ""must cite"".': 1.097557544708252, 'Minor comments:': 1.098131537437439, 'Notations are unusual, with ""a"" denoting a state rather than an action, this is potentially confusing and I see no reason to stray away from standard RL notations': 1.0980876684188843, 'Using a dot for tensor concatenation is not a great choice either, since the dot usually indicates a dot product': 0.8507503867149353, 'The r_i in 3.2.2 is a residual that has nothing to do with r_i the reward': 1.0986123085021973, 'c_i is defined as ""The control that was performed at time i"", but instead it seems to be the control performed at time i-1': 1.0986123085021973, 'There is a recurrent confusion between mean and median in 3.2.2': 1.0986123085021973, 'x should not be used in Observation 1 since the x from Fig.': 1.0986123085021973, '3 does not go through layer normalization': 1.0986123085021973, 'The inequality in Observation 1 should be about |x_i|, not x_i': 1.0986123085021973, 'Observation 1 (with its proof) takes too much space for such a simple result': 1.0986123085021973, 'In 3.2.3 the first r_j should be r_i': 1.0986123085021973, 'The probability of dying comes out of nowhere in 3.3, since we do not know yet it will be an output of the model': 1.0986123085021973, '""Our approach is not able to learn from good strategies"" => did you mean ""*only* from good strategies""?': 1.0986123085021973, 'Please say that in Fig. 4 ""fc"" means ""fully connected""': 1.0986123085021973, 'It would be nice also to say how the architecture of Fig.': 1.0986123085021973, '4 differs from the classical DQN architecture from Mnih et al (2015)': 1.0986123085021973, 'Please clarify r_j2 as per your answer in OpenReview comments': 1.0986123085021973, 'Table 3 says ""After one iteration"" but has ""PRL Iteration 2"" in it, which is confusing': 1.0986123085021973, '""Figure 5 shows that not only there is no degradation in Pong and Demon Attack""=> to me it seems to be a bit worse, actually': 1.0867111682891846, '""A model that has learned only from random play is able to play at least 7 times better.""': 1.095704197883606, '=> not clear where this 7 comes from': 1.0986123085021973, '""Demon Attack\'s plot in Figure 5c shows a potential problem we mentioned earlier"" => where was it mentioned?': 1.0986119508743286, 'The term strategy is a bit ambiguous.': 1.0986123085021973, 'Could you please explain more in formal terms what is strategy?': 1.0986123085021973, 'Is r the discounted Return at time t, or the reward at time t?': 1.0986123085021973, 'Could the author compare the method to TD learning?': 1.0986123085021973, 'The paper is vague and using many RL terms with different meanings without clarifying those diversions.': 0.4207960069179535, '""So, the output for a given state-actions pair is always same"".': 1.0986123085021973, 'Q function by definition is the value of (state, action).': 1.0986123085021973, 'So as long as the policy is deterministic the output would be always same too.': 1.0986123085021973, ""How's this different from Q learning?"": 1.0986123085021973, ""The model description doesn't specify what is the policy, and it's only being mentioned in data generation part."": 0.8593020439147949, 'Why is it a model based approach?': 1.0986123085021973, 'The learning curves are only for 19 iterations, which does not give any useful information.': 1.0986100435256958, 'The final results are clearly nothing comparable to previous works.': 1.0986123085021973, 'The model is only being tested on three games.': 1.0986123085021973, 'The paper is vague and using informal language or sometimes misusing the common RL terms.': 1.0986123085021973, 'The experiments are very small scale and even in that scenario performing very bad.': 1.0986123085021973, ""It's not clear, why it's a model-based approach."": 1.0986123085021973, 'This paper proposes a new approach to model based reinforcement learning and': 1.0986123085021973, 'evaluates it on 3 ATARI games.': 1.0986123085021973, 'The approach involves training a model that': 1.0986123085021973, 'predicts a sequence of rewards and probabilities of losing a life given a': 1.0986123085021973, 'context of frames and a sequence of actions.': 1.0986123085021973, 'The controller samples random': 1.0986123085021973, 'sequences of actions and executes the one that balances the probabilities of': 1.0986123085021973, 'earning a point and losing a life given some thresholds.': 1.0986123085021973, 'The proposed system': 1.0986123085021973, 'learns to play 3 Atari games both individually and when trained on all 3 in a': 1.0986123085021973, 'multi-task setup at super-human level.': 1.0986123085021973, 'The results presented in the paper are very encouraging but there are many': 0.9869927167892456, 'ad-hoc design choices in the design of the system.': 0.7422624826431274, 'The paper also provides': 1.0288769006729126, 'little insight into the importance of the different components of the system.': 0.419223815202713, 'Main concerns:': 1.0986123085021973, 'The way predicted rewards and life loss probabilities are combined is very ad-hoc.': 1.0986123085021973, 'The natural way to do this would be by learning a Q-value, instead different': 1.0985698699951172, 'rules are devised for different games.': 1.0179895162582397, 'Is a model actually being learned and improved?': 0.47605395317077637, 'It would be good to see': 1.0974271297454834, 'predictions for several actions sequences from some carefully chosen start': 1.080108642578125, 'states.': 1.0986123085021973, 'This would be good to see both on a game where the approach works and': 0.9922535419464111, 'on a game where it fails.': 0.024382829666137695, 'The learning progress could also be measured by': 0.9805662631988525, 'plotting the training loss on a fixed holdout set of sequences.': 1.003280520439148, 'How important is the proposed RRNN architecture?': 0.8926278352737427, 'Would it still work without': 0.8585682511329651, 'the residual connections?': 0.41871124505996704, 'Would a standard LSTM also work?': 1.0984269380569458, 'Minor points:': 0.9536539316177368, 'Intro, paragraph 2 - There is a lot of much earlier work on using models in': 1.0976940393447876, 'RL.': 1.0986123085021973, 'For example, see Dyna and ""Memory approaches to reinforcement learning in': 1.025865077972412, 'non-Markovian domains"" by Lin and Mitchell to name just two.': 1.0532466173171997, 'Section 3.1 - Minor point, but using a_i to represent the observation is': 1.0986123085021973, 'unusual.': 1.0986111164093018, 'Why not use o_i for observations and a_i for actions?': 1.0986074209213257, 'Section 3.2.2 - Notation again, r_i was used earlier to represent the': 1.0986123085021973, 'reward at time': 0.5599654316902161, 'i': 1.0982534885406494, 'but it is being used again for something else.': 1.0121104717254639, 'Observation 1 seems somewhat out of place.': 1.0986123085021973, 'Citing the layer normalization': 1.0986123085021973, 'paper for the motivation is enough.': 1.0986123085021973, 'Section 3.2.2, second last paragraph - How is memory decoupled from': 1.0986123085021973, 'computation here?': 1.0986123085021973, 'Models like neural turning machines accomplish this by using': 1.0986123085021973, 'an external memory, but this looks like an RNN with skip connections.': 1.0986123085021973, 'Section 3.3, second paragraph - Whether the model overfits or not depends on': 1.0986123085021973, 'the data.': 1.0986123085021973, ""The approach doesn't work with demonstrations precisely because it"": 1.0986123085021973, 'would overfit.': 1.0986123085021973, 'Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy': 1.0986123085021973, 'instead of Morimoto et al.': 1.0986123085021973, 'Overall I think the paper has some really promising ideas and encouraging': 1.0986123085021973, 'results but is missing a few exploratory/ablation experiments and some polish.': 1.0986123085021973}"
425,https://openreview.net/forum?id=rJeKjwvclx,"{'Paper Summary:': 1.0986123085021973, 'The paper introduces a question answering model called Dynamic Coattention Network (DCN).': 1.0986123085021973, 'It extracts co-dependent representations of the document and question, and then uses an iterative dynamic pointing decoder to predict an answer span.': 1.0986123085021973, 'The proposed model achieves state-of-the-art performance, outperforming all published models.': 1.0986123085021973, 'Paper Strengths:': 1.0986123085021973, 'The proposed model introduces two new concepts to QA models': 1.0986123085021973, '1) using attention in both directions, and 2) a dynamic decoder which iterates over multiple answer spans until convergence or maximum number of iterations.': 1.0986123085021973, 'The paper also presents ablation study of the proposed model which shows the importance of their design choices.': 1.0986123085021973, 'It is interesting to see the same idea of co-attention performing well in 2 different domains': 1.0986123085021973, 'Visual Question Answering and machine reading comprehension.': 1.0986123085021973, 'The performance breakdown over document and question lengths (Figure 6) strengthens the importance of attention for QA task.': 1.098610758781433, 'The proposed model achieves state-of-the-art result on SQuAD dataset.': 1.089051365852356, 'The model architecture has been clearly described.': 1.098584771156311, 'Paper Weaknesses / Future Thoughts:': 0.7474381327629089, ""The paper provides model's performance when the maximum number of iterations is 1 and 4."": 1.0986121892929077, 'I would like to see how the performance of the model changes with the number of iterations, i.e., the model performance when that number is 2 and 3.': 1.0986099243164062, 'Is there a clear trend?': 1.0986047983169556, 'What type of questions is the model able to get correct with more iterations?': 1.0986123085021973, 'As with many deep learning approaches, the overall architecture seems quite complex, and the design choices seem to be driven by performance numbers.': 1.0976296663284302, 'As future work, authors might try to analyze qualitative advantages of different choices in the proposed model.': 1.0986123085021973, 'What type of questions are correctly answered because of co-attention mechanism instead of attention in a single direction, when using Maxout Highway Network instead of a simple MLP, etc?': 1.0986051559448242, 'Preliminary Evaluation:': 1.0955721139907837, 'Novel and state-of-the-art question answering approach.': 1.0986123085021973, 'Model is clearly described in detail.': 1.0986123085021973, 'In my thoughts, a clear accept.': 1.0986123085021973, 'Summary: The paper proposes a novel deep neural network architecture for the task of question answering on the SQuAD dataset. The model consists of two main components': 1.0986108779907227, 'coattention encoder and dynamic pointer decoder.': 0.45909667015075684, 'The encoder produces attention over the question as well as over the document in parallel and thus learns co-dependent representations of the question and the document.': 1.0985774993896484, 'The decoder predicts the starting and the end token of the answer iteratively, with the motivation that multiple iterations will help the model escape local maxima and thus will reduce the errors made by the model.': 1.0986120700836182, 'The proposed model achieved state-of-art result on SQuAD dataset at the time of writing the paper.': 0.4340507686138153, 'The paper reports some analyses of the results such as performance across question types, document, question, answer lengths, etc.': 1.0944174528121948, 'The paper also performs some ablation studies such as performing only single round of iteration on decoder, etc.': 1.0986123085021973, 'Strengths:': 1.0986123085021973, '1. The paper is well-motivated with two main motivations': 1.0986123085021973, 'co-attending to the document and the question, and iteratively producing the answer.': 1.0986123085021973, '2. The proposed model architecture is novel and the design choices made seem reasonable.': 1.0986123085021973, '3. The experiments show that the proposed model outperforms the existing model (at the time of writing the paper) on the SQuAD dataset by significant margin.': 1.0986123085021973, ""4. The analyses of the results and the ablation studies performed (as per someone's request) provide insights into the various modelling design choices made."": 1.0986123085021973, 'Weaknesses/Questions/Suggestions:': 1.0986123085021973, '1. In order to gain insights into how much each additional iteration in the decoder help, I would like to see the following': 1.0986123085021973, 'for every iteration report the mean F1 for questions that converged in that iteration along with the number of questions that converged in that iteration.': 1.0986123085021973, '2. Example of Question 3 in figure 5 is an interesting example where the model is unable to decide between multiple local maxima despite several iterations. Could authors please report how often this happens?': 1.0986123085021973, '3. In order to estimate how much modelling of attention in the encoder helps, it would be good if authors could report the performance of the model when attention is not modeled at all in the encoder (neither over question, nor over document).': 1.0872191190719604, '4. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required.': 1.0600498914718628, ""5. In Wang and Jiang (2016), the attention is predicted over question for each word in the document. But in table 2, when performing ablation study to make the proposed model similar to Wang and Jiang, C^D is set to C^Q. But isn’t C^Q attention over document for each word in the question? So, how is this similar to Wang and Jiang’s attention? I think QA^D will be similar to Wang and Jiang's attention since QA^D is attention over question for each word in the document. Please clarify."": 0.7282348871231079, '6. In section 2.1, “n” and “m” are swapped when explaining the Document and Question encoding matrix. Please fix it.': 1.049272894859314, 'Review Summary: The paper presents a novel and interesting model for the task of question answering on SQuAD dataset and shows that the model outperforms existing models.': 1.098578691482544, 'However, to gain more insights into the functioning of the model, I would like see more analyses of the results and one more ablation study (see weaknesses section above).': 1.096962809562683, 'This paper proposed a dynamic coattention network for the question answering task with long contextual documents.': 1.0986123085021973, 'The model is able to encode co-dependent representations of the question and the document, and a dynamic decoder iteratively pointing the potential answer spans to locate the final answer.': 1.0986123085021973, 'Overall, this is a well-written paper.': 1.0985132455825806, 'Although the model is a bit complicated (coattention encoder, iterative dynamic pointering decoder and highway maxout network), the intuitions behind and the details of the model are clearly presented.': 1.0986121892929077, 'Also the performance on the SQuAD dataset is good.': 1.0986123085021973, 'I would recommend this paper to be accepted.': 1.0986123085021973}"
426,https://openreview.net/forum?id=rJfMusFll,"{'This paper extends neural conversational models into the batch reinforcement learning setting.': 1.0986123085021973, 'The idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive.': 1.0986123085021973, 'Thus, it is natural to use off-policy learning – training a base policy on unsupervised data, deploying that policy to collect human scores, and then learning off-line from those scores.': 1.0985472202301025, 'While the overall contribution is modest (extending off-policy actor-critic to the application of dialogue generation), the approach is well-motivated, and the paper is written clearly and is easy to understand.': 1.0986120700836182, 'My main concern is that the primary dataset used (restaurant recommendations) is very small (6000 conversations).': 1.0986119508743286, 'In fact, it is several orders of magnitude smaller than other datasets used in the literature (e.g. Twitter, the Ubuntu Dialogue Corpus) for dialogue generation.': 1.0986112356185913, 'It is a bit surprising to me that RNN chatbots (with no additional structure) are able to generate reasonable utterances on such a small dataset.': 1.0986123085021973, 'Wen et al. (2016) are able to do this on a similarly small restaurant dataset, but this is mostly because they map directly from dialogue states to surface form, rather than some embedding representation of the context.': 1.0986123085021973, 'Thus, it remains to be seen if the approaches in this paper also result in improvements when much more unsupervised data is available.': 1.0985761880874634, 'References:': 1.0986037254333496, 'Wen, Tsung-Hsien, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young.': 0.9805996417999268, '""A Network-based End-to-End Trainable Task-oriented Dialogue System.""': 1.0986123085021973, 'arXiv preprint arXiv:1604.04562 (2016).': 1.0986123085021973, 'The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots.': 1.0986123085021973, 'The approach is well motivated and the paper is well written, except for some intuitions for why the batch version outperforms the on-line version (see comments on ""clarification regarding batch vs. online setting"").': 1.0986123085021973, 'The artificial experiments are instructive, and the real-world experiments were performed very thoroughly although the results show only modest improvement.': 1.0867266654968262, 'The paper discuss a ""batch"" method for RL setup to improve chat-bots.': 1.0986123085021973, 'The authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem.': 1.0986123085021973, 'They make a comparison to the online version and explore several modeling choices.': 1.0986114740371704, 'I find the writing clear, and the algorithm a natural extension of the online version.': 1.0985925197601318, 'Below are some constructive remarks:': 1.0784327983856201, 'Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was.': 1.0986123085021973, 'It will be good to understand why, and add this to the discussion.': 1.0984667539596558, 'Here is one option:': 1.0986120700836182, 'For the artificial task it seems like you are giving the constant value function an unfair advantage, as it can update all the weights of the model, and not just the top layer, like the per-state value function.': 1.0986123085021973, 'section 2.2:': 1.0986123085021973, ""sentence before last: s' is not defined."": 0.6289652585983276, 'last sentence: missing ""... in the stochastic case.""': 1.0986086130142212, 'at the end.': 1.0986123085021973, 'Section 4.1 last paragraph: ""While Bot-1 is not significant ...""': 1.0978268384933472, '=> ""While Bot-1 is not significantly different from ML ...""': 1.0820732116699219}"
427,https://openreview.net/forum?id=rJg_1L5gg,"{'First up, I want to point out that this paper is really long.': 1.0986123085021973, 'Like 17 pages long': 1.0986123085021973, 'without any supplementary material.': 1.0986123085021973, ""While ICLR does not have an official page limit, it would be nice if authors put themselves in the reviewer's shoes and did not take undue advantage of this rule."": 1.0986123085021973, 'Having 1 or 2 pages in addition to the conventional 8 page limit is ok, but more than doubling the pages is quite unfair.': 1.0986123085021973, 'Now for the review: The paper proposes a new artificial dataset for sequence learning.': 1.0986123085021973, 'I call it artificial because it was artificially generated from the original MNIST dataset which is a smallish dataset of real images of handwritten digits.': 1.0986123085021973, 'In addition to the dataset, the authors propose to train recurrent networks using a schedule over the length of the sequence, which they call ""incremental learning"".': 1.0986123085021973, 'The experiments show that their proposed schedule is better than not having any schedule on this data set.': 1.0986123085021973, 'Furthermore, they also show that their proposed schedule is better than a few other intuitive schedules.': 1.0986123085021973, 'The authors verify this by doing some ablation studies over the model on the proposed dataset.': 1.0986123085021973, 'I have following issues with this paper:': 1.0986123085021973, 'I did not find anything novel in this paper.': 1.0986123085021973, 'The proposed incremental learning schedule is nothing new and is a natural thing to try when learning sequences.': 1.0986123085021973, 'Similar idea have already been tried by a number of authors, including Bengio 2015, and Ranzato 2015.': 1.0949599742889404, 'The only new piece of work is the ablation studies which the authors conduct to tease out and verify that indeed the improvement in performance is due to the curriculum used.': 1.0986123085021973, 'Furthermore, the authors only test their hypothesis on a single dataset which they propose and is artificially generated.': 1.09678053855896, 'Why not use it on a real sequential dataset, such as, language modeling.': 1.0985229015350342, 'Does the technique not work in that scenario?': 1.0986111164093018, 'In fact I am quite positive that for language modeling where the vocabulary size is huge, the performance gains will be no where close to the 74% reported in the paper.': 1.098230004310608, ""I'm not convinced about the value of having this artificial dataset."": 1.0986123085021973, 'Already there are so many real world sequential dataset available, including in text, speech, finance and other areas.': 1.0984574556350708, 'What exactly does this dataset bring to the table is not super clear to me.': 1.0956023931503296, 'While having another dataset may not be a bad thing in itself, I almost felt that this dataset was created for the sole purpose of making the proposed ideas work.': 0.41373205184936523, 'It would have been so much better had the authors shown experiments on other datasets.': 0.496002733707428, 'As I said, the paper is way too long.': 0.6323226690292358, 'A significant part of the length of the paper is due to a collection of experiments which are completely un-related to the main message of the paper.': 1.0950425863265991, 'For instance, the experiment in Section 6.2 is completely unrelated to the story of the paper.': 0.4331628978252411, 'Same is true with the transfer learning experiments of Section 6.4.': 0.8900889754295349, 'The submitted paper proposes a new way of learning sequence predictors.': 1.0758365392684937, 'In the lines of incremental learning and curriculum learning, easier samples are presented first and the complexity is increased during training.': 0.4867289364337921, 'The particularity here is that the complexity is defined as the length of the sequences given for training, the premise being is that longer sequences are harder to learn, since they need a more complex internal representation.': 0.9973004460334778, 'The targeted application is sequence prediction from primed prefixes, tested on a single dataset, which the authors extract themselves from MNIST.': 1.0986123085021973, 'The idea in the paper is interesting and worth reading.': 1.0986123085021973, 'There are also many interesting aspects of evaluation part, as the authors perform several ablation studies to rule out side-effects of the tests.': 1.0986123085021973, 'The proposed learning strategy is compared to other strategies.': 1.0986123085021973, 'However, my biggest concern is still with evaluation.': 1.0986123085021973, 'The authors tested the method on a single dataset, which is non standard and derived from MNIST.': 1.0986123085021973, 'Given the general nature of the claim, in order to confirm the interest of the proposed algorithm, it need to be tested on other datasets, public datasets, and on a different application.': 1.0986123085021973, 'The paper is too long and should be trimmed significantly.': 1.0986123085021973, 'The transfer learning part (from prediction to classification) is a different story and I do not see a clear connection to the main contribution of the paper.': 1.0986123085021973, 'The presentation and organization of the paper could be improved.': 1.0986123085021973, ""It is quite sequentially written and sometimes reads like a student's report."": 1.0986123085021973, 'The loss given in the long unnumbered equation on page 6 should be better explained: provide explanations for each term, and make clearer what the different symbols mean.': 1.0986123085021973, 'Learning is supervised, so which variables are predictions, and which are observations from the data (ground truth).': 1.0986123085021973, 'Names in table 2 do not correspond to the descriptions in section 4.': 1.0986123085021973, 'This paper presents a thorough analysis of different methods to do curriculum learning.': 1.0986123085021973, 'The major issue I have with it is that the dataset used seems very specific and does not necessarily justified, as mentioned by AnonReviewer3.': 1.0986123085021973, 'It would have been great to see experiments on more standard tasks.': 1.09861159324646, ""Also, I really can't understand how the performance of FFNN models can be so good, please elaborate on this (see last comment)."": 1.0986123085021973, 'However, the paper is well written, the comparisons of the described methods are interesting and would probably apply to some other datasets as well.': 1.0986028909683228, 'The paper is way too long (18 pages!).': 1.03047513961792, 'Please reduce it or move some of the results to an appendix section.': 1.0986123085021973, 'The method described is extremely similar to the one described in Reinforcement learning neural turing machines (Zaremba et al., 2016, https://arxiv.org/pdf/1505.00521v3.pdf) where the authors progressively increase the length of training examples until the performance exceeds a given threshold.': 1.098604440689087, 'Maybe you should mention it.': 1.0860111713409424, 'Could you explain very briefly in the paper what ""4-connected"" and ""8-connected"" mean, for people not familiar with these terms?': 1.098602533340454, 'I agree that having gold pen stroke sequences would be nice and probably very good features to have for image classification.': 1.0986123085021973, 'But how accurate are the constructed ones?': 1.0986123085021973, 'Typically, the example given in figure 1 does not represent the way people write a ""3"".': 1.0986123085021973, ""I'm just concerned about the validity of the proposed dataset and what these sequences really represent (although I agree that it can still be relevant as a sequence learning dataset, even if it does not reflect the way people write)."": 1.0986123085021973, ""In figure 5, for the blue curve, I was expecting to see an increase of the error when new data are added to the set, but there doesn't seem to be much correlation between these two phenomenons."": 1.0986123085021973, 'Can you explain why?': 1.0986123085021973, 'Also, could you explain the important error rate increase at about 7e+07 steps for the regular sequence learning?': 1.0986123085021973, 'The method used to test the H1 hypothesis is interesting, but did you try something even simpler like not using batch (ie batch size of 1 sequence)?': 1.0986123085021973, 'This would alleviate this ""different number of points by batch"" effect and the results would probably very different than in figure 5.': 0.4058648943901062, 'The performance of the FFNN models seem too good compared to the RNN ones.': 1.0962358713150024, 'How is this possible?': 1.0968958139419556, 'RNN models should perform at least as well.': 0.5187880396842957, 'Even the ""Incremental sequence learning"" RNN barely beats its FFNN equivalent.': 1.0986123085021973, 'Do the ""dx"" and ""dy"" values always take values in [-1, 0, 1]?': 1.087904691696167, 'If so, the number of possible mappings is very small (from [-1, 0, 1] to [-1, 0, 1]), how could a mapping between two successive points be so accurate without looking at the history?': 0.6221724152565002, 'Please clarify on this.': 1.0986077785491943}"
428,https://openreview.net/forum?id=rJiNwv9gg,"{'This work proposes a new approach for image compression using auto encoders.': 1.0986123085021973, 'The results are impressive, besting the state of the art in this field.': 1.0986123085021973, 'Pros:': 1.0986123085021973, '+ Very clear paper.': 1.0986123085021973, 'It should be possible to replicate these results should one be inclined to do so.': 1.0986123085021973, '+': 1.0986123085021973, 'The results, when compared to other work in this field are very promising.': 1.0986123085021973, ""I need to emphasize, and I think the authors should have emphasized this fact as well: this is very new technology and it should not be surprising it's not better than the state of the art in image compression."": 1.0986123085021973, ""It's definitely better than other neural network approaches to compression, though."": 1.0986123085021973, 'Cons:': 1.0986123085021973, 'The training procedure seems clunky.': 1.0986123085021973, 'It requires multiple training stages, freezing weights, etc.': 1.0986123085021973, ""The motivation behind Figure 1 is a bit strange, as it's not clear what it's trying to illustrate, and may confuse readers (it talks about effects on JPEG, but the paper discusses a neural network architecture, not DCT quantization)"": 1.0986123085021973, 'This paper proposes an autoencoder approach to lossy image compression by minimizing the weighted sum of reconstruction error and code length.': 1.0986123085021973, 'The architecture consists of a convolutional encoder and a sub-pixel convolutional decoder.': 1.0986123085021973, 'Experiments compare PSNR, SSIM, and MS-SSIM performance against JPEG, JPEG-2000, and a recent RNN-based compression approach.': 1.0986123085021973, 'A mean opinion score test was also conducted.': 1.0986123085021973, 'The paper is clear and well-written.': 1.0986123085021973, 'The decoder architecture takes advantage of recent advances in convolutional approaches to image super-resolution.': 1.0986123085021973, 'The proposed approaches to quantization and rate estimation are sensible and well-justified.': 1.0986123085021973, 'The experimental baselines do not appear to be entirely complete.': 1.0986123085021973, 'The task of using autoencoders to perform compression is important and has a large practical impact.': 1.0986123085021973, 'Though directly optimizing the rate-distortion tradeoff is not an entirely novel enterprise, there are enough differences (e.g. the quantization approach and sub-pixel convolutional decoder) to sufficiently distinguish this from earlier work.': 1.0986123085021973, 'I am not an image compression expert but the approach and results both seem compelling.': 1.0986123085021973, 'The main shortcoming is that the implementation of Toderici et al. 2016b appears to be incomplete, and there is no comparison to Balle et al. 2016.': 1.0986123085021973, 'Overall, I feel that the fact that this architecture achieves competitive performance with JPEG-2000 while simultaneously setting the stage for future work that varies the encoder/decoder size and data domain means the community will find this work to be of significant interest.': 1.0986123085021973, 'I have no further specific comments at this time as they were answered sufficiently in the pre-review questions.': 1.0986123085021973, 'The paper proposes a neural approach to learning an image compression-decompression scheme as an auto-encoder.': 1.0986123085021973, 'While the idea is certainly interesting and well-motivated, in practice, it turns out to achieve effectively identical rates to JPEG-2000.': 1.0986123085021973, 'Now, as the authors argue, there is some value to the fact that this scheme was learned automatically rather than by expert design': 1.0986123085021973, ""which means it has benefits beyond the compression of natural images (e.g., it could be used to automatically learning a compression scheme for signals for which we don't have as much domain knowledge)."": 1.0986123085021973, 'However, I still believe that this makes the paper unsuitable for publication in its current form because of the following reasons': 1.0986123085021973, '1. Firstly, the fact that the learned encoder is competitive': 0.6841013431549072, 'and not clearly better': 1.0986121892929077, 'than JPEG 2000 means that the focus of the paper should more be about the aspects in which the encoder is similar to, and the aspects in which it differs, from JPEG 2000.': 1.061140537261963, 'Is it learning similar filters or completely different ones ?': 1.0985984802246094, 'For what kinds of textures does it do better and for what kinds does it do worse (the paper could show the best and worst 10 patches at different bit-rates) ?': 1.0986123085021973, ""2. Secondly, I think it's crucial that the paper demonstrate that the benefits come from a better coding scheme (as opposed to just a better decoder), as suggested in my initial pre-review question. How would a decoder trained on JPEG-2000 codes (and perhaps also on encoded random projections) do worse or better ?"": 1.0984810590744019, ""3. Finally, I think the fact that it does as well/worse than JPEG-2000 significantly diminishes the case for using a 'deep' auto-encoder. JPEG-2000 essentially uses a wavelet transform, which is a basis that past studies have shown could be recovered using a simple sparse dictionary algorithm like K-SVD. This is why I feel that the method needs to clearly outperform JPEG-2000, or show comparisons to (or atleast discuss) a well-crafted traditional/generative model-based baseline."": 1.0727908611297607}"
429,https://openreview.net/forum?id=rJo9n9Feg,"{'Game of tic-tac-toe is considered.': 0.42463016510009766, '1029 tic-tac-toe board combinations are chosen so that a single move will result into victory of either the black or the white player.': 0.9692238569259644, 'There are 18 possible moves - 2 players x 9 locations.': 0.969590425491333, 'A CNN is trained from a visual rendering of the game board to these 18 possible outputs.': 1.0985993146896362, 'CAM technique is used to visualize the salient regions in the inputs responsible for the prediction that CNN makes.': 1.0986090898513794, 'Authors find that predictions correspond to the winning board locations.': 1.0986123085021973, 'Authors claim that this:': 1.0986106395721436, '1. is a very interesting finding.': 0.6755289435386658, '2. CNN has figured out game rules.': 1.0835577249526978, '3. Cross modal supervision is applicable to higher-level semantics.': 0.8235582709312439, ""I don't think (2) be can be claimed because the knowledge of game rules is not tested by any experiment."": 1.0986117124557495, 'There is only ""one"" stage of a game - i.e. last move that is considered.': 1.0986123085021973, 'Further, the results are on the training set itself - the bare minimum requirement of any implicit or explicit representation of game rules is the ability to act in previously unseen states (i.e. generalization).': 1.0986123085021973, 'Even if the CNN did generalize, I would avoid making any claims about knowledge of game rules.': 1.0986123085021973, ""For (3), author's definition of cross-modal seems to be training from images to games moves."": 1.0986123085021973, 'In image-classification we go from images': 1.0986123085021973, '> labels (i.e. between two different domains).': 1.086054801940918, 'We already know CNNs can perform such mappings.': 0.65022873878479, 'CNNs have been used to map images to actions such as in DQN my Mnih et al., or DDPG by Lillicrap et al. and a lot of other classical work such as ALVIN.': 1.0984225273132324, ""It's unclear what points authors are trying to make."": 1.0940418243408203, 'For (1): how interesting is an implicit attention mechanism is a subjective matter.': 1.0986123085021973, 'The authors claim a difference between the concepts of ""what do do"" and ""what will happen"".': 1.0985779762268066, 'They claim by supervising for ""what will happen"", the CNN can automatically learn about ""what to do"".': 1.0982741117477417, 'This is extensively studied in the model predictive control literature.': 1.0986078977584839, 'Where model is ""what will happen next"", and the model is used to infer a control law - ""what to do"".': 1.093064785003662, 'However, in the experimental setup presented in the paper what will happen and what to do seem to be the exact same things.': 1.098479986190796, 'For further analysis of what the CNN has learnt I would recommend:': 0.5784499645233154, '(a) Visualizing CAM with respect to incorrect classes.': 1.0985214710235596, 'For eg, visualize the CAM with respect to player would lose (instead of winning).': 1.0985409021377563, '(b) Split the data into train/val and use the predictions on the val-set for visualization.': 1.0853192806243896, 'These would be much more informative about what kind of ""generalizable"" features the CNN pays attention to.': 1.09770667552948, ""In summary, understanding why CNN's make what decisions they make is a very interesting area of research."": 1.0561567544937134, 'While the emergence of an implicit attention mechanism may be considered to be an interesting finding by some, many claims made by the authors are not supported by experiments (see comments above).': 1.0986123085021973, 'Summary': 1.0986123085021973, '===': 1.0986123085021973, 'This paper presents tic-tac-toe as toy problem for investigating CNNs.': 1.0986123085021973, 'A dataset is created containing tic-tac-toe boards where one player is one': 1.0986123085021973, 'move away from winning and a CNN is trained to label boards according': 1.0986123085021973, 'to (1) the player who can win (2 choices) and (2) the position they may move': 0.5147760510444641, 'to win (9 choices), resulting in 18 labels.': 0.5968481302261353, 'The CNN evaluated in this paper': 1.0924056768417358, ""performs perfectly at the task and the paper's goal is to inspect how the"": 0.4221915006637573, 'CNN works.': 1.0984177589416504, 'The fundamental mechanism for this inspection is Class Activation': 1.0911564826965332, 'Mapping (CAM) (Zhou et.': 1.0965033769607544, 'al. 2016), which identifies regions of implicit attention': 0.40551233291625977, 'in the CNN.': 1.0958924293518066, 'These implicit attention maps (localization heat maps) are used to': 1.0986090898513794, 'derive actions (which square each player should move).': 1.0897096395492554, 'The attention maps': 0.40643423795700073, '(1) attend to squares in the tic-tac-toe board rather than arbitrary': 1.0985476970672607, 'blobs, despite the fact that one square in a board has uniform color, and': 1.0936198234558105, '(2) they can be used to pick correct (winning) actions.': 1.0832524299621582, 'This experiment are used to support assertions that the network understands': 1.0986123085021973, '(1) chess (tic-tac-toe) boards': 1.0982215404510498, '(2) a rule for winning tic-tac-toe': 1.0748693943023682, '(3) that there are two players.': 0.8884603977203369, 'Some follow up experiments indicate similar results under various renderings': 1.0986123085021973, 'of the tic-tac-toe boards and an incomplete training regime.': 0.7823384404182434, 'More Clarifying Questions': 1.0986123085021973, '* I am not quite sure precisely how CAM is implemented here.': 1.092703938484192, 'In the original CAM': 1.0949610471725464, 'one must identify a class of interest to visualize (e.g., cat or dog).': 0.9024393558502197, ""I don't"": 1.0950257778167725, 'think this paper identifies such a choice.': 1.088272213935852, 'How is one of the 18 possible classes': 1.098611831665039, 'chosen for creating the CAM visualization and through that visualization': 1.0957412719726562, 'choosing an action?': 0.6659328937530518, '* How was the test set for this dataset for the table 1 results created?': 0.4649500548839569, 'How many of the final 1029 states were used for test and was the': 0.8695539236068726, 'distribution of labels the same in train and test?': 0.8523737192153931, '* How is RCO computed?': 1.0510658025741577, 'Is rank correlation or Pearson correlation used?': 1.0185884237289429, 'If Pearson correlation is used then it may be good to consider rank correlation,': 1.0986095666885376, 'as argued in ""Human Attention in Visual Question Answering: Do Humans and': 0.3990526795387268, 'Deep Networks Look at the Same Regions?"" by Das et.': 0.34239694476127625, 'al.': 1.0984416007995605, 'in EMNLP 2016.': 1.098611831665039, 'In table 1, what does the 10^3 next to RCO mean?': 0.2880949378013611, 'Pros': 1.09427011013031, '*': 1.0976884365081787, 'The proposed method, deriving an action to take from the result of a': 1.0898263454437256, 'visualization technique, is very novel.': 0.7914645671844482, '* This paper provides an experiment that clearly shows a CNN relying on context': 0.7960032224655151, 'to make accurate predictions.': 1.0779788494110107, 'The use of a toy tic-tac-toe domain to study attention in CNNs': 1.0925806760787964, '(implicit or otherwise) is a potentially fruitful setting that may': 0.30878570675849915, 'lead to better understanding of implicit and maybe explicit attention mechanisms.': 1.0984952449798584, 'Cons': 1.0977213382720947, '* This work distinguishes between predictions about ""what will happen""': 0.9364352822303772, '(will the white player win?)': 0.12185673415660858, 'and ""what to do"" (where should the white': 1.0746355056762695, 'player move to win?).': 1.0568300485610962, 'The central idea is generalization from ""what will happen""': 1.0836069583892822, 'to ""what to do"" indicates concept learning (sec. 2.1).': 0.3921196460723877, 'Why should an ability to': 0.8208794593811035, 'act be any more indicative of a learned concept than an ability to predict': 0.7278367877006531, 'future states.': 1.0760481357574463, 'I see a further issue with the presentation of this approach and': 1.0983624458312988, 'a potential correctness problem:': 1.0986123085021973, '1. (correctness)': 0.6958335638046265, 'In the specific setting proposed I see no difference between ""what to do""': 0.9658589363098145, 'and ""what will happen.""': 1.080092430114746, 'Suppose one created labels dictating ""what to do"" for each example in the': 0.8674424886703491, 'proposed dataset.': 1.0984151363372803, 'How would these differ from the labels of ""what will happen""': 1.0749021768569946, 'in the proposed dataset?': 1.0942052602767944, 'In this case ""what will happen"" labels include': 0.9259233474731445, 'both player identity (who wins) and board position (which position they move': 0.8937676548957825, 'to win).': 0.6706353425979614, 'Wouldn\'t the ""what to do"" labels need to indicate board position?': 1.0986123085021973, 'They could also chosen to indicate player identity, which would make them': 0.9915028214454651, 'identical to the ""what will happen"" labels (both 18-way softmaxes).': 0.4679208993911743, '2. (presentation)': 0.6073262095451355, 'I think this distinction would usually be handled by the Reinforcement Learning': 1.0535807609558105, 'framework, but the proposed method is not presented in that framework or': 0.7270311117172241, 'related to an RL based approach.': 0.8852465748786926, 'In RL ""what will happen"" is the reward an': 1.0917919874191284, 'agent will receive for making a particular action and ""what to do"" is the': 0.8345808386802673, 'action an agent should take.': 1.0467768907546997, 'From this point of view, generalization from': 0.4715173840522766, '""what will happen"" to ""what to do"" is not a novel thing to study.': 0.50335294008255, 'Alternate models include:': 0.4015983045101166, '* A deep Q network (Mnih. et.': 1.058927059173584, 'al. 2015) could predict the value of': 1.0290414094924927, 'every possible action where an action is a (player, board position) tuple.': 0.9610547423362732, ""* The argmax of the current model's softmax could be used as an action"": 0.45771509408950806, 'prediction.': 1.0985701084136963, 'The deep Q network approach need not be implemented, but differences between': 1.020967960357666, 'methods should be explained because of the uniqueness of the proposed approach.': 1.0922313928604126, '* Comparison to work that uses visualization to investigate deep RL networks': 1.0980280637741089, 'is missing.': 1.0986123085021973, 'In particular, other work in RL has used Simonyan et.': 1.0978286266326904, '(arXiv 2013) style saliency maps to investigate network behavior.': 1.0985013246536255, 'For example,': 1.0252889394760132, '""Dueling Network Architectures for Deep Reinforcement Learning"" by Wang et.': 0.8974881768226624, 'in (ICML 2016) uses saliency maps to identify differences between their': 0.9108616709709167, 'state-value and advantage networks.': 1.069682240486145, 'In ""Graying the black box:': 1.0961706638336182, 'Understanding DQNs"" by Zahavy et.': 1.0473111867904663, '(ICML 2016)': 0.8997346758842468, 'these saliency maps are': 1.0700762271881104, 'also used to analyze network behavior.': 0.18824070692062378, 'In section 2.3, saliency maps of Simonyan et.': 0.5861928462982178, 'al. are said to not be able to': 0.9367029070854187, 'activate on grid squares because they have constant intensity, yet no empirical': 0.04305368661880493, 'or theoretical evidence is provided for this claim.': 0.990120530128479, 'On a related note, what precisely is the notion of information referenced in': 0.08343829214572906, 'section 2.3 and why is it relevant?': 0.15947696566581726, 'Is it entropy of the distribution of pixel': 0.8962511420249939, 'intensities in a patch?': 1.0112062692642212, 'To me it seems that any measure which depends only': 0.6339540481567383, 'on one patch is irrelevant because the methods discussed (e.g., saliency maps)': 0.8387935161590576, 'depend on context as well as the intensities within a patch.': 0.9779354333877563, 'The presentation in the paper would be improved if the results in section 7': 0.917965829372406, 'were presented along with relevant discussion in preceding sections.': 1.0858876705169678, 'Overall Evaluation': 0.8552029728889465, 'The experiments presented here are novel, but I am not sure they are very': 0.40367332100868225, 'significant or offer clear conclusions.': 0.4220992624759674, 'The methods and goals are not presented': 1.0875418186187744, 'clearly and lack the broader relevant context mentioned above.': 0.5006377696990967, 'Furthermore, I': 1.0084264278411865, 'find the lines of thought mentioned in the Cons section possibly incorrect': 0.4207799732685089, 'or incomplete.': 1.0986123085021973, 'As detailed with further clarifying questions, upon closer': 1.0986123085021973, 'inspection I do not see how some aspects of the proposed approach were': 1.0986123085021973, 'implemented, so my opinion may change with further details.': 0.6153841614723206, '1029 tic-tac-toe boards are rendered (in various ways).': 0.9357460737228394, 'These 1029 boards are legal boards where the next legal play can end the game.': 1.0986123085021973, 'There are 18 categories of such boards': 0.6704556941986084, '9 for the different locations of the next play, and 2 for the color of the next play.': 0.1587362289428711, 'The supervision is basically saying ""If you place a black square in the middle right, black will win"" or ""if you place a white square in the upper left, white will win"".': 1.0986123085021973, 'A CNN is trained to predict these 18 categories and can do so with 100% accuracy.': 1.0986027717590332, ""The focus of the paper is using Zhou et al's Class Activation Mapping to show where the CNN focuses when making it's decision."": 1.0985502004623413, 'As I understand it, an input to CAM is the class of interest.': 1.0981336832046509, ""So let's say it is class 1 (black wins with a play to the bottom right square, if I've deciphered figure 2 correctly."": 1.0986123085021973, 'Figure 2 should really be more clear about what each class is).': 1.0985804796218872, 'So we ask CAM to determine the area of focus of the CNN for deciding whether class 1 is exhibited.': 1.0976355075836182, ""The focus ends up being on the empty bottom right square (because certainly you can't exhibit class 1 if the bottom right square is occupied)."": 1.0986123085021973, 'The CNN also needs to condition its decision on other parts of the board': 1.0986123085021973, 'it needs to know whether there will be 3 in a row from some direction.': 1.098611831665039, 'But maybe that conditioning is weaker?': 1.0986098051071167, ""That's kind of interesting"": 1.0983593463897705, ""but I'm not sure about the deeper statements about discovering game rules that the paper hints at."": 0.38598015904426575, ""I'm also not sure about the connection of this work to weakly supervised learning or multi-modal learning."": 1.09859037399292, ""The paper is pretty well written, overall, with some grammatical mistakes, but I simply don't see the surprising discovery of this work."": 1.0985772609710693, 'I also have some concerns about how contrived this scenario is': 1.090147614479065, 'using a big, expressive CNN for such a simple game domain and using a particular CNN visualization method.': 0.5809934735298157, ""I am not an expert in reinforcement learning (which isn't happening in this paper, but is in related works on CNN game playing), so maybe I'm not appreciating the paper appropriately."": 0.44659677147865295}"
430,https://openreview.net/forum?id=rJqBEPcxe,"{'The authors propose a conceptually simple method for regularisation of recurrent neural networks.': 1.0986123085021973, 'The idea is related to dropout, but instead of zeroing out units, they are instead set to their respective values at the preceding time step element-wise with a certain probability.': 1.0979324579238892, 'Overall, the paper is well written.': 1.0986123085021973, 'The method is clearly represented up to issues raised by reviewers during the pre-review question phase.': 1.0986123085021973, 'The related work is complete and probably the best currently available on the matter of regularising RNNs.': 1.0986123085021973, 'The experimental section focuses on comparing the method with the current SOTA on a set of NLP benchmarks and a synthetic problem.': 1.0986123085021973, 'All of the experiments focus on sequences over discrete values.': 1.0986123085021973, 'An additional experiment also shows that the sequential Jacobian is far higher for long-term dependencies than in the dropout case.': 1.0986120700836182, 'Overall, the paper bears great potential.': 1.0986123085021973, 'However, I do see some points.': 1.0986123085021973, '1) As raised during the pre-review questions, I would like to see the results of experiments that feature a complete hyper parameter search.': 1.0986123085021973, 'I.e. a proper model selection process,as it should be standard in the community.': 1.0986123085021973, 'I do not see why this was not done, especially as the author count seems to indicate that the necessary resources are available.': 1.0986123085021973, 'I want to repeat at this point that Table 2 of the paper shows that validation error is not a reliable estimator for testing error in the respective data set.': 1.0986119508743286, 'Thus, overfitting the model selection process is a serious concern here.': 1.0198590755462646, 'Zoneout does not seem to improve that much in the other tasks.': 0.4412492513656616, '2) Zoneout is not investigated well mathematically.': 1.0954220294952393, 'E.g. an analysis of the of the form of gradients from unit K at time step T to unit K’ at time step T-R would have been interesting, especially as these are not necessarily non-zero for dropout.': 1.0936927795410156, 'Also, the question whether zoneout has a variational interpretation in the spirit of Yarin Gal’s work is an obvious one.': 0.29630428552627563, 'I can see that it is if we treat zoneout in a resnet framework and dropout on the incremental parts.': 1.0985431671142578, 'Overall, little effort is done answering the question *why* zoneout works well, even though the literature bears plenty of starting points for such analysis.': 0.8379526138305664, '3)': 1.0985461473464966, 'The data sets used are only symbolic.': 1.0986123085021973, 'It would have been great if more ground was covered, i.e. continuous data such as from dynamical systems.': 1.0986123085021973, 'To me it is not obvious whether it will transfer right away.': 1.0986123085021973, 'An extreme amount of “tricks” is being published currently for improved RNN training.': 1.0986123085021973, 'How does zoneout stand out?': 1.0986123085021973, 'It is a nice idea, and simple to implement.': 1.0986123085021973, 'However, the paper under delivers: the experiments do not convince me (see 1) and 3)).': 1.0986123085021973, 'There authors do not provide convincing theoretical insights either.': 1.0986123085021973, '(2)': 1.0986123085021973, 'Consequently, the paper reduces to a “epsilon improvement, great text, mediocre experimental evaluation, little theoretical insight”.': 1.0986123085021973, 'Paper Summary': 1.0986123085021973, 'This paper proposes a variant of dropout, applicable to RNNs, in which the state': 0.8746365308761597, 'of a unit is randomly retained, as opposed to being set to zero.': 1.0986123085021973, 'This provides': 1.0986123085021973, 'noise which gives the regularization effect, but also prevents loss of': 1.0986123085021973, 'information over time, in fact making it easier to send gradients back because': 1.0986123085021973, 'they can flow right through the identity connections without attenuation.': 1.0986123085021973, 'Experiments show that this model works quite well.': 1.0986123085021973, 'It is still worse that': 1.0986123085021973, 'variational dropout on Penn Tree bank language modeling task, but given the': 1.0986123085021973, 'simplicity of the idea it is likely to become widely useful.': 1.0986123085021973, 'Strengths': 1.0986123085021973, 'Simple idea that works well.': 1.0986123085021973, 'Detailed experiments help understand the effects of the zoneout probabilities': 1.0986123085021973, 'and validate its applicability to different tasks/domains.': 1.0986123085021973, 'Weaknesses': 1.0986123085021973, 'Does not beat variational dropout (but maybe better hyper-parameter tuning': 1.0977730751037598, 'will help).': 1.0986123085021973, 'Quality': 1.0986123085021973, 'The experimental design and writeup is high quality.': 1.0986123085021973, 'Clarity': 1.0986123085021973, 'The paper clear and well written, experimental details seem adequate.': 1.0986123085021973, 'Originality': 1.0986123085021973, 'The proposed idea is novel.': 1.0986123085021973, 'Significance': 1.0986123085021973, 'This paper will be of interest to anyone working with RNNs (which is a large': 1.0986123085021973, 'group of people!).': 1.0986123085021973, 'Minor suggestion': 1.0986123085021973, 'As the authors mention - Zoneout has two things working for it - the noise and': 1.0986123085021973, 'the ability to pass gradients back without decay.': 1.0986123085021973, 'It might help to tease apart': 1.0986123085021973, 'the contribution from these two factors.': 1.0986123085021973, 'For example, if we use a fixed': 1.098610520362854, 'mask over the unrolled network (different at each time step) instead of resampling': 1.0967594385147095, 'it again for every training case, it would tell us how much help comes from the': 1.005398154258728, 'identity connections alone.': 1.0986123085021973, 'This paper tests zoneout against a variety of datasets - character level, word level, and pMNIST classification - showing applicability in a wide range of scenarios.': 1.0986123085021973, 'While zoneout acts as a regularizer to prevent overfitting, it also has similarities to residual connections.': 1.098611831665039, 'The continued analysis of this aspect, including analyzing how the gradient flow improves the given tasks, is of great interest and helps show it as an inherent property of zoneout.': 1.0986123085021973, 'This is a well written paper with a variety of experiments that support the claims.': 1.0986123085021973, 'I have also previously used this technique in a recurrent setting and am confident on the positive impact it can have upon tasks.': 1.0986123085021973, 'This is likely to become a standard technique used within RNNs across various frameworks.': 1.0986123085021973}"
431,https://openreview.net/forum?id=rJqFGTslg,"{'The idea of ""pruning where it matters"" is great.': 1.3852710723876953, 'The authors do a very good job of thinking it through, and taking to the next level by studying pruning across different layers too.': 1.2872685194015503, 'Extra points for clarity of the description and good pictures.': 1.3862943649291992, 'Even more extra points for actually specifying what spaces are which layers are mapping into which (\\mathbb symbol - two thumbs up!).': 1.3862943649291992, 'The experiments are well done and the results are encouraging.': 1.3862943649291992, 'Of course, more experiments would be even nicer, but is it ever not the case?': 1.3826327323913574, 'My question/issue - is the proposed pruning criterion proposed?': 1.3862943649291992, 'Yes, pruning on the filter level is what in my opinion is the way to go, but I would be curious how the ""min sum of weights"" criterion compares to other approaches.': 1.3862943649291992, 'How does it compare to other pruning criteria?': 1.3862943649291992, 'Is it better than ""pruning at random""?': 1.3862943649291992, 'Overall, I liked the paper.': 1.3410896062850952, 'This paper prunes entire groups of filters in CNN so that they reduce computational cost and at the same time do not result in sparse connectivity.': 1.3862943649291992, 'This result is important to speed up and compress neural networks while being able to use standard fully-connected linear algebra routines.': 1.3862943649291992, 'The results are a 10% improvements in ResNet-like and ImageNet, which may be also achieved with better design of networks.': 1.3862941265106201, 'New networks should have been also compared, but this we know it is time-consuming.': 1.3862942457199097, 'A good paper with some useful results.': 1.3862943649291992, 'This paper proposes a simple method for pruning filters in two types of architecture to decrease the time for execution.': 1.3862943649291992, 'Pros:': 1.3862943649291992, 'Impressively retains accuracy on popular models on ImageNet and Cifar10': 1.3862943649291992, 'Cons:': 1.3862943649291992, 'There is no justification for for low L1 or L2 norm being a good selection criteria.': 1.3862943649291992, 'There are two easy critical missing baselines of 1) randomly pruning filters, 2) pruning filters with low activation pattern norms on training set.': 1.3862943649291992, 'There is no direct comparison to the multitude of other pruning and speedup methods.': 1.3862943649291992, 'While FLOPs are reported, it is not clear what empirical speedup this method gives, which is what people interested in these methods care about.': 1.3862943649291992, 'Wall-clock speedup is trivial to report, so the lack of wall-clock speedup is suspect.': 1.3862943649291992, 'This paper proposes a very simple idea (prune low-weight filters from ConvNets) in order to reduce FLOPs and memory consumption.': 1.385992407798767, 'The proposed method is experimented on with VGG-16 and ResNets on CIFAR10 and ImageNet.': 1.3862941265106201, 'Creates *structured* sparsity, which automatically improves performance without changing the underlying convolution implementation': 1.3843892812728882, 'Very simple to implement': 1.3862943649291992, 'No evaluation of how pruning impacts transfer learning': 1.3862943649291992, ""I'm generally positive about this work."": 1.3862943649291992, 'While the main idea is almost trivial, I am not aware of any other papers that propose exactly the same idea and show a good set of experimental results.': 0.7281628251075745, ""Therefore I'm inclined to accept it."": 1.3862943649291992, 'The only major downside is that the paper does not evaluate the impact of filter pruning on transfer learning.': 1.3862943649291992, 'For example, there is not much interest in the tasks of CIFAR10 or even ImageNet.': 1.386293888092041, 'Instead, the main interest in both academia and industry is the value of the learned representation for transferring to other tasks.': 1.3556888103485107, 'One might expect filter pruning (or any other kind of pruning) to harm transfer learning.': 1.3862930536270142, ""It's possible that the while the main task has about the same performance, transfer learning is strongly hurt."": 1.3862943649291992, 'This paper has missed an opportunity to explore that direction.': 1.3862943649291992, 'Nit: Fig 2 title says VGG-16 in (b) and VGG_BN in (c).': 1.3862943649291992, 'Are these the same models?': 1.3862943649291992}"
432,https://openreview.net/forum?id=rJq_YBqxx,"{'*': 1.0974255800247192, 'Summary: This paper proposes a neural machine translation model that translates the source and the target texts in an end to end manner from characters to characters.': 1.0985841751098633, 'The model can learn morphology in the encoder and in the decoder the authors use a hierarchical decoder.': 1.0725971460342407, 'Authors provide very compelling results on various bilingual corpora for different language pairs.': 1.0986123085021973, 'The paper is well-written, the results are competitive compared to other baselines in the literature.': 1.0986121892929077, '* Review:': 1.0986123085021973, '- I think the paper is very well written, I like the analysis presented in this paper. It is clean and precise.': 0.6650456190109253, '- The idea of using hierarchical decoders have been explored before, e.g. [1]. Can you cite those papers?': 0.5481000542640686, '- This paper is mainly an application paper and it is mainly the application of several existing components on the character-level NMT tasks. In this sense, it is good that authors made their codes available online. However, the contributions from the general ML point of view is still limited.': 0.2858375310897827, '* Some Requests:': 1.0621514320373535, '-Can you add the size of the models to the Table 1?': 1.076825499534607, 'Can you add some of the failure cases of your model, where the model failed to translate correctly?': 1.0986123085021973, 'An Overview of the Review:': 1.0986123085021973, 'Pros:': 1.0984407663345337, '- The paper is well written': 0.9816979765892029, '- Extensive analysis of the model on various language pairs': 1.0982885360717773, '- Convincing experimental results.': 0.8374925255775452, 'Cons:': 1.0986123085021973, '- The model is complicated.': 0.9021557569503784, '- Mainly an architecture engineering/application paper(bringing together various well-known techniques), not much novelty.': 0.5463033318519592, '- The proposed model is potentially slower than the regular models since it needs to operate over the characters instead of the words and uses several RNNs.': 0.4006388187408447, '[1] Serban IV, Sordoni A, Bengio Y, Courville A, Pineau J. Hierarchical neural network generative models for movie dialogues.': 0.2880694568157196, 'arXiv preprint arXiv:1507.04808.': 1.0959581136703491, '2015': 1.0986123085021973, 'Jul 17.': 1.0986123085021973, ""Update after reading the authors' responses & the paper revision dated Dec 21:"": 1.0986123085021973, 'I have removed the comment ""insufficient comparison to past work"" in the title & update the score from 3 -> 5.': 1.0973496437072754, 'The main reason for the score is on novelty.': 1.0986123085021973, 'The proposal of HGRU & the use of the R matrix are basically just to achieve the effect of ""whether to continue from character-level states or using word-level states"".': 1.0986123085021973, 'It seems that these solutions are specific to symbolic frameworks like Theano (which the authors used) and TensorFlow.': 1.0986123085021973, 'This, however, is not a problem for languages like Matlab (which Luong & Manning used) or Torch.': 1.0986123085021973, 'This is a well-written paper with good analysis in which I especially like Figure 5.': 1.0986123085021973, 'However I think there is little novelty in this work.': 1.0685449838638306, 'The title is about learning morphology but there is nothing specifically enforced in the model to learn morphemes or subword units.': 1.098609447479248, ""For example, maybe some constraints can be put on the weights in w_i in Figure 1 to detect morpheme boundaries or some additional objective like MDL can be used (though it's not clear how these constraints can be incorporated cleanly)."": 1.0986123085021973, ""Moreover, I'm very surprised that litte comparison (only a brief mention) was given to the work of (Luong & Manning, 2016)"": 1.0986123085021973, '[1], which trains deep 8-layer word-character models and achieves much better results on English-Czech, e.g., 19.6 BLEU compared to 17.0 BLEU achieved in the paper.': 0.943862795829773, 'I think the HGRU thing is over-complicated in terms of presentation.': 0.8850774168968201, 'If I read correctly, what HGRU does is basically either continue the character decoder or reset using word-level states at boundaries, which is what was done in [1].': 1.098476767539978, 'Luong & Manning (2016) even make it more efficient by not having to decode all target words at the morpheme level & it would be good to know the speed of the model proposed in this ICLR submission.': 1.0986121892929077, 'What end up new in this paper are perhaps different analyses on what a character-based model learns & adding an additional RNN layer in the encoder.': 1.0986123085021973, 'One minor comment: annotate h_t in Figure 1.': 0.15674550831317902, '[1] Minh-Thang Luong and Christopher D. Manning.': 1.0847193002700806, '2016.': 0.43516016006469727, 'Achieving Open Vocabulary Neural Machine Translation': 1.0986123085021973, 'with Hybrid Word-Character Models.': 0.9577690958976746, 'ACL.': 0.5396695137023926, 'https://arxiv.org/pdf/1604.00788v2.pdf': 0.9679998755455017, 'The paper presents one of the first neural translation systems that operates purely at the character-level, another one being https://arxiv.org/abs/1610.03017 , which can be considered a concurrent work.': 1.0985991954803467, 'The system is rather complicated and consists of a lot of recurrent networks.': 1.0985325574874878, 'The quantitative results are quite good and the qualitative results are quite encouraging.': 1.097873330116272, 'First, a few words about the quality of presentation.': 1.097908854484558, 'Despite being an expert in the area, it is hard for me to be sure that I exactly understood what is being done.': 1.0986113548278809, 'The Subsections 3.1 and 3.2 sketch two main features of the architecture at a rather high-level.': 1.0986123085021973, 'For example, does the RNN sentence encoder receive one vector per word as input or more?': 1.0986123085021973, 'Figure 2 suggests that it’s just one.': 0.7627331018447876, 'The notation h_t is overloaded, used in both Subsection 3.1 and 3.2 with clearly different meaning.': 1.0986123085021973, 'An Appendix that explains unambiguously how the model works would be in order.': 1.0986123085021973, 'Also, the approach appears to be limited by its reliance on the availability of blanks between words, a trait which not all languages possess.': 1.0986123085021973, 'Second, the results seem to be quite good.': 1.0986123085021973, 'However, no significant improvement over bpe2char systems is reported.': 1.0986123085021973, 'Also, I would be curious to know how long it takes to train such a model,  because from the description it seems like the model would be very slow to train (400 steps of BiNNN).': 1.0986123085021973, 'On a related note, normally an ablation test is a must for such papers, to show that the architectural enhancements applied were actually necessary.': 1.0986123085021973, 'I can imagine that this would take a lot of GPU time for such a complex model.': 1.0986123085021973, 'On the bright side, Figure 3 presents some really interesting properties that of the embeddings that the model learnt.': 1.0986123085021973, 'Likewise interesting is Figure 5.': 1.0986123085021973, 'To conclude, I think that this an interesting application paper, but the execution quality could be improved.': 1.0986123085021973, 'I am ready to increase my score if an ablation test confirms that the considered encoder is better than a trivial baseline, that e.g. takes the last hidden state for each RNN.': 1.0986123085021973}"
433,https://openreview.net/forum?id=rJsiFTYex,"{'The paper proposes and analyses three methods applied to traditional LSTMs: Monte Carlo test-time model averaging, average pooling, and residual connections.': 1.098597764968872, 'It shows that those methods help to enhance traditional LSTMs on sentiment analysis.': 1.0986123085021973, 'Although the paper is well written, the experiment section is definitely its dead point.': 1.0986123085021973, 'Firstly, although it shows some improvements over traditional LSTMs, those results are not on par with the state of the art.': 1.0984692573547363, 'Secondly, if the purpose is to take those extensions as strong baselines for further research, the experiments are not adequate: the both two datasets which were used are quite similar (though they have different statistics).': 1.0986123085021973, 'I thus suggest to carry out more experiments on more diverse tasks, like those in ""LSTM: A Search Space Odyssey"").': 1.097557544708252, 'Besides, those extensions are not really novel.': 1.0986123085021973, 'This paper presents three improvements to the standard LSTM architecture used in many neural NLP models: Monte Carlo averaging, embed average pooling, and residual connections.': 1.0986123085021973, 'Each of the modifications is trivial to implement, so the paper is definitely of interest to any NLP researchers experimenting with deep learning.': 1.098495364189148, 'With that said, I am concerned about the experiments and their results.': 1.0826839208602905, 'The residual connections do not seem to consistently help performance; on SST the vertical residuals help but the lateral residuals hurt, and on IMDB it is the opposite.': 1.0986028909683228, 'More fundamentally, there need to be more tasks than just sentiment analysis here.': 1.0985280275344849, ""I'm not quite sure why the paper's focus is on text classification, as any NLP task using an LSTM encoder could conceivably benefit from these modifications."": 1.0986123085021973, 'It would be great to see a huge variety of tasks like QA, MT, etc., which would really make the paper much stronger.': 1.084470510482788, ""At this point, while the experiments that are included in the paper are very thorough and the analysis is interesting, there need to be more tasks to convince me that the modifications generalize, so I don't think the paper is ready for publication."": 1.0986120700836182, 'I agree with the other reviewer that the application areas are limited in the paper.': 1.0986123085021973, 'I agree with the overall sentiment of the paper to evaluate effectiveness of some of the more recent techniques in this area, in conjunction with the recurrent networks.': 1.098610758781433, 'The paper advertises itself as a method (or a list of methods) of improving the recurrent baselines when performing experiments, however fails (or not shown) to generalize to other tasks.': 1.0986123085021973, 'Effectiveness of these methods need to be shown across a wide variety of tasks if we intend to replace traditional baselines in general, rather than a specific subset of applications.': 1.0986123085021973, 'I like the desire to evaluate many of the recent techniques and having many replications of experiments towards this end (which is a strong point of the paper).': 1.0986123085021973, 'However, whether there are synergies of some of the enhancements with sentiment analysis or not, we cannot see from these results.': 1.0986123085021973, 'It would be interesting to see whether some of these results generalize across a wide variety of tasks.': 1.0986123085021973}"
434,https://openreview.net/forum?id=rJxDkvqee,"{'Pros:': 0.4112173318862915, 'Interesting training criterion.': 1.0986121892929077, 'Cons:': 1.0979865789413452, 'Missing proper ASR technique based baselines.': 1.0986119508743286, 'Comments:': 1.0986065864562988, 'The dataset is quite small.': 1.0976567268371582, 'ROC curves for detection, and more measurements, e.g. EER would probably be helpful besides AP.': 0.2869594097137451, 'More detailed analysis of the results would be necessary, e.g. precision of words seen during training compared to the detection': 1.0986120700836182, 'performance of out-of-vocabulary words.': 1.0442330837249756, 'It would be interesting to show scatter plots for embedding vs. orthographic distances.': 1.0986123085021973, 'This paper proposes an approach to learning word vector representations for character sequences and acoustic spans jointly.': 1.0986123085021973, 'The paper is clearly written and both the approach and experiments seem reasonable in terms of execution.': 1.0986123085021973, 'The motivation and tasks feel a bit synthetic as it requires acoustics spans for words that have already been segmented from continuous speech - - a major assumption.': 1.0986123085021973, 'The evaluation tasks feel a bit synthetic overall and in particular when evaluating character based comparisons it seems there should also be phoneme based comparisons.': 1.0986123085021973, ""There's a lot of discussion of character edit distance relative to acoustic span similarity."": 1.0986123085021973, 'It seems very natural to also include phoneme string edit distance in this discussion and experiments.': 1.098609447479248, 'This is especially true of the word similarity test.': 1.0965479612350464, 'Rather than only looking at levenshtein edit distance of characters you should evaluate edit distance of the phone strings relative to the acoustic embedding distances.': 1.0986123085021973, 'Beyond the evaluation task the paper would be more interesting if you compared character embeddings with phone string embeddings.': 1.0576167106628418, ""I believe the last function could remain identical it's just swapping out characters for phones as the symbol set."": 1.0986123085021973, 'finally in this topic the discussion and experiments should look at homophones As if not obvious what the network would learn to handle these.': 0.5304344892501831, 'the vocabulary size and training data amount make this really a toy problem.': 0.6602670550346375, 'although there are many pairs constructed most of those pairs will be easy distinctions.': 1.0234121084213257, 'the experiments and conclusions would be far stronger with a larger vocabulary and word segment data set with subsampling all pairs perhaps biased towards more difficult or similar pairs.': 0.4680538773536682, 'it seems this approach is unable to address the task of keyword spotting in longer spoken utterances.': 1.0931086540222168, ""If that's the case please add some discussion as to why you are solving the problem of word embeddings given existing word segmentations."": 1.0986123085021973, 'The motivating example of using this approach to retrieve words seems flawed if a recognizer must be used to segment words beforehand': 0.5141577124595642, 'this proposes a multi-view learning approach for learning representations for acoustic sequences.': 1.0986123085021973, 'they investigate the use of bidirectional LSTM with contrastive losses.': 1.0986123085021973, 'experiments show improvement over the previous work.': 1.0986123085021973, 'although I have no expertise in speech processing, I am in favor of accepting this paper because of following contributions:': 1.0986123085021973, 'investigating the use of fairly known architecture on a new domain.': 1.0986123085021973, 'providing novel objectives specific to the domain': 1.0986123085021973, 'setting up new benchmarks designed for evaluating multi-view models': 1.0986123085021973, 'I hope authors open-source their implementation so that people can replicate results, compare their work, and improve on this work.': 1.0986123085021973}"
435,https://openreview.net/forum?id=rJxdQ3jeg,"{""Two things I'd like to see."": 1.608264684677124, '1) Specifics about the JPEG and JPEG2000 implementations used, and how they were configured.': 1.604040503501892, 'One major weakness I see in many papers is they do not include specific encoders and configuration used in comparisons.': 1.609397053718567, ""Without knowing this, it's hard to know if the comparison was done with a suitably strong JPEG implementation that was properly configured, for example."": 1.6094379425048828, '2) The comparison to JPEG2000 is unfortunately not that interesting, since that codec does not have widespread usage and likely never will.': 0.9858250617980957, 'A better comparison would be with WebP performance.': 1.6094218492507935, 'Or, even better, both.': 1.6094378232955933, 'Very nice results.': 1.6094379425048828, 'Is a software implementation of this available to play with?': 1.5288931131362915, 'This is the most convincing paper on image compression with deep neural networks that I have read so far.': 1.6094379425048828, 'The paper is very well written, the use of the rate-distortion theory in the objective fits smoothly in the framework.': 1.6094379425048828, 'The paper is compared to a reasonable baseline (JPEG2000, as opposed to previous papers only considering JPEG).': 1.6094379425048828, 'I would expect this paper to have a very good impact.': 1.6094379425048828, 'Yes, please include results on Lena/Barbara/Baboon (sorry, not Gibbons), along with state-of-the-art references with more classical methods such as the one I mentioned in my questions.': 1.6094379425048828, 'I think it is important to clearly state how NN compare to best previous methods.': 1.6094379425048828, ""From the submitted version, I still don't know how both categories of methods are positioned."": 1.6094379425048828, 'This paper extends an approach to rate-distortion optimization to deep encoders and decoders, and from a simple entropy encoding scheme to adaptive entropy coding.': 1.6094354391098022, 'In addition, the paper discusses the approach’s relationship to variational autoencoders.': 1.6074435710906982, 'Given that the approach to rate-distortion optimization has already been published, the novelty of this submission is arguably not very high (correct me if I missed a new trick).': 1.6094343662261963, 'In some ways, this paper even represents a step backward, since earlier work optimized for a perceptual metric where here MSE is used.': 1.6094379425048828, 'However, the results are a visible improvement over JPEG 2000, and I don’t know of any other learned encoding which has been shown to achieve this level of performance.': 1.6094378232955933, 'The paper is very well written.': 1.6094379425048828, 'Equation 10 appears to be wrong and I believe the partition function should depend on g_s(y; theta).': 1.5608994960784912, 'This would mean that the approach is not equivalent to a VAE for non-Euclidean metrics.': 1.5110325813293457, 'What was the reason for optimizing MSE rather than a perceptual metric as in previous work?': 1.6094379425048828, 'Given the author’s backgrounds, it is surprising that even the evaluation was only performed in terms of PSNR.': 1.5976637601852417, 'What is the contribution of adaptive entropy coding versus the effect of deeper encoders and decoders?': 1.6094379425048828, 'This seems like an important piece of information, so it would be interesting to see the performance without adaptation as in the previous paper.': 1.5935348272323608, 'More detail on the adaptive coder and its effects should be provided, and I will be happy to give a higher score when the authors do.': 1.6093871593475342, 'This is a nice paper that demonstrates an end-to-end trained image compression and decompression system, which achieves better bit-rate vs quality trade-offs than established image compression algorithms (like JPEG-2000).': 1.6094379425048828, 'In addition to showing the efficacy of \'deep learning\' for a new application, a key contribution of the paper is the introduction of a differentiable version of ""rate"" function, which the authors show can be used for effective training with different rate-distortion trade-offs.': 1.6094379425048828, 'I expect this will have impact beyond the compression application itself': 0.9059407711029053, 'for other tasks that might benefit from differentiable approximations to similar functions.': 1.3549293279647827, 'The authors provided a thoughtful response to my pre-review question.': 1.6086094379425049, 'I would still argue that to minimize distortion under a fixed range and quantization, a sufficiently complex network would learn automatically produce  codes within a fixed range with the highest-possible entropy (i.e., it would meet the upper bound).': 1.6094379425048828, 'But the second argument is convincing': 1.6094379425048828, 'doing so forces a specific ""form"" on how the compressor output is used, which to match the effective compression of the current system, would require a more complex network that is able to carry out the computations currently being done by a separate variable rate encoder used to store q.': 1.4555866718292236, 'This nicely written paper presents an end-to-end learning method for image compression.': 1.609437346458435, 'By optimizing for rate-distortion performance and a clever relaxation the method is able to learn an efficient image compression method by optimizing over a database of natural images.': 1.6094379425048828, ""As the method is interesting, results are interesting and analysis is quite thorough it's easy for me to recommend acceptance."": 1.6094375848770142}"
436,https://openreview.net/forum?id=rJzaDdYxx,"{'This paper proposes a new method, interior gradients, for analysing feature importance in deep neural networks.': 1.0986123085021973, 'The interior gradient is the gradient measured on a scaled version of the input.': 1.0986123085021973, 'The integrated gradient is the integral of interior gradients over all scaling factors.': 1.0986123085021973, 'Visualizations comparing integrated gradients with standard gradients on real images input to the Inception CNN show that integrated gradients correspond to an intuitive notion of feature importance.': 1.0983870029449463, 'While motivation and qualitative examples are appealing, the paper lacks both qualitative and quantitative comparison to prior work.': 1.0986123085021973, 'Only the baseline (simply the standard gradient) is presented as reference for qualitative comparison.': 1.0986123085021973, 'Yet, the paper cites numerous other works (DeepLift, layer-wise relevance propagation, guided backpropagation) that all attack the same problem of feature importance.': 1.0884640216827393, 'Lack of comparison to any of these methods is a major weakness of the paper.': 1.0986123085021973, 'I do not believe it is fit for publication without such comparisons.': 1.0986123085021973, 'My pre-review question articulated this same concern and has not been answered.': 1.0986123085021973, 'The authors propose to measure “feature importance”, or specifically, which pixels contribute most to a network’s classification of an image.': 1.0296469926834106, 'A simple (albeit not particularly effective) heuristic for measuring feature importance is to measure the gradients of the predicted class wrt each pixel in an input image I.': 0.40570300817489624, 'This assigns a score to each pixel in I (that ranks how much the output prediction would change if a given pixel were to change).': 1.098588466644287, 'In this paper, the authors build on this and propose to measure feature importance by computing gradients of the output wrt scaled version of the input image, alpha*I, where alpha is a scalar between 0 and 1, then summing across all values of alpha to obtain their feature importance score.': 0.7113813161849976, 'Here the scaling is simply linear scaling of the pixel values (alpha=0 is all black image, alpha=1 is original image).': 1.0984939336776733, 'The authors call these scaled images “counterfactuals” which seems like quite an unnecessarily grandiose name for literally, a scaled image.': 1.094122052192688, 'The authors show a number of visualizations that indicate that the proposed feature importance score is more reasonable than just looking at gradients only with respect to the original image.': 0.767448902130127, 'They also show some quantitative evidence that the pixels highlighted by the proposed measure are more likely to fall on the objects rather than spurious parts of the image (in particular, see figure 5).': 1.091139316558838, 'The method is also applied to other types of networks.': 1.0985547304153442, 'The quantitative evidence is quite limited and most of the paper is spent on qualitative results.': 1.096238136291504, 'While the goal of understanding deep networks is of key importance, it is not clear whether this paper really help elucidate much.': 1.098568081855774, 'The main interesting observation in this paper is that scaling an image by a small alpha (i.e. creating a faint image) places more “importance” on pixels on the object related to the correct class prediction.': 1.0986123085021973, 'Beyond that, the paper builds a bit on this, but no deeper insight is gained.': 1.0986123085021973, 'The authors propose some hand-wavy explanation of why using small alpha (faint image) may force the network to focus on the object, but the argument is not convincing.': 1.0986123085021973, 'It would have been interesting to try to probe a bit deeper here, but that may not be easy.': 1.0986123085021973, 'Ultimately, it is not clear how the proposed scheme for feature importance ranking is useful.': 1.0986123085021973, 'First, it is still quite noisy and does not truly help understand what a deep net is doing on a particular image.': 1.0986123085021973, 'Performing a single gradient descent step on an image (or on the collection of scaled versions of the image) hardly begins to probe the internal workings of a network.': 1.0986123085021973, 'Moreover, as the authors admit, the scheme makes the assumption that each pixel is independent, which is clearly false.': 1.0986123085021973, 'Considering the paper presents a very simple idea, it is far too long.': 1.0986123085021973, 'The main paper is 14 pages, up to 19 with references and appendix.': 1.0986123085021973, 'In general the writing is long-winded and overly verbose.': 1.0986123085021973, 'It detracted substantially from the paper.': 1.0986123085021973, 'The authors also define unnecessary terminology.': 1.0986123085021973, '“Gradients of Coutnerfactuals” sounds quite fancy, but is not very related to the ideas explored in the writing.': 1.0986123085021973, 'I would encourage the authors to tighten up the writing and figures down to a more readable page length, and to more clearly spell out the ideas explored early on.': 1.0986123085021973, 'This work proposes to use visualization of gradients to further understand the importance of features (i.e. pixels) for visual classification.': 1.0986123085021973, 'Overall, this presented visualizations are interesting, however, the approach is very ad hoc.': 1.0986123085021973, ""The authors do not explain why visualizing regular gradients isn't correlated with the importance of features relevant to the given visual category and proceed to the interior gradient approach."": 1.09860360622406, 'One particular question with regular gradients at features that form the spatial support of the visual class.': 1.0986123085021973, 'Is it the case that the gradients of the features that are confident of the prediction remain low, while those with high uncertainty will have strong gradients?': 1.0986123085021973, 'With regards to the interior gradients, it is unclear how the scaling parameter \\alpha affects the feature importance and how it is related to attention.': 1.0986123085021973, 'Finally, does this model use batch normalization?': 1.0986123085021973}"
437,https://openreview.net/forum?id=rk5upnsxe,"{'The authors present a unified framework for various divisive normalization schemes, and then show that a somewhat novel version of normalization does somewhat better on several tasks than some mid-strength baselines.': 1.0986123085021973, 'Pros:': 1.0986123085021973, '* It has seemed for a while that there are a bunch of different normalization methods out there, of varying importance in varying applications, so having a standardized framework for them all, and evaluating them carefully and systematically, is a very useful contribution.': 1.0667263269424438, '*': 1.0986123085021973, 'The paper is clearly written.': 1.0986123085021973, '* From an architectural standpoint, the actual comparisons seem well motivated.': 0.6294545531272888, ""(For instance, I'm glad they tried DN* and BN*"": 0.7801674008369446, ""if they hadn't tried those, I would have wanted them too.)"": 1.098589301109314, 'Cons:': 1.0986123085021973, ""* I'm not really sure what the difference is between their new DN method and standard cross-channel local contrast normalization."": 0.4192509055137634, '(Oh, actually': 1.098571538925171, 'looking at the other reviews, everyone else seems to have noticed this too.': 1.0985270738601685, ""I'll not beat a dead horse about this any further.)"": 1.0986123085021973, ""* I'm nervous that the conclusions that they state might not hold on larger, stronger tasks, like ImageNet, and with larger deeper models."": 1.0903629064559937, 'I myself have found that while with smaller models on simpler tasks (e.g. Caltech 101), contrast normalization was really useful, that it became much less useful for larger architectures on larger tasks.': 1.0986028909683228, ""In fact, if I recall correctly, the original AlexNet model had a type of cross-unit normalization in it, but this was dispensed with in more recent models (I think after Zeiler and Fergus 2013) largely because it didn't contribute that much to performance but was somewhat expensive computationally."": 1.0982639789581299, 'Of course, batch normalization methods have definitely been shown to contribute performance on large problems with large models, but I think it would be really important to show the same with the DN methods here, before any definite conclusion could be reached.': 1.098602056503296, 'This paper empirically studies multiple combinations of various tricks to improve the performance of deep neural networks on various tasks.': 1.0986123085021973, 'Authors investigate various combinations of normalization techniques together with additional regularizations.': 1.0986123085021973, 'The paper makes few interesting empirical observations, such that the L1 regularizer on top of the activations is relatively useful for most of the tasks.': 1.0986123085021973, 'In general, it seems that this work can be significantly improved by providing more precise study of existing normalization techniques.': 1.0986123085021973, 'Also, studying more closely the overall volumes of the summation and suppression fields (e.g. how many samples one needs to collect for a robust enough normalization) would be useful.': 1.0986123085021973, 'In more detail, the work seems to have the following issues:': 1.0986123085021973, '* Divisive normalization, is used extensively in Krizhevsky12 (LRN).': 1.0986123085021973, 'It is almost exactly the same definition as in equation 1, however with slightly different constants.': 1.0986123085021973, 'Therefore claiming that it is less explored is questionable.': 1.0986123085021973, '* It is not clear whether the Divisive normalization does subtract the mean from the activation as there is a contradiction in its definition in equation 1 and 3.': 1.0986123085021973, 'This questions whether the ""General Formulation of Normalization"" is correct.': 1.0986123085021973, 'In seems that Divisive normalization is used also in Jarrett09, called Contrast Normalization, with a definition more similar to equation 3 (subtracting the mean).': 1.0986123085021973, 'In case of the RNN experiments, it would be more clear to provide the absolute size of the summation and suppression field as BN may be inferior to DN due to a small batch size.': 1.0986123085021973, '* It is unclear what and how are measured the results shown in Table 10.': 1.0986123085021973, 'Also it is unclear what are the sizes of the suppression/summation fields for the CIFAR and Super Resolution experiments.': 1.0986123085021973, 'Minor, relatively irrelevant issues:': 1.0986123085021973, '* It is usually better to pick a stronger baseline for the tasks.': 1.0980244874954224, 'The selected CIFAR model from Caffe seems to be quite far from the state of the art on the CIFAR dataset.': 1.0860086679458618, 'A stronger baseline (e.g. the widely available ResNet) would allow to see whether the proposed techniques are useful for the more recent models as well.': 1.0986121892929077, '* Double caption for Table 7/8.': 1.098611831665039, '*** Paper Summary ***': 1.0986123085021973, 'This paper proposes a unified view on normalization.': 1.098061442375183, 'The framework encompases layer normalization, batch normalization and local contrast normalization.': 0.9989629983901978, 'It also suggests decorrelating the inputs through L1 regularization of the activations.': 1.0985918045043945, 'Results are reported on three tasks: CIFAR classification, PTB Language models and super resolution on Berkeley dataset.': 0.9989991784095764, '*** Review Summary ***': 1.0968211889266968, 'Overall, I feel it is good to refresh the community about local normalization schemes and other mechanism to favor unit competitions.': 0.852149486541748, 'The paper reads well and reports results on various setups, with sufficient discussion.': 1.0405704975128174, '*** Detailed Review ***': 1.0986123085021973, 'The paper is clear and reads well.': 1.0750527381896973, 'It lacks a few reference to prior research.': 1.0984556674957275, 'Also I am surprised that ""Local Contrast Normalization"" is not said anywhere, as it is a common terminology in the neural network and vision literature.': 0.7173959612846375, 'It is unclear to me why you chose to pair L1 regularization of the activation and normalization.': 1.0927013158798218, 'They seem complementary.': 1.0984838008880615, 'Would it make sense to apply L1 regularization to the baseline to highlight it is helpful on its own.': 0.526938796043396, 'Overall, it seems the only thing that brings a consistent improvement across all setups.': 0.9750009775161743, 'On related work, maybe it would be worthwhile to insist that Local Contrast Normalization (LCN) used to be very popular': 1.0974780321121216, '[Pinto et al, 2008, Jarret et al 2009, Sermanet et al 2012; Quoc Le 2013] and effective.': 0.5009106993675232, 'It is great to connect this litterature to current work on layer normalization and batch normalization.': 0.8736669421195984, 'Similarly, sparsity or group sparsity of the activation has shown effective in the past [Rozell et al 08, Kavukcuoglu et al 09] and need more exposure today.': 0.6120902895927429, 'Finally, since dropout is so popular but interact poorly with normalizer estimates, I feel it would be worthwhile to report results with dropout beyond the baseline and discuss how the different normalization scheme interact with it.': 1.0850675106048584, 'The paper reads well and reports results on various setups, with sufficent discussion.': 1.0973286628723145, '*** References ***': 1.098045825958252, 'Jarrett, Kevin, Koray Kavukcuoglu, and Yann Lecun.': 0.4034879803657532, '""What is the best multi-stage architecture for object recognition?.""': 1.0986123085021973, '2009 IEEE 12th International Conference on Computer Vision.': 0.6059406995773315, 'IEEE, 2009.': 1.0986123085021973, 'Pinto, N., Cox, D., DiCarlo, J.: Why is real-world visual object recognition hard?': 1.0887229442596436, 'PLoS Comput Biol 4 (2008)': 1.0986123085021973, 'Le, Quoc V. ""Building high-level features using large scale unsupervised learning.""': 1.0986123085021973, '2013 IEEE international conference on acoustics, speech and signal processing.': 1.0986123085021973, 'IEEE, 2013.': 1.0986123085021973, 'P. Sermanet, S. Chintala, and Y. LeCun.': 1.0986123085021973, 'Convolutional neural networks applied to house': 1.0986123085021973, 'numbers digit classification.': 1.0986123085021973, 'In ICPR, 2012.': 1.0986123085021973, 'C. Rozell, D. Johnson, and B. Olshausen.': 1.0986123085021973, 'Sparse coding via thresholding and local competition in neural circuits.': 1.0986123085021973, 'Neural Computation, 2008.': 1.0986123085021973, 'K. Kavukcuoglu, M. Ranzato, R. Fergus, and Y. LeCun.': 1.0986123085021973, 'Learning invariant features through topographic filter maps.': 1.0986123085021973, 'In CVPR, 2009.': 1.0986123085021973}"
438,https://openreview.net/forum?id=rk9eAFcxg,"{'The work combines variational recurrent neural networks, and adversarial neural networks to handle domain adaptation for time series data.': 1.0986088514328003, 'The proposed method, along with several competing algorithms are compared on two healthcare datasets constructed from MIMIC-III in domain adaptation settings.': 1.0986119508743286, 'The new contribution of the work is relatively small.': 0.7939950823783875, 'It extends VRNN with adversarial training for learning domain agnostic representations.': 1.098609209060669, 'From the experimental results, the proposed method clearly out-performs competing algorithms.': 0.6238152384757996, 'However, it is not clear where the advantage is coming from.': 1.075791358947754, 'The only difference between the proposed method and R-DANN is using variational RNN vs RNN.': 0.5984698534011841, 'Little insights were provided on how this could bring such a big difference in terms of performance and the drastic difference in the temporal dependencies captured by these two methods in Figure 4.': 1.0986123085021973, 'Detailed comments:': 1.0986123085021973, '1. Please provide more details on what is plotted in Figure 1. Is 1 (b) is the t-sne projection of representations learned by DANN or R-DANN? The text in section 4.4 suggests it’s the later case. It is surprising to see such a regular plot for VRADA.  What do you think are the two dominant latent factors encoded in figure 1 (c)?': 0.4132230579853058, '2. In Table 2, the two baselines have quite significant difference in performance testing on the entire target (including validation set) vs on the test set only. VRADA, on the other hand, performs almost identical in these two settings. Could you please offer some explanation on this?': 0.47001582384109497, '3. Please explain figure 3 and 4 in more details. how to interpret the x-axis of figure 3, and the x and y axes of figure 4. Again the right two plots in figure 4 are extremely regular comparing to the ones on the left.': 0.9215126037597656, 'Update: I thank the authors for their comments!': 1.098565697669983, 'After reading them, I still think the paper is not novel enough': 1.0866237878799438, ""so I'm leaving the rating untouched."": 1.0986123085021973, 'This paper proposes a domain adaptation technique for time series.': 1.016841173171997, 'The core of the approach is a combination of variational recurrent neural networks and adversarial domain adaptation (at the last time step).': 1.0986123085021973, 'Pros:': 1.0986123085021973, '1. The authors consider a very important application of domain adaptation.': 1.0986123085021973, '2. The paper is well-written and relatively easy to read.': 1.0986123085021973, '3. Solid empirical evaluation. The authors compare their method against several recent domain adaptation techniques on a number of datasets.': 1.0986123085021973, 'Cons:': 1.0986123085021973, '1. The novelty of the approach is relatively low: it’s just a straightforward fusion of the existing techniques.': 1.0986123085021973, '2. The paper lacks any motivation for use of the particular combination (VRNN and RevGrad). I still believe comparable results can be obtained by polishing R-DANN (e.g. carefully penalizing domain discrepancy at every step)': 1.0986123085021973, 'Additional comments:': 1.0986123085021973, '1. I’m not convinced by the discussion presented in Section 4.4. I don’t think the visualization of firing patterns can be used to support the efficiency of the proposed method.': 1.0986123085021973, '2. Figure 1(c) looks very suspicious. I can hardly believe t-SNE could produce this _very_ regular structure for non-degenerate (non-synthetic, real-world) data.': 1.0986123085021973, 'Overall, it’s a solid paper but I’m not sure if it is up to the ICLR standard.': 1.0986123085021973, 'This paper combines variational RNN (VRNN) and domain adversarial networks (DANN) for domain adaptation in the sequence modelling domain.': 1.0986123085021973, 'The VRNN is used to learn representations for sequential data, which is the hidden states of the last time step.': 1.0986123085021973, 'The DANN is used to make the representations domain invariant, therefore achieving cross domain adaptation.': 1.0986123085021973, 'Experiments are done on a number of data sets, and the proposed method (VRADA) outperforms baselines including DANN, VFAE and R-DANN on almost all of them.': 1.0986123085021973, ""I don't have questions about the proposed model, the model is quite clear and seems to be a simple combination of VRNN and DANN."": 1.0986123085021973, 'But a few questions came up during the pre-review question phase:': 1.0986123085021973, 'As the authors have mentioned, DANN in general outperforms MMD based methods, however, the VFAE method which is based on MMD regularization on the representations seems to outperform DANN across the board.': 1.0986123085021973, 'That seems to indicate VRNN + MMD should also be a good combination.': 1.0986123085021973, 'One baseline the authors showed in the experiments is R-DANN, which is an RNN version of DANN.': 1.0986123085021973, 'There are two differences between R-DANN and VRADA: (1) R-DANN uses deterministic RNN for representation learning, while VRADA uses variational RNN; (2) on target domain R-DANN only optimizes adversarial loss, while VRADA optimizes both adversarial loss and reconstruction loss for feature learning.': 1.0986123085021973, 'It would be good to analyze further where the performance gain comes from.': 1.0986123085021973}"
439,https://openreview.net/forum?id=rkE3y85ee,"{'This paper introduces a continuous relaxation of categorical distribution,  namely the the Gumbel-Softmax distribution, such that generative models with categorical random variables can be trained using reparameterization (path-derivative) gradients.': 1.0986123085021973, 'The method is shown to improve upon other methods in terms of the achieved log-likelihoods of the resulting models.': 1.0986123085021973, 'The main contribution, namely the method itself, is simple yet nontrivial and worth publishing, and seems effective in experiments.': 1.0986123085021973, 'The paper is well-written, and I applaud the details provided in the appendix.': 1.0984174013137817, 'The main application seems to be semi-supervised situations where you really want categorical variables.': 1.0986123085021973, '- P1: ""differentiable sampling mechanism for softmax"". ""sampling"" => ""approximate sampling"", since it\'s technically sampling from the Gumbal-softmax.': 1.0985040664672852, '- P3: ""backpropagtion""': 1.0954629182815552, '- Section 4.1: Interesting experiments.': 0.4807114899158478, '- It would be interesting to report whether there is any discrepancy between the relaxed and non-relaxed models in terms of log-likelihood. Currently, only the likelihoods under the non-relaxed models are reported.': 0.9457498788833618, '- It is slightly discouraging that the temperature (a nuisance parameter) is used differently across experiments. It would be nice to give more details on whether you were succesful in learning the temperature, instead of annealing it; it would be interesting if that hyper-parameter could be eliminated.': 0.9219889640808105, 'The authors propose a method for reparameterization gradients with categorical distributions.': 1.0986120700836182, 'This is done by using the Gumbel-Softmax distribution, a smoothened version of the Gumbel-Max trick for sampling from a multinomial.': 1.0775376558303833, 'The paper is well-written and clear.': 1.0986123085021973, 'The application to the semi-supervised model in Kingma et al. (2014) makes sense for large classes, as well as its application to general stochastic computation graphs (Schulman et al., 2015).': 1.0986119508743286, 'One disconcerting point is that (from my understanding at least), this does not actually perform variational inference for discrete latent variable models.': 1.0986123085021973, 'Rather, it changes the probability model itself and performs approximate inference on the modified (continuous relaxed) version of the model.': 1.0986100435256958, ""This is fine in practice given that it's all approximate inference, but unlike previous variational inference advances either in more expressive approximations or faster computation (as noted by the different gradient estimators they compare to), the probability model is fundamentally changed."": 1.0986123085021973, 'Two critical points seem key: the sensitivity of the temperature, and whether this applies for non-one hot encodings of the categorical distribution (and thus sufficiently scale to high dimensions).': 1.0986123085021973, 'Comments by the authors on this are welcome.': 1.0986123085021973, 'There is a related work by Rolfe (2016) on discrete VAEs, who also consider a continuous relaxed approach.': 1.0986119508743286, 'This is worth citing and comparing to (or at least mentioning) in the paper.': 1.0706946849822998, 'References': 1.0986123085021973, 'Rolfe, J. T. (2016).': 1.0985807180404663, 'Discrete Variational Autoencoders. arXiv.org.': 1.0985506772994995, 'The paper combines Gumbel distribution with the popular softmax function to obtain a continuous distribution on the simplex that can approximate categorical samples.': 1.0986119508743286, 'It is not surprising that Gumbel softmax outperforms other single sample gradient estimators.': 1.0754034519195557, 'However, I am curious about how Gumbel compares with Dirichlet experimentally.': 1.0973589420318604, 'The computational efficiency of the estimator when training semi-supervised models is nice.': 1.0985783338546753, ""However, the advantage will be greater when the number of classes are huge, which doesn't seem to be the case in a simple dataset like MNIST."": 1.0986123085021973, 'I am wondering why the experiments are not done on a richer dataset.': 1.0986123085021973, 'The presentation of the paper is neat and clean.': 1.0986123085021973, 'The experiments settings are clearly explained and the analysis appears to be complete.': 1.0986123085021973, 'The only concern I have is the novelty of this work.': 1.0986123085021973, 'I consider this work as a nice but may be incremental (relatively small) contribution to our community.': 1.0986123085021973}"
440,https://openreview.net/forum?id=rkE8pVcle,"{'The paper introduces a simulator and a set of synthetic question answering tasks where interaction with the ""teacher"" via asking questions is desired.': 0.4077273905277252, 'The motivation is that an intelligent agent can improve its performance by asking questions and getting corresponding feedback from users.': 1.0983408689498901, 'The paper studies this problem in an offline supervised and an online reinforcement learning settings.': 0.8945416808128357, 'The results show that the models improve by asking questions.': 1.0227724313735962, 'The idea is novel, and is relatively unexplored in the research community.': 1.0510631799697876, 'The paper serves as a good first step in that direction.': 1.0594652891159058, 'The paper studies three different types of tasks where the agent can benefit from user feedback.': 1.0549944639205933, 'The paper is well written and provides a clear and detailed description of the tasks, models and experimental settings.': 1.0908262729644775, 'Other comments/questions:': 1.0975831747055054, 'What is the motivation behind using both vanilla-MemN2N AND Cont-MemN2N?': 1.029737114906311, ""Is using both resulting in any conclusions which are adding to the paper's contributions?"": 1.0986117124557495, 'In the Question Clarification setting, what is the distribution of misspelled words over question entity, answer entity, relation entity or none of these?': 1.0955512523651123, 'If most of the misspelled words come from relation entities, it might be a much easier problem than it seems.': 1.0904849767684937, 'The first point on Page 10 ""The performance of TestModelAQ is worse than TestAQ but better than TestQA.""': 1.0496686697006226, 'is not true for Task 2 from the numbers in Tables 2 and 4.': 1.0986121892929077, 'What happens if the conversational history is smaller or none?': 1.0986014604568481, 'Figure 5, Task 6, why does the accuracy for good student drop when it stops asking questions?': 0.6151084899902344, 'It already knows the relevant facts, so asking questions is not providing any additional information to the good student.': 1.0847352743148804, 'Figure 5, Task 2, the poor student is able to achieve almost 70% of the questions correct even without asking questions.': 1.0178970098495483, 'I would expect this number to be quite low.': 1.0985901355743408, 'Any explanation behind this?': 1.0986123085021973, 'Figure 1, Task 2 AQ, last sentence should have a negative response ""(-)"" instead of positive as currently shown.': 0.8497520685195923, 'Preliminary Evaluation:': 1.0986123085021973, 'A good first step in the research direction of learning dialogue agents from unstructured user interaction.': 1.0984406471252441, 'The goal of this paper is to analyze the behaviour of dialogue agents when they must answer factoid questions, but must query an oracle for additional information.': 1.0986123085021973, 'This can be interpreted as a form of interaction between the dialogue agent and a ‘teacher’.': 1.0971782207489014, 'The problem under investigation is indeed very important.': 1.0703989267349243, 'The authors create a synthetic environment in which to test their agent.': 1.0957038402557373, 'The main strength of the paper is that the paper tests many different combinations of environments, where either some knowledge is missing (and the agent has to query for it), or there is some misspelling in the teacher’s question, and different ways the agent can ask for extra information.': 1.0986123085021973, 'I am a bit concerned that many of the tasks are too easy (e.g. the AQ question paraphrase), and I am also concerned that the environment presented is very limited, and quite far (in terms of richness of linguistic structure) from how real humans would interact with chatbots.': 1.0986123085021973, 'I think the paper would be better positioned as testing the basic reasoning capabilities of agents/ their ability to do question answering, rather than dialogue.': 1.0986123085021973, 'However, I think the ‘ground-up’ approach that starts with simple environments is indeed worthy of analysis, and this paper makes an interesting contribution in that direction.': 1.0986123085021973, 'Of course, the paper would be much more convincing with human experiments.': 1.0355178117752075, 'Additional notes:': 1.0982056856155396, 'I think the simulation, in the synthetic environment, for the first mistake a learner can make during dialogue: “the learner has problems understanding the surface form of the text of the dialogue partner, e.g., the phrasing of a question”, is particularly limited since only word misspellings are considered (and the models used don’t work at the character level), which is of course only a tiny fraction of ways an agent can misunderstand the context.': 1.0956021547317505, 'I would be particularly interested to see some discussion of how the authors plan to scale this up to more realistic settings.': 1.0952274799346924, 'EDIT: I have updated my score to reflect the addition of the Mechanical Turk experiments': 1.098487138748169, ""This paper introduces a simulator and a set of synthetic tasks for evaluating a dialogue agent's ability to learn from user feedback."": 1.0958163738250732, 'For solving these tasks, the paper uses memory networks (Sukhbaatar et al., 2015) learned through previously proposed supervised learning and reinforcement learning methods.': 1.0986123085021973, 'In this setup, it is demonstrated that the agent learning from feedback (e.g. through question asking or question clarification) performs better.': 1.0841935873031616, 'The motivation for the paper is excellent; dialogue agents which learn directly from unstructured human feedback (as opposed to reward signals alone) could be very useful in real-world applications.': 1.0986123085021973, 'However, the paper falls short on the execution.': 1.0911208391189575, 'All the numerous experiments presented are based on the synthetic dialogue simulator, which is highly artificial and different from real-world dialogues.': 1.082410454750061, 'The simulator is based on a simple factoid question-answering framework, which normally is not considered dialogue and which appears to be solvable with a few hand-crafted rules.': 1.0986114740371704, ""The framework also assumes that the user's feedback is always correct and is given in one of a handful of forms (e.g. paraphrase of original question without typos) and that the agent can learn from examples of another agent asking questions or making clarifications, which simplifies the task even further."": 1.09861159324646, 'Because of the artificial setting and limited scope of the experiments, it seems difficult to draw conclusions about how to learn from unstructured user feedback.': 1.0985889434814453, 'To test the hypothesis that it is possible to learn from such user feedback, I would strongly recommend the authors to continue working on this project by carrying out experiments with real human users (even in the factoid question answering domain, if necessary).': 1.0986106395721436, 'This would provide much stronger evidence that a dialogue agent can learn from such feedback.': 0.2772590219974518, 'Other comments:': 1.0986123085021973, 'The abstract uses the phrase ""interactive dialogue agents"".': 1.0986123085021973, 'What is meant by ""interactive"" dialogue agents?': 1.0959837436676025, ""All dialogue agents interact with the user, so isn't it redundant to call them interactive?"": 0.6201878190040588, 'A major limitation of the experiments is that the questions the agent can ask are specified a priori.': 1.0621525049209595, 'If I understand correctly, in the supervised learning setting the agent is trained to imitate the questions of another rule-based agent.': 1.0985419750213623, 'While in the RL setting, the paper states ""For each dialogue, the bot takes two sequential actions : to ask or not to ask a question (denoted as a_1 ); and guessing the final answer (denoted as a_2)"".': 0.47943824529647827, 'This means the agent learns *when* to ask questions but not *what* questions to ask.': 1.073724389076233, 'Related to the previous comment, in the sub-section ""ONLINE REINFORCEMENT LEARNING (RL)"" the paper states ""We also explored scenarios where the student learns the ability to decide when to ask a question and what to ask."".': 0.22850242257118225, 'Please clarify this by removing the part ""what to ask"".': 1.064092755317688, 'The paper presents an overwhelming amount of results.': 1.0983465909957886, 'I understand the benefit of synthetic tasks is precisely the ability to measure many aspects of model performance, but in this case it confuses the reader to present so many results.': 1.0974819660186768, 'For example, what was the reason for including the ""TrainAQ(+FP)"" and ""TrainMix"" training settings?': 1.0986123085021973, 'How do these results help validate the original hypothesis?': 1.0986123085021973, ""If they don't, they should be taken out or moved to the appendix."": 0.6447371244430542, 'Since the contribution of the paper lies in the tasks and evaluation, it might be better to move either the vanilla-MemN2N (Table 2) to the appendix or to move the Cont-MemN2N results (Table 3) to the appendix.': 0.41580700874328613, 'UPDATE': 1.0986123085021973, 'Following the discussion below and the additional experiments provided by the authors, I have increased my score to 8.': 1.092492699623108}"
441,https://openreview.net/forum?id=rkEFLFqee,"{'This paper introduces an approach for future frame prediction in videos by decoupling motion and content to be encoded separately, and additionally using multi-scale residual connections.': 1.0986123085021973, 'Qualitative and quantitative results are shown on KTH, Weizmann, and UCF-101 datasets.': 1.0986123085021973, 'The idea of decoupling motion and content is interesting, and seems to work well for this task.': 1.0986123085021973, 'However, the novelty is relatively incremental given previous cited work on multi-stream networks, and it is not clear that this particular decoupling works well or is of broader interest beyond the specific task of future frame prediction.': 1.0986123085021973, 'While results on KTH and Weizmann are convincing and significantly outperform baselines, the results are less impressive on less constrained UCF-101 dataset.': 1.0986123085021973, 'The qualitative examples for UCF-101 are not convincing, as discussed in the pre-review question.': 1.0986123085021973, 'Overall this is a well-executed work with an interesting though not extremely novel idea.': 1.0986123085021973, 'Given the limited novelty of decoupling motion and content and impact beyond the specific application, the paper would be strengthened if this could be shown to be of broader interest e.g. for other video tasks.': 1.0986123085021973, 'The paper presents a method for predicting video sequences in the lines of Mathieu et al.': 0.8773117065429688, 'The contribution is the separation of the predictor into two different networks, picking up motion and content, respectively.': 0.9359086751937866, 'The paper is very interesting, but the novelty is low compared to the referenced work.': 1.0965031385421753, 'As also pointed out by AnonReviewer1, there is a similarity with two-stream networks (and also a whole body of work building on this seminal paper).': 1.0880804061889648, 'Separating motion and content has also been proposed for other applications, e.g. pose estimation.': 1.0986123085021973, 'Details :': 1.0986123085021973, 'The paper can be clearly understood if the basic frameworks (like GANs) are known, but the presentation is not general and good enough for a broad public.': 1.0985788106918335, 'Example : Losses (7) to (9) are well known from the Matthieu et al. paper.': 0.8156709671020508, 'However, to make the paper self-contained, they should be properly explained, and it should be mentioned that they are ""additional"" losses.': 1.0956640243530273, 'The main target is the GAN loss.': 1.0986123085021973, 'The adversarial part of the paper is not properly enough introduced.': 1.0227795839309692, 'I do agree, that adversarial training is now well enough known in the community, but it should still be properly introduced.': 1.0926001071929932, 'This also involves the explanation that L_Disc is the loss for a second network, the discriminator and explaining the role of both etc.': 1.0984810590744019, 'Equation (1) : c is not explained (are these motion vectors)?': 1.0986123085021973, ""c is also overloaded with the feature dimension c'."": 1.0986123085021973, 'The residual nature of the layer should be made more apparent in equation (3).': 1.097503423690796, 'There are several typos, absence of articles and prepositions (""of"" etc.).': 1.0935412645339966, 'The paper should be reread carefully.': 1.0986123085021973, '1) Summary': 1.0986123085021973, 'This paper investigates the usefulness of decoupling appearance and motion information for the problem of future frame prediction in natural videos.': 1.0986123085021973, 'The method introduces a novel two-stream encoder-decoder architecture, MCNet, consisting of two separate encoders': 0.8080949783325195, 'a convnet on single frames and a convnet+LSTM on sequences of temporal differences': 1.0986123085021973, 'followed by combination layers (stacking + convolutions) and a deconvolutional network decoder leveraging also residual connections from the two encoders.': 1.0986076593399048, 'The architecture is trained end-to-end using the objective and adversarial training strategy of Mathieu et al.': 1.0985456705093384, '2) Contributions': 1.0986123085021973, '+': 1.0986123085021973, 'The architecture seems novel and is well motivated.': 1.022947072982788, 'It is also somewhat related to the two-stream networks of Simonyan & Zisserman, which are very effective for real-world action recognition.': 1.0812917947769165, 'The qualitative results are numerous, insightful, and very convincing (including quantitatively) on KTH & Weizmann, showing the benefits of decoupling content and motion for simple scenes with periodic motions, as well as the need for residual connections.': 1.098331332206726, '3) Suggestions for improvement': 1.0986123085021973, 'Static dataset bias:': 1.0986123085021973, 'In response to the pre-review concerns about the observed static nature of the qualitative results, the authors added a simple baseline consisting in copying the pixels of the last observed frame.': 1.0986123085021973, 'On the one hand, the updated experiments on KTH confirm the good results of the method in these conditions.': 1.0986123085021973, ""On the other hand, the fact that this baseline is better than all other methods (not just the authors's) on UCF101 casts some doubts on whether reporting average statistics on UCF101 is insightful enough."": 1.0986123085021973, 'Although the authors provide some qualitative analysis pertaining to the quantity of motion, further quantitative analysis seems necessary to validate the performance of this and other methods on future frame prediction.': 1.0985963344573975, 'At least, the results on UCF101 should be disambiguated with respect to the type of scene, for instance by measuring the overall quantity of motion (e.g., l2 norm of time differences) and reporting PSNR and SSIM per quartile / decile.': 0.4071035385131836, 'Ideally, other realistic datasets than UCF101 should be considered in complement.': 1.098351240158081, 'For instance, the Hollywood 2 dataset of Marszalek et al would be a good candidate, as it focuses on movies and often contains complex actor, camera, and background motions that would make the ""pixel-copying"" baseline very poor.': 1.0610922574996948, 'Experiments on video datasets beyond actions, like the KITTI tracking benchmark, would also greatly improve the paper.': 1.0986123085021973, 'Additional recognition experiments:': 1.0986123085021973, 'As mentioned in pre-review questions, further UCF-101 experiments on action recognition tasks by fine-tuning would also greatly improve the paper.': 1.0985783338546753, 'Classifying videos indeed requires learning both appearance and motion features, and the two-stream encoder + combination layers of the MCNet+Res architecture seem particularly adapted, if they indeed allowed for unsupervised pre-trainining of content and motion representations, as postulated by the authors.': 0.48723137378692627, 'These experiments would also contribute to dispelling the aforementioned concerns about the static nature of the learned representations.': 1.0951625108718872, '4) Conclusion': 0.5441102981567383, 'Overall, this paper proposes an interesting architecture for an important problem, but requires additional experiments to substantiate the claims made by the authors.': 0.7399753332138062, 'If the authors make the aforementioned additional experiments and the results are convincing, then this paper would be clearly relevant for ICLR.': 0.8048758506774902, '5) Post-rebuttal final decision': 1.0780085325241089, 'The authors did a significant amount of additional work, following the suggestions made by the reviewers, and providing additional compelling experimental evidence.': 0.6628820300102234, 'This makes this one of the most experimentally thorough ones for this problem.': 0.759671151638031, 'I, therefore, increase my rating, and suggest to accept this paper.': 1.098581075668335, 'Good job!': 1.0986123085021973}"
442,https://openreview.net/forum?id=rkFBJv9gg,"{'This paper introduces MusicNet, a new dataset.': 1.0986123085021973, 'Application of ML techniques to music have been limited due to scarcity of exactly the kind of data that is provided here: meticulously annotated, carefully verified and organized, containing enough ""hours"" of music, and where genre has been well constrained in order to allow for sufficient homogeneity in the data to help ensure usefulness.': 1.0963537693023682, 'This is great for the community.': 1.0986123085021973, 'The description of the validation of the dataset is interesting, and indicates a careful process was followed.': 1.0986123085021973, 'The authors provide just enough basic experiments to show that this dataset is big enough that good low-level features (i.e. expected sinusoidal variations) can indeed be learned in an end-to-end context.': 1.0377174615859985, 'One might argue that in terms of learning representations, the work presented here contributes more in the dataset than in the experiments or techniques used.': 1.0682860612869263, 'However, given the challenges of acquiring good datasets, and given the essential role such datasets play for the community in moving research forward and providing baseline reference points, I feel that this contribution carries substantial weight in terms of expected future rewards.': 1.0985791683197021, '(If research groups were making great new datasets available on a regular basis, that would place this in a different context.': 1.0986123085021973, 'But so far, that is not the case.)': 1.0986123085021973, 'In otherwords, while the experiments/techniques are not necessarily in the top 50% of accepted papers (per the review criteria), I am guessing that the dataset is in the top 15% or better.': 1.0908031463623047, 'This paper describes the creation of a corpus of freely-licensed classical music recordings along with corresponding MIDI-scores aligned to the audio.': 1.0986123085021973, 'It also describes experiments in polyphonic transcription using various deep learning approaches, which show promising results.': 1.0986123085021973, 'The paper is a little disorganised and somewhat contradictory in parts.': 1.0986123085021973, 'For example, I find the first sentence in section 2 (MusicNet) would better be pushed one paragraph below so that the section be allowed to begin with a survey of the tools available to researchers in music.': 1.0986123085021973, 'Also, the description for Table 3 should probably appear somewhere in the Methods section.': 1.0986123085021973, 'Last example: the abstract/intro says the purpose is note prediction; later (4th paragraph of intro) there\'s a claim that the focus is ""learning low-level features of music....""': 1.0986123085021973, 'I find this slightly disorienting.': 1.0986123085021973, 'Although others (Uehara et al., 2016, for example) have discussed collection platforms and corpora, this work is interesting because of its size and the approach for generating features.': 1.0986123085021973, ""I'm interested in what the authors will to do expand the offerings in the corpus, both in terms of volume and diversity."": 1.0986123085021973, 'The paper introduces a new dataset called MusicNet (presumably analogous to ImageNet), featuring dense ground truth labels for 30+ hours of classical music, which is provided as raw audio.': 1.0981568098068237, 'Such a dataset is extremely valuable for music information retrieval (MIR) research and a dataset of this size has never before been publicly available.': 1.0986123085021973, 'It has the potential to dramatically increase the impact of modern machine learning techniques (e.g. deep learning) in this field, whose adoption has previously been hampered by a lack of available datasets that are large enough.': 1.059438705444336, 'The paper is clear and well-written.': 1.0986123085021973, 'The paper also features some ""example"" experiments using the dataset, which I am somewhat less excited about.': 1.0985901355743408, 'The authors decided to focus on one single task that is not particularly challenging: identifying pitches in isolated segments of audio.': 1.0986123085021973, 'Pitch information is a fairly low-level characteristic of music.': 1.0986123085021973, ""Considering that isolated fragments are used as input, this is a relatively simple problem that probably doesn't even require machine learning to solve adequately, e.g. peak picking on a spectral representation could already get you pretty far."": 1.0986123085021973, ""It's not clear what value the machine learning component in the proposed approach actually adds, if any."": 1.0986123085021973, ""I could be wrong about this as I haven't done the comparison myself, but I think the burden is on the authors to demonstrate that using ML here is actually useful."": 1.0986109972000122, 'I would argue that one of the strenghts of the dataset is the variety of label information it provides, so a much more convincing setup would have been to demonstrate many different prediction tasks for both low-level (e.g. pitch, onsets) and high-level (e.g. composer) characteristics, perhaps with fewer and simpler models': 1.0986123085021973, 'maybe even sticking to spectrogram input and forgoing raw audio input for the time being, as this comparison seems orthogonal to the introduction of the dataset itself.': 0.45104876160621643, 'As it stands, I feel that the fact that the experiments are relatively uninteresting detracts from the main point of the paper, which is to introduce a new public dataset that is truly unique in terms of its scale and scope.': 1.0986104011535645, 'That said, the experiments seem to have been conducted in a rigorous fashion and the evaluation and analysis of the resulting models is properly executed.': 1.0986123085021973, 'Re: Section 4.5, it is rather unsurprising to me that a pitch detector would learn filters that resemble pitches (i.e. sinusoids), although the observation that this requires a relatively large amount of data is interesting.': 1.0983242988586426, 'However, it would be more interesting to demonstrate that this is also the case for higher-level tasks.': 1.0986123085021973, 'The authors favourably compare the features learnt by their model with prior work on end-to-end learning from raw audio, but neglect that the tasks considered in this work were much more high-level.': 1.098602056503296, ""Some might also question whether ICLR is the appropriate venue to introduce a new dataset, but personally I think it's a great idea to submit it here, seeing as it will reach the right people."": 1.096956491470337, ""I suppose this is up to the organisers and the program committee, but I thought it important to mention this, because I don't think this paper merits acceptance based on its experimental results alone."": 1.0978748798370361}"
443,https://openreview.net/forum?id=rkFd2P5gl,"{'This paper describe an implementation of delayed synchronize SGD method for multi-GPU deep ne training.': 1.0986123085021973, 'Comments': 1.0986123085021973, '1) The described manual implementation of delayed synchronization and state protection is helpful.': 0.3172646760940552, 'However, such dependency been implemented by a dependency scheduler, without doing threading manually.': 1.0982329845428467, '2) The overlap of computation and communication is a known technique implemented in existing solutions such as TensorFlow(as described in Chen et.al) and MXNet.': 0.5114951729774475, 'The claimed contribution of this point is somewhat limited.': 0.4415101408958435, '3)': 1.070672631263733, 'The convergence accuracy is only reported for the beginning iterations and only on AlexNet.': 1.098611831665039, 'It would be more helpful to include convergence curve till the end for all compared networks.': 1.0986123085021973, 'In summary, this is paper implements a variant of delayed SyncSGD approach.': 1.0986123085021973, 'I find the novelty of the system somewhat limited (due to comment (2)).': 1.0986121892929077, 'The experiments should have been improved to demonstrate the advantage of proposed approach.': 1.0984630584716797, 'The authors present methods to speed-up gradient descent by leveraging asynchronicity in a layer-wise manner.': 1.0986123085021973, 'While they obtain up-to 1.7x speedup compared to synchronous training, their baseline is weak.': 1.0986123085021973, 'More importantly, they dismiss parameter-server based methods, which are becoming standard, and so effectively just do not compare to the current state-of-the-art.': 1.0986120700836182, 'They also do not present wall-time measurements.': 1.0986123085021973, 'With these flaws, the paper is not ready for ICLR acceptance.': 1.0986108779907227, 'This paper is relatively difficult to parse.': 1.0986073017120361, 'Much of the exposition of the proposed algorithm could be better presented using pseudo-code describing the compute flow, or a diagram describing exactly how the updates take place.': 1.0986123085021973, ""As it stands, I'm not sure I understand everything."": 1.0986123085021973, 'I would also have liked to see exactly described what the various labels in Fig 1 correspond to (""SGD task-wise, 1 comm""?': 1.0986123085021973, 'Did you mean layer-wise?).': 1.098610281944275, 'There are a couple of major issues with the evaluation: first, no comparison is reported against baseline async methods such as using a parameter server.': 1.0985978841781616, 'Second, using AlexNet as a benchmark is not informative at all.': 1.0981038808822632, 'AlexNet looks very different from any SOTA image recognition model, and in particular it has many fewer layers, which is especially relevant to the discussion in 6.3.': 1.0986123085021973, 'It also uses lots of fully-connected layers which affect the compute/communication ratios in ways that are not relevant to most interesting architectures today.': 1.0986123085021973}"
444,https://openreview.net/forum?id=rkGabzZgl,"{'summary': 1.0986123085021973, 'The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed and is accordingly marginalised.': 0.5852972269058228, 'Maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple Monte Carlo approximation of ML for this model.': 0.9066376686096191, 'The paper then introduces a theoretical framework for analysing the discrepancy (called inference gap) between the model at training (model ensemble, or here the latent variable model), and the model at testing (where usually what should be an expectation over the activations over many models becomes the activation of one model with averaged weights).': 1.0979015827178955, 'This framework introduces several notions (e.g. expectation linearity) which allow the study of which transition functions (and more generally layers) can have a small inference gap.': 1.0986123085021973, 'Theorem 3 gives a bound on the inference gap.': 0.8971319794654846, 'Finally a new regularisation term is introduced to account for minimisation of the inference gap during learning.': 0.4122827649116516, 'Experiments are performed on MNIST, CIFAR-10 and CIFAR-100 and show that the method has the potential to perform better than standard dropout and at the level of Monte Carlo Dropout (the standard method to compute the real dropout outputs consistently with the training assumption of an ensemble, of course quite expensive computationally)': 1.093765139579773, 'The study gives a very interesting theoretical model for dropout as a latent variable model where standard dropout is then a monte carlo approximation.': 0.6590633392333984, 'This is very probably widely applicable to further studies of dropout.': 0.48722440004348755, 'The framework for the study of the inference gap is interesting although maybe somewhat less widely applicable.': 1.0762687921524048, 'The proposed model is convincing although 1.': 1.0976288318634033, 'it is tested on simple datasets 2.': 1.0985944271087646, 'the gains are relatively small and 3. there is an increased computational cost during training because a new hyper-parameter is introduced.': 1.097489356994629, 'p6 line 8 typo: expecatation': 1.0986123085021973, 'This paper introduces dropout as a latent variable model (LVM).': 0.08309665322303772, 'Leveraging this formulation authors analyze the dropout “inference gap” which they define to be the gap between network output during training (where an instance of dropout is used for every training sample) and test (where expected dropout values are used to scale node outputs).': 0.7316954135894775, 'They introduce the notion of expectation linearity and use this to derive bounds on the inference gap under some (mild) assumptions.': 1.0984771251678467, 'Furthermore, they propose use of per-sample based inference gap as a regularizer, and present analysis of accuracy of models with expectation-linearization constraints as compared to those without.': 1.098545789718628, 'One relatively minor issue I see with the LVM view of dropout is that it seems applicable only to probabilistic models whereas dropout is more generally applicable to deep networks.': 0.5496494770050049, 'However I’d expect that the regularizer formulation of dropout would be effective even in non-probabilistic models.': 0.602568507194519, 'MC dropout on page 8 is not defined, please define.': 1.0986123085021973, 'On page 9 it is mentioned that with the proposed regularizer the standard dropout networks achieve better results than when Monte Carlo dropout is used.': 1.0922789573669434, 'This seems to be the case only on MNIST dataset and not on CIFAR?': 1.0986123085021973, 'From Tables 1 and 2 it also appears that MC dropout achieves best performance across tasks and methods but it is of course an expensive procedure.': 1.0916411876678467, 'Comments on the computational efficiency of various dropout procedures - to go with the accuracy results - would be quite valuable.': 1.0986123085021973, 'Couple of typos:': 1.0986123085021973, 'Pg.': 1.0986123085021973, '2 “ … x is he input …” -> “ … x is the input …”': 1.0986123085021973, '5 “ … as defined in (1), is …” -> ref.': 1.0986123085021973, 'to (1) is not right at two places in this paragraph': 1.0986123085021973, 'Overall it is a good paper, I think should be accepted and discussed at the conference.': 1.0986123085021973, 'This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization.': 1.0986123085021973, 'The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective.': 1.0986123085021973, 'They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities.': 1.0986123085021973, 'In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.': 1.0986123085021973, 'Their proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel.': 1.0986123085021973, 'small scale benchmark problems.': 1.0986123085021973, 'I therefore don’t belief that this approach will have a large impact on how practitioner train models.': 1.0986123085021973, 'But their general perspective is well aligned with the recently proposed idea of “Dropout as a bayesian approximation” and the insights and theorems in this paper might enable future work in that direction.': 1.0986123085021973}"
445,https://openreview.net/forum?id=rkKCdAdgx,"{'Description:': 1.0986123085021973, 'This paper aims at compressing binary inputs and outputs of neural network models with unsupervised ""Bloom embeddings"".': 1.0985888242721558, 'The embedding is based on Bloom filters: projecting an element of a set to different positions of a binary array by several independent hash functions, which allows membership checking with no missed but with possibly some false positives.': 1.0986114740371704, 'Inputs and outputs are assumed to be sparse.': 0.7773862481117249, 'The nonzero elements of an input are then simply encoded by Bloom filters onto the same binary array.': 1.097158670425415, 'The neural network is run with the embedded inputs.': 1.098611831665039, 'Desired outputs are assumed to be a softmax-type ranking of different alternatives.': 1.0986123085021973, 'a sort of back-projection step is needed to recover a probability ranking of the desired ground truth alternatives from the lower-dimensional output.': 1.0359231233596802, 'For each ground-truth class, this is simply approximated as a product of the output values at the Bloom-filtered hash positions of that class.': 1.0961735248565674, 'The paper simply applies this idea, testing it on seven data sets.': 0.6479632258415222, 'Scores and training times are compared to the baseline networks without embeddings.': 1.0986123085021973, 'Comparison embedding methods are mostly very traditional (hashing trick, error-correcting output codes, linear projection by canonical correlation analysis) but include one recent pairwise mutual information based approach.': 1.0986123085021973, 'Evaluation:': 1.0986123085021973, 'It is hard to see a lot of novelty in this paper.': 1.0942949056625366, 'The Bloom filters are an existing technique, which is applied very straightforwardly here.': 1.0921231508255005, 'The back-projection step of equation 2 is also a straightforward continuous-valued variant of the Bloom-filter membership test.': 1.0985780954360962, 'The way of recovering outputs is heuristic, since the neural network inbetween the embedded inputs and outputs is not really aware that the outputs will be run through a further back-projection step.': 1.0985863208770752, 'In the comparisons of Table 3, only two embedding dimensionalities are used for each data set.': 1.0728009939193726, 'This is insufficient, since it leaves open the question whether other methods could get improved performance for higher/lower embeddings, relative to the proposed method.': 1.0938968658447266, '(In appendix B, Figure 4, authors do compare their method to a variant of it for many different embedding dimensionalities; why not do this for all comparison methods too?)': 1.0110359191894531, 'Overall, this seems for the most part too close to off-the-shelf existing embedding to be acceptable.': 1.0969868898391724, 'Minor points:': 0.48518773913383484, 'As the paper notes, dimensionality reduction of inputs by various techniques is common.': 1.0967252254486084, 'The paper lists some simple embeddings such as SVD based ones, CCA etc. but a more thorough review of other approaches including the vast array of nonlinear dimensionality reduction solutions should be mentioned.': 1.097968339920044, 'The experiments in the paper seem to have an underlying assumption that inputs and outputs need to have the same type of embedding dimension.': 0.5225597620010376, 'This seems unnecessary.': 1.0054681301116943, 'The paper presents the idea of using Bloom filter encodings on the input features and the output layer of deep network to reduce the size of the network.': 1.0986123085021973, 'Bloom-filter like encodings were proposed in Shi et al.': 1.0986123085021973, '(JMLR 2009)': 1.0986123085021973, '""Hash Kernels for Structured Data"" (see Theorem 2 and its proof), whereas it was proposed to encode the outputs in the context of multilabel classification in Cisse et al. (NIPS 2013)': 1.0986123085021973, '""Robust Bloom Filters for Large MultiLabel Classification Tasks"".': 1.0986123085021973, 'The paper still joins both ideas together, applies it to ranking problems, and presents extensive experiments in the context of deep neural networks and language modelling.': 1.0986123085021973, 'The main motivation of the paper is the reduction of model size for deep neural networks.': 1.0961179733276367, 'There is a whole body of literature on this topic,  that is not mentioned at all in the paper, where the baselines are based on weight quantization (with an available implementation in TensorFlow https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantization).': 1.0986123085021973, 'More recent methods are e.g.,': 0.428512841463089, '1) the model compression approach of https://arxiv.org/abs/1510.00149': 0.7154062986373901, '2) training with integer/binary weights https://arxiv.org/abs/1511.00363': 0.36327531933784485, 'Overall, the advantage of the Bloom filter approach is its simplicity and its wide applicability': 0.40511035919189453, 'it could, as well, be used in conjunction with weight quantization and other compression methods.': 0.8394032120704651, 'The main merits of the paper is to present extensive experiments on how well the vanilla Bloom filter approach can perform, but overall the novelty is fairly limited.': 1.0986045598983765, 'The authors propose a simple scheme based on Bloom filters to generate embeddings for inputs and outputs.': 1.0986123085021973, ""This reduces memory while introducing limited computational overhead, and it is simple enough to implement that it can easily be added to any practitioner's toolbox."": 1.0986123085021973, 'Pros:': 1.0986123085021973, 'Can be applied to practically any model, either at the input or hte output.': 1.0986123085021973, 'Method is very straightforward: apply multiple hashes, instead of single one.': 1.0986123085021973, 'The algorithm for decoding is nice too.': 1.0986123085021973, 'Cons:': 1.0986123085021973, ""The paper is a bit difficult to read; the idea is really simple so it doesn't seem like it warrants such a complex description."": 1.0986123085021973, 'The novelty of the approach is a bit limited since, as other reviewers have mentioned, Bloom filters are in use in a lot of areas including multi-label classification.': 1.0986123085021973, 'This seems like it would be a good short paper.': 1.0986123085021973, ""There's a lot of well fleshed out experiments, but the core of the paper is a bit incremental."": 1.0986123085021973}"
446,https://openreview.net/forum?id=rkYmiD9lg,"{'The paper presents an application of a tensor factorization to linear models, which allows to consider higher-order interactions between variables in classification (and regression) problems, and that maintains computational feasibility, being linear in the dimension.': 1.3862943649291992, 'The factorization employed is based on the TT format, first proposed by Oseledests (2011).': 1.3862941265106201, 'The authors also propose the adoption of a Riemannian optimization scheme to explicit consider the geometry of the tensor manifold, and thus speed up convergence.': 1.3862502574920654, 'The paper in general is well written, it presents an interesting application of the TT tensor format for linear models (together with an application of Riemannian optimization), which in my opinion is quite interesting since it has a wide range of possible applications in different algorithms in machine learning.': 1.3862943649291992, 'On the other side, I have some concerns are about the experimental part, which I consider not at the level of the rest of the paper, for instance in terms of number of experiments on real datasets, role of dropout in real datasets, comparison with other algorithms on real datasets.': 1.3862943649291992, 'Moreover the authors do not take into account explicitly the problem of the choice of the rank to be used in the experiments.': 1.3862417936325073, 'In general the experimental section seems a collection of preliminary experiments where different aspects have been tested by not in a organic way.': 1.3862943649291992, ""I think the paper is close to a weak acceptance / weak rejection, I don't rate it as a full acceptance paper, mainly due to the non-satisfactory experiment setting."": 1.3862943649291992, 'In case of extra experiments confirming the goodness of the approach, I believe the paper could have much better scores.': 1.3859137296676636, 'Some minor comments:': 1.2148500680923462, 'formula 2: Obvious comment: learning the parameters of the model in (1) can be done as in (2), but also in other ways, depending on the approach you are using.': 1.300946593284607, 'the fact that the rank is bounded by 2r, before formula 9, is explained in Lubich et al., 2015?': 1.3862943649291992, 'after formula 10: why the N projections in total they cost O(dr^2(r+N)), it should be O(Ndr^2(r+1)), no?': 1.3862943649291992, 'since each of the elements of the summation has rank 1, and the cost for each of them is O(dr^2(r+TT_rank(Z)^2)), where TT-rank(Z)=1.': 1.3862943649291992, 'Am I wrong?': 1.3862943649291992, 'section 6.2: can you explain why the random initialization freezes the convergence?': 1.3862943649291992, 'This seems interesting but not motivated.': 1.3862943649291992, 'Any guess?': 1.3862943649291992, 'section 6.3: you adopt dropout: can you comment in particular on the advantages it gives in the context of the exponential machines?': 1.3862943649291992, 'did you use it on real datasets?': 1.3862943649291992, 'how do you choose r_0 in you experiments?': 1.3862943649291992, 'with a validation set?': 1.3862943649291992, ""in section 7: why you don't have x_1 x_2 among the variables?"": 1.3862943649291992, 'section 8: there is a typo in ""experiments""': 1.3862943649291992, 'section 8.1: ""We simplicity, we binarized"" I think there\'s a problem with the English language in this sentence': 1.3862943649291992, 'section 8.3: ""we report that dropout helps"".. this is quite general statement, only tested on a synthetic dataset': 0.8452761769294739, 'section 8.5: can you provide more results for this dataset, for instance in terms of training and inference time? or test wrt other algorithms?': 1.2329157590866089, 'This paper introduces a polynomial linear model for supervised classification tasks.': 1.3857425451278687, 'The model is based on a combination of the Tensor Train (TT) tensor decomposition method and a form of stochastic Riemannian  optimization.': 1.3862943649291992, 'A few empirical experiments are performed that demonstrate the good performance of the proposed model relative to appropriate baselines.': 1.3862943649291992, 'From a theoretical standpoint, I think the approach is interesting and elegant.': 1.386292576789856, 'The main machinery underlying this work are the TT decomposition and the geometric structure of the manifold of tensors with fixed TT-rank, which have been established in prior work.': 1.3862942457199097, 'The novelty of this paper is in the combination of this machinery to form an efficient polynomial linear model.': 1.3760793209075928, 'As such, I would have hoped that the paper mainly focused on the efficacy of this combination and how it is superior to obvious alternatives.': 1.386291265487671, 'For example, I would have really appreciated seeing how FMs performed when optimized over the manifold of positive definite matrices, as another reviewer mentioned.': 1.3862943649291992, 'Instead, there is a bit too much effort devoted to explaining prior work.': 1.3862942457199097, 'I think the empirical analysis could be substantially improved.': 1.3862943649291992, 'I am particularly puzzled by the significant performance boost obtained from initializing with the ordinary logistic regression solution.': 1.3862943649291992, 'I would have liked some further analysis of this effect, especially whether or not it is possible to obtain a similar performance boost with other models.': 1.3862942457199097, 'Regarding the synthetic data, I think an important baseline would be against a vanilla feed forward neural network, which would help readers understand how complicated the interactions are and how difficult the dataset is to model.': 1.386292815208435, 'I agree with the previous reviewer regarding a variety of other possible improvements to the experimental section.': 1.3861591815948486, ""A few typos: 'Bernoulli distrbution', 'reproduce the experiemnts', 'generilize better'."": 1.3861991167068481, 'Overall, I am on the fence regarding this paper.': 1.378973126411438, 'The main idea is quite good, but insufficient attention was devoted to analyzing the aspects of the model that make it interesting and novel.': 1.3862943649291992, 'This paper proposes to use the tensor train (TT) decomposition to represent the full polynomial linear model.': 1.3842333555221558, 'The TT form can reduce the computation complexity in both of inference and model training.': 1.3862943649291992, 'A stochastic gradient over a Riemann Manifold has been proposed to solve the TT based formulation.': 1.3862943649291992, 'The empirical experiments validate the proposed method.': 1.3862943649291992, 'The proposed approach is very interesting and novel for me.': 1.3862943649291992, 'I would like to vote acceptance on this paper.': 1.3862943649291992, 'My only suggestion is to include the computational complexity per iteration.': 1.3862943649291992, 'The paper describes how to use a tensor factorization method called Tensor Train for modeling the interactions between features for supervised classification tasks.': 1.3862943649291992, 'Tensor Train approximates tensors of any dimensions using low rank products of matrices.': 1.3862943649291992, 'The rank is used as a parameter for controlling the complexity of the approximation.': 1.3862943649291992, 'Experiments are performed on different datasets for binary classification problems.': 1.3862943649291992, 'The core of the paper consists in demonstrating how the TT formalism developed by one of the authors could be adapted for modeling interactions between features.': 1.3862943649291992, 'Another contribution is a gradient algorithm that exploits the geometrical structure of the factorization.': 1.3862943649291992, 'These ideas are probably new in Machine learning.': 1.3862943649291992, 'The algorithm itself is of reasonable complexity for the inference and could probably be adapted to large size problems although this is not the case for the experiments here.': 1.3862943649291992, 'The experimental section is not well structured, it is incomplete and could be improved.': 1.3862943649291992, 'We miss a description of the datasets characteristics.': 1.3862943649291992, 'The performance on the different datasets are not provided.': 1.3862943649291992, 'Each dataset has been used for illustrating one aspect of the model, but you could also provide classification performance and a comparison with baselines for all the experiments.': 1.3862943649291992, 'The experiments on the 2 UCI datasets show optimization performance on the training set, you could provide the same curves on test sets to show how the algorithm generalizes.': 1.3862943649291992, 'The comparison to other approaches (section 8.4) is only performed on artificial data, which are designed with interacting features and are not representative of diverse situations.': 1.3862943649291992, 'The same holds for the role of Dropout.': 1.3862943649291992, 'The comparison on the Movielens dataset is incomplete.': 1.3862943649291992, 'Besides, all the tests are performed on small size problems.': 1.3862943649291992, 'Overall, there are original contributions which could be worth a publication.': 1.3862943649291992, 'The experiments are incomplete and not conclusive.': 1.3862943649291992, 'A more detailed comparison with competing methods, like Factorization Machines, could also improve the paper.': 1.3862943649291992}"
447,https://openreview.net/forum?id=rkaRFYcgl,"{'The author proposes the use of low-rank matrix in feedfoward and RNNs.': 1.0579224824905396, 'In particular, they try their approach in a GRU and a feedforward highway network.': 1.0986117124557495, 'Author also presents as a contribution the passthrough framework, which can describe feedforward and recurrent networks.': 0.547837495803833, 'However, this framework seems hardly novel, relatively to the formalism introduced by LSTM or highway networks.': 1.0975685119628906, 'An empirical evaluation is performed on different datasets (MNIST, memory/addition tasks, sequential permuted MNIST and character level penntreebank).': 1.0986123085021973, 'However, there are few problems with the evaluation:': 0.961798369884491, 'In the highway network experiment, the author does not compare with a baseline.': 1.0688302516937256, 'We can not assess what it the impact of the low-rank parameterization.': 1.098610758781433, 'Also, it would be interesting to compare the result with a highway network that have this capacity bottleneck across layer  (first layer of size , second layer of size , third layer of size ) and not in the gate functions.': 1.0980405807495117, 'Also, how did you select the hyperparameter values?.': 1.0245896577835083, 'It is unfortunate that the character level penntreebank does not use the same experimental setting than previous works as it prevents from direct comparison.': 1.0986026525497437, 'Also the overall bpc perplexity seems relatively high for this dataset.': 1.0182116031646729, 'It is therefore not clear how low-rank decomposition would perform on this task applied on a stronger baseline.': 1.0976426601409912, 'Author claims state-of-art in the memory task.': 1.0794503688812256, 'However, their approach uses  more parameters than the uRNN (41K against 6.5K for the memory) which makes the comparison a little bit unfair toward uRNN.': 1.0734373331069946, 'It would be informative to see how low-rank RNN performs using overall 6.5K parameters.': 1.097930669784546, 'Generally, it would be good to see what is the impact of the matrix rank given a fix state size.': 0.662162721157074, 'It would be informative as well to have the baseline and the uRNN curve in Figure 2 for the memory/addition task.': 1.0971250534057617, 'it is not clear when to use low-rank or low-rank + diagonal from the experiments.': 1.066085696220398, 'Overall, the evaluation in its current form in not really convincing, except for the sequential MNIST dataset.': 0.5436580777168274, 'The paper proposes a low-rank version of pass-through networks to better control capacity, which can be useful in some cases, as shown in the experiments.': 1.0986123085021973, 'That said, I found the results not very convincing overall.': 1.0986104011535645, 'Results are overall not as good as state-of-the-art on sequential MNIST or the memory task, but add one more hyper-parameter to tune.': 1.0966497659683228, 'As I said, it would help to show in Tables and/or Figures competing approaches like uRNNs.': 1.0972987413406372, 'The authors study the use of low-rank approximation to the matrix-multiply in RNNs.': 1.0986123085021973, 'This reduces the number of parameters by a large factor, and with a diagonal addition (called low-rank plus diagonal)': 1.097434401512146, 'it is shown to work as well as a fully-parametrized network on a number of tasks.': 1.0807733535766602, 'The paper is solid, the only weakness being some claims about conceptual unification (e.g., the first line of the conclusion': 1.0986123085021973, '""We presented a framework that unifies the description various types of recurrent and feed-forward': 1.0986123085021973, 'neural networks as passthrough neural networks.""': 1.0986123085021973, 'claiming this framework as a contribution of this paper is untrue, the general framework is well known in the community and RNNs have been presented in this way before.)': 1.0986123085021973, 'Aside from the above small point, the true contribution is in making low-rank RNNs work, the results are generally as good as fully-parametrized networks.': 1.0986123085021973, 'They are hardly better though, which makes it unclear why low-rank networks should be used.': 1.0986123085021973, 'The contribution is thus not very strong in terms of results, but even achieving the same results with fewer parameters is not easy and the studies were well-executed and explained.': 1.0986123085021973}"
448,https://openreview.net/forum?id=rkjZ2Pcxe,"{'The authors consider a simple optimization technique consisting of adding gradient noise with a specific schedule.': 1.0986123085021973, 'They test their method on a number of recently proposed neural networks for simulating computer logic (end-to-end memory network, neural programmer, neural random access machines).': 1.0986117124557495, 'On these networks, the question of optimization has so far not been studied as extensively as for more standard networks.': 1.0986093282699585, 'A study specific to this class of models is therefore welcome.': 0.6524815559387207, 'Results consistently show better optimization properties from adding noise in the training procedure.': 1.0986088514328003, 'One issue with the paper is that it is not clear whether the proposed optimization strategy permits to learn actually good models, or simply better than those that do not use noise.': 1.0986123085021973, 'A comparison to results obtained in the literature would be desirable.': 1.0986123085021973, 'For example, in the MNIST experiments of Section 4.1, the optimization procedure reaches in the most favorable scenario an average accuracy level of approximately 92%, which is still far from having actually learned an interesting problem representation (a linear model would probably reach similar accuracy).': 1.0986123085021973, 'I understand that the architecture is specially designed to be difficult to optimize (20 layers of 50 HUs), but it would have been more interesting to consider a scenario where depth is actually beneficial for solving the problem.': 1.098559021949768, 'This paper presents a simple method of adding gradient noise to improve the training of deep neural networks.': 1.0986016988754272, 'This paper first appeared on arXiv over a year ago and while there have been many innovations in the area of improving the training of deep neural networks in tha time (batch normalization for RNNs, layer normalization, normalization propagation, etc.)': 1.0985864400863647, 'this paper does not mention or compare to these methods.': 1.098611831665039, 'In particular, the authors state ""However, recent work on applying batch normalization to recurrent networks (Laurent et al., 2015) has not shown promise in improving generalization ability for recur- rent architectures, which are the focus of this work.""': 1.0986123085021973, 'This statement is simply incorrect and was thoroughly explored in, e.g. Cooijmans et al. (2016) that establish that batch normalization is effective for RNNs.': 1.0986123085021973, 'The proposed method itself is extremely simple and is similar to numerous training strategies that have previously been advocated in the literature.': 1.0986123085021973, 'As a result the contribution would be incremental at best and could be significant with sufficiently strong empirical results supporting this particular variant.': 1.0986123085021973, 'However, as discussed above there are now multiple training strategies and algorithms in the literature that are not empirically compared.': 1.0986123085021973, 'Unfortunately, this paper is now fairly seriously out of date.': 1.0986123085021973, 'It would not be appropriate to publish this at ICLR 2017.': 1.0986123085021973, 'The authors propose to add noise to the gradients computed while optimizing deep neural networks with stochastic gradient based methods.': 1.0986123085021973, 'They show results multiple data sets which indicate that the method can counteract bad parameter initialization and that it can be especially beneficial for training more complicated architectures.': 0.5594156384468079, 'The method is tested on a multitude of different tasks and architectures.': 1.0986123085021973, 'The results would be more convincing if they would be accompanied by confidence intervals but I understand that some of the experiments must have taken very long to run.': 0.7225555777549744, 'I like that the results include both situations in which the gradient noise helps a lot and situations in which it doesn’t seem to add much to the other optimization or initialization tools employed.': 1.0978798866271973, 'The quantity of the experiments and the variety of the models provide quite convincing evidence that the effect of the gradient noise generalizes to many settings.': 0.4738616645336151, 'The results were not always that convincing.': 1.0986123085021973, 'In Section 4.2, the method only helped significantly when a sub-optimal training scheme was used, for example.': 1.0986123085021973, 'The results on MNIST are not very good compared to the state-of-the-art.': 1.0986123085021973, 'Since the method is so simple, I was hoping to see more theoretical arguments for its usefulness.': 1.0986123085021973, 'That said, the experimental investigations into the importance of the annealing procedure, the comparison with the effect of gradient stochasticity and the comparison with weight noise, provide some additional insight.': 1.0361124277114868, 'The paper is well written and cites relevant prior work.': 0.7513734102249146, 'The proposed method is described clearly and concisely, which is to be expected given its simplicity.': 0.40624505281448364, 'The proposed idea is not very original.': 1.094419240951538, 'As the authors acknowledge, very similar algorithms have been used for training and it is pretty much identical to simulating Langevin dynamics but with the goal of finding a single optimum in mind rather than approximating an expected value.': 1.0986123085021973, 'The work is the evaluation of an old tool in a new era where models have become bigger and more complex.': 1.098610281944275, 'Despite the lack of novelty of the method, I do think that the results are valuable.': 0.4740566909313202, 'The method is so easy to implement and seems to be so useful for complicated model which are hard to initialize, that it is important for others in the field to know about it.': 1.0939319133758545, 'I suspect many people will at least try the method.': 0.5058151483535767, 'The variety of the architectures and tasks for which the method was useful suggests that many people may also add it to their repertoire of optimization tricks.': 1.0985503196716309, 'Pros:': 1.0986123085021973, '*': 1.0986123085021973, 'The idea is easy to implement.': 1.0986123085021973, 'The method is evaluated on a variety of tasks and for very different models.': 1.0124826431274414, 'Some interesting experiments which compare the method with similar approaches and investigate the importance of the annealing scheme.': 1.0986120700836182, 'The paper is well-written.': 0.7191535830497742, 'Cons:': 1.0986123085021973, 'The idea is not very original.': 1.0984957218170166, '* There is no clear theoretical motivation of analysis.': 1.0986109972000122, '* Not all the results are convincing.': 1.0986123085021973}"
449,https://openreview.net/forum?id=rkmDI85ge,"{'The authors introduce an adaptive softmax approximation tailored for faster performance on GPUs.': 1.0986065864562988, 'The key idea, which is very sensible, is to use a class-based hierarchical softmax, but where the clusters/hierarchy are distributed such that the resulting matrix multiplications are optimally-sized for GPU computation, based on their empirical tests.': 1.0986123085021973, 'Their results indicate that the system does indeed work very well.': 1.0986123085021973, 'In terms of presentation, I found the paper to have both clear and unclear elements.': 1.0986120700836182, 'Fortunately, the underlying concepts and logic seem quite clear.': 1.0986123085021973, 'Unfortunately, at various points, the writing is not.': 1.0986123085021973, ""There are various minor typos (as mentioned by AnonReviewer2, in addition to some other spots, e.g. the notation describing recurrent network in Section 3 mentions an x_t which is surely different from the x_t used in the previous paragraph on regular feedforward NN's, i think it belonged in the equation for h_t; the use of the two matrices A and P in Eq2 is strange, etc)."": 1.0986123085021973, 'Also, while Section 4.2 (Intuition for 2-cluster case) was a good idea to include and helpful, and while the *concepts* underlying the complexity analysis were straightforward, it could be made a lot clearer by (a) adding an additional figure such as Figure 2, along with (b) a few well-placed additional sentences unpacking the logic of the argument into easier-to-follow steps.': 1.0986123085021973, 'For example, it was only when I saw Eq (6) and (7) combined with Fig(2) that the analysis on the previous page made more sense in terms of arriving at the eq for the complexity of putting the head of the distribution in the root of the tree.': 1.0986049175262451, '(Perhaps an Appendix might be the most appropriate place to add such an explanation).': 1.0527219772338867, 'SYNOPSIS:': 1.0986050367355347, 'The authors introduce an efficient approximation to the softmax function that speeds up the empirical calculation of the softmax on GPUs.': 1.0986121892929077, 'They leverage the unbalanced distribution of words and specific empirical timings of matrix multiplies on GPUs to devise an algorithm that selects an optimal placement of the vocabulary into clusters.': 1.0986123085021973, 'They show empirical results that show speedups over alternative methods, while not losing much accuracy compared to the full softmax.': 1.0986123085021973, 'THOUGHTS:': 1.0986123085021973, ""Since the goal of this work is to speed up training, I'm curious why you compare only to the flat 2-level HSM (O(sqrt(V)) speedup at best), and not the deeper binary-tree HSM (O(lgV) speedup at best)?"": 1.0986123085021973, 'Overall, the paper is clear, easy to understand, and well written, bar a few notation issues as pointed out by other reviewers.': 1.0986123085021973, 'It adds an interesting extra tool in the language modeling toolbox.': 1.0986123085021973, 'The idea is based on several previous works that aim to optimize vocabulary clustering to improve the speed-accuracy tradeoff often experienced in practice with hierarchical methods.': 1.0986123085021973, ""The interesting result here seems to be that this particular clustering objective improves speed (what it was designed for), while apparently not losing much i.t.o. accuracy (what it wasn't designed for)."": 1.0986123085021973, 'Although the authors do not speculate  reasons for the latter part at all, I suspect it is largely related to the fact that the flat region on the timing graph (Fig 1) means that the head group V_h can actually include a sizeable portion of the most frequent words in the vocabulary at constant cost.': 1.0986123085021973, 'This reduces the approximation error (regions of no support in P_approx(next | previous) compared to P_real ), which in turn mitigates the hit in perplexity compared to the full softmax.': 1.0986123085021973, 'However, since the method is intimately related to the speed-optimal method proposed by Zweig et al. (2013) (albeit without the explicit tailoring towards GPU), I feel that a direct comparison is warranted (I understand this is underway).': 1.0986123085021973, 'If the performance and accuracy improvements still hold, I will update my rating to a 7.': 1.0986123085021973, 'he authors provide an interesting, computational-complexity-driven approach for efficient softmax computation for language modeling based on GPUs.': 1.0986123085021973, 'An adaptive softmax approach is proposed based on a hierarchical model.': 1.0986123085021973, 'Dynamic programming is applied to optimize the structure of the hierarchical approach chosen here w.r.t.': 1.0986123085021973, 'computational complexity based on GPUs.': 1.0986123085021973, 'However, it remains unclear, how robust the specific configuration obtained from dynamic programming is w.r.t.': 1.0986123085021973, 'performance/perplexity.': 1.0986123085021973, 'Corresponding comparative results with perplexity-based clustering would be desirable.': 1.0986123085021973, 'Especially, in Sec. 5, Paragraph Baselines, and Table 1, respectively, it would be interesting to see a result on HSM(PPL)': 1.0986123085021973, '(cf.': 1.0986123085021973, 'Zweig et al. 2013).': 1.0986123085021973, 'AFAIK, the first successful application of an LSTM-based language model for large vocabulary was published by Sundermeyer et al. 2012 (see below), which is missing in the sumary of prior work on the bottom of p. 3.': 1.0986123085021973, 'Mainly, the paper is well written and accessible, though notation in some cases should be improved, see detailed comments below.': 1.0986123085021973, 'Prior work on LSTM language modeling:': 1.0986123085021973, '- Sundermeyer et al.: LSTM Neural Networks for Language Modeling, Interspeech, pp. 194-197, 2012.': 1.0986123085021973, 'Notation:': 1.0986123085021973, '- use of g(k) vs. g(k,B,d): g(k) should be clearly defined (constant B and d?)': 1.0986123085021973, '- notation should not be reused (B is matrix in Eq. (3), and batch size in Sec. 4.1).': 1.0986123085021973, '- notation p_{i+j} (Eq. (10) and before) is kind of misleading, as p_{i+j} is not the same as p_{(i+j)}': 1.0986123085021973, 'Minor comments:': 1.0986123085021973, '- p. 1, item list at bottom, first item: take -> takes': 1.0986123085021973, '- p. 5, second paragraph: will then contained -> will then contain': 1.0986123085021973, '- p. 5, third paragaph: to associated -> to associate': 1.0986123085021973, '- Sec. 4.3, first paragraph: At the time being -> For the time being': 1.0986123085021973, '- below Eq. (9): most-right -> right-most': 1.0986123085021973, '- below Eq. (10): the second term of this equation -> the second term of the right-hand side of this equation': 1.0986123085021973, '- p. 6, second to last line: smaller that the -> smaller than the': 1.0986123085021973, '- p. 7, Sec. 5, itemize, first item: 100 millions -> 100 million': 1.0986123085021973, '- p. 8, last sentence: we are the -> ours is the': 1.0986123085021973}"
450,https://openreview.net/forum?id=rkpACe1lx,"{'A well known limitation in deep neural networks is that the same parameters are typically used for all examples, even though different examples have very different characteristics.': 1.3862943649291992, 'For example, recognizing animals will likely require different features than categorizing flowers.': 1.3862709999084473, 'Using different parameters for different types of examples has the potential to greatly reduce underfitting.': 1.3862943649291992, 'This can be seen in recent results with generative models, where image quality is much better for less diverse datasets.': 1.3862943649291992, 'However, it is difficult to use different parameters for different examples because we typically train using minibatches, which relies on using the same parameters for all examples in a minibatch (i.e. doing matrix multiplies in a fully-connected network).': 1.3862943649291992, 'The hypernetworks paper cleverly proposes to get around this problem by adapting different ""parameters"" for different time steps in recurrent networks and different.': 1.3862943649291992, 'The basic insight is that a minibatch will always include many different examples from the same time step or spatial position, so there is no computational issue involved with using different ""parameters"".': 1.3862943649291992, 'In this paper, the ""parameters"" are modified for different positions based on the output from a hypernetwork which conditions on the time step.': 1.3862919807434082, 'Hypothetically, this hypernetwork could also condition on other features that are shared by all sequences in the minibatch.': 1.3862940073013306, 'I expect this method to become standard for training RNNs, especially where the length of the sequences is the same during the training and testing phases.': 1.3862417936325073, 'Penn Treebank is a highly competitive baseline, so the SOTA result reported here is impressive.': 1.3610191345214844, 'The experiments on convolutional networks are less experimentally impressive.': 1.0964689254760742, 'I suspect that the authors were aiming to achieve state of the art results here but settled with achieving a reduction in the number of parameters.': 1.3860820531845093, 'It might even be worthwhile to consider a synthetic experiment where two completely different types of image are appended (i.e. birds on the left and flowers on the right) and show that the hypernetwork helps in this situation.': 1.3862943649291992, 'It may be the case that for convnets, the cases where hypernetworks help are very specific.': 0.543135404586792, 'For RNNs, it seems to be the case that explicitly changing the nature of the computation depending on the position in the sequence greatly improves generalization.': 1.3803249597549438, 'While a usual RNN could learn to store a counter (indicating the position in the sequence), the hypernetwork could be a more efficient way to add capacity.': 1.3862911462783813, 'Applications to time series forecasting and modeling could be an interesting area for future work.': 0.7929840683937073, 'Although the trainable parameters might be reduced significantly, unfortunately the training and recognition speech cannot be reduced in this way.': 1.3862000703811646, 'Unfortunately, as the results show, the authors could not get better results with less parameters.': 0.7927216291427612, 'However, the proposed structure with even more number of parameters shows significant gain e.g. in LM.': 1.32050621509552, 'The paper should be reorganized, and shortened.': 1.0716450214385986, 'It is sometimes difficult to follow and sometimes inconsistent.': 1.3854632377624512, 'E.g.: the weights of the feedforward network depend only on an embedding vector (see also my previous comments on linear bottlenecks), whereas in recurrent network the generated weights also depend on the input observation or its hidden representation.': 1.379728078842163, 'Could the authors provide the num. of trainable parameters for Table 6?': 1.386209487915039, 'Probably presenting less results could also improve the readability.': 1.370389699935913, 'Only marginal accept due to the writing style.': 1.3862836360931396, 'This paper proposes an interesting new method for training neural networks, i.e., a hypernetwork is used to generate the model parameters of the main network.': 1.3862943649291992, 'The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task.': 1.3862943649291992, 'In particular, the hyperLSTM with non-shared weights can achieve excellent results compared to conventional LSTM and its variants on a couple of LM talks, which is very inspiring.': 1.3862943649291992, 'pros': 1.3847951889038086, 'This work demonstrates that it is possible to generate the neural network model parameters using another network that can achieve competitive results by a few relative large scale experiments.': 1.3862943649291992, 'The idea itself is very inspiring, and the experiments are very solid.': 1.3862943649291992, 'cons': 1.2625927925109863, 'The paper would be much stronger if it was more focused.': 1.3862942457199097, 'In particular, it is unclear what is the key advantage of this hypernetwork approach.': 1.3862799406051636, 'It is argued that in the paper that can achieve competitive results using smaller number of trainable model parameters.': 1.3862943649291992, 'However, in the running time, the computational complexity is the same as the standard main network for static networks, such as ConvNet, and the computational cost is even larger for dynamic networks such as LSTMs.': 1.3862941265106201, 'The improvements of hyperLSTMs over conventional LSTM and its variants seem mainly come from increasing the number of model parameters.': 1.386293649673462, 'minor question,': 1.3862943649291992, 'The ConvNet and LSTM used in the experiments do not have a large softmax layer.': 1.3862874507904053, 'For most of the word-level tasks for either LM or MT, the softmax layer could be more than 100K. Is it going to be challenging for the hyperNetwork generate large number of weights for that case, and is it going to slowing the training down significantly?': 1.3862943649291992, '*** Paper Summary ***': 1.3862943649291992, 'The paper proposes to a new neural network architecture.': 1.3862721920013428, 'The layer weights of a classical network are computed as a function of a latent representation associated with the layer.': 1.3862943649291992, 'Two instances are presented (i) a CNN where each layer weight is computed from a lower dimensional layer embedding vector; (ii) an RNN where each layer weight is computed from a secondary RNN state.': 1.386183500289917, '*** Review Summary ***': 1.3862905502319336, 'Pros:': 1.385536551475525, 'I like the idea of bringing multiplicative RNNs and their predecessors back into the spotlight.': 0.857439398765564, 'LM and MT results are excellent.': 1.3862884044647217, 'Cons:': 1.384714126586914, 'The paper could be better written.': 1.3283071517944336, 'It is too long for the conference format and need refocussing.': 1.3862943649291992, 'On related work, the relation with multiplicative RNN and their generic tensor product predecessor (Order 2 networks, wrt C. Lee Giles definition) should be mentioned in the related work section and the differences with earlier research need to be explained and motivated (by the way it is better to say that something is revisiting an old idea or training it at modern scale/on modern tasks than ommitting it).': 1.3857247829437256, 'on focus, it is not clear if your goal is to achieve better performance or more compact networks.': 0.9453222751617432, 'In the RNN section you lean toward the former, in the CNN section you seem to lean toward the latter.': 0.9737095832824707, 'I would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication.': 1.352802038192749, 'The relation with multiplicative/order 2 networks and eventual differences need to be explained.': 0.9721012711524963, '*** Detailed Review ***': 1.3862943649291992, 'Multiplicative networks are an extremely powerfull architecture and bringing them back into the spotlight is excellent.': 1.3771038055419922, 'This paper has excellent results but suffer poor presentation, lack of a clear focus.': 1.3576900959014893, 'It spends time on details and ommit important points.': 1.3862943649291992, 'In its current form, it is much too long to long and his not self contained without the appendices.': 1.0383318662643433, 'Spending more time on multiplicative RNNs, order 2 networks at the begining of the paper would be excellent.': 1.3735567331314087, 'This will let you highlight the difference between this paper and earlier work.': 1.3861216306686401, 'It would also be necessary to spend a little time on why multiplicative RNN were less used than gated RNN: it seems that the optimization problem their training involve is tricker and it would be helpful to explain whether you had a harder time tweaking optimization parameters or whether you needed longer training sessions compared to LSTMs, regular CNN.': 1.3638678789138794, 'On name, I am not sure that ""hypernetwork"" help the reader understand better what the proposed architecture compared to multiplicative interactions.': 1.377373218536377, 'In section 3.2, you seem to imply that there are different settings of hypernetworks that allow to vary from an RNN to a CNN, this is not clear to me, maybe you could show how this would work on a simple temporal problem with equations.': 0.5841046571731567, 'The work on CNN and RNN are rather disconnected to me: for CNN, you seem to be interested in a low rank structure of the weights, showing that similar performance can be achieved with less weights.': 1.0263246297836304, 'It is not clear to me why to pursue that goal.': 1.2901588678359985, 'Do you expect speedups?': 1.2839579582214355, 'less memory for embedded applications?': 1.3785545825958252, 'In that case you should compare with alternative strategies, e.g. model compression (Caruana et al 2006, aka Dark Knowledge, Hinton et al 2014) or hashed networks (Chen et al 2015).': 0.5998021960258484, 'For RNN, you seem to target better perplexity/BLEU and model compactness is not a priority.': 1.1318464279174805, 'Instead of making the weights have a simpler structure, you make them richer, i.e. dependent over time.': 0.7948318123817444, 'It seems in that case models might be bigger and take longer to train.': 0.7657738924026489, 'You might want to comment on training time, inference time, memory requirement in that case, as you highlight it might be an important goal in the CNN section.': 1.2043471336364746, 'Overall, I am not sure it helps to have this mixed message.': 1.3533391952514648, 'I would rather see the paper fit in the conference format with the RNN results alone and a clearer explanation and defers the publications of the CNN results when a proper comparison with memory concerned methods is performed.': 1.3853474855422974, 'Some of the discussions are not clear to me, I am not sure what message the reader should get from Figure 2 or from the discussion on saturation statistics (p10, Figure 5).': 1.3861640691757202, 'Similarly, I am not sure if Figure 4 is showing anything: everything should change more drastically at word boundaries even in a regular LSTM (states, gates units should look very different before/after a space); without such a comparison it is hard to see if this is unique to your network.': 1.3552943468093872, 'The results on handwriting generation are harder to compare for me.': 0.9304194450378418, 'Log-loss are hard to understand, I have no sense whether the difference between models is significant (what would be the variance in this metric under boostrap sampling of the training set?).': 1.3862942457199097, 'I am not sold either on qualitative metric were human can assess quality but human cannot evaluate if the network is repeating the training set.': 1.3844163417816162, 'Did you thing at precision/recall metric for ink, possibly with some spatial tolerance ?': 1.3635594844818115, '(e.g. evaluation of segmentation tasks in vision).': 0.8899905681610107, 'The MT experiments are insufficiently discussed in the main text.': 1.3852338790893555, 'Overall, I would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication.': 1.37062668800354, 'You need to properly discuss the relation to multiplicative/order 2 networks and highlight the differences.': 0.6188721060752869, 'Unclear discussion can be eliminated to make the experimental setup and the results presentation clearer in the main text.': 1.3283029794692993, '*** References ***': 1.3862943649291992, 'M.W. Goudreau, C.L. Giles, S.T. Chakradhar, D. Chen, ""First-Order Vs.': 1.057303547859192, 'Second-Order Single Layer Recurrent Neural Networks,""IEEE Trans. on Neural Networks, 5 (3), p. 511, 1994.': 0.22649483382701874, 'Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil, ""Model Compression,"" The Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2006), August 2006, pp. 535-541.': 1.3862943649291992, 'Dark knowledge, G Hinton, O Vinyals, J Dean 2014': 1.3862943649291992, 'W. Chen, J. Wilson, S. Tyree, K. Weinberger and Y. Chen, Compressing Neural Networks with the Hashing Trick, Proc.': 0.8551031351089478, 'International Conference on Machine Learning (ICML-15)': 0.6813707947731018}"
451,https://openreview.net/forum?id=rkpdnIqlx,"{'The authors present a method for training probabilistic models by maximizing a stochastic variational-lower-bound-type objective.': 1.0985873937606812, 'Training involves sampling and then learning a transition-based inference to ""walk back"" samples to the data.': 1.0986123085021973, 'Because of its focus on transitions, it can be used to learn a raw transition operator rather than purely learning an energy-based model.': 1.0722483396530151, 'The objective is intuitively appealing because of its similarity to previous successful but less principled training methods for MRFs like Contrastive Divergence.': 0.5319522023200989, 'The idea for the algorithm is appealing, and it looks like it could find a nice place in the literature.': 1.0986123085021973, 'However, the submission in its current form is not yet ready for publication.': 1.0986123085021973, 'Experiments are qualitative and the generated samples are not obviously indicative of a high model quality.': 1.0986123085021973, 'As pointed out elsewhere, the mathematical analysis does not currently demonstrate tightness of the variational bound in the case of a learned transition operator.': 1.0985995531082153, 'More evaluation using e.g. annealed importance sampling to estimate held-out likelihoods is necessary.': 1.0986076593399048, 'Assuming that the analysis can be repaired, the ability to directly parametrize a transition operator, an interesting strength of this method, should be explored in further experiments and contrasted with the more standard energy-based modeling.': 1.0982402563095093, 'This looks like a promising idea, and other reviews and questions have already raised some important technical points which should help strengthen this paper for future submission.': 1.0986123085021973, 'I very much like the underlying idea for this paper.': 1.0708727836608887, ""I wasn't convinced by the execution in its current state."": 1.0986123085021973, ""My primary concern is the one I expressed in my pre-review question below, which I don't think the authors addressed."": 1.0986117124557495, ""Specifically, I think the choice of q(s | s') = p(s | s') will make the forward and reverse trajectories almost pathologically mismatched to each other, and will thus make the variational bound extremely loose and high variance."": 1.0986037254333496, 'The claim about the tightness of the bound in Appendix D relies on the assumption that the transition distribution obeys detailed balance.': 1.0968059301376343, 'The learned transition distribution in the paper does not obey detailed balance, and therefore the tightness claim in Appendix D does not hold.': 1.0979034900665283, '(In Section 2.1 you briefly discuss the idea of learning an energy function, rather than directly learning a transition distribution.': 1.0796306133270264, 'I think this would be excellent, and in that case you could choose an MCMC transition operator that does obey detailed balance for that energy function.)': 1.0986120700836182, 'I did not go through Appendix D beyond this step.': 1.0985931158065796, 'The experimental results were not visually impressive.': 1.0985761880874634, 'I suspect this is primarily driven by the mismatch between generative and inference trajectories.': 1.0986123085021973, 'See my concern above and in the pre-review question below.': 1.0986123085021973, 'Also, see note below for sec.': 1.0986123085021973, '5. I suspect some terms are being dropped from the training gradient.': 1.0986123085021973, 'The paper is optimizing a variational bound on log likelihood.': 1.0986123085021973, 'You should really, really, really report and compare log likelihoods against competing methods!': 1.0986123085021973, 'Detailed comments below.': 1.0986123085021973, 'Some of these were written based on a previous version of the paper.': 1.0986123085021973, 'sec 1.2 - first paragraph is very difficult to follow': 1.0986123085021973, '""these modes these spurious modes"" -> ""these spurious modes""': 1.0986123085021973, 'sec 2.1 - ""s = (v,h)"" -> ""s = {v,h}""': 1.0986123085021973, 'sec 2.2 - ""with an MCMC"" -> ""with an MCMC chain""': 1.0224238634109497, '""(ideally an MCMC)"" -> ""(e.g. via MCMC)""': 1.0985928773880005, 'MCMC is not ideal ...': 1.098387360572815, ""it's just often the best we can do."": 1.0449233055114746, 'sec 3, last bullet - could make the temperature infinite for the last step, in which case the last step will sample directly from the prior, and the posterior and the prior will be exactly the same.': 1.0972838401794434, 'sec. 4': 1.0985372066497803, 'Using an energy function would be great!!': 1.0972940921783447, 'Especially, because many MCMC transition operators obey detailed balance, you would be far less prone to suffer from the forward/backward transition mismatch that is my primary concern about this technique.': 1.0979148149490356, 'eq. 12,13': 1.0986121892929077, 'What is alpha?': 1.0968890190124512, 'How does it depend on the temperature.': 1.0986123085021973, ""It's never specified."": 1.0986123085021973, 'sec. 5, last paragraph in GSN section': 1.0986123085021973, 'Note that q also depends on theta, so by not backpropagating through the full q chain you are dropping terms from the gradient.': 0.34795552492141724, 'sec. 5, non-equilibrium thermodynamics': 1.0986123085021973, 'Note that the noneq.': 1.0986123085021973, 'paper also increases the noise variance as the distance from the data increases.': 1.0985749959945679, 'Fig. 1': 1.0986123085021973, 'right/left mislabeled': 1.0926271677017212, 'Fig. 2': 1.0986123085021973, 'label panes': 1.0986123085021973, 'Fig. 3': 1.0986123085021973, 'After how many walkback steps?': 1.0986123085021973, 'This paper proposes a new kind of generative model based on an annealing process, where the transition probabilities are learned directly to maximize a variational lower bound on the log-likelihood.': 1.0986123085021973, 'Overall, the idea is clever and appealing, but I think the paper needs more quantitative validation and better discussion of the relationship with prior work.': 1.0986123085021973, 'In terms of prior work, AIS and RAISE are both closely related algorithms, and share much of the mathematical structure with the proposed method.': 1.0986123085021973, 'For this reason, it’s not sufficient to mention them in passing in the related work section; those methods and their relationship to variational walkback need to be discussed in detail.': 1.0986123085021973, 'If I understand correctly, the proposed method is essentially an extension of RAISE where the transition probabilities are learned rather than fixed based on an existing MRF.': 1.0986123085021973, 'I think this is an interesting and worthwhile extension, but the relationship to existing work needs to be clarified.': 1.0986120700836182, 'The analysis of Appendix D seems incorrect.': 1.0986123085021973, 'It derives a formula for the ratios of prior and posterior probabilities, but this formula only holds under the assumption of constant temperature (in which case the ratio is very large).': 1.0984083414077759, 'When the temperature is varied, the analysis of Neal (2001) applies, and the answer is different.': 0.7988028526306152, 'One of the main selling points of the method is that it optimizes a variational lower bound on the log-likelihood; even more accurate estimates can be obtained using importance sampling.': 1.0986123085021973, 'It ought to be easy to report log-likelihood estimates for this method, so I wonder why such estimates aren’t reported.': 1.0986121892929077, 'There are lots of prior results to compare against on MNIST.': 1.0986123085021973, '(In addition, a natural baseline would be RAISE, so that one can check if the ability to learn the transitions actually helps.)': 1.0985479354858398, 'I think the basic idea here is a sound one, so I would be willing to raise my score if the above issues are addressed in a revised version.': 1.0986123085021973, 'Minor comments:': 1.0986123085021973, '“A recognized obstacle to training undirected graphical models… is that ML training requires sampling from MCMC chains in the inner loop of training, for each example.”': 1.0986123085021973, 'This seems like an unfair characterization, since the standard algorithm is PCD, which usually takes only a single step per mini-batch.': 1.0986123085021973, 'Some of the methods discussed in the related work are missing citations.': 1.0986123085021973, 'The method is justified in terms of “carving the energy function in the right direction at each point”, but I’m not sure this is actually what’s happening.': 1.0986101627349854, 'Isn’t the point of the method that it can optimize a lower bound on the log-likelihood, and therefore learn a globally correct allocation of probability mass?': 1.0986123085021973}"
452,https://openreview.net/forum?id=rksfwnFxl,"{'This paper presents an anomaly-based host intrusion detection method.': 1.0986121892929077, 'LSTM RNN is used to model the system-call sequences and the averaged sequence likelihood is then used to determine anomaly, which is the attack.': 1.0984328985214233, 'This paper also compares an ensemble method with two baselines as classification model.': 0.9668945670127869, '+This is is well written and more of ideas are clearly presented.': 1.0819274187088013, '+It demonstrates an interesting application of LSTM sequential modeling to HIDS problem': 1.0943217277526855, 'The overall novelty is limited considering the major technical components like LSTM RNN and ensemble method are already established.': 1.0986078977584839, 'The contribution of the proposed ensemble method needs further evaluation because it is also possible to use ensemble ideas in kNN and kMC baselines.': 1.0986123085021973, 'The authors propose using an LSTM on a sequence of system calls to perform network intrusion detection (NIDS).': 1.098597526550293, 'The idea of using neural networks (in general) for NIDS is old [1].': 1.0986123085021973, 'The idea of using some sort of NN on top of a sequence of system calls for NIDS is published [2].': 1.0986123085021973, 'The idea of using LSTMs for NIDS is published [2].': 1.0986123085021973, 'The paper in [2] operates on counts of N-grams of system calls, rather than on the raw sequence, but that pre-processing does not seem ""heavy"" to me.': 1.0980106592178345, 'Overall, the proposed system works as well as other proposed NIDS system, and the paper checks portability (which is good).': 1.0986120700836182, ""But, on the con side, I don't see this paper as adding a lot to the state-of-the-art in NIDS."": 1.0986113548278809, 'Nor does is the paper well-matched to ICLR.': 1.0986123085021973, ""I didn't learn a lot about representations from this paper: many people have thrown LSTM at sequence problems."": 1.0985944271087646, ""Therefore, I think it's below threshold for ICLR."": 0.9717304110527039, 'The authors may wish to submit to a security conference.': 1.0986123085021973, 'References:': 1.0933635234832764, '1. Debar, Herve, Monique Becker, and Didier Siboni. ""A neural network component for an intrusion detection system."" Research in Security and Privacy, 1992. Proceedings., 1992 IEEE Computer Society Symposium on. IEEE, 1992.': 0.49361056089401245, '2. Creech, Gideon, and Jiankun Hu. ""A semantic approach to host-based intrusion detection systems using contiguousand discontiguous system call patterns."" IEEE Transactions on Computers 63.4 (2014): 807-819.': 0.7777376174926758, '3. Staudemeyer, Ralf C. ""Applying long short-term memory recurrent neural networks to intrusion detection."" South African Computer Journal 56.1 (2015).': 1.0565075874328613, 'In this paper a novel approach for anomaly detection is considered for the task of intrusion detection based on system call sequence.': 1.09861159324646, 'The system call sequence is regarded as a language, and multiple lstm-rnn language models are trained and ensembled.': 1.0986123085021973, 'Diversity in the ensemble is achieved by choosing different hyper parameters for each lstm-LM.': 1.0986123085021973, 'The combination of the LMs is done by averaging transformations of the likelihoods.': 1.0316998958587646, 'I really like the fact that no attack data is used during training, and I like the LM and ensemble approach.': 1.0986120700836182, ""The only high level drawbacks I have are the following, which might have a simple answer as I'm not an expert in this field:"": 1.0986123085021973, 'Relaying of system calls seems weak: If the attacker has access to some ""normal"" sequences of system calls, all she can fool the system by interleaving its malicious system calls with normal ones, in a way that will artificially raise the likelihood of the sequence.': 0.4641912579536438, 'A few lines covering other anomaly detection tasks, where RNNs are used, can be added to the introduction, to give a better idea about the novelty of the approach.': 1.0986121892929077}"
453,https://openreview.net/forum?id=rkuDV6iex,"{'First of all, I would like to thank the authors for putting this much work into a necessary but somewhat tedious topic.': 1.3862942457199097, 'While I think the paper is somewhat below the standard of a conference paper (see detailed comments below), I would definitely love to see a version of this paper published with some of the issues ironed out.': 1.3862943649291992, 'I also agree with many of the points raised by other reviewers and will not repeat them here.': 1.3862943649291992, 'Major points:': 1.386270523071289, '""As we saw in the previous section, the minima of deep network loss functions are for the most part decent.""': 1.3515771627426147, 'All you said in the previous section was that theory shows that there are no bad minima under ""strong assumptions"".': 1.3862831592559814, 'There is no practical proof that minima do not vary in quality.': 1.2756872177124023, '""This implies that we probably do not need to take many precautions to avoid bad minima in practice.': 1.3333803415298462, 'If all minima are decent, then the task of finding a ""decent minima quickly"" is reduced to the task of finding any minima quickly.""': 1.3862886428833008, 'First of all, as one of the reviewers pointed out, we are never guaranteed in practice to actually reach a local minimum.': 1.3862937688827515, 'We could always hit a region of the objective function where the algorithm makes essentially no further progress.': 1.3862942457199097, 'The final error level, in practice, actually does depend significantly on many factors such as (i) optimization algorithm (ii) learning rate schedule (iii) initialization of weights (iv) presence of unsupervised pretraining (v) whether neurons are added or eliminated during training etc. etc.': 1.3862943649291992, 'Therefore, the task of optimizing neural networks is far from being ""reduced to finding any minima quickly"".': 1.3862943649291992, 'Figure 1': 1.3862943649291992, ""I don't like Figure 1, because it suggests to me that you diagnosed exactly where the transition between the two phases happened, which I don't think you did."": 1.2408519983291626, 'Also, the concept of having a fast-decaying error followed by a slow-decaying error is simple enough for readers to understand without a dedicated graph.': 1.3815027475357056, ""Minor point on presentation: The red brace is positioned lower in the figure than the blue brace and the braces don't join up horizontally."": 1.3760274648666382, 'Please be more careful.': 1.3862943649291992, 'Misuse of the transient phase / minimization phase concept': 1.3862943649291992, 'In section 4.3, you talk about the transient and minimization phase of optimization.': 1.3862943649291992, 'However, you have no way of diagnosing when or if your algorithm reaches the minimization phase.': 1.3862943649291992, 'You seem to think that the minimization phase is simply the part of the optimization process where the error decreases slowly.': 1.3862943649291992, 'AFAIK, this is not the case.': 1.3862943649291992, 'The minimization phase is where the optimization algorithm enters the vicinity of the local minimum that can be approximated by the second-order Taylor expansion.': 1.3862943649291992, 'For this to even occur, one would have to verify, for example, that the learning rate is small enough.': 0.9292117357254028, 'You change the algorithm after 25%, 50% and 75% of training, but these points seem arbitrary.': 1.344616413116455, 'What is the minimization phase was reached at 99%, or 10 epochs after you decided to stop training?': 1.3636746406555176, 'Only 1 dataset': 1.3862943649291992, 'You run most experiments on only 1 dataset (CIFAR).': 1.3862943649291992, 'Please replicate with at least one more dataset.': 1.3862929344177246, 'Many figures are unclear': 1.3862943649291992, 'For each figure, the following information are relevant: network used; dataset used; learning rate used; batch norm yes / no; whether figure shows train, test, or validation error.': 1.2628847360610962, 'It should be easy for the reader to ascertain this information for all figures, not just for some.': 1.3808302879333496, 'You say at the beginning of section 4.1 that each algorithm finds a different minimum as if this is a significant finding.': 1.3856080770492554, 'However, this is obvious because the updates taken by these algorithms vary wildly.': 1.1843490600585938, 'Keep in mind that there is an exponentially large number of minima.': 1.3808090686798096, 'The probability of different algorithms choosing the same minimum is essentially zero because of their sheer number.': 1.384374737739563, 'The same would be true if you even shift the learning rate slightly or use a different random seed for minibatch generation etc. etc.': 1.1223472356796265, 'Lack of confidence intervals': 1.3862899541854858, 'The value of Figures 1, 2, 3 and 6 is limited is because it is unclear how these plots would change if the random seed were changed.': 1.3372904062271118, 'We only get information for a single weight initialization and a single minibatch sequence.': 1.1124786138534546, 'While figures 5 and 7 can be used to try and infer what confidence intervals around plots in figures 1, 2, 3 and 6 might look like, I think those confidence intervals should still be shown for at least a subset of the configurations presented.': 1.1298637390136719, 'Lack of information regarding learning rate': 1.3862773180007935, 'There is big question mark left open regarding how all your results would change if different learning rates were used.': 1.3186901807785034, ""You don't even tell us how you chose the learning rates from the intervals you gave in section 3.4."": 1.3862943649291992, 'Lack of information regarding the absolute distance of interpolated points': 1.3862943649291992, 'In most figures, you interpolate between two or three trained weight configuration.': 1.3862943649291992, 'However, you do not say how far the interpolated points are apart.': 1.3862943649291992, 'This is highly significant, because if points are close together and there is a big ""hump"" between them, it means that those points are more ""brittle"" than if they are far apart and there is a big ""hump"" between them.': 1.3862943649291992, 'Minor points:': 1.3862943649291992, 'LSTM is not a fixed network architecture like NiN or VGG, but a layer type.': 1.3862943649291992, 'LSTM would be equivalent to CNN.': 1.3862943649291992, 'Also, the VGG paper has multiple versions of VGG.': 1.3862943649291992, 'You should specify which one you used.': 1.3862943649291992, 'The font size for the legends in the upper triangle of Table 1 is too small.': 1.3862943649291992, 'You can\'t just write ""best viewed in zoom"" in the table caption and pretend that somehow fixes the problem.': 1.3862943649291992, 'Personally, I prefer no legend over an unreadable legend.': 1.3855246305465698, 'The paper is dedicated to better understanding the optimization landscape in deep learning, in particular when explored with different optimization algorithms, and thus it also characterizes the behavior of these algorithms.': 1.3862942457199097, 'It heavily re-uses the approach of Goodfellow et al. (2015).': 1.3862943649291992, 'I find it hard to understand the contributions of the paper, for example: is it surprising that different algorithms reach different solutions when starting from the same initialization?': 1.386252999305725, 'It would be useful if the authors build such basic intuition in the paper.': 1.3861685991287231, 'I also did not receive a clear answer to the question I posed to reviewers regarding clarifying how does the findings of the paper can contribute to future works on optimization in deep learning.': 1.3861035108566284, 'And this is what I find fundamentally missing.': 1.3859622478485107, 'So for example, there are probably plenty of ways to modify approach of Goodfellow et al. (2015), and similar works, and come up with interesting visualization methods for deep learning - but the question is: how is this helpful in terms of designing better algorithms, gaining more intuition how the optimization surface looks like in general, etc.?': 1.3862943649291992, 'This is an interesting paper, though I am fairly confident it is a better fit for the journal than this conference.': 1.3862943649291992, 'It would be interesting and instructive, even for sanity check, to plot the eigenspectra of the solutions recovered by the algorithms to see the order of critical points recovered.': 1.372717261314392, 'I appreciate the work': 1.3862943649291992, 'but I do not think the paper is clear enough.': 1.3862943649291992, 'Moreover, the authors say ""local minimia"" ~70 times but do not show (except for Figure 11?) that the solutions found are not necessarily local minima.': 1.3855376243591309, 'The authors do not talk about that fact that slices of a non-convex problem can look like the ones they show.': 1.3862943649291992, 'It is well-known that the first-order methods may just fail to deal with certain non-convex ill-conditioned problems even in low-dimensional noiseless cases, the place/solution where they fail to make progress is not necessarily a local minimum.': 0.7519410252571106, 'Some sentences like the one given below suggest that the study is too superficial:': 1.3862943649291992, '""One of the interesting empirical observation is that we often observe is that the incremental improvement': 1.3862943649291992, 'of optimization methods decreases rapidly even in non-convex problems.""': 1.3862943649291992, 'This paper provides an extensive analysis of the error loss function for different optimization methods.': 1.3862943649291992, 'The presentation is well done and informative.': 1.3862943649291992, 'The experimental procedure is clarified sufficiently well.': 1.3862943649291992, 'Theoretical evaluations like this are crucial for a wide range of applications and help to better understand and improve the convergence behavior for a given system.': 1.3862870931625366, 'Pros:': 1.3862943649291992, 'Important analysis': 1.3862943649291992, 'Good visualizations': 1.3862943649291992, 'Cons:': 1.3862943649291992, ""The paper describes mostly the observation that the optima vary for different methods, however doesn't attempt to explain why it happens and how to solve it (aside from batch-norm)"": 1.3862943649291992, 'Some fonts are very small (e.g. Fig. 5)': 1.3862943649291992}"
454,https://openreview.net/forum?id=rky3QW9le,"{'This paper trains a generative model of image patches, where dictionary elements undergo gated linear transformations before being combined.': 1.0986123085021973, 'The transformations are motivated in terms of Lie group operators, though in practice they are a set of fixed linear transformations.': 1.0986123085021973, 'This is motivated strongly in terms of learning a hierarchy of transformations, though only one layer is used in the experiments (except for a toy case in the appendix).': 1.0986123085021973, 'I like the motivation for this algorithm.': 1.0986123085021973, 'The realization seems very similar to a group or block sparse coding implementation.': 1.0986123085021973, 'I was disappointed by the restriction to linear transformations.': 1.0986123085021973, 'The experiments were all toy cases, demonstrating that the algorithm can learn groups of Gabor- or center surround-like features.': 1.0986123085021973, ""They would have been somewhat underpowered five years ago, and seemed extremely small by today's standards."": 1.0986123085021973, 'Specific comments:': 1.0986123085021973, 'Based on common practices in ML literature, I have a strong bias to think of  as inputs and  as network weights.': 1.0986123085021973, 'Latent variables are often  or .': 1.0986123085021973, 'Depending on your target audience, I would suggest permuting your choice of symbols so the reader can more quickly interpret your model.': 1.0986082553863525, 'nit: number all equations for easier reference': 1.0986123085021973, 'sec 2.2': 1.0986123085021973, ""It's weird that the transformation is fixed, but is still written as a function of x."": 1.0986123085021973, 'sec 2.3': 1.0986123085021973, 'The updated text here confuses me actually.': 1.0986123085021973, 'I had thought that you were using a fixed set of linear transformations, and were motivating in terms of Lie groups, but were not actually taking matrix exponentials in your algorithm.': 1.0986123085021973, 'The equations in the second half of this section suggest you are working with matrix exponentials though.': 1.0986123085021973, ""I'm not sure which direction I'm confused in, but probably good to clarify the text either way."": 1.0986123085021973, 'BTW': 1.0986123085021973, ""there's another possible solution to the local minima difficulty, which is the one used in Sohl-Dickstein, 2010."": 1.0986123085021973, 'There, they introduce blurring operators matched to each transformation operator, and gradient descent can escape local minima by detouring through coarser (more blurred) scales.': 1.0986123085021973, 'sec 3.2': 1.0986123085021973, 'I believe by degrees of freedom you mean the number of model parameters, not the number of latent coefficients that must be inferred?': 1.0986123085021973, 'Should make this more clear.': 1.0986123085021973, 'Is it more appropriate to compare reconstruction error while matching number of model parameters, or number of latent variables?': 1.0986123085021973, 'I wonder if a convolutional version of this algorithm would be practical / would make it more suited as a generative model of whole images.': 1.0986123085021973, '====': 1.0986123085021973, 'post rebuttal update': 1.0986123085021973, 'Thank you for taking the time to write the rebuttal!': 1.0986123085021973, 'I have read it, but it did not significantly effect my rating.': 1.0986123085021973, 'This paper proposes an approach to unsupervised learning based on a modification to sparse coding that allows for explicit modeling of transformations (such as shift, rotation, etc.), as opposed to simple pooling as is typically done in convnets.': 1.0986123085021973, 'Results are shown for training on natural images, demonstrating that the algorithm learns about features and their transformations in the data.': 1.0986121892929077, 'A comparison to traditional sparse coding shows that it represents images with fewer degrees of freedom.': 1.0813400745391846, 'This seems like a good and interesting approach, but the work seems like its still in its early formative stages rather than a complete work with a compelling punch line.': 1.0981101989746094, ""For example one of the motivations is that you'd like to represent pose along with the identity of an object."": 1.0666158199310303, ""While this work seems well on its way to that goal, it doesn't quite get there - it leaves a lot of dots still to be connected."": 0.9954152703285217, ""Also there are a number of things that aren't clear in the paper:"": 1.098483920097351, 'o The central idea of the paper it seems is the use of a transformational sparse coding tree to make tractable the inference of the Lie group parameters x_k.': 1.0986015796661377, 'But how exactly this is done is not at all clear.': 1.0986123085021973, 'For example, the sentence: ""The main idea is to gradually marginalize over an increasing range of transformations,"" is suggestive but not clear.': 1.0986101627349854, 'This needs to be much better defined.': 1.0986123085021973, 'What do you mean by marginalization in this context?': 1.0986123085021973, 'o': 1.0986123085021973, 'The connection between the Lie group operator and the tree leaves and weights w_b is not at all clear.': 1.098494052886963, 'The learning rule spells out the gradient for the Lie group operator, but how this is used to learn the leaves of the tree is not clear.': 0.8688033819198608, 'A lot is left to the imagination here.': 1.0986123085021973, 'This is especially confusing because although the Lie group operator is introduced earlier, it is then stated that its not tractable for inference because there are too many local minima, and this motivates the tree approach instead.': 1.0986090898513794, 'So its not clear why you are learning the Lie group operator.': 1.0986123085021973, 'o It is stated that ""Averaging over many data points, smoothens the surface of the error function.""': 1.0986123085021973, ""I don't understand why you would average over many data points."": 1.0986123085021973, 'It seems each would have its own transformation, no?': 1.0986123085021973, 'o What data do you train on?': 1.0986123085021973, 'How is it generated?': 1.0986123085021973, 'Do you generate patches with known transformations and then show that you can recover them?': 1.0986123085021973, 'Please explain.': 1.0986123085021973, 'The results shown in Figure 4 look very interesting, but given the lack of clarity in the above, difficult to interpret and understand what this means, and its significance.': 1.0986007452011108, 'I would encourage the authors to rewrite the paper more clearly and also to put more work into further developing these ideas, which seem very promising.': 1.0986123085021973, 'A new sparse coding model is introduced that learns features jointly with their transformations.': 1.0986123085021973, 'It is found that inference over per-image transformation variables is hard, so the authors suggest tying these variables across all data points, turning them into global parameters, and using multiple transformations for each feature.': 1.0986123085021973, 'Furthermore, it is suggested to use a tree of transformations, where each path down the tree generates a feature by multiplying the root feature by the transformations associated with the edges.': 1.0986123085021973, 'The one-layer tree model achieves similar reconstruction error as traditional sparse coding, while using fewer parameters.': 1.0986123085021973, 'This is a nice addition to the literature on sparse coding and the literature on learning transformation models.': 1.0986123085021973, 'The authors identify and deal with a difficult inference problem that can occur in transformation models.': 1.0986123085021973, 'That said, I am skeptical about the usefulness of the general approach.': 1.0986123085021973, 'The authors take it as a given that “learning sparse features and transformations jointly” is an important goal in itself, but this is never really argued or demonstrated with experiments.': 1.098589539527893, 'It doesn’t seem like this method enables new applications, extends our understanding of learning what/where pathways in the brain, or improve our ability to model natural images.': 1.0986121892929077, 'The authors claim that the model extracts pose information, but although the model explicitly captures the transformation that relates different features in a tree, at test time, inference is only performed over the (sparse) coefficient associated with each (feature, transformation) combination, just like in sparse coding.': 1.0986121892929077, 'It is not clear what we gain by knowing that each coefficient is associated with a transformation, especially since there are many models that do this general “what / where” split.': 1.0020304918289185, 'It would be good to check that the x_{v->b} actually change significantly from their initialization values.': 1.0982050895690918, 'The loss surface still looks pretty bad even for tied transformations, so they may actually not move much.': 1.0970253944396973, 'Does the proposed model work better according to some measure, compared to a model where x_{v->b} are fixed and chosen from some reasonable range of parameter values (either randomly or spaced evenly)?': 1.098581075668335, 'One of the conceptually interesting aspects of the paper is the idea of a tree of transformations, but the advantage of deeper trees is never demonstrated convincingly.': 0.8671941757202148, 'It looks like the authors have only just gotten this approach to work on toy data with vertical and horizontal bars.': 0.4466784596443176, 'Finally, it is not clear how the method could be extended to have multiple layers.': 0.9169230461120605, 'The transformation operators T can be defined in the first layer because they act on the input space, but the same cannot be done in the learned feature space.': 0.6997590661048889, 'It is also not clear how the pose information should be further processed in a hierarchical manner, or how learning in a deep version should work.': 0.6179490685462952, 'In summary, I do not recommend this paper for publication, because it is not clear what problem is being solved, the method is only moderately novel and the novel aspects are not convincingly shown to be beneficial.': 0.5658106803894043}"
455,https://openreview.net/forum?id=ry18Ww5ee,"{'This was an interesting paper.': 1.0986121892929077, 'The algorithm seems clear, the problem well-recognized, and the results are both strong and plausible.': 1.094427466392517, 'Approaches to hyperparameter optimization based on SMBO have struggled to make good use of convergence during training, and this paper presents a fresh look at a non-SMBO alternative (at least I thought it did, until one of the other reviewers pointed out how much overlap there is with the previously published successive halving algorithm - too bad!).': 1.056786060333252, ""Still, I'm excited to try it."": 1.0986123085021973, ""I'm cautiously optimistic that this simple alternative to SMBO may be the first advance to model search for the skeptical practitioner since the case for random search > grid search (http://www.jmlr.org/papers/v13/bergstra12a.html, which this paper should probably cite in connection with their random search baseline.)"": 1.087470531463623, 'I would suggest that the authors remove the (incorrect?) claim that this algorithm is ""embarrassingly parallel"" as it seems that there are number of synchronization barriers at which state must be shared in order to make the go-no-go decisions on whatever training runs are still in progress.': 1.0986121892929077, ""As the authors themselves point out as future work, there are interesting questions around how to adapt this algorithm to make optimal use of a cluster (I'm optimistic that it should carry over, but it's not trivial)."": 1.0986123085021973, ""For future work, the authors might be interested in Hutter et al's work on Bayesian Optimization With Censored Response Data (https://arxiv.org/abs/1310.1947) for some ideas about how to use the dis-continued runs."": 1.0986123085021973, 'This paper discusses Hyperband, an extension of successive halving by Jamieson & Talwalkar (AISTATS 2016).': 1.0986123085021973, 'Successive halving is a very nice algorithm that starts evaluating many configurations and repeatedly cuts off the current worst half to explore many configuration for a limited budget.': 1.0986123085021973, 'Having read the paper for the question period and just rereading it again, I am now not entirely sure what its contribution is meant to be: the only improvement of Hyperband vs. successive halving is in the theoretical worst case bounds (not more than 5x worse than random search), but you can (a) trivially obtain that bound by using a fifth of your time for running random configurations to completion and (b) the theoretical analysis to show this is said to be beyond the scope of the paper.': 1.0986123085021973, 'That makes me wonder whether the theoretical results are the contribution of this paper, or whether they are the subject of a different paper and the current paper is mostly an empirical study of the method?': 1.0980477333068848, 'I hope to get a response by the authors and see this made clearer in an updated version of the paper.': 1.0965988636016846, ""In terms of experiments, the paper fails to show a case where Hyperband actually performs better than the authors' previous algorithm successive halving with its most agressive setting of bracket b=4."": 1.0985519886016846, 'Literally, in every figure, bracket b=4 is at least as good (and sometimes substantially better) than Hyperband.': 1.0986108779907227, 'That makes me think that in practice I would prefer successive halving with b=4 over Hyperband.': 1.0986123085021973, ""(And if I really want Hyperband's guarantee of not being more than 5x worse than random search I can run random search on a fifth of my machines.)"": 0.40466225147247314, 'The experiments also compare to some Bayesian optimization methods, but not to the most relevant very closely related Multi-Task Bayesian Optimization methods that have been dominating effective methods for deep learning in that area in the last 3 years: ""Multi-Task Bayesian Optimization"" by Swersky, Snoek, and Adams (2013) already showed 5x speedups for deep learning by starting with smaller datasets, and there have been several follow-up papers showing even larger speedups.': 1.0549895763397217, 'Given that this prominent work on multitask Bayesian optimization exists, I also think the introduction, which sells Hyperband as a very new approach to hyperparameter optimization is misleading.': 0.30451446771621704, 'I would\'ve much preferred a more down-to-earth pitch that says ""configuration evaluation"" has been becoming a very important feature in hyperparameter optimization, including Bayesian optimization, that sometimes yields very large speedups (this can be quantified by examples from existing papers) and this paper adds some much-needed theoretical understanding to this and demonstrates how important configuration evaluation is even in the simplest case of being used with random search.': 1.0866811275482178, 'I think this could be done easily and locally by adding a paragraph to the intro.': 0.43693777918815613, 'As another point regarding novelty, I think the authors should make clear that approaches for adaptively deciding how many resources to use for which evaluation have been studied for (at least) 23 years in the ML community': 0.47853347659111023, 'see Maron & Moore, NIPS 1993: ""Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation"" (https://papers.nips.cc/paper/841-hoeffding-races-accelerating-model-selection-search-for-classification-and-function-approximation).': 0.9161907434463501, 'Again, this could be done by a paragraph in the intro.': 1.0986052751541138, 'Overall, I think for this paper having the related work section at the end leads to many concepts appearing to be new in the paper that turn out not to be new in the end, which is a bit of a let-down.': 0.6485645771026611, 'I encourage the authors to prominently discuss related work, including the recent trends in Bayesian optimization towards configuration evaluation, in the beginning, and then clearly state the contribution of this paper by positioning it in the context of that related work and saying what exactly is new.': 1.0982909202575684, '(I think the answer is ""very simple method"", ""great empirical results for several deep learning tasks"" and ""much-needed new theoretical results"", which is a very nice contribution.)': 0.30310118198394775, ""I'm giving an accepting score trusting that the authors will follow this suggestion."": 1.0979148149490356, 'I have some responses to some of the author responses:': 0.7281610369682312, '1) ""In response to your question, we ran an experiment modeled after the empirical studies in Krueger et al tuning 2 hyperparameters of a kernel SVM to compare CVST (Krueger et al 2015) and Hyperband.': 0.9547042846679688, 'Hyperband is 3-4x faster than CVST on this experiment and the two achieve similar test performance.': 0.6940391063690186, 'Notably, CVST was only 50% faster than standard holdout.': 1.0986007452011108, 'For the experiments in our paper, we excluded CVST due to the aforementioned theoretical differences and because CVST is not an anytime algorithm, but as we perform more experiments, we will update the draft to reflect this comparison.""': 1.0986123085021973, 'Great, I am looking forward to seeing the details on these experiments before the decision phase.': 1.0986123085021973, '2) ""Hyperband makes no assumptions on the shape or rate of convergence of the validation error, just that it eventually converges.""': 1.0986123085021973, ""It's only the worst-case analysis that makes no assumption, but of course one would not be happy with that worst-case performance of being 5x worse than random search."": 1.0986123085021973, '(The 5x is what the authors call ""modestly worse, by a log factor""; it\'s the logarithm of the dataset size or of the number of epochs, both of which tend to be large numbers).': 1.0986123085021973, 'I think this number of 5x should be stated explicitly somewhere for the authors choice of Hyperband parameters.': 1.0328607559204102, ""(E.g., at the beginning of the experiments, when Hyperband's parameters are stated.)"": 1.0703563690185547, '3) ""Like random search, it is also embarrassingly parallel.""': 1.0949722528457642, 'I think this is not quite correct.': 0.981711208820343, ""Let's say I want to tune hyperparameters on ImageNet and each hyperparameter evaluation takes 1 week, but I have 100 GPUs, then random search will give a decent solution (the best of 100 random configurations) after 1 week."": 0.6603148579597473, 'However, Hyperband will require 5 weeks before it will give any solution.': 1.0986123085021973, 'Again, the modest log factor is a factor of 5.': 1.0986123085021973, 'To me, ""embarassingly parallel"" would mean making great predictions after a week if you throw enough resources at it.': 1.0986123085021973, 'This paper presents Hyperband, a method for hyperparameter optimization where the model is trained by gradient descent or some other iterative scheme.': 1.0986123085021973, 'The paper builds on the successive halving + random search approach of Jamieson and Talwalkar and addresses the tradeoff between training fewer models for a longer amount of time, or many models for a shorter amount of time.': 1.0986123085021973, 'Effectively, the idea is to perform multiple rounds of successive halving, starting from the most exploratory setting, and then in each round exponentially decreasing the number of experiments, but granting them exponentially more resources.': 1.0986123085021973, 'In contrast to other recent papers on this topic, the approach here does not rely on any specific model of the underlying learning curves and therefore makes fewer assumptions about the nature of the model.': 1.0986123085021973, 'The results seem to show that this approach can be highly effective, often providing several factors of speedup over sequential approaches.': 1.0986123085021973, 'Overall I think this paper is a good contribution to the hyperparameter optimization literature.': 1.0986123085021973, 'It’s relatively simple to implement, and seems to be quite effective for many problems.': 1.0986123085021973, 'It seems like a natural extension of the random search methodology to the case of early stopping.': 1.0986123085021973, 'To me, it seems like Hyperband would be most useful on problems where a) random search itself is expected to perform well and b) the computational budget is sufficiently constrained so that squeezing out the absolute best performance is not feasible and near-optimal performance is sufficient.': 1.0986123085021973, 'I would personally like to see the plots in Figure 3 run out far enough that the other methods have had time to converge in order to see what this gap between optimal and near-optimal really is (if there is one).': 1.0986123085021973, 'I’m not sure I agree with the use of random2x as a baseline.': 1.0986123085021973, 'I can see why it’s a useful comparison because it demonstrates the benefit of parallelism over sequential methods, but virtually all of these other methods also have parallel extensions.': 1.0986123085021973, 'I think if random2x is shown, then I would also like to see SMAC2x, Spearmint2x, TPE2x, etc.': 1.0986123085021973, 'I also think it would be worth seeing 3x, 10x, and so forth and how Hyperband fares against these baselines.': 1.0986123085021973}"
456,https://openreview.net/forum?id=ry2YOrcge,"{'The paper presents an end-to-end neural network model for the problem of designing natural language interfaces for database queries.': 1.0981521606445312, 'The proposed approach uses only weak supervision signals to learn the parameters of the model.': 1.0428119897842407, 'Unlike in traditional approaches, where the problem is solved by semantically parsing a natural language query into logical forms and executing those logical forms over the given data base, the proposed approach trains a neural network in an end-to-end manner which goes directly from the natural language query to the final answer obtained by processing the data base.': 1.0327759981155396, 'This is achieved by formulating a collection of operations to be performed over the data base as continuous operations, the distributions over which is learnt using the now-standard soft attention mechanisms.': 1.0986123085021973, 'The model is validated on the smallish WikiTableQuestions dataset, where the authors show that a single model performs worse than the approach which uses the traditional Semantic Parsing technique.': 1.0986121892929077, 'However an ensemble of 15 models (trained in a variety of ways) results in comparable performance to the state of the art.': 1.0985932350158691, 'I feel that the paper proposes an interesting solution to the hard problem of learning natural language interfaces for data bases.': 1.0979971885681152, 'The model is an extension of the previously proposed models of Neelakantan 2016.': 1.0986123085021973, 'The experimental section is rather weak though.': 1.0986123085021973, 'The authors only show their model work on a single smallish dataset.': 1.0986123085021973, 'Would love to see more ablation studies of their model and comparison against fancier version of memnns (i do not buy their initial response to not testing against memory networks).': 1.0986123085021973, 'I do have a few objections though.': 1.0986123085021973, 'The details of the model are rather convoluted and the Section 2.1 is not very clearly written.': 0.46297499537467957, 'In particular with the absence of the accompanying code the model will be super hard to replicate.': 1.0986096858978271, 'I wish the authors do a better job in explaining the details as to how exactly the discrete operations are modeled, what is the role of the ""row selector"", the ""scalar answer"" and the ""lookup answer"" etc.': 0.8791559934616089, 'The authors do a full attention over the entire database.': 1.0286353826522827, 'Do they think this approach would scale when the data bases are huge (millions of rows)?': 1.098576545715332, 'Wish they experimented with larger datasets as well.': 0.8035351037979126, 'This paper proposes a weakly supervised, end-to-end neural network model for solving a challenging natural language understanding task.': 0.9906133413314819, 'As an extension of the Neural Programmer, this work aims at overcoming the ambiguities imposed by natural language.': 1.0985780954360962, 'By predefining a set of operations, the model is able to learn the interface between the language reasoning and answer composition using backpropagation.': 1.0986123085021973, 'On the WikiTableQuestions dataset, it is able to achieve a slightly better performance than the traditional semantic parser methods.': 1.0934115648269653, 'Overall, this is a very interesting and promising work as it involves a lot of real-world challenges about natural language understanding.': 1.0986123085021973, 'The intuitions and design of the model are very clear, but the complication makes the paper a bit difficult to read, which means the model is also difficult to be reimplemented.': 1.0986121892929077, 'I would expect to see more details about model ablation and it would help us figure out the prominent parts of the model design.': 1.0986123085021973, 'This paper proposes a weakly supervised, end-to-end neural network model to learn a natural language interface for tables.': 1.0985833406448364, 'The neural programmer is applied to the WikiTableQuestions, a natural language QA dataset and achieves reasonable accuracy.': 1.0985889434814453, 'An ensemble further boosts the performance by combining components built with different configurations, and achieves comparable performance as the traditional natural language semantic parser baseline.': 1.0985655784606934, 'Dropout and weight decay seem to play a significant role.': 0.4194414019584656, ""It'll be interesting to see more error analysis and the major reason for the still low accuracy compared to many other NLP tasks."": 1.0964144468307495, ""What's the headroom and oracle number with the current approach?"": 1.0985116958618164}"
457,https://openreview.net/forum?id=ry3iBFqgl,"{'It would seem that the shelf life of a dataset has decreased rapidly in recent literature.': 1.0978885889053345, 'SQuAD dataset has been heavily pursued as soon as it hit online couple months ago, the best performance on their leaderboard now reaching to 82%.': 1.0986123085021973, 'This is rather surprising when taking into account the fact that the formal conference presentation of the dataset took place only a month ago at EMNLP’16, and that the reported machine performance (at the time of paper submission) was only at 51%.': 1.0985984802246094, 'One reasonable speculation is that the dataset may have not been hard enough.': 1.0977747440338135, 'NewsQA, the paper in submission, aims to address this concern by presenting a dataset of a comparable scale created through different QA collection strategies.': 1.0986123085021973, 'Most notably, the authors solicit questions without requiring answers from the same turkers, in order to promote more diverse and hard-to-answer questions.': 1.0986121892929077, 'Another notable difference is that the questions are gathered without showing the content of the news articles, and the dataset makes use of a bigger subset of CNN/Daily corpus (12K / 90K), as opposed to a much smaller subset (500 / 90K) used by SQuAD.': 1.0966033935546875, 'In sum, I think NewsQA dataset presents an effort to construct a harder, large-scale reading comprehension challenge, a recently hot research topic for which we don’t yet have satisfying datasets.': 1.0968047380447388, 'While not without its own weaknesses, I think this dataset presents potential values compared to what are available out there today.': 1.0985193252563477, 'That said, the paper does read like it was prepared in a hurry, as there are numerous small things that the authors could have done better.': 1.0977108478546143, 'As a result, I do wonder about the quality of the dataset.': 1.098608136177063, 'For one, human performance of SQuAD measured by the authors (70.5 - 82%) is lower than that reported by SQuAD (80.3 - 90.5%).': 1.0985952615737915, 'I think this sort of difference can easily happen depending on the level of carefulness the annotators can maintain.': 0.6424704790115356, 'After all, not all humans have the same level of carefulness or even the same level of reading comprehension.': 1.0649244785308838, 'I think it’d be the best if the authors can try to explain the reason behind these differences, and if possible, perform a more careful measurement of human performance.': 0.6725773215293884, 'If anything, I don’t think it looks favorable for NewsQA if the human performance is only at the level of 74.9%, as it looks as if the difficulty of the dataset comes mainly from the potential noise from the QA collection process, which implies that the low model performance could result from not necessarily because of the difficulty of the comprehension and reasoning, but because of incorrect answers given by human annotators.': 0.9586718082427979, 'I’m also not sure whether the design choice of not presenting the news article when soliciting the questions was a good one.': 1.059460163116455, 'I can imagine that people might end up asking similar generic questions when not enough context has been presented.': 0.9680314064025879, 'Perhaps taking a hybrid, what I would like to suggest is to present news articles where some sentences or phrases are randomly redacted, so that the question generators can have a bit more context while not having the full material in front of them.': 0.8814563155174255, 'Yet another way of encouraging the turkers from asking too trivial questions is to engage an automatic QA system on the fly — turkers must construct a QA pair for which an existing state-of-the-art system cannot answer correctly.': 1.0985549688339233, 'Paper Summary:': 1.0986123085021973, 'This paper presents a new comprehension dataset called NewsQA dataset, containing 100,000 question-answer pairs from over 10,000 news articles from CNN.': 1.0986123085021973, 'The dataset is collected through a four-stage process': 1.0986123085021973, 'article filtering, question collection, answer collection and answer validation.': 1.0986123085021973, 'Examples from the dataset are divided into different types based on answer types and reasoning required to answer questions.': 1.0986123085021973, 'Human and machine performances on NewsQA are reported and compared with SQuAD.': 1.0986123085021973, 'Paper Strengths:': 1.0986123085021973, 'I agree that models can benefit from diverse set of datasets.': 1.0986123085021973, 'This dataset is collected from news articles, hence might pose different sets of problems from current popular datasets such as SQuAD.': 1.0986123085021973, 'The proposed dataset is sufficiently large for data hungry deep learning models to train.': 1.0986123085021973, 'The inclusion of questions with null answers is a nice property to have.': 1.0986123085021973, 'A good amount of thought has gone into formulating the four-stage data collection process.': 1.0986123085021973, 'The proposed BARB model is performing as good as a published state-of-the-art model, while being much faster.': 1.0986123085021973, 'Paper Weaknesses:': 1.0986123085021973, 'Human evaluation is weak.': 1.0986123085021973, ""Two near-native English speakers' performance on 100 examples each can hardly be a representative of the complete dataset."": 1.0986123085021973, 'Also, what is the model performance on these 200 examples?': 1.0986123085021973, 'Not that it is necessary for this paper, but to clearly demonstrate that this dataset is harder than SQuAD, the authors should either calculate the human performance the same way as SQuAD or calculate human performances on both NewsQA and SQuAD in some other consistent manner on large enough subsets which are good representatives of the complete datasets.': 1.0986123085021973, 'Dataset from other communities such as VQA dataset (Antol et al., ICCV 2015) also use the same method as SQuAD to compute human performance.': 1.0986123085021973, 'Section 3.5 says that 86% of questions have answers agreed upon by atleast 2 workers.': 1.0986123085021973, 'Why is this number inconsistent with the 4.5% of questions which have answers without agreement after validation (last line in Section 4.1)?': 1.0986123085021973, 'Is the same article shown to multiple Questioners?': 1.0986123085021973, 'If yes, is it ensured that the Questioners asking questions about the same article are not asking the same/similar questions?': 1.0986123085021973, 'Authors mention that they keep the same hyperparameters as SQuAD.': 1.0986123085021973, 'What are the accuracies if the hyperparameters are tuned using a validation set from NewsQA?': 1.0986123085021973, '500 examples which are labeled for reasoning types do not seem enough to represent the complete dataset.': 1.0986123085021973, 'Also, what is the model performance on these 500 examples?': 1.0986123085021973, ""Which model's performance has been shown in Figure 1?"": 1.0986123085021973, 'Are the two ""students"" graduate/undergraduate students or researchers?': 1.0986123085021973, 'Test set seems to be very small.': 1.0986123085021973, 'Suggestion: Answer validation step is nice, but maybe the dataset can be released in 2 versions': 1.0986123085021973, 'one with all the answers collected in 3rd stage (without the validation step), and one in the current format with the validation step.': 1.0986123085021973, 'Preliminary Evaluation:': 0.5501433610916138, 'The proposed dataset is a large scale machine comprehension dataset collected from news articles, which in my suggestion, is diverse enough from existing datasets that state-of-the-art models can definitely benefit from it.': 1.0965100526809692, 'With a better human evaluation, I think this paper will make a good poster.': 0.47209566831588745, 'Summary: The paper proposes a novel machine comprehension dataset called NEWSQA. The dataset consists of over 100,000 question answer pairs based on over 10,000 news articles from CNN. The paper analyzes the different types of answers and the different types of reasoning required to answer questions in the dataset. The paper evaluates human performance and the performance of two baselines on the dataset and compares them with the performance on SQuAD dataset.': 1.0986061096191406, 'Strengths:': 0.9374216198921204, '1. The paper presents a large scale dataset for machine comprehension.': 1.0719923973083496, '2. The question collection method seems reasonable to collect exploratory questions. Having an answer validation step is desirable.': 1.0792168378829956, '3. The paper proposes a novel (computationally more efficient) implementation of the match-LSTM model.': 1.0657806396484375, 'Weaknesses:': 1.0986123085021973, '1. The human evaluation presented in the paper is not satisfactory because the human performance is reported on a very small subset (200 questions). So, it seems unlikely that these 200 questions will provide a reliable measure of the human performance on the entire dataset (which consists of thousands of questions).': 1.080398440361023, '2. NEWSQA dataset is very similar to SQuAD dataset in terms of the size of the dataset, the type of dataset': 0.9728834629058838, 'natural language questions posed by crowdworkers, answers comprising of spans of text from related paragraphs.': 0.8303382992744446, 'The paper presents two empirical ways to show that NEWSQA is more challenging than SQuAD': 1.0986123085021973, '1) the gap between human and machine performance in NEWSQA is larger than that in SQuAD.': 1.0970779657363892, 'However, since the human performance numbers are reported on very small subset, these trends might not carry over when human performance is computed on all of the dataset.': 1.0986123085021973, '2) the sentence-level accuracy on SQuAD is higher than that in NEWSQA.': 1.097358226776123, 'However, as the paper mentions, the differences in accuracies could likely be due to different lengths of documents in the two datasets.': 1.0981948375701904, 'So, even this measure does not truly reflect that SQuAD is less challenging than NEWSQA.': 1.0986121892929077, 'So, it is not clear if NEWSQA is truly more challenging than SQuAD.': 1.0986037254333496, '3. Authors mention that BARB is computationally more efficient and faster compared to match-LSTM. However, the paper does not report how much faster BARB is compared to match-LSTM.': 0.8462103009223938, '4. On page 7, under ""Boundary pointing"" paragraph, the paper should clarify what ""s"" in ""n_s"" refers to.': 1.0986123085021973, 'Review summary: While the dataset collection method seems interesting and promising, I would be more convinced after I see the following': 1.0986123085021973, '1. Human performance on all (or significant percentage of the dataset).': 1.0986123085021973, '2. An empirical study that fairly shows that NEWSQA is more challenging (or better in some other way) than SQuAD.': 1.0986123085021973}"
458,https://openreview.net/forum?id=ry4Vrt5gl,"{'This papers adds to the literature on learning optimizers/algorithms that has gained popularity recently.': 1.0965585708618164, 'The authors choose to use the framework of guided policy search at the meta-level to train the optimizers.': 1.098536491394043, 'They also opt to train on random objectives and assess transfer to a few simple tasks.': 1.0986123085021973, 'As pointed below, this is a useful addition.': 1.098607063293457, 'However, the argument of using RL vs gradients at the meta-level that appears below is not clear or convincing.': 1.0975936651229858, 'I urge the authors to run an experiment comparing the two approaches and to present comparative results.': 1.0986101627349854, 'This is a very important question, and the scalability of this approach could very well hinge on this fact.': 0.414133220911026, 'Indeed, demonstrating both scaling to large domains and transfer to those domains is the key challenge in this domain.': 1.098549485206604, 'In summary, the idea is a good one, but the experiments are weak.': 0.8201612234115601, 'The current version of the paper is improved w.r.t.': 1.0986027717590332, 'the original arXiv version from June.': 1.0986086130142212, 'While the results are exactly the same, the text does not oversell them as much as before.': 1.0927737951278687, 'You may also consider to avoid words like ""mantra"", etc.': 1.0986123085021973, 'I believe that my criticism given in my comment from 3 Dec 2016 about ""randomly generated task"" is valid and you answer is not.': 1.0986123085021973, 'This paper proposes an approach to learning a custom optimizer for a given class optimization problems.': 0.5300776362419128, 'I think in the case of training machine learning algorithms, a class would represent a model like “logistic regression”.': 0.775071382522583, 'The authors cleverly cast this as a reinforcement learning problem and use guided policy search to train a neural network to map the current location and history onto a step direction/magnitude.': 0.7495871782302856, 'Overall I think this is a great idea and a very nice contribution to the fast growing meta-learning literature.': 1.0838741064071655, 'However, I think that there are some aspects that could be touched on to make this a stronger paper.': 1.0956428050994873, 'My first thought is that the authors claim to train the method to learn the regularities of an entire class of optimization problems, rather than learning to exploit regularities in a given set of tasks.': 1.0986095666885376, 'The distinction here is not terribly clear to me.': 1.0745890140533447, 'For example, in learning an optimizer for logistic regression, the authors seem to claim that learning on a randomly sampled set of logistic regression problems will allow the model to learn about logistic regression itself.': 1.0985217094421387, 'I am not convinced of this, because there is bias in the randomly sampled data itself.': 0.9350557923316956, 'From the paper in this case, “The instances are drawn randomly from two multivariate Gaussians with random means and covariances, with half drawn from each.”': 0.8376380205154419, 'It seems the optimizer is then trained to optimize instances of logistic regression *given this specific family of training inputs* and not logistic regression problems in general.': 0.39187324047088623, 'A simple experiment to prove the method works more generally would be to repeat the existing experiments, but where the test instances are drawn from a completely different distribution.': 0.906091034412384, 'It would be even more interesting to see how this changes as the test distribution deviates further from the training distribution.': 1.0929323434829712, 'Can the authors comment on the choice of architecture used here?': 1.0986123085021973, 'Why one layer with 50 hidden units and softplus activations specifically?': 1.0982306003570557, 'Why not e.g., 100 units, 2 layers and ReLUs?': 1.098611831665039, 'Presumably this is to prevent overfitting, but given the limited capacity of the network, how do these results look when the dimensionality of the input space increases beyond 2 or 3?': 0.30236566066741943, 'I would love to see what kind of policy the network learns on e.g., a 2D function using a contour plot.': 1.0439914464950562, 'What do the steps look like on a random problem instance when compared to other hand-engineered optimizers?': 1.0565873384475708, 'Overall I think this a really interesting paper with a great methodological contribution.': 1.0986123085021973, 'My main concern is that the results may be oversold as the problems are still relatively simple and constrained.': 1.090444803237915, 'However, if the authors can demonstrate that this approach produces robust policies for a very general set of problems then that would be truly spectacular.': 0.08274880051612854, 'Minor notes below.': 1.0986123085021973, 'Section 3.1 should you be using \\pi_T^* to denote the optimal policy?': 1.0985560417175293, 'You use \\pi_t^* and \\pi^* currently.': 1.0986121892929077, 'Are the problems here considered noiseless?': 1.0986123085021973, 'That is, is the state transition given an action deterministic?': 1.0986123085021973, 'It would be very interesting to see this on noisy problems.': 1.0986123085021973}"
459,https://openreview.net/forum?id=ry54RWtxx,"{'This paper takes a first step towards learning to statically analyze source code.': 0.4164302349090576, 'It develops a simple toy programming language that includes loops and branching.': 1.0982648134231567, 'The aim is to determine whether all variables in the program are defined before they are used.': 1.0905711650848389, ""The paper tries a variety of off-the-shelf sequence classification models and develops a new model that makes use of a ``differentiable set'' to keep track of which variables have been defined so far."": 1.0986111164093018, 'Result show that an LSTM model can achieve 98% accuracy, and the differentiable set model can achieve 99.3% accuracy with sequence-level supervision and 99.7% accuracy with strong token-level supervision.': 1.0195719003677368, 'An additional result is used whereby an LSTM language model is trained over correct code, and then low probability (where a threshold to determine low is tuned by hand) tokens are highlighted as sources of possible error.': 1.0986123085021973, 'One further question is if the authors could clarify what reasoning patterns are needed to solve these problems.': 1.0982141494750977, 'Does the model need to, e.g., statically determine whether an `if` condition can ever evaluate to true in order to solve these tasks?': 1.0986119508743286, 'Or is it just as simple as checking whether a variable appears on a LHS before it appears on a RHS later in the textual representation of the program?': 0.6494188904762268, 'Strengths:': 1.098405122756958, 'Learning a static analyzer is an interesting concept, and I think there is good potential for this line of work': 0.2279880791902542, 'The ability to determine whether variables are defined before they are used is certainly a prerequisite for more complicated static analysis.': 1.0881373882293701, 'The experimental setup seems reasonable': 1.0986123085021973, 'The differentiable set seems like a useful (albeit simple) modelling tool': 1.0985547304153442, 'Weaknesses:': 1.0986123085021973, ""The setup is very toy, and it's not clear to me that this makes much progress towards the challenges that would arise if one were trying to learn a static analyzer"": 1.0051047801971436, 'The models are mostly very simple.': 1.0783809423446655, ""The one novelty on the modelling front (the differentiable set) provides a small win on this task, but it's not clear if it is a useful general construct or not."": 1.0568180084228516, 'Overall:': 1.0986087322235107, ""I think it's an interesting start, and I'm eager to see how this line of work progresses."": 1.098538875579834, ""In my opinion, it's a bit too early to accept this work to ICLR, but I'd be excited about seeing what happens as the authors try to push the system to learn to analyze more properties of code, and as they push towards scenarios where the learned static analyzer would be useful, perhaps leveraging strengths of machine learning that are not available to standard programming languages analyses."": 1.0985703468322754, 'The authors are trying to understand whether static analysis can be learned.': 1.0986123085021973, 'As I hinted in my question, I think that all of the interesting complexity of static analysis has been removed in the toy language': 1.0986123085021973, 'extraordinarily simple logic using a set can solve the problem posed, and an LSTM (unsurprisingly) can learn the extraordinarily simple logic (when given a differentiable set object).': 1.0986123085021973, 'This extreme simplicity gives me no confidence that a more realistic static analysis problem can be solved.': 1.0986123085021973, 'LSTMs (and deep learning) have had remarkable successes in solving messy real-world language problems.': 1.0986123085021973, ""It's certainly possible that LSTMs could solve static analysis"": 1.0986123085021973, 'but being technically timid is not the right way to go about it.': 1.0986123085021973, 'The authors explore the idea of deep-learning a static analyzer.': 1.0986123085021973, 'They do it with a toy programming language and a very simplified analysis problem': 1.0986123085021973, 'just checking if all variables are initalized.': 1.0986123085021973, 'While the idea is interesting and might be developped into a tool in the future, the toy task presented in this paper is too simple to warrant an ICLR submission.': 1.0986123085021973, 'Just detecting whether a variable is initialized in a string is a toy algorihtmic task, similar to the ones solved in a number of paper in recent years by models such as the Neural Turing Machine, Stack RNNs, Neural GPU, or Differentiable Neural Computer.': 1.0986123085021973, 'All these architectures perform almost perfectly on a number of algorithmic tasks, so it is highly probable that they would also solve this one.': 1.0986123085021973, 'Unluckily, the authors only compare to much more basic models, such as HMMs.': 1.0986123085021973, 'Since the code for many of the above-mentioned models is available online, a paper without these baselines is not ready for ICLR.': 1.0986123085021973, 'Moreover, there is a risk that existing models already solve this problem very well, making the contribution unclear.': 1.0986123085021973}"
460,https://openreview.net/forum?id=ry7O1ssex,"{'Our understanding of GAN to date is still vague.': 1.3579158782958984, 'Although there have been some efforts relating GAN to energy models, I personally consider that the perspective of this paper, namelying understanding GAN (a variant of GAN, to be more precise) as variational training of an energy model is the most natural and elegant.': 1.3796051740646362, 'The derivation up to equation (5) and the reduction to (7) are very nice.': 1.386051058769226, 'I think this is the most important contribution of the paper.': 1.3745386600494385, 'The techniques introduced in sections 5 and 6 are somewhat ad hoc, and lack clarity.': 1.1515250205993652, 'Referring to the version I looked at (not sure though if it is the latest), section 6 contain some errors/typos (stuff around p_z(x|\\tilde{x}).': 1.3862886428833008, 'The presentation of section 6 needs to improve in clarity.': 1.385263442993164, 'But I think this does not shadow the main contribution of the paper, namely, that perspectives given in sections 2-4.': 1.3861008882522583, 'Overall I very much enjoy the presented insight of this paper into GAN.': 1.386061668395996, 'I do have some comment/question regarding equation (7).': 1.3355555534362793, 'This equation formulates a variant of GAN, or a model resembling GAN.': 1.3086802959442139, 'I am happy to see that the entropy term pops up there, which should save GAN from degenerating its generative distribution or from missing modes.': 1.386247158050537, 'The swapping of the min-max order in this formulation however makes me wonder if this variant of GAN indeed reflects the ""principle"" of GAN, or it is in fact a different principle, which happens to gives rise to a model  that ""resembles"" GAN.': 1.3862926959991455, ""Of course, my question may be merely philosophical rather than mathematical, and I won't expect a precise anwer."": 1.3854626417160034, 'Nonetheless, if the author can provide additional insignts on this, it would be appreciated.': 1.3862582445144653, ""The paper drives a variant of GAN's min max game optimizations from EBM's NLL minimization and propose a new generative model from this derivation."": 1.3862943649291992, 'In introduction, could you elaborate why accurate distance metric is needed for unsupervised learning.': 1.3862943649291992, ""It's a bit hard to read the paper as it jumps from points to points without clear connection and laying down the background."": 1.386293888092041, 'The introduction, refers to many different concepts without any clear connection.': 1.3862943649291992, 'And other sections as well is very incomprehensible even if one is familiar with the concepts.': 1.3862943649291992, 'There has been lots of interests in similar area recently, Authors do cite some of them in introduction and related work but the relationship and comparison are missing.': 1.3588452339172363, ""In the results, if it's comparison of the quality of samples it has to include other recent works and compare those as well."": 1.3862403631210327, ""And if it's semi-supervised learning, again, the numbers should be compared to other works."": 1.3862943649291992, 'In summary, unfortunately the paper is very clear and cumbersome to read which makes it hard to judge it fairly.': 1.3862943649291992, 'I strongly suggest re-write of the paper in more coherent matter to make it easier to read.': 1.3862943649291992, 'And also extend the experiments with more comparisons with other works.': 1.3862943649291992, 'This paper presents an adversarial training formulation for energy models.': 1.3862943649291992, 'Although the relationship between energy-based models and GANs is abundantly clear in the literature, the contributions of this paper seems to be a multimodal energy estimate, and training a transition operator instead of a sampler.': 1.3108106851577759, 'The core technical contribution of the paper is not clear.': 1.3862943649291992, ""The only two quantitative results in the paper demonstrate that the proposed method learns better features than a traditional GAN, and that the proposed method does a better job in SSL than the authors' chosen baseline."": 1.159277081489563, 'In either case, no comparison to existing literature is made, even though GANs have been used for SSL previously (https://arxiv.org/abs/1606.03498).': 1.363304615020752, 'The qualitative results (visual comparison of quality of samples) is inconclusive.': 1.3862943649291992, 'The lack of comparison to existing literature makes this paper a clear reject.': 1.3862943649291992, 'To improve this paper, the authors need to make the core contribution of the paper much better, and situate the paper very clearly wrt existing literature.': 1.3862667083740234, 'The motivation for the development of the model should be clearer.': 1.3862943649291992, 'Further, experiments need to compare with existing literature.': 1.3862943649291992, 'This paper presents a bridging of energy-based models and GANs, where': 1.3862943649291992, 'starting from the energy-based formalism': 1.3862943649291992, 'they derive an additional entropy term that is not present in the original GAN formulation and is also absent in the EBGAN model of Zhao et al (2016, cited work).': 1.3862943649291992, 'The relation between GANs and energy-based models was, as far as I know, first described in Kim and Bengio (2016, cited work) who also introduced the entropy regularization.': 1.3862943649291992, 'It is also discussed in another ICLR submission (Dai et al.': 1.3862943649291992, '""Calibrating Energy-based Generative Adversarial Networks"").': 1.3862943649291992, 'There are two novel contribution of this paper: (1) VGAN: the introduction of a novel entropy approximation; (2) VCD: variational contrastive divergence is introduced as a  novel learning algorithm (however, in fact, a different model).': 1.3862943649291992, 'The specific motivation for this second contribution is not particularly clear.': 1.3862943649291992, 'The two contributions offered by the authors are quite interesting and are well motivated in the sense that the address the important problem of avoiding dealing with the generally intractable entropy term.': 1.3862943649291992, 'However, unfortunately the authors present no results directly supporting either contribution.': 1.3862943649291992, 'For example, it is not at all clear what role, if any, the entropy approximation plays in the samples generated from the VGAN model.': 1.3862943649291992, 'Especially in the light of the impressive samples from the EBGAN model that has no corresponding term.': 1.3862943649291992, 'The results provided to support the VCD algorithm, come in the form of a comparison of samples and reconstructions.': 1.3862943649291992, 'But the samples provided here strike me as slightly less impressive compared to either the VGAN results or the SOTA in the literature - this is, of course, difficult to evaluate.': 1.3862943649291992, 'The results the authors do present include qualitative results in the form of reasonably compelling samples from the model trained on CIFAR-10, MNIST and SVHN datasets.': 1.3862943649291992, 'They also present quantitative results int he form of semi-supervised learning tasks on the MNIST and SVHN.': 1.3862943649291992, 'However these': 1.3862943649291992, 'quantitative results are not particularly compelling as they show limited improvement over baselines.': 1.3862943649291992, 'Also, there is no reference to the many': 1.3862943649291992, 'existing semi-supervised results on these datasets.': 1.3862943649291992, 'Summary: The authors identify an important problem and offer two novel and intriguing solutions. Unfortunately the results to not sufficiently support either': 1.3862943649291992, 'contribution.': 1.3862943649291992}"
461,https://openreview.net/forum?id=ryAe2WBee,"{'The paper proposes a semantic embedding based approach to multilabel classification.': 1.0986121892929077, 'Conversely to previous proposals, SEM considers the underlying parameters determining the': 1.0922352075576782, 'observed labels are low-rank rather than that the observed label matrix is itself low-rank.': 1.0985983610153198, 'However, It is not clear to what extent the difference between the two assumptions is significant': 1.0984768867492676, 'SEM models the labels for an instance as draws from a multinomial distribution': 1.0986098051071167, 'parametrized by nonlinear functions of the instance features.': 0.791736364364624, 'As such, it is a neural network.': 0.23771008849143982, 'The proposed training algorithm is slightly more complicated than vanilla backprop.': 1.0852431058883667, 'The significance of the results compared to NNML (in particular on large datasets Delicious and EUrlex) is not very clear.': 1.0986123085021973, 'The paper is well written and the main idea is clearly presented.': 1.0986119508743286, 'However, the experimental results are not significant enough to compensate the lack of conceptual novelty.': 1.0986123085021973, 'The paper presents the semantic embedding model for multi-label prediction.': 1.0974241495132446, ""In my questions, I pointed that the proposed approach assumes the number of labels to predict is known, and the authors said this was an orthogonal question, although I don't think it is!"": 1.0986123085021973, 'I was trying to understand how different is SEM from a basic MLP with softmax output which would be trained with a two step approach instead of stochastic gradient descent.': 1.0986123085021973, 'It seems reasonable given their similarity to compare to this very basic baseline.': 1.0986123085021973, 'Regarding the sampling strategy to estimate the posterior distribution, and the difference with Jean et al, I agree it is slightly different but I think you should definitely refer to it and point to the differences.': 1.0986088514328003, 'One last question: why is it called ""semantic"" embeddings?': 1.0986123085021973, ""usually this term is used to show some semantic meaning between trained embeddings, but this doesn't seem to appear in this paper."": 0.6089275479316711, 'This paper proposes SEM, a simple large-size multilabel learning algorithm which models the probability of each label as softmax(sigmoid(W^T X) + b), so a one-layer hidden network.': 1.0986123085021973, 'This in and of itself is not novel, nor is the idea of optimizing this by adagrad.': 1.0986123085021973, ""Though it's weird that the paper explicitly derives the gradient and suggests doing alternating adagrad steps instead of the more standard adagrad steps; it's unclear whether this matters at all for performance."": 1.0986123085021973, 'The main trick responsible for increasing the efficiency of this model is the candidate label sampling, which is done in a relatively standard way by sampling labels proportionally to their frequency in the dataset.': 1.0986123085021973, ""Given that neither the model nor the training strategy is novel, it's surprising that the results are better than the state-of-the-art in quality and efficiency (though non-asymptotic efficiency claims are always questionable since implementation effort trades off fairly well against performance)."": 1.0986123085021973, ""I feel like this paper doesn't quite meet the bar."": 1.0984958410263062}"
462,https://openreview.net/forum?id=ryCcJaqgl,"{'Updated review: the authors did an admirable job of responding to and incorporating reviewer feedback.': 0.810518741607666, 'In particular, they put a lot of effort into additional experiments, even incorporating a new and much stronger baseline (the ConvNet -> LSTM baseline requested by multiple reviewers).': 0.5636642575263977, 'I still have two lingering concerns previously stated': 1.0491979122161865, ""that each model's architecture (# hidden units, etc.) should be tuned independently and that a pure time series forecasting baselines (without the trend preprocessing) should be tried."": 1.0986123085021973, ""I'm going to bump up my score from a clear rejection to a borderline."": 1.0986123085021973, 'This paper is concerned with time series prediction problems for which the prediction targets include the slope and duration of upcoming local trends.': 1.0986123085021973, 'This setting is of great interest in several real world problem settings (e.g., financial markets) where decisions (e.g., buy or sell) are often driven by local changes and trends.': 1.0986117124557495, 'The primary challenge in these problems is distinguishing true changes and trends (i.e., a downturn in share price) from noise.': 1.0985723733901978, 'The authors tackle this with an interesting hybrid architecture (TreNet) with four parts: (1) preprocessing to extract trends, (2) an LSTM that accepts those trends as inputs to ostensibly capture long term dependencies, (3) a ConvNet that accepts a local window of raw data as its input at each time step, and (4) a higher ""feature fusion"" (i.e., dense) layer to combine the LSTM\'s and ConvNet\'s outputs.': 1.0976084470748901, 'On three univariate time series data sets, the TreNet outperforms the competing baselines including those based on its constituent parts (LSTM + trend inputs, CNN).': 1.0985944271087646, 'Strengths:': 1.0986123085021973, 'A very interesting problem setting that can plausibly be argued to differ from other sequential modeling problems in deep learning (e.g., video classification).': 1.0986019372940063, 'This is a nice example of fairly thoughtful task-driven machine learning.': 1.0922178030014038, ""Accepting the author's assumptions as true for the moment, the proposed architecture seems intuitive and well-designed."": 0.75951087474823, 'Weaknesses:': 1.0855481624603271, 'Although this is an interesting problem setting (decisions driven by trends and changes), the authors did not make a strong argument for why they formulated the machine learning task as they did.': 1.0985807180404663, 'Trend targets are not provided from ""on high"" (by data oracle) but extracted from raw data using a deterministic algorithm.': 1.0986123085021973, 'Thus, one could just easily formulate this as plain time series forecasting problem in which we forecast the next 100 steps and then apply the trend extractor to convert those predictions into a trend.': 1.0986123085021973, 'If the forecasts are accurate, so will be the extracted trends.': 1.0985795259475708, 'The proposed architecture, while interesting, is not justified, in particular the choice to feed the extracted trends and raw data into separate LSTM and ConvNet layers that are only combined at the end by a shallow MLP.': 1.087829828262329, 'An equally straightforward but more intuitive choice would have been to feed the output of the ConvNet into the LSTM, perhaps augmented by the trend input.': 1.0986113548278809, 'Without a solid rationale, this unconventional choice comes across as arbitrary.': 0.4964786469936371, 'Following up on that point, the raw->ConvNet->LSTM and {raw->ConvNet,trends}->LSTM architectures are natural baselines for experiments.': 1.0985807180404663, 'The paper presupposes, rather than argues, the value of the extracted trends and durations as inputs.': 0.6152496933937073, 'It is not unreasonable to think that, with enough training data, a sufficiently powerful ConvNet->LSTM architecture should be able to learn to detect these trends in raw data, if they are predictive.': 0.43779733777046204, 'Following up on that point, two other obvious baselines that were omitted: raw->LSTM and {raw->ConvNet,trends}->MLP.': 1.0986123085021973, 'Basically, the authors propose a complex architecture without demonstrating the value of each part (trend extraction, LSTM, ConvNet, MLP).': 1.0986117124557495, 'The baselines are unnecessarily weak.': 1.0918591022491455, 'One thing I am uncertain about in general: the validity of the practice of using the same LSTM and ConvNet architectures in both the baselines and the TreNet.': 0.9733132719993591, 'This *sounds* like an apples-to-apples comparison, but in the world of hyperparameter tuning, it could in fact disadvantage either.': 0.6375575065612793, 'It seems like a more thorough approach would be to optimize each architecture independently.': 1.0985863208770752, 'Regarding related work and baselines: I think it is fair to limit the scope of in-depth analysis and experiments to a set of reasonable, representative baselines, at least in a conference paper submitted to a deep learning conference.': 0.3615007996559143, 'That said, the authors ignored a large body of work on financial time series modeling using probabilistic models and related techniques.': 1.0986123085021973, 'This is another way to frame the above ""separate trends from noise"" problem: treat the observations as noisy.': 1.0986123085021973, 'One semi-recent example: J. Hernandez-Lobato, J. Lloyds, and D. Hernandez-Lobato.': 1.0986123085021973, 'Gaussian process conditional copulas with applications to financial time series.': 1.0985896587371826, 'NIPS 2013.': 1.0986123085021973, 'I appreciate this research direction in general, but at the moment, I believe that the work described in this manuscript is not suitable for inclusion at ICLR.': 1.0917037725448608, 'My policy for interactive review is to keep an open mind and willingness to change my score, but a large revision is unlikely.': 1.078611135482788, 'I would encourage the authors to instead use their time and energy': 1.0986119508743286, 'and reviewer feedback': 1.0986123085021973, 'in order to prepare for a future conference deadline (e.g., ICML).': 1.0986075401306152, '1) Summary': 1.0986123085021973, 'This paper proposes an end-to-end hybrid architecture to predict the local linear trends of time series.': 1.0635374784469604, 'A temporal convnet on raw data extracts short-term features.': 0.5779765248298645, 'In parallel, long term representations are learned via a LSTM on piecewise linear approximations of the time series.': 1.0827926397323608, 'Both representations are combined using a MLP with one hidden layer (in two parts, one for each stream), and the entire architecture is trained end-to-end by minimizing (using Adam) the (l2-regularized) euclidean loss w.r.t.': 1.0986123085021973, 'ground truth local trend durations and slopes.': 1.0986123085021973, '2) Contributions': 1.0986123085021973, '+ Interesting end-to-end architecture decoupling short-term and long-term representation learning in two separate streams in the first part of the architecture.': 0.5907617211341858, '+': 1.0986123085021973, 'Comparison to deep and shallow baselines.': 1.0986123085021973, '3) Suggestions for improvement': 1.0986123085021973, 'Add a LRCN baseline and discussion:': 1.0986123085021973, 'The benefits of decoupling short-term and long-term representation learning need to be assessed by comparing to the popular ""long-term recurrent convolutional network"" (LRCN) of Donahue et al (https://arxiv.org/abs/1411.4389).': 1.0986123085021973, 'This approach stacks a LSTM on top of CNN features and is typically used on time series of video frames for tasks that are more general than local linear trend prediction.': 1.0986123085021973, 'Furthermore, LRCN does not require the hand-crafted preprocessing of time series to extract piecewise linear approximations needed by the LSTM of the TreNet architecture proposed here.': 1.0953161716461182, 'Finally, LRCN might be more parameter efficient, as it does not have the fully connected fusion layers of TreNet (eq. 8).': 0.766180157661438, 'Add more complex multivariate datasets:': 0.40581363439559937, 'The currently used 3 datasets are limited, especially compared to modern research in representation learning for time series forecasting.': 1.0156095027923584, 'For instance, and of particular interest to ICLR, I would suggest investigating future frame prediction on natural video datasets like UCF101 where CNN+LSTM are typically used albeit with a more complex loss (cf. for instance the popular adversarial one of Mathieu et al).': 1.0986032485961914, 'Although different from the task of local linear trend prediction, it would be interesting to see how TreNet could be applied to the encoder stage of existing encoder-decoder architectures for frame prediction.': 1.0976899862289429, 'It seems that decoupling short term and long term motion representation learning (for instance) could be beneficial in natural videos, as they often contain fast object motions together with slower camera ones.': 1.0986123085021973, 'Clarification about the target variables:': 1.0986123085021973, 'The authors need to clarify whether they handle separately or jointly the duration and slope.': 0.9618940949440002, 'The text is ambiguous and seems to suggest training two separate models, one for slope, one for duration, which is particularly puzzling considering that predicting them jointly is in fact much easier (just two output variables instead of one), makes more sense, and is entirely feasible with the current method.': 1.0868825912475586, 'Other parts of the text can be improved too.': 1.0756889581680298, 'For instance, the authors can vastly compress the generic description of standard convnet and LSTM equations in section 4, while the preprocessing of the time series needs to appear much earlier.': 1.0986123085021973, '4) Conclusion': 1.0986123085021973, 'Although the architecture seems promising, the current experiments are too preliminary to validate its usefulness, in particular to existing alternatives like LRCN, which are not compared to.': 1.0986123085021973, 'Revision of the review:': 1.0986119508743286, 'The authors did a commendable job of including additional references and baseline experiments.': 1.098323106765747, 'This paper presents a hybrid architecture for time series prediction, focusing on the slope and duration of linear trends.': 1.0986018180847168, 'The architecture consists of combining a 1D convnet for local time series and an LSTM for time series of trend descriptors.': 1.0985729694366455, 'The convnet and LSTM features are combined into an MLP for predicting either the slope or the duration of the next trend in a 1D time series.': 1.0985383987426758, 'The method is evaluated on 3 small datasets.': 1.0986108779907227, 'Summary:': 1.0613430738449097, 'This paper, while relative well written and presenting an interesting approach, has several methodology flaws, that should be handled by new experiments.': 1.0985904932022095, 'Pros:': 0.41005444526672363, 'The idea of extracting upward or downward trends from time series - although these should, ideally be learned, not rely on an ad-hoc technique, given that this is a submission for ICLR.': 1.098610758781433, 'Methodology:': 1.0858010053634644, '*': 1.0986123085021973, 'In section 3, what do you mean by predicting “either [the duration]': 0.898423433303833, 'or [slope]': 1.0985772609710693, '” of the trend?': 0.7859968543052673, 'Predictions are valid only if those two predictions are done jointly.': 1.09861159324646, 'The two losses should be combined during training.': 1.0586587190628052, '* In the entire paper, the trend slope and duration need to be predicted jointly.': 1.0980618000030518, 'Predicting a time series without specifying the horizon of the prediction is meaningless.': 0.9014168977737427, 'If the duration of the trends is short, the time series could go up or down alternatively; if the duration of the trend is long, the slope might be close to zero.': 1.0848932266235352, 'Predictions at specific horizons are needed.': 0.6060179471969604, '* In general, time series prediction for such applications as trading and load forecasting is pointless if no decision is made.': 0.7866500020027161, 'A trading strategy would be radically different for short-term and noisy oscillations or from long-term, stable upward or downward trend.': 1.0986123085021973, 'An actual evaluation in terms of trading profit/loss should be added for each of the baselines, including the naïve baselines.': 1.0986121892929077, 'As mentioned earlier in the pre-review questions, an important baseline is missing: feeding the local time series to the convnet and connecting the convnet directly to the LSTM, without ad-hoc trend extraction.': 1.0986120700836182, 'The convnet -> LSTM architecture would need an analysis of the convnet filters and trend prediction representation.': 1.0811866521835327, '* Trend prediction/segmentation by the convnet could be an extra supervised loss.': 1.0986123085021973, 'The detailed analysis of the trend extraction technique is missing.': 1.0986123085021973, 'In section 5, the SVM baselines have local trend and local time series vectors concatenated.': 1.0986123085021973, 'Why isn’t the same approach used for LSTM baselines (as a multivariate input)': 0.6763086318969727, 'and why the convnet operates only on local': 1.084830641746521, '* An important, “naïve” baseline is missing: next local trend slope and duration = previous local trend slope and duration.': 1.0793942213058472, 'Missing references:': 1.098608374595642, 'The related work section is very partial and omits important work in hybrid convnet + LSTM architectures, in particular:': 0.6889467835426331, 'Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru.': 0.5020619630813599, 'Show and tell: A neural image caption generator.': 0.5047138929367065, 'CoRR, abs/1411.4555, 2014.': 0.6948020458221436, 'Donahue, Jeff, Hendricks, Lisa Anne, Guadarrama, Sergio, Rohrbach, Marcus, Venugopalan, Subhashini, Saenko, Kate, and Darrell, Trevor.': 0.786949634552002, 'Long-term recurrent convolutional networks for visual recognition and description.': 0.3052765727043152, 'CoRR, abs/1411.4389, 2014.': 0.41588887572288513, 'Karpathy, Andrej, Toderici, George, Shetty, Sanketh, Leung, Thomas, Sukthankar, Rahul, and Fei-Fei, Li.': 0.38393381237983704, 'Large-scale video classification with convolutional neural networks.': 1.0461945533752441, 'In CVPR, 2014.': 0.646881639957428, 'The organization of the paper needs improvement:': 1.0821759700775146, '* Section 3 does not explain the selection of the maximal tolerable variance in each trend segment.': 0.9673724174499512, 'The appendix should be moved to the core part of the paper.': 1.0591102838516235, '* Section 4 is unnecessarily long and gives well known details and equations about convnets and LSTMs.': 0.4135218858718872, 'The only variation from standard algorithm descriptions is that   are concatenated.': 0.9451857209205627, 'The feature fusion layer can be expressed by a simple MLP on the concatenation of R(T(l)) and C(L(t)).': 0.7943296432495117, 'Details could be moved to the appendix.': 1.0029149055480957, 'Additional questions:': 1.098414421081543, '*In section 5, how many datapoints are there in each dataset?': 1.0617693662643433, 'Listing only the number of local trends is uninformative.': 0.30040469765663147, 'Typos:': 1.0972189903259277, '* p. 5, top “duration and slop”': 1.0434367656707764}"
463,https://openreview.net/forum?id=ryEGFD9gl,"{'The paper discusses sub modular sum-product networks as a tractable extension for classical sum-product networks.': 1.0986123085021973, 'The proposed approach is evaluated on semantic segmentation tasks and some early promising results are provided.': 1.0986123085021973, 'Summary:': 1.0986123085021973, '———': 1.0986123085021973, 'I think the paper presents a compelling technique for hierarchical reasoning in MRFs but the experimental results are not yet convincing.': 1.0986123085021973, 'Moreover the writing is confusing at times.': 1.0986123085021973, 'See below for details.': 1.0986123085021973, 'Quality: I think some of the techniques could be described more carefully to better convey the intuition.': 1.0986123085021973, 'Clarity: Some of the derivations and intuitions could be explained in more detail.': 1.0986123085021973, 'Originality: The suggested idea is great.': 1.0986123085021973, 'Significance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.': 0.7202064990997314, 'Detailed comments:': 1.0986123085021973, ""1. I think the clarity of the paper would benefit significantly from fixes to inaccuracies. E.g., \\alpha-expansion and belief propagation are not `scene-understanding algorithms’ but rather approaches for optimizing energy functions. Computing the MAP state of an SSPN in time sub-linear in the network size seems counterintuitive because it means we are not allowed to visit all the nodes in the network. The term `deep probabilistic model’ should probably be defined. The paper states that InferSSPN computes `the approximate MAP state of the SSPN (equivalently, the optimal parse of the image)’ and I’m wondering how the `approximate MAP state' can be optimal. Etc."": 1.098605990409851, '2. Albeit being formulated for scene understanding tasks, no experiments demonstrate the obtained results of the proposed technique. To assess the applicability of the proposed approach a more detailed analysis is required. More specifically, the technique is evaluated on a subset of images which makes comparison to any other approach impossible. According to my opinion, either a conclusive experimental evaluation using, e.g., IoU metric should be given in the paper, or a comparison to publicly available results is possible.': 0.5527284741401672, '3. To simplify the understanding of the paper a more intuitive high-level description is desirable. Maybe the authors can even provide an intuitive visualization of their approach.': 0.49807366728782654, 'This paper is about submodular sum product networks applied to scene understanding.': 1.0986123085021973, 'SPNs have shown great success in deep linear models since the work of Poon 2011.': 1.0986123085021973, 'The authors propose an extension to the initial SPNs model to be submodular, introducing submodular unary and pairwise potentials.': 1.098567247390747, 'The authors propose a new inference algorithm.': 1.0986123085021973, 'The authors evaluated their results on Stanford Background Dataset and compared against multiple baselines.': 0.4144800007343292, 'Pros:': 1.0986123085021973, '+ New formulation of SPNs': 1.0986123085021973, '+ New inference algorithm': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'The authors did not discuss how the SSPN structure is learned and how the generative process chooses the a symbol (operation) at each level)': 1.0986123085021973, 'The evaluations is lacking.': 1.0986123085021973, 'The authors only showed results on their own approach and baselines, leaving out every other approach.': 1.0986123085021973, 'Evaluations could have been also done on BSD for regular image segmentation (hierarchical segmentation).': 1.0986123085021973, 'The idea is great, however, the paper needs more work to be published.': 1.0986123085021973, 'I would also recommend for the authors to include more details about their approach and present a full paper with extended experiments and full learning approach.': 1.0986123085021973, 'This paper develops Submodular Sum Product Networks (SSPNs) and': 1.0986123085021973, 'an efficient inference algorithm for approximately computing the': 1.0986123085021973, 'most probable labeling of variables in the model.': 1.0986123085021973, 'The main': 1.0986123085021973, 'application in the paper is on scene parsing.': 1.0986123085021973, 'In this context,': 1.0986123085021973, 'SSPNs define an energy function with a grammar component for': 1.0986123085021973, 'representing a hierarchy of labels and an MRF for encoding': 1.0986123085021973, 'smoothness of labels over space.': 1.0986123085021973, 'To perform inference,': 1.0986123085021973, 'the': 1.0986123085021973, 'authors develop a move-making algorithm, somewhat in the spirit': 1.0986123085021973, 'of fusion moves (Lempitsky et al., 2010) that repeatedly improves': 1.054876685142517, 'a solution by considering a large neighborhood of alternative segmentations': 0.9689668416976929, 'and solving an optimization problem to choose the best neighbor.': 0.47508496046066284, 'Empirical results show that the proposed algorithm achieves better': 1.098591685295105, 'energy that belief propagation of alpha expansion and is much faster.': 0.4894118010997772, 'This is generally a well-executed paper.': 1.0985318422317505, 'The model is interesting': 1.0961062908172607, 'and clearly defined, the algorithm is well presented with proper': 1.0279558897018433, 'analysis of the relevant runtimes and guarantees on the': 0.4622308313846588, 'behavior.': 1.0986123085021973, 'Overall, the algorithm seems effective at minimizing': 1.098611831665039, 'the energy of SSPN models.': 0.43227818608283997, ""Having said that, I don't think this paper is a great fit for"": 1.098130464553833, 'ICLR.': 0.7831975817680359, 'The model is even somewhat to the antithesis of the idea of': 1.0950982570648193, 'learning representations, in that a highly structured form of': 0.9182500243186951, 'energy function is asserted by the human modeller, and then': 0.9852932095527649, 'inference is performed.': 1.0986109972000122, ""I don't see the connection to learning"": 1.098584771156311, 'representations.': 0.9649534225463867, 'One additional issue is that while the proposed': 0.9919412732124329, 'algorithm is faster than alternatives, the times are still on the': 1.098326325416565, 'order of 1-287 seconds per image, which means that the': 1.0986123085021973, 'applicability of this method (as is) to something like training': 1.0986123085021973, 'ConvNets is limited.': 1.0986123085021973, 'Finally, there is no attempt to argue that the model produces': 1.0986123085021973, 'better segmentations than alternative models.': 1.0986123085021973, 'The only': 1.0986123085021973, 'evaluations in the paper are on energy values achieved and on': 1.0986123085021973, 'training data.': 1.0986123085021973, 'So overall I think this is a good paper that should be published': 1.0986123085021973, ""at a good machine learning conference, but I don't think ICLR is"": 1.0986123085021973, 'the right fit.': 1.0986123085021973}"
464,https://openreview.net/forum?id=ryF7rTqgl,"{'This paper proposes a method that attempts to ""understand"" what is happening within a neural network by using linear classifier probes which are inserted at various levels of the network.': 1.0985634326934814, 'I think the idea is nice overall because it allows network designers to better understand the representational power of each layer in the network, but at the same time, this works feels a bit rushed.': 1.097756028175354, 'In particular, the fact that the authors did not provide any results in ""real"" networks, which are used to win competitions makes the results less strong, since researchers who want to created competitive network architectures don\'t have enough evidence from this work to decides whether they should use it or not.': 1.0986098051071167, 'Ideally, I would encourage the authors to consider continuing this line of research and show how to use the information given by these linear classifiers to construct better network architectures.': 1.098544716835022, ""Unfortunately, as is, I don't think we have enough novelty to justify accepting this work in the conference."": 0.9913003444671631, 'This paper proposes to use a linear classifier as the probe for the informativeness of the hidden activations from different neural network layers.': 1.0986123085021973, 'The training of the linear classifier does not affect the training of the neural network.': 1.0986123085021973, 'The paper is well motivated for investigating how much useful information (or how good the representations are) for each layer.': 1.0986123085021973, 'The observations in this paper agrees with existing insights, such as, 1) (Fig 5a) too many random layers are harmful.': 1.0986123085021973, '2) (Fig 5b) training is helpful.': 1.0986123085021973, '3) (Fig 7) lower layers converge faster than higher layer.': 1.0986123085021973, '4) (Fig 8) too deep network is hard to train, and skip link can remedy this problem.': 1.0986123085021973, 'However, this paper has following problems:': 1.0986123085021973, '1. It is not sufficiently justified why the linear classifier is a good probe. It is not crystal clear why good intermediate features need to show high linear classification accuracy. More theoretical analysis and/or intuition will be helpful.': 1.0986123085021973, '2. This paper does not provide much insight on how to design better networks based on the observations. Designing a better network is also the best way to justify the usefulness of the analysis.': 1.0986123085021973, 'Overall, this paper is tackling an interesting problem, but the technique (the linear classifier as the probe) is not novel and more importantly need to be better justified.': 1.0986123085021973, 'Moreover, it is important to show how to design better neural networks using the observations in this paper.': 1.0986123085021973, 'The authors propose a method to investigate the predictiveness of intermediate layer activations.': 1.0986123085021973, 'To do so, they propose training linear classifiers and evaluate the error on the test set.': 1.0986123085021973, 'The paper is well motivated and aims to shed some light onto the progress of model training and hopes to provide insights into deep learning architecture design.': 1.0986123085021973, 'The two main reasons for why the authors decided to use linear probes seem to be:': 1.0986123085021973, 'convexity': 1.0986123085021973, 'The last layer in the network is (usually) linear': 1.0986123085021973, 'In the second to last paragraph of page 4 the authors point out that it could happen that the intermediate features are useless for a linear classifier.': 1.0986123085021973, 'This is correct and what I consider the main flaw of the paper.': 1.0986123085021973, 'I am missing any motivation as to the usefulness of the suggested analysis to architecture design.': 1.0986123085021973, ""In fact, the example with the skip connection (Figure 8) seems to suggest that skip connections shouldn't be used."": 1.0986123085021973, ""Doesn't that contradict the recent successes of ResNet?"": 1.0986123085021973, ""While the results are interesting, they aren't particularly surprising and I am failing to see direct applicability to understanding deep models as the authors suggest."": 1.0986123085021973}"
465,https://openreview.net/forum?id=ryHlUtqge,"{'In supervised learning, a significant advance occurred when the framework of semi-supervised learning was  adopted, which used the weaker approach of unsupervised learning to infer some property, such as a distance measure or a smoothness regularizer, which could then be used with a small number of labeled examples.': 1.0986123085021973, 'The approach rested on the assumption of smoothness on the manifold, typically.': 1.0986123085021973, 'This paper attempts to stretch this analogy to reinforcement learning, although the analogy is somewhat incoherent.': 1.0986123085021973, 'Labels are not equivalent to reward functions, and positive or negative rewards do not mean the same as positive and negative labels.': 1.0986123085021973, 'Still, the paper makes a worthwhile attempt to explore this notion of semi-supervised RL, which is clearly an important area that deserves more attention.': 1.0986123085021973, 'The authors use the term ""labeled MDP"" to mean the typical MDP framework where the reward function is unknown.': 1.0986123085021973, 'They use the confusing term ""unlabeled MDP"" to mean the situation where the reward is unknown, which is technically not an MDP (but a controlled Markov process).': 1.0986123085021973, 'In the classical RL transfer learning setup, the agent is attempting to transfer learning from a source ""labeled"" MDP to a target ""labeled"" MDP (where both reward functions are known, but the learned policy is known only in the source MDP).': 1.0986123085021973, 'In the semi-supervised RL setting, the target is an ""unlabeled"" CMP, and the source is both a ""labeled"" MDP and an ""unlabeled"" CMP.': 1.0986123085021973, 'The basic approach is to use inverse RL to infer the unknown ""labels"" and then attempt to construct transfer.': 1.0986123085021973, 'A further restriction is made to linearly solvable MDPs for technical reasons.': 1.0986123085021973, 'Experiments are reported using three relatively complex domains using the Mujoco physics simulator.': 1.0986123085021973, 'The work is interesting, but in the opinion of this reviewer, the work fails to provide a simple sufficiently general notion of semi-supervised RL that will be of sufficiently wide interest to the RL community.': 1.0986123085021973, 'That remains to be done by a future paper, but in the interim, the work here is sufficiently interesting and the problem is certainly a worthwhile one to study.': 1.0986123085021973, 'This paper formalizes the problem setting of having only a subset of available MDPs for which one has access to a reward.': 1.0986123085021973, 'The authors name this setting ""semi-supervised reinforcement learning"" (SSRL), as a reference to semi-supervised learning (where one only has access to labels for a subset of the dataset).': 1.0986123085021973, 'They provide an approach for solving SSRL named semi-supervised skill generalization (S3G), which builds on the framework of maximum entropy control.': 1.0986123085021973, 'The whole approach is straightforward and amounts to an EM algorithm with partial labels': 1.0986123085021973, '(: they alternate iteratively between estimating a reward function (parametrized) and fitting a control policy using this reward function.': 1.0986123085021973, 'They provide experiments on 4 tasks (obstacle, 2-link reacher, 2-link reacher with vision, half-cheetah) in MuJoCo.': 1.0986123085021973, 'The paper is well-written, and is overall clear.': 1.0986123085021973, 'The appendix provides some more context, I think a few implementation details are missing to be able to fully reproduce the experiments from the paper, but they will provide the code.': 1.0986123085021973, 'The link to inverse reinforcement learning seems to be done correctly.': 1.0986123085021973, 'However, there is no reference to off-policy policy learning, and, for instance, it seems to me that the \\tau \\in D_{samp} term of equation (3) could benefit from variance reduction as in e.g. TB(\\lambda)': 1.0986123085021973, '[Precup et al. 2000] or Retrace(\\lambda)': 1.096858263015747, '[Munos et al. 2016].': 1.0986123085021973, 'The experimental section is convincing, but I would appreciate a precision (and small discussion) of this sentence ""To extensively test the generalization capabilities of the policies learned with each method, we measure performance on a wide range of settings that is a superset of the unlabeled and labeled MDPs"" with numbers for the different scenarios (or the replacement of superset by ""union"" if this is the case).': 1.0388398170471191, 'It may explain better the poor results of ""oracle"" on ""obstacle"" and ""2-link reacher"", and reinforce* the further sentences ""In the obstacle task, the true reward function is not sufficiently shaped for learning in the unlabeled MDPs.': 0.9506544470787048, 'Hence, the reward regression and oracle methods perform poorly"".': 0.4069080650806427, 'Correction on page 4: ""5-tuple M_i = (S, A, T, R)"" is a 4-tuple.': 1.098067045211792, 'Overall, I think that this is a good and sound paper.': 0.8214393258094788, 'I am personally unsure as to if all the parallels and/or references to previous work are complete, thus my confidence score of 3.': 1.0954312086105347, '(* pun intended)': 1.0986123085021973, 'The paper proposes to study the problem of semi-supervised RL where one has to distinguish between labelled MDPs that provide rewards, and unlabelled MDPs that are not associated with any reward signal.': 1.0986123085021973, 'The underlying is very simple since it aims at simultaneously learning a policy based on the REINFORCE+entropy regularization technique, and also a model of the reward that will be used (as in inverse reinforcement learning) as a feedback over unlabelled MDPs.': 1.0986119508743286, 'The experiments are made on different continous domains and show interesting results': 1.0986123085021973, 'The paper is well written, and easy to understand.': 1.0986123085021973, 'It is based on a simple but efficient idea of simultaneously learning the policy and a model of the reward and the resulting algorithm exhibit interesting properties.': 1.0986123085021973, 'The proposed idea is quite obvious, but the authors are the first ones to propose to test such a model.': 1.0986123085021973, 'The experiments could be made stronger by mixing continuous and discrete problems but are convincing.': 1.0986123085021973}"
466,https://openreview.net/forum?id=ryMxXPFex,"{'Paper proposes a novel Variational Encoder architecture that contains discrete variables.': 1.0986123085021973, 'Model contains an undirected discrete component that captures distribution over disconnected manifolds and a directed hierarchical continuous component that models the actual manifolds (induced by the discrete variables).': 0.7202739715576172, 'In essence the model clusters the data and at the same time learns a continuous manifold representation for the clusters.': 1.0986109972000122, 'The training procedure for such models is also presented and is quite involved.': 1.0986123085021973, 'Experiments illustrate state-of-the-art performance on public datasets (including MNIST, Omniglot, Caltech-101).': 0.8093814849853516, 'Overall the model is interesting and could be useful in a variety of applications and domains.': 1.0986120700836182, 'The approach is complex and somewhat mathematically involved.': 1.0986123085021973, ""It's not exactly clear how the model compares or relates to other RBM formulations, particularly those that contain discrete latent variables and continuous outputs."": 1.0978831052780151, 'As a prime example:': 1.0986123085021973, 'Graham Taylor and Geoffrey Hinton.': 1.0986123085021973, 'Factored conditional restricted Boltzmann machines for modeling motion style.': 1.0984270572662354, 'In Proc. of the 26th International Conference on Machine Learning (ICML), 1025–1032, 2009.': 1.0986123085021973, 'Discussion of this should certainly be added.': 1.0986123085021973, 'This paper presents a way of training deep generative models with discrete hidden variables using the reparameterization trick.': 1.0986100435256958, 'It then applies it to a particular DBN-like architecture, and shows that this architecture achieves state-of-the-art density modeling performance on MNIST and similar datasets.': 1.092116117477417, 'The paper is well written, and the exposition is both thorough and precise.': 1.0986123085021973, 'There are several appendices which justify various design decisions in detail.': 0.8169645071029663, 'I wish more papers in our field would take this degree of care with the exposition!': 1.0986123085021973, 'The log-likelihood results are quite strong, especially given that most of the competitive algorithms are based on continuous latent variables.': 1.0985537767410278, 'Probably the main thing missing from the experiments is some way to separate out the contributions of the architecture and the inference algorithm.': 1.0903406143188477, '(E.g., what if a comparable architecture is trained with VIMCO, or if the algorithm is applied to a previously published discrete architecture?)': 1.0980052947998047, 'I’m a bit concerned about the variance of the gradients in the general formulation of the algorithm.': 0.8970889449119568, 'See my comment “variance of the derivatives of F^{-1}” below.': 1.0986120700836182, 'I think the response is convincing, but the problem (as well as “engineering principles” for the smoothing distribution) are probably worth pointing out in the paper itself, since the problem seems likely to occur unless the user is aware of it.': 0.744006335735321, '(E.g., my proposal of widely separated normals would be a natural distribution to consider until one actually works through the gradients — something not commonly done in the age of autodiff frameworks.)': 1.092299461364746, 'Another concern is how many sequential operations are needed for inference in the RBM model.': 1.0986123085021973, '(Note: is this actually an RBM, or a general Boltzmann machine?)': 1.0986123085021973, 'The q distribution takes the form of an autoregressive model where the variables are processed one at a time.': 1.0986123085021973, 'Section 3 mentions the possibility of grouping together variables in the q distribution, and this is elaborated in detail in Appendix A.': 0.7755862474441528, 'But the solution requires decomposing the joint into a product of conditionals and applying the CDFs sequentially.': 1.0986123085021973, 'So either way, it seems like we’re stuck handling all the variables sequentially, which might get expensive.': 1.0986123085021973, 'Minor: the second paragraph of Section 3 needs a reference to Appendix A.': 1.0986123085021973, 'This is an interesting paper on how to handle reparameterization in VAEs when you have discrete variables.': 1.098610520362854, 'The idea is to introduce a smoothing transformation that is shared between the generative model and the recognition model (leading to cancellations).': 1.098002314567566, 'A second contribution is to introduce an RBM as the prior model P(z) and to use autoregressive connections in generative and recognition models.': 1.0912021398544312, 'The whole package becomes a bit entangled and complex and it is hard to figure out what causes the claimed good performance.': 1.0840963125228882, 'Experiments that study these contributions separately would have been nice.': 1.0986123085021973, 'The framework does become a little complex but this should not be a problem if nice software is delivered that can be used in a plug and play mode.': 0.9481916427612305, 'Overall, the paper is very rich with ideas': 1.0986123085021973, 'so I think it would be a great contribution to the conference.': 1.0986123085021973}"
467,https://openreview.net/forum?id=ryPx38qge,"{'In this paper, the authors explicitly design geometrical structure into a CNN by combining it with a Scattering network.': 1.098360538482666, 'This aids stability and limited-data performance.': 1.0986073017120361, 'The paper is well written, the contribution of combining Scattering and CNNs is novel and the results seem promising.': 1.0986123085021973, 'I feel that such work was a missing piece in the Scattering literature to make it useful for practical applications.': 1.0986123085021973, 'I wish the authors would have investigated the effect of the stable bottom layers with respect to adversarial examples.': 1.0986095666885376, 'This can be done in a relatively straightforward way with software like cleverhans [1] or deep fool [2].': 1.0986123085021973, ""It would be very interesting if the first layer's stability in the hybrid architectures increases robustness significantly, as this would tell us that these fooling images are related to low-level geometry."": 1.0986120700836182, 'Finding that this is not the case, would be very interesting as well.': 0.37873053550720215, 'Further, the proposed architecture is not evaluated on real limited data problems.': 1.0986123085021973, 'This would further strengthen the improved generalization claim.': 1.0986117124557495, 'However, I admit that the Cifar-100 / Cifar-10 difference already seems like a promising indicator in this regard.': 1.0986123085021973, 'If one of the two points above will be addressed in an additional experiment, I would be happy to raise my score from 6 to 7.': 1.0986123085021973, 'Summary:': 1.0986123085021973, '+': 0.7019147872924805, 'An interesting approach is presented that might be useful for real-world limited data scenarios.': 1.0986120700836182, '+ Limited data results look promising.': 1.0041857957839966, 'Adversarial examples are not investigated in the experimental section.': 0.270406574010849, 'No realistic small-data problem is addressed.': 1.0790246725082397, 'Minor:': 0.832548975944519, 'The authors should add a SOTA ResNet to Table 3, as NiN is indeed out of fashion these days.': 1.0982699394226074, 'Some typos: tacke, developping, learni.': 1.0986123085021973, '[1] https://arxiv.org/abs/1610.00768v3': 0.3614051342010498, '[2] https://arxiv.org/abs/1511.04599': 0.3743509352207184, 'The paper investigates a hybrid network consisting of a scattering network followed by a convolutional network.': 1.0986121892929077, 'By using scattering layers, the number of parameters is reduced, and the first layers are guaranteed to be stable to deformations.': 1.0985487699508667, 'Experiments show that the hybrid network achieves reasonable performance, and outperforms the network-in-network architecture in the small-data regime.': 1.0983293056488037, 'I have often heard researchers ask why it is necessary to re-learn low level features of convolutional networks every time they are trained.': 1.0986123085021973, 'In theory, using fixed features could save parameters and training time.': 1.0986123085021973, 'As far as I am aware, this paper is the first to investigate this question.': 1.0985718965530396, 'In my view, the results show that using scattering features in the bottom layers does not work as well as learned CNN features.': 1.0986121892929077, 'This is not completely obvious a priori, and so the results are interesting, but I disagree with the framing that the hybrid network is superior in terms of generalization.': 1.0986123085021973, 'For the low-data regime, the hybrid network sometimes gives better accuracy than NiN, but this is quite an old architecture and its capacity has not been tuned to the dataset size.': 1.0986123085021973, 'For the full dataset, the hybrid network is clearly outperformed by fully learned models.': 1.0986120700836182, 'If I understood correctly, the authors have not simply compared identical architectures with and without scattering as the first layers, which further complicates drawing a conclusion.': 1.0986123085021973, 'The authors claim the hybrid network has the theoretical advantage of stability.': 1.0609458684921265, 'However, only the first layers of a hybrid network will be stable, while the learned ones can still create instability.': 1.098004937171936, 'Furthermore, if potentially unstable deep networks outperform stable scattering nets and partially-stable hybrid nets, we have to question the importance of stability as defined in the theory of scattering networks.': 1.0986121892929077, 'In conclusion, I think the paper investigates a relevant question, but I am not convinced that the hybrid network really generalizes better than standard deep nets.': 1.098396897315979, 'Faster computation (at test time) could be useful e.g. in low power and mobile devices, but this aspect is not really fleshed out in the paper.': 1.0986123085021973, 'Minor comments:': 1.0986117124557495, 'section 3.1.2: “learni”': 1.0986123085021973, 'Thanks a lot for your detailed response and clarifications.': 1.0986123085021973, 'The paper proposes to use a scattering transform as the lower layers of a deep network.': 1.0986123085021973, 'This fixed representation enjoys good geometric properties (local invariance to deformations) and can be thought as a form of regularization or prior.': 1.0986123085021973, 'The top layers of the network are trained to perform a given supervised task.': 1.0986123085021973, 'This is the final model can be thought as plugging a standard deep convolutional network on top of the scattering transform.': 1.0986123085021973, 'Evaluation on CIFAR 10 and 100 shows that the proposed approach achieves performance competitive with high performing baselines.': 1.0984634160995483, 'I find the paper very interesting.': 0.912805438041687, 'The idea of cascading these representations seems very natural thing to try.': 1.0986123085021973, 'To the best of my knowledge this is the first work that combines predefined and generic representations with modern CNN architectures achieving competitive performance to high performing approaches.': 1.0986123085021973, ""While the state of the art (Resnets and variants) achieves significantly higher performances, I believe that this work strongly delivers it's point."": 1.0986123085021973, 'The paper convincingly shows that lower level invariances can be obtained from analytic representations (scattering transform), simplifying the training process (using less parameters) and allowing for faster evaluation.': 1.0986123085021973, 'The of the hybrid approach become crucial in the low data regime.': 1.0982909202575684, 'The author argues that with the scattering initialisation instabilities cannot occur in the first layers contrary as the operator is non-expansive.': 1.0986123085021973, 'This naturally suggests that the model is more robust to adversarial examples.': 1.0986109972000122, 'It would be extremely interesting to present an empirical evaluation of this task.': 1.0986117124557495, ""What's the practical impact?"": 1.0986123085021973, 'Can this hybrid network be fooled with adversarial?': 0.7052108645439148, 'If this is the case, it would render the use of scattering initialization very attractive.': 1.0631327629089355}"
468,https://openreview.net/forum?id=ryQbbFile,"{'Summary: The paper presents an approach – Neural Answer Construction Model for the task of answering non-factoid questions, in particular, love-advice questions. The two main features of the proposed model are the following – 1) it incorporates the biases of semantics behind questions into word embeddings, 2) in addition to optimizing for closeness between questions and answers, it also optimizes for optimum combination of sentences in the predicted answer. The proposed model is evaluated using the dataset from a Japanese online QA service and is shown to outperform the baseline model (Tan et al. 2015) by 20% relatively (6% absolutely). The paper also experiments with few other baseline models (ablations of the proposed model).': 1.0947630405426025, 'Strengths:': 1.0986123085021973, '1. The two motivations behind the proposed approach – need to understand the ambiguous use of words depending on context, and need to generate new answers rather than just selecting from answers held by QA sites – are reasonable.': 1.0986123085021973, '2. The novelty in the paper involves the following – 1) incorporating biases of semantics behind questions into word embeddings using paragraph2vec like model, modified to take as inputs - words from questions, question title token and question category token, 2) modelling optimum combination of sentences (conclusion and supplement sentences) in the predicted answer, 3) designing abstract scenario for answers, inspired by automated web-service composition framework (Rao & Su (2005)), and 4) extracting important topics in conclusion sentence and emphasizing them in supplemental sentence using attention mechanism (attention mechanism is similar to Tan et al. 2016).': 1.0986123085021973, '3. The proposed method is shown to outperform the current best method (Tan et al. 2015) by 20% relatively (6% absolutely) which seems to be significant improvement.': 1.0986123085021973, '4. The paper presents few ablations studies that provide insights on how much different components of the model (such as incorporating biases into word embeddings, incorporating attention from conclusion to supplement) are helping towards performance improvement.': 1.0986123085021973, 'Weaknesses/Suggestions/Questions:': 1.0986123085021973, '1. How are the abstract patterns determined, i,e., how did the authors determine that the answers to love-advice questions generally constitute of sympathy, conclusion, supplement for conclusion and encouragement? How much is the improvement in performance when using abstract patterns compared to the case when not using these patters, i.e. when candidate answers are picked from union of all corpuses rather than picking from respective corpuses (corpuses for sympathy, conclusion etc.).': 1.0986123085021973, '2. It seems that the abstract patterns are specific to the type of questions. So, the abstract patterns for love-advice will be different from those for business advice. Thus, it seems like the abstract patterns need to be hand-coded for different types and hence one model cannot generalize across different types.': 1.0915923118591309, '3. The paper should present explicit analysis of how much combinational optimization between sentences help – comparison between model performance with and without combinational optimization keeping rest of the model architecture same. The authors could also plot the accuracy of the model as a function of the combinational optimization scores. This will provide insights into how significant are the combinational optimization scores towards overall model accuracy.': 1.0733976364135742, '4. Paper says that current systems designed for non-factoid QA cannot generalize to questions outside those stored in QA sites and claims that this is one of the contributions of this paper. In order to ground that claim, the paper should show experimentally how well the proposed method generalized to such out-of-domain questions. Although the questions asked by human experts in the human evaluation were not from the evaluation datasets, the paper should analyze how different those questions were compared to the questions present in the evaluation datasets.': 1.0974023342132568, '5. For human evaluation, were the outputs of the proposed model and that of the QA-LSTM model judged each judged by both the human experts OR one of the human experts judged the outputs of one system and the other human expert judged the outputs of the other system? If both the sets of outputs were each judged by both the human experts, how were the ratings of the two experts combined for every questions?': 1.071201205253601, '6. I wonder why the authors did not do a human evaluation where they just ask human workers (not experts) to compare the output of the proposed model with that of the QA-LSTM model – which of the two outputs they would like to hear when asking for advice. Such an evaluation would not get biased by whether each sentence is good or not, whether the combination is good or not. Looking at the qualitative examples in Table 4, I personally like the output of the QA-LSTM more than that of the proposed model because they seem to provide a direct answer to the question (e.g., for the first example the output of the QA-LSTM says “You should wait until you feel excited”, whereas the output of the proposed model says “It is better to concentrate on how to confess your love to her” which seems a bit indirect to the question asked.)': 1.0936163663864136, '7. Given a question, is the ground-truth answer different in the two tasks': 1.0975788831710815, 'answer selection and answer construction?': 1.0986123085021973, '8. The paper mentions that Attentive LSTM (Tan et al. 2016) is evaluated as the current best answer selection method (section 5.2). So, why is its accuracy lower than that of QA-LSTM in table 1. The authors explain this by pointing out the issue of questions being very long compared to answers and hence the attention being noisy. But, did these issues not exist in the dataset used by Tan et al. 2016?': 1.0980000495910645, '9. The paper says the proposed method achieves 20% gain over current best (in Conclusion section) where they refer to QA-LSTM as the current best method. However, in the description of Attentive LSTM (section 5.2), the paper mentions that Attention LSTM is the current best method. So, could authors please clarify the discrepancy?': 1.0966633558273315, '10. Minor correction: remove space between 20 and % in abstract.': 0.8417512774467468, 'Review Summary: The problem of non-factoid QA being dealt with in the paper is an interesting and useful problem to solve.': 1.0592701435089111, 'The motivations presented in the paper behind the proposed approach are reasonable.': 0.9936844110488892, 'The experiments show that the proposed model outperforms the baseline model.': 1.0986123085021973, 'However, the use of abstract patterns to determine the answer seems like hand-designing and hence it seems like these abstract patterns need to be designed for every other type of non-factoid question and hence the proposed approach is not generalizable to other types.': 1.0986123085021973, 'Also, the paper needs more analysis of the results to provide insights into the contribution of different model components.': 1.0986123085021973, 'This paper extends mostly on top of the work of QA-biLSTM and QA-biLSTM with attentions, as proposed in Tan et al. 2015 and Tan et al. 2016, in the following 2 ways:': 1.0986123085021973, '1. It trains a topic-specific word embedding using an approach similar to Paragraph2vec by leveraging the topic and title information provided in the data.': 1.0986123085021973, '2. It considers the multiple-unit answer selection problem (e.g., one sentence selected from answer section, and another selected from supplemental section) vs. the single answer selection problem as studied in Tan et al 2015 and 2016. The mechanism used to retain the coherence between different parts of the answers is inspired by the attention mechanism introduced by Tan et al. 2016.': 1.0986123085021973, 'While the practical results presented in the paper is interesting, the main innovations of this paper are rather limited.': 1.0986123085021973, 'This paper proposes a neural architecture for answering non-factoid questions.': 1.0986123085021973, ""The author's model improves over previous neural models for answer sentence selection."": 1.0986123085021973, 'Experiments are conducted on a Japanese love advice corpus; the coolest part of the paper for me was that the model was actually rolled out to the public and its answers were rated twice as good as actual human contributors!': 1.0986123085021973, 'It was hard for me to determine the novelty of the contribution.': 1.0986123085021973, 'The authors mention that their model ""fills the gap': 1.0986123085021973, 'between answer selection and generation""; however, no generation is actually performed by the model!': 1.0986123085021973, 'Instead, the model appears to be very similar to the QA-LSTM of Tan et al., 2015 except that there are additional terms in the objective to handle conclusion and supplementary sentences.': 1.0986123085021973, 'The structure of the answer is fixed to a predefined template (e.g., conclusion': 1.0986123085021973, '> supplementary), so the model is not really learning how to order the sentences.': 1.0986123085021973, 'The other contribution is the ""word embedding with semantics"" portion described in sec 4.1, which is essentially just the paragraph vector model except with ""titles"" and ""categories"" instead of paragraphs.': 1.0986123085021973, 'While the result of the paper is a model that has actually demonstrated real-life usefulness, the technical contributions do not strike me as novel enough for publication at ICLR.': 1.0986123085021973, 'Other comments:': 1.0986123085021973, ""One major issue with the reliance of the model on the template is that you can't evaluate on commonly-used non-factoid QA datasets such as InsuranceQA."": 1.0986123085021973, 'If the template were not fixed beforehand (but possibly learned by the model), you could conceivably evaluate on different datasets.': 1.0986123085021973, ""The examples in Table 4 don't show a clear edge in answer quality to your model; QA-LSTM seems to choose good answers as well."": 1.0986123085021973, ""Doesn't the construction model have an advantage over the vanilla QA-LSTM in that it knows which sentences are conclusions and which are supplementary?"": 1.0986123085021973, 'Or does QA-LSTM also get this distinction?': 1.0986123085021973}"
469,https://openreview.net/forum?id=ryT4pvqll,"{'This paper proposes a novel exploration strategy that promotes exploration of under-appreciated reward regions.': 1.0986123085021973, 'Proposed importance sampling based approach is a simple modification to REINFORCE and experiments in several algorithmic toy tasks show that the proposed model is performing better than REINFORCE and Q-learning.': 1.0986123085021973, 'This paper shows promising results in automated algorithm discovery using reinforcement learning.': 1.0986123085021973, 'However it is not very clear what is the main motivation of the paper.': 1.0986123085021973, 'Is the main motivation better exploration for policy gradient methods?': 1.0986119508743286, 'If so, authors should have benchmarked their algorithm with standard reinforcement learning tasks.': 1.0986123085021973, 'While there is a huge body of literature on improving REINFORCE, authors have considered a simple version of REINFORCE on a non-standard task and say that UREX is better.': 1.0985826253890991, 'If the main motivation is improving the performance in algorithm learning tasks, then the baselines are still weak.': 1.0986117124557495, 'Authors should make it clear which is the main motivation.': 1.0985974073410034, 'Also the action space is too small.': 1.0981638431549072, 'In the beginning authors raise the concern that entropy regularization might not scale to larger action spaces.': 1.0985342264175415, 'So a comparison of MENT and UREX in a large action space problem would give more insights on whether UREX is not affected by large action space.': 1.0986120700836182, 'After rebuttal:': 1.0959852933883667, 'I missed the action sequences argument when I pointed about small action space issue.': 1.0986123085021973, 'For question regarding weak baseline, there are several tricks used in the literature to tackle high-variance issue for REINFORCE.': 1.044805645942688, 'For example, see Mnih & Gregor, 2014.': 1.0853283405303955, 'I have increased my rating from 6 to 7.': 1.0986123085021973, 'I still encourage the authors to improve their baseline.': 1.0986123085021973, 'overview:': 1.0986123085021973, 'This work proposes to link trajectory log-probabilities and rewards by defining under-appreciated rewards.': 1.0986123085021973, 'This suggests that there is a linear relationship between trajectory rewards and their log-probability which can be exploited by measuring the resulting mismatch.': 1.0986123085021973, 'That is, when an action sequence under-appreciates its reward, its log-probability is increased.': 1.0986123085021973, 'This method is a simple modification to the well-known REINFORCE method, requiring only one extra hyperparameter \\tau, and intuitively provides us with a better exploration mechanism than \\epsilon-greedy or random exploration.': 1.0986123085021973, 'The method is tested on algorithmic environments, and compared to entropy-regularized REINFORCE and double Q-learning, and performs equally or better than those two baselines, especially in more complex environments.': 1.098610281944275, 'remarks:': 1.0985991954803467, 'the focus in the introduction on algorithmic tasks may be a double-edged sword.': 1.0986123085021973, 'It is an interesting domain to test your hypothesis and benchmark your method.': 1.0986123085021973, 'At the same time, it distracts the reader from the (IMO) generality of the proposed method.': 0.8082190752029419, 'in the introduction you say the reward is sparse, in section 6, on tasks 1-5, you then say there is a reward at each correct emission, i.e. each time step.': 0.3601510226726532, ""This is only 'corrected' to end-of-episode-reward in section 7.4, after having discussed results."": 1.0986123085021973, ""I'd move or mention this in section 6."": 1.0984336137771606, 'approach seems quite sensible to tau being in the same range as logpi(a|h), but you only try tau=0.1 for UREX.': 0.05869940668344498, ""I'm not sure I understand nor agree with this experimentation choice."": 1.0986120700836182, 'an alternative to grid search is random search (Bergstra&Bengio, 2012).': 1.0984941720962524, 'It may illustrate better hyperparameter robustness, and allow you to explore more in the same number of experiment.': 1.0652203559875488, 'opinion:': 1.0986120700836182, 'An interesting approach to policy-gradient, to be sure.': 1.0986123085021973, 'It tackles the very important question of ""how should agents explore?""': 1.0986123085021973, ""I'm ambivalent to claiming that an algorithm is robust to hyperparmeters simply because it performs better on the selected hyperparameter range."": 1.0986123085021973, 'All you really show is that it performs well some amount of time when the hyperparams lay in that range.': 1.0986123085021973, ""Couldn't it be that MENT needs different hyperparameters?"": 0.8046233057975769, ""(Just being devil's advocate here)"": 1.0625486373901367, 'I see why matching 1/tau with logpi is the obvious choice, but it implies a very strong prior: that the reward (to a factor of 1/tau) lies in the same space as the log policy.': 1.0962120294570923, ""One point of failure I see (but correct me if I'm wrong) is that as the length of the trajectory grows the reward is expected to grow linearly, so short ways to get some reward will be less explored than long ways of getting the same reward, creating an imbalance unless the reward is shaped such that shorter trajectories get more reward (which is only the case in task 6)."": 1.0984500646591187, 'It might have been good to also compare with methods explicitly trying to explore better with value-functions (e.g. prioritized experience replay, Schaul et al 2015)': 0.8600656390190125, 'At the risk of repeating myself, tau plays a major role in this method, but there is little analysis on its effect on experiments.': 1.0863289833068848, 'The methodology and reasoning is clearly explained and I think this paper communicates its message very well.': 1.0845776796340942, 'That message is novel, albeit a minor modification to a well-known algorithm, it is well motivated and, I think, a welcome addition to literature concerning exploration in RL.': 1.0894197225570679, 'The experiments are chosen accordingly, and results seem to reflect the hypothesis of the authors.': 1.09372878074646, 'I realize the tyranny of extensive experimentation and the scarcity of time, but I do think that this paper would benefit from more (or cleverer) experimentation, as well as demonstrating more explicitly the impact of the method on exploration.': 0.6215680241584778, ""Reading this paper convinced me that measuring mismatch between a trajectory's observed reward and its probability given the current policy is a clever (and well motivated) thing to do."": 1.0906164646148682, 'Yet, I think that the paper could have a more convincing empirical argument, even if it is for toy tasks.': 1.0983091592788696, 'The paper proposes a new algorithm based on REINFORCE which aims at exploring under-appreciate action sequences.': 1.0986082553863525, 'The idea is to compare the probability of a sequence of actions under the current policy with the estimated reward.': 1.098611831665039, 'Actions where the current policy under-estimate the reward will provide a higher feedback, thus encouraging exploration of particular sequences of actions.': 1.0986123085021973, 'The UREX model is tested on 6 algortihmic RL problems and show interesting properties in comparison to the standard regularized REINFORCE (MENT) model and to Q-Learning.': 1.0985761880874634, 'The model is interesting, well defined and well explained.': 0.28637537360191345, 'As far as I know, the UREX model is an original model which will certainly be useful for the RL community.': 1.0977579355239868, 'The only drawback of the paper is to restrict the evaluation of this algortihm to algorithmic problems that are specific while it would be easy to test the proposed model onto other standard RL problems.': 1.0986123085021973, 'This would clearly help to make the article stronger and I greatly encourage the authors to add some other tasks in their paper.': 1.0986123085021973}"
470,https://openreview.net/forum?id=ryT9R3Yxe,"{'While this paper has some decent accuracy numbers, it is hard to argue for acceptance given the following:': 1.0986123085021973, ""1) motivation based on the incorrect assumption that the Paragraph Vector wouldn't work on unseen data"": 0.4842536151409149, '2) Numerous basic formatting and Bibtex citation issues.': 1.041517972946167, 'Lack of novelty of yet another standard directed LDA-like bag of words/bigram model.': 1.0986123085021973, 'It feels that this paper is structured around a shortcoming of the original paragraph vectors paper, namely an alleged inability to infer representation for text outside of the training data.': 1.0986121892929077, 'I am reasonably sure that this is not the case.': 1.097243070602417, 'Unfortunately on that basis, the premise for the work presented here no longer holds, which renders most of the subsequent discussion void.': 1.0986119508743286, 'While I recommend this paper be rejected, I encourage the authors to revisit the novel aspects of the idea presented here and see if that can be turned into a different type of paper going forward.': 1.0986114740371704, 'This work reframes paragraph vectors from a generative point of view and in so doing, motivates the existing method of inferring paragraph vectors as well as applying a L2 regularizer on the paragraph embeddings.': 1.09861159324646, 'The work also motivates joint learning of a classifier on the paragraph vectors to perform text classification.': 1.0986098051071167, 'The paper has numerous citation issues both in formatting within the text and the formatting of the bibliography, e.g. on some occasions including first names, on others not.': 1.0227818489074707, 'I suggest the authors use a software package like BibTex to have a more consistent bibliography.': 1.097171664237976, 'There seems to be little novelty in this work.': 0.984603226184845, 'The authors claim that there is no proposed method for inferring unseen documents for paragraph vectors.': 1.0986123085021973, 'This is untrue.': 1.0986123085021973, 'In the original paragraph vector paper, the authors show that to get a new vector, the rest of the model parameters are held fixed and gradient descent is performed on the new paragraph vector.': 1.0986123085021973, 'This means the original dataset is not needed when inferring a paragraph vector for new text.': 1.0986123085021973, 'This work seems to be essentially doing the same thing when finding the MAP estimate for a new vector.': 1.0986123085021973, 'Thus the only contribution from the generative paragraph vector framing is the regularization on the embedding matrix.': 1.0986123085021973, 'The supervised generative paragraph vector amounts to jointly training a linear classifier on the paragraph vectors, while inference for the paragraph vector is unchanged.': 1.0986123085021973, 'For the n-gram based approach, the authors should cite Li et al., 2015.': 1.0986123085021973, 'In the experiments, table 1 and 2 are badly formatted with .0 being truncated.': 1.0986123085021973, 'The authors also do not state the size of the paragraph vector.': 1.0986123085021973, 'Finally the SGPV results are actually worse than that reported in the original paragraph vector paper where SST-1 got 48.7 and SST-2 got 86.3.': 1.0986123085021973, 'Bofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao, Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews, 2015.': 1.0081210136413574}"
471,https://openreview.net/forum?id=ryTYxh5ll,"{'The problem of utilizing all available information (across modalities) about a product to learn a meaningful ""joint"" embedding is an interesting one, and certainly seems like it a promising direction for improving recommender systems, especially in the ""cold start"" scenario.': 1.0986121892929077, ""I'm unaware of approaches combining as many modalities as proposed in this paper, so an effective solution could indeed be significant."": 1.0985926389694214, 'However, there are many aspects of the proposed architecture that seem sub-optimal to me:': 1.0986123085021973, ""1. A major benefit of neural-network based systems is that the entire system can be trained end-to-end, jointly. The proposed approach sticks together largely pre-trained modules for different modalities... this can be justifiable when there is very little training data available on which to train jointly. With 10M product pairs, however, this doesn't seem to be the case for the Amazon dataset (although I haven't worked with this dataset myself so perhaps I'm missing something... either way it's not discussed at all in the paper). I consider the lack of a jointly fine-tuned model a major shortcoming of the proposed approach."": 0.572533130645752, '2. The discussion of ""pairwise residual units"" is confusing and not well-motivated. The residual formulation (if I understand it correctly) applies a ReLU layer to the concatenation of the modality specific embeddings, giving a new similarity (after dot products) that can be added to the similarity obtained from the concatenation directly. Why not just have an additional fully-connected layer that mixes the modality specific embeddings to form a final embedding (perhaps of lower dimensionality)? This should at least be presented as a baseline, if the pairwise residual unit is claimed as a contribution... I don\'t find the provided explanation convincing (in what way does the residual approach reduce parameter count?).': 0.6100273132324219, '3. More minor: The choice of TextCNN for the text embedding vectors seems fine (although I wonder how an LSTM-based approach would perform)... However the details surrounding how it is used are obscured in the paper. In response to a question, the authors mention that it runs on the concatenation of the first 10 words of the title and product description. Especially for the description, this seems insufficiently long to contain a lot of information to me.': 0.5556480884552002, 'More care could be given to motivating the choices made in the paper.': 0.464728981256485, ""Finally, I'm not familiar with state of the art on this dataset..."": 0.8538715243339539, 'do the comparisons accurately reflect it?': 1.0986123085021973, 'It seems only one competing technique is presented, with none on the more challenging cold-start scenarios.': 0.8256242871284485, 'Minor detail: In the second paragraph of page 3, there is a reference that just says (cite Julian).': 1.0980782508850098, 'This paper proposes combining different modalities of product content (e.g. review text, images, co-purchase info ...etc) in order to learn one unified product representation for recommender systems.': 1.0986123085021973, 'While the idea of combining multiple sources of information is indeed an effective approach for handling data sparsity in recommender systems, I have some reservations on the approach proposed in this paper:': 1.0986121892929077, '1) Some modalities are not necessarily relevant for the recommendation task or item similarity.': 0.5059958100318909, 'For example, cover images of books or movies (which are product types in the experiments of this paper) do not tell us much about their content.': 1.0986123085021973, 'The paper should clearly motivate and show how different modalities contribute to the final task.': 1.0986123085021973, '2) The connection between the proposed joint product embedding and residual networks is a bit awkward.': 1.0986123085021973, 'The original residual layers are composed of adding the original input vector to the output of an MLP, i.e. several affine transformations followed by non-linearities.': 1.0986123085021973, 'These layers allow training very deep neural networks (up to 1000 layers) as a result of easier gradient flow.': 1.0986123085021973, 'In contrast, the pairwise residual unit of this paper adds the dot product of two item vectors to the dot product of the same vectors but after applying a simple non-linearity.': 1.0986123085021973, 'The motivation of this architecture is not very obvious, and is not well motivated in the paper.': 1.0986123085021973, '3) While it is a minor point, but the choice of the term embedding for the dot product of two items is not usual.': 1.0986123085021973, 'Embeddings usually refer to vectors in R^n, and for specific entities.': 1.0986123085021973, 'Here it refers to the final output, and renders the output layer in Figure 2 pointless.': 1.0986123085021973, 'Finally, I believe the paper can be improved by focusing more on motivating architectural choices, and being more concise in your description.': 1.0986123085021973, 'The paper is currently very long (11 pages) and I strongly encourage you to shorten it.': 1.0986123085021973, 'The paper proposes a method to combine arbitrary content into recommender systems, such as images, text, etc.': 1.0986123085021973, ""These various features have been previously used to improve recommender systems, though what's novel here is the contribution of a general-purpose framework to combine arbitrary feature types."": 1.0986123085021973, 'Positively, the idea of combining many heterogeneous feature types into RS is ambitious and fairly novel.': 1.0986123085021973, 'Previous works have certainly sought to include various feature types to improve RSs, though combining different features types successfully is difficult.': 1.0986123085021973, 'Negatively, there are a few aspects of the paper that are a bit ad-hoc.': 1.0986123085021973, 'In particular:': 1.0986123085021973, 'There are a lot of pieces here being ""glued together"" to build the system.': 1.0986123085021973, 'Different parts are trained separately and then combined together using another learning stage.': 1.0986123085021973, 'There\'s nothing wrong with doing things in this way (and indeed it\'s the most straightforward and likely to work approach), but it pushes the contribution more toward the ""system building"" direction as opposed to the ""end-to-end learning"" direction which is more the focus of this conference.': 1.0986123085021973, 'Further to the above, this makes it hard to say how easily the model would generalize to arbitrary feature types, say e.g. if I had audio or video features describing the item.': 1.0986123085021973, 'To incorporate such features into the system would require a lot of implementation work, as opposed to being a system where I can just throw more features in and expect it to work.': 1.0986123085021973, 'The pre-review comments address some of these issues.': 1.0986123085021973, 'Some of the responses aren\'t entirely convincing, e.g. it\'d be better to have the same baselines across tables, rather than dropping some because ""the case had already been made elsewhere"".': 1.0986123085021973, 'Other than that, I like the effort to combine several different feature types in real recommender systems datasets.': 1.0986123085021973, ""I'm not entirely sure how strong the baselines are, they seem more like ablation-style experiments rather than comparison against any state-of-the-art RS."": 1.0986123085021973}"
472,https://openreview.net/forum?id=ryUPiRvge,"{'Thank you for an interesting perspective on the neural approaches to approximate physical phenomenon.': 1.0986123085021973, 'This paper describes a method to extrapolate a given dataset and predict formulae with naturally occurring functions like sine, cosine, multiplication etc.': 1.0986123085021973, 'Pros': 1.0986123085021973, 'The approach is rather simple and hence can be applied to existing methods.': 1.0986123085021973, 'The major difference is incorporating functions with 2 or more inputs which was done successfully in the paper.': 1.0986123085021973, 'It seems that MLP, even though it is good for interpolation, it fails to extrapolate data to model the correct function.': 1.0986123085021973, 'It was a great idea to use basis functions like sine, cosine to make the approach more explicit.': 1.0986123085021973, 'Cons': 1.0986123085021973, 'Page 8, the claim that x2 cos(ax1 + b) ~ 1.21(cos(-ax1 + π + b + 0.41x2) + sin(ax1 + b + 0.41x2)) for y in [-2,2] is not entirely correct.': 1.0986123085021973, ""There should be some restrictions on 'a' and 'b' as well as the approximate equality doesn't hold for all real values of 'a' and 'b'."": 1.0986123085021973, 'Although, for a=2*pi and b=pi/4, the claim is correct so the model is predicting a correct solution within certain limits.': 1.0986123085021973, 'Most of the experiments involve up to 4 variables.': 1.0986123085021973, 'It would be interesting to see how the neural approach models hundreds of variables.': 1.0986123085021973, 'Another way of looking at the model is that the non-linearities like sine, cosine, multiplication act as basis functions.': 1.0986123085021973, 'If the data is a linear combination of such functions, the model will be able to learn the weights.': 1.0986123085021973, 'As division is not one of the non-linearities, predicting expressions in Equation 13 seems unlikely.': 1.0986123085021973, 'Hence, I was wondering, is it possible to make sure that this architecture is a universal approximator.': 1.0986123085021973, 'Suggested Edits': 1.0986123085021973, 'Page 8, It seems that there is a typographical error in the expression 1.21(cos(ax1 + π + b + 0.41x2) + sin(ax1 + b + 0.41x2)).': 1.0986123085021973, 'When compared with the predicted formula in Figure 4(b), it should be 1.21(cos(-ax1 + π + b + 0.41x2) + sin(ax1 + b + 0.41x2)).': 1.0986123085021973, 'The authors attempt to extract analytical equations governing physical systems from observations - an important task.': 1.0986123085021973, 'Being able to capture succinct and interpretable rules which a physical system follows is of great importance.': 1.0986123085021973, 'However, the authors do this with simple and naive tools which will not scale to complex tasks, offering no new insights or advances to the field.': 1.0986123085021973, 'The contribution of the paper (and the first four pages of the submission!) can be summarised in one sentence:': 1.0986123085021973, '""Learn the weights of a small network with cosine, sinusoid, and input elements products activation functions s.t.': 1.0986123085021973, 'the weights are sparse (L1)"".': 1.0986123085021973, 'The learnt network weights with its fixed structure are then presented as the learnt equation.': 1.0986123085021973, ""This research uses tools from literature from the '90s (I haven't seen the abbreviation ANN (page 3) for a long time) and does not build on modern techniques which have advanced a lot since then."": 1.0986123085021973, 'I would encourage the authors to review modern literature and continue working on this important task.': 1.0986123085021973, 'Thank you for an interesting read.': 1.0986123085021973, 'To my knowledge, very few papers have looked at transfer learning with **no** target domain data (the authors called this task as ""extrapolation"").': 1.0986123085021973, 'This paper clearly shows that the knowledge of the underlying system dynamics is crucial in this case.': 1.0986123085021973, 'The experiments clearly showed the promising potential of the proposed EQL model.': 1.0966944694519043, 'I think EQL is very interesting also from the perspective of interpretability, which is crucial for data analysis in scientific domains.': 1.09861159324646, 'Quesions and comments:': 0.4588608741760254, ""1. Multiplication units. By the universal approximation theorem, multiplication can also be represented by a neural network in the usual sense. I agree with the authors' explanation of interpolation and extrapolation, but I still don't quite understand why multiplication unit is crucial here. I guess is it because this representation generalises better when training data is not that representative for the future?"": 1.0229755640029907, ""2. Fitting an EQL vs. fitting a polynomial. It seems to me that the number of layers in EQL has some connections to the degree of the polynomial. Assume we know the underlying dynamics we want to learn can be represented by a polynomial. Then what's the difference between fitting a polynomial (with model selection techniques to determine the degree) and fitting an EQL (with model selection techniques to determine the number of layers)? Also your experiments showed that the selection of basis functions (specific to the underlying dynamics you want to learn) is crucial for the performance. This means you need to have some prior knowledge on the form of the equation anyway!"": 1.0961552858352661, '3. Ben-David et al. 2010 has presented some error bounds for the hypothesis that is trained on source data but tested on the target data. I wonder if your EQL model can achieve better error bounds?': 0.4603753089904785, '4. Can you comment on the comparison of your method to those who modelled the extrapolation data with **uncertainty**?': 1.0986123085021973}"
473,https://openreview.net/forum?id=ryWKREqxx,"{'The paper aims to consolidate some recent literature in simple types of ""reading comprehension"" tasks involving matching questions to answers to be found in a passage, and then to explore the types of structure learned by these models and propose modifications.': 1.0986123085021973, 'These reading comprehension datasets such as CNN/Daily Mail are on the simpler side because they do not generally involve chains of reasoning over multiple pieces of supporting evidence as can be found in datasets like MCTest.': 1.0986120700836182, 'Many models have been proposed for this task, and the paper breaks down these models into ""aggregation readers"" and ""explicit reference readers.""': 1.0985701084136963, 'The authors show that the aggregation readers organize their hidden states into a predicate structure which allows them to mimic the explicit reference readers.': 1.0986123085021973, 'The authors then experiment with adding linguistic features, including reference features, to the existing models to improve performance.': 1.0986123085021973, 'I appreciate the re-naming and re-writing of the paper to make it more clear that the aggregation readers are specifically learning a predicate structure, as well as the inclusion of results about dimensionality of the symbol space.': 1.0696271657943726, 'Further, I think the effort to organize and categorize several different reading comprehension models into broader classes is useful, as the field has been producing many such models and the landscape is unclear.': 1.0986123085021973, 'The concerns with this paper are that the predicate structure demonstrated is fairly simple, and it is not clear that it provides insight towards the development of better models in the future, since the ""explicit reference readers"" need not learn it, and the CNN/Daily Mail dataset has very little headroom left as demonstrated by Chen et al. 2016.': 1.0986121892929077, 'The desire for ""dramatic improvements in performance"" mentioned in the discussion section probably cannot be achieved on these datasets.': 1.098523497581482, 'More complex datasets would probably involve multi-hop inference which this paper does not discuss.': 1.0986100435256958, 'Further, the message of the paper is a bit scattered and hard to parse, and could benefit from a bit more focus.': 0.17765262722969055, 'I think that with the explosion of various competing neural network models for NLP tasks, contributions like this one which attempt to organize and analyze the landscape are valuable, but that this paper might be better suited for an NLP conference or journal such as TACL.': 1.098556637763977, 'This paper aims to provide an insightful and analytic survey over the recent literature on reading comprehension with the distinct goal of investigating whether logical structure (or predication, as the authors rephrased in their response) arises in many of the recent models.': 1.0986123085021973, 'I really like the spirit of the paper and appreciate the efforts to organize rather chaotic recent literature into two unified themes: ""aggregation readers"" and ""explicit reference models”.': 1.0986123085021973, 'Overall the quality of writing is great and section 3 was especially nice to read.': 0.3993760049343109, 'I’m also happy with the proposed rewording from ""logical structure"" to “predication"", and the clarification by the authors was detailed and helpful.': 1.0986123085021973, 'I think I still have slight mixed feelings about the contribution of the work.': 1.0986123085021973, 'First, I wonder whether the choice of the dataset was ideal in the first place to accomplish the desired goal of the paper.': 1.0986123085021973, 'There have been concerns about CNN/DailyMail dataset (Chen et al. ACL’16)': 1.0986121892929077, 'and it is not clear to me whether the dataset supports investigation on logical structure of interesting kinds.': 1.0888704061508179, 'Maybe it is bound to be rather about lack of logical structure.': 1.0986123085021973, 'Second, I wish the discussion on predication sheds more practical insights into dataset design or model design to better tackle reading comprehension challenges.': 1.0818063020706177, 'In that sense, it may have been more helpful if the authors could make more precise analysis on different types of reading comprehension challenges, what types of logical structure are lacking in various existing models and datasets, and point to specific directions where the community needs to focus more.': 1.0984630584716797, 'The paper proposed to analyze several recently developed machine readers and found that some machine readers could potentially take advantages of the entity marker (given that the same marker points out to the same entity).': 1.0986123085021973, 'I usually like analysis papers, but I found the argument proposed in this paper not very clear.': 1.0986123085021973, 'I like the experiments on the Stanford reader, which shows that the entity marker in fact helps the Stanford reader on WDW.': 1.0986123085021973, 'I found that results rather interesting.': 1.0986123085021973, 'However, I found the organization and the overall message of this paper quite confusing.': 1.0986123085021973, 'First of all, it feels that the authors want to explain the above behavior with some definition of the “structures”.': 1.0986123085021973, 'However, I am not sure that how successful the attempt is.': 1.0986123085021973, 'For me, it is still not clear what the structures are.': 1.0986123085021973, 'This makes reading section 4 a bit frustrating.': 1.0986123085021973, 'I am also not sure what is the take home message of this paper.': 1.0986123085021973, 'Does it mean that the entity marking should be used in the MR models?': 1.0986123085021973, 'Should we design models that can also model the entity reference at the same time?': 1.0986123085021973, 'What are the roles of the linguistic features here?': 1.0986123085021973, 'Should we use linguistic structure to overcome the reference issue?': 1.0986123085021973, 'Overall, I feel that the analysis is interesting, but I feel that the paper can benefit from having a more focused argument.': 1.0986123085021973}"
474,https://openreview.net/forum?id=ryXZmzNeg,"{'This paper attempts to learn a Markov chain to estimate a probability distribution over latent variables Z, such that P(X | Z) can be eased to generate samples from a data distribution.': 1.0986008644104004, 'The paper in its current form is not acceptable due to the following reasons:': 1.0986119508743286, '1. No quantitative evaluation. The authors do include samples from the generative model, which however are insufficient to judge performance of the model. See comment 2.': 0.6392267346382141, '2. The description of the model is very unclear. I had to indulge in a lot of charity to interpret what the authors ""must be doing"". What does Q(Z) mean? Does it mean the true posterior P(Z | X) ? What is the generative model here? Typically, it\'s P(Z)P(X|Z). VAEs use a variational approximation Q(Z | X) to the true posterior P(Z | X). Are you trying to say that your model can sample from the true posterior P(Z | X)?': 0.9194484949111938, 'Comments:': 1.0986123085021973, '1. Using additive noise in the input does not seem like a reasonable idea. Any justification of why this is being done?': 0.5465338826179504, '2. Approaches which learn transition operators are usually very amenable to data augmentation-based semi-supervised learning. I encourage the authors to improve their paper by testing their model on semi-supervised learning benchmarks.': 0.8201185464859009, 'The authors propose to sample from VAEs through a Markov chain [z_t ~ q(z|x=x_{t-1}), x_t ~ p(x|z=z_t)].': 1.0986123085021973, 'The paper uses confusing notation, oversells the novelty, ignoring some relevant previous results.': 1.0986123085021973, 'The qualitative difference between regular sampling and this Gibbs chain is not very convincing, judging from the figures.': 1.098610758781433, 'It would be a great workshop paper (perhaps more), if the authors fix the notation, fix the discussion to related work, and produce more convincing (perhaps simply upscaled?) figures.': 1.0986121892929077, ""- Rezende et al's (2014) original VAE paper already discusses the Markov chain, which is ignored in this paper"": 1.0985997915267944, '- Notation is nonstandard / confusing. At page 1, it’s unclear what the authors mean with “p(x|z) which is approximated as q(x|z)”.': 1.0983251333236694, 'It’s also not clear what’s meant with q(z).': 1.0986123085021973, 'At page 2, q(z) is called the learned distribution, while p(z) can in general also be a learned distribution.': 0.842101514339447, 'It’s not true that it’s impossible to draw samples from q(z): one can sample x ~ q(x) from the dataset, then draw z ~ q(z|x).': 1.0986123085021973, ""It's not explained whether the analysis only applies to continuous observed spaces, or also discrete observed spaces"": 1.0984216928482056, 'Figures 3 and 4 are not very convincing.': 1.0985736846923828, 'The authors argues that the standard ancestral sampling from stochastic autoencoders (such as the Variational Autoencoder and the Adversarial': 1.0986123085021973, 'Autoencoder) imposes the overly-restrictive constraint that the encoder distribution must marginally match the latent variable prior.': 0.51310795545578, 'They propose, as an alternative, a Markov Chain Monte Carlo approach that avoids the need to specify a simple parametric form for the prior.': 1.095970630645752, 'The paper is not clearly written.': 1.0982736349105835, 'Most critically, the notation the authors use is either deeply flawed, or there are simple misunderstanding with respect to the manipulations of probability distributions.': 1.0986123085021973, 'For example, the authors seem to suggest that both distributions Q(Z|X) and Q(X|Z) are parametrized.': 1.0986123085021973, 'For this to be true the model must either be trivially simple, or an energy-based model.': 1.0986123085021973, 'There is no indication that they are speaking of an energy-based model.': 1.0986123085021973, 'Another example of possible confusion is the statement that the ratio of distributions Q(Z|X)/P(Z) = 1.': 1.0986123085021973, 'I believe this is supposed to be a ratio of marginals: Q(Z)/P(X) =': 1.0986123085021973, '1.': 1.0986123085021973, 'Overall, it seems like there is a confusion of what Q and P represent.': 1.0986123085021973, 'The standard notation used in VAEs is to use P to represent the decoder distribution and': 1.0986123085021973, 'Q to represent the encoder distribution.': 1.0986123085021973, 'This seems not to be how the authors are using these terms.': 1.0986123085021973, 'Nor does it seem like there is a single consistent interpretation.': 1.0986123085021973, 'The empirical results consist entirely of qualitative results (samples and reconstructions) from a single dataset (CelebA).': 1.0986123085021973, 'The samples are also not at all up to the quality of the SOTA models.': 1.0986123085021973, 'The interpolations shown in Figures 1 and 3 both seems to look like interpolation in pixel space for both the VAE model and the proposed DVAE.': 1.0986123085021973}"
475,https://openreview.net/forum?id=ryZqPN5xe,"{'This paper proposes a method of augmenting pre-trained networks for one task with an additional inference path specific to an additional task, as a replacement for the standard “fine-tuning” approach.': 1.0986123085021973, 'Pros:': 1.0986123085021973, 'The method is simple and clearly explained.': 0.6649036407470703, 'Standard fine-tuning is used widely, so improvements to and analysis of it should be of general interest.': 0.7660733461380005, 'Experiments are performed in multiple domains': 1.0986121892929077, 'vision and NLP.': 1.0986121892929077, 'Cons:': 1.0986123085021973, 'The additional modules incur a rather large cost, resulting in 2x the parameters and roughly 3x the computation of the original network (for the “stiched” network).': 1.0986123085021973, 'These costs are not addressed in the paper text, and make the method significantly less practical for real-world use where performance is very often important.': 1.0986123085021973, 'Given these large additional costs, the core of the idea is not sufficiently validated, to me.': 1.0986123085021973, 'In order to verify that the improved performance is actually coming from some unique aspects of the proposed technique, rather than simply the fact that a higher-capacity network is being used, some additional baselines are needed:': 1.0986121892929077, '(1) Allowing the original network weights to be learned for the target task, as well as the additional module.': 1.0985686779022217, 'Outperforming this baseline on the validation set would verify that freezing the original weights provides an interesting form of regularization for the network.': 1.0986121892929077, '(2) Training the full module/stitched network from scratch on the *source* task, then fine-tuning it for the target task.': 1.0567039251327515, 'Outperforming this baseline would verify that having a set of weights which never “sees” the source dataset is useful.': 1.0986121892929077, 'The method is not evaluated on ImageNet, which is far and away the most common domain in which pre-trained networks are used and fine-tuned for other tasks.': 1.098611831665039, 'I’ve never seen networks pre-trained on CIFAR deployed anywhere, and it’s hard to know whether the method will be practically useful for computer vision applications based on CIFAR results': 1.0986052751541138, 'often improved performance on CIFAR does not translate to ImageNet.': 1.0737550258636475, '(In other contexts, such as more theoretical contributions, having results only on small datasets is acceptable to me, but network fine-tuning is far enough on the “practical” end of the spectrum that claiming an improvement to it should necessitate an ImageNet evaluation.)': 0.23407086730003357, 'Overall I think the proposed idea is interesting and potentially promising, but in its current form is not sufficiently evaluated to convince me that the performance boosts don’t simply come from the use of a larger network, and the lack of ImageNet evaluation calls into question its real-world application.': 1.0660637617111206, '===============': 1.098573923110962, 'Edit (1/23/17): I had indeed missed the fact that the Stanford Cars does do transfer learning from ImageNet': 1.0986123085021973, 'thanks for the correction.': 0.7758493423461914, 'However, the experiment in this case is only showing late fusion ensembling, which is a conventional approach compared with the ""stitched network"" idea which is the real novelty of the paper.': 0.9468362927436829, 'Furthermore the results in this case are particularly weak, showing only that an ensemble of ResNet+VGG outperforms VGG alone, which is completely expected given that ResNet alone is a stronger base network than VGG (""ResNet+VGG > ResNet"" would be a stronger result, but still not surprising).': 1.0385632514953613, ""Demonstrating the stitched network idea on ImageNet, comparing with the corresponding VGG-only or ResNet-only finetuning, could be enough to push this paper over the bar for me, but the current version of the experiments here don't sufficiently validate the stitched network idea, in my opinion."": 0.19057771563529968, 'This paper proposed to perform finetuning in an augmentation fashion by freezing the original network and adding a new model aside it.': 1.0986123085021973, 'The idea itself is interesting and complements existing training and finetuning approaches, although I think there are a few baseline approaches that can be compared against, such as:': 1.0986120700836182, '(1) Ensemble: in principle, the idea is similar to an ensembling approach where multiple networks are ensembled together to get a final prediction.': 1.0986123085021973, 'The approach in Figure 1 should be compared with such ensemble baselines - taking multiple source domain predictors, possibly with the same modular setting as the proposed method, and compare the performance.': 1.0986123085021973, '(2) comparison with late fusion: if we combine the pretrained network and a network finetuned from the pretrained one, and do a late fusion?': 1.0986123085021973, 'Basically, I think it is a valuable argument in section 3.2 (and Figure 4) that finetuning with a small amount of data may hurt the performance in general.': 1.0986123085021973, 'This builds the ground for freezing a pretrained network and only augmenting it, not changing it.': 1.0986123085021973, 'I agree with the authors on this argument, although currently other than Figure 4 there seem to be little empirical study that justifies it.': 1.0986123085021973, 'It is worth noting that Figure 3 seems to suggest that some of the module filters are either not converging or are learning unuseful features - like the first two filters in 3(a).': 1.0986076593399048, 'Overall I think it is an interesting idea and I would love to see it better developed, thus I am giving a weak accept recommendation, but with a low confidence as the experiments section is not very convincing.': 1.0986121892929077, 'This paper presents a new technique for adapting a neural network to a new task for which there is not a lot of training data.': 1.0986123085021973, 'The most widely used current technique is that of fine-tuning.': 1.0986123085021973, 'The idea in this paper is to instead learn a network that learns features that are complementary to the fixed network.': 1.098604679107666, 'Additionally, the authors consider the setting where the new network/features are “stitched” to the old one at various levels in the hieararchy, rather that it just being a parallel “tower”.': 1.0986123085021973, 'This work is similar in spirit (if not in some details) to the Progressive Nets paper by Rusu et al, as already discussed.': 1.098454236984253, 'The motivations and experiments are certainly different so this submission has merit on its own.': 1.098611831665039, 'The idea of learning a “residual” with the stitched connnections is very similar in spirit to the ResNet work.': 1.0986063480377197, 'It would be nice to compare and contrast those approaches.': 0.9400273561477661, 'I’ve never seen a batch being used 5 times in a row during training, does this work better than just regular SGD?': 1.0986051559448242, 'In Figure 5 it’d be nice to label the y-axis.': 1.0986114740371704, 'That Figure would also benefit from not being a bar chart, but simply emulating Figure 4, which is much more readable!': 1.0986123085021973, 'Figure 5 again: what is an untrained model?': 1.0986121892929077, 'It’s not immediately obvious why this is a good idea at all.': 1.0784083604812622, 'Is TFT-1 simply fine-tuning one more layer than “Retrain Softmax”?': 1.0985060930252075, 'I think that the results at the end of section 3 are a bit weak because of usage of a big network.': 1.096387505531311, 'I would definitely like to see how the results change if using a smaller net.': 1.0244548320770264, 'The authors claim throughout the paper that the purpose of the added connections and layers is to learn *complementary* features': 0.9085174798965454, 'and they show this with some figures.': 0.9306809306144714, 'The latter are a convinving evidence, but not proof or guarantee that this is what is actually happening.': 0.39569616317749023, 'I suggest the authors consider adding an explicit constraint in their loss that encourages that, e.g. by having a soft orthogonality constraing (assuming one can project intermediate features to some common feature dimensionality).': 1.0986123085021973, 'The usage of very small L2 regularization maybe achieves the same thing, but there’s no evidence for that in the paper (in that we don’t have any visualizations of what happens if there’s no L2 reg.).': 1.098412275314331, 'One of the big questions for me while reading the paper was how would an ensemble of 2 pre-trained nets would do on the tasks that the authors consider.': 1.0985867977142334, 'This is especially relevant in the cars classification example, where I suspect that a strong baseline is that of fine-tuning VGG on this task, fine-tuning resnet on this task, and possibly training a linear combination of the two outputs or just averaging them naively.': 1.0974076986312866, 'Disappointing that there are no results in figure 4, 5 and 8 except the ones from this paper.': 0.7766433358192444, 'It’s really hard to situate this paper if we don’t actually know how it compares to previously published results.': 1.0854880809783936, 'In general, this was an interesting and potentially useful piece of work.': 0.988271176815033, 'The problem of efficiently reusing the previously trained classifier for retraining on a small set is certainly interesting to the community.': 1.0967825651168823, 'While I think that this paper takes a good step in the right direction, it falls a bit short in some dimensions (comparisons with more serious baselines, more understanding etc).': 0.16819977760314941}"
476,https://openreview.net/forum?id=ry_4vpixl,"{'My main objection with this work is that it operates under a hypothesis (that is becoming more and more popular in the literature) that all we need is to have gradients flow in order to solve long term dependency problems.': 1.0986123085021973, 'The usual approach is then to enforce orthogonal matrices which (in absence of the nonlinearity) results in unitary jacobians, hence the gradients do not vanish and do not explode.': 1.0986123085021973, ""However this hypothesis is taken for granted (and we don't know it is true yet) and instead of synthetic data, we do not have any empirical evidence that is strong enough to convince us the hypothesis is true."": 1.0986099243164062, ""My own issues with this way of thinking is: a) what about representational power; restricting to orthogonal matrices it means we can not represent the same family of functions as before (e.g. we can't have complex attractors and so forth if we run the model forward without any inputs)."": 1.0986123085021973, 'You can only get those if you have eigenvalues larger than 1.': 1.0986123085021973, 'It also becomes really hard to deal with noise (since you attempt to preserve every detail of the input, or rather every part of the input affects the output).': 1.0986121892929077, 'Ideally you would want to preserve only what you need for the task given limited capacity.': 1.0986123085021973, ""But you can't learn to do that."": 1.0986123085021973, 'My issue is that everyone is focused on solving this preserved issue without worrying of the side-effects.': 1.0979065895080566, 'I would like one of these papers going for jacobians having eigenvalues of 1 show this helps in realistic scenarios, on complex datasets.': 1.0986123085021973, 'This paper discusses recurrent networks with an update rule of the form h_{t+1} = R_x R h_{t}, where R_x is an embedding of the input x into the space of orthogonal or unitary matrices, and R is a shared orthogonal or unitary matrix.': 1.0986123085021973, 'While this is an interesting model, it is by no means a *new* model:  the idea of using matrices to represent input objects (and multiplication to update state) is often used in the embedding-knowledge-bases or embedding-logic literature (e.g. Using matrices to model symbolic relationships by Ilya Sutskever and Geoffrey Hinton, or Holographic Embeddings of Knowledge Graphs by Maximillian Nickel et al.).': 1.0986123085021973, ""I don't think the experiments or analysis in this work add much to our understanding of it."": 1.0986112356185913, 'In particular, the experiments are especially weak, consisting only of a very simplified version of the copy task (which is already very much a toy).': 1.0986123085021973, 'I know several people who have played with this model in the setting of language modeling, and as the other reviewer notes, the inability of the model to forget is an actual annoyance.': 1.0986123085021973, 'I think it is incumbent on the authors to show how this model can be really useful on a nontrivial task; as it is we should not accept this paper.': 1.0986121892929077, 'Some questions:  is there any reason to use the shared R instead of absorbing it into all the R_x?': 1.0986080169677734, 'Can you find any nice ways of using the fact that the model is linear in h or linear in R_x ?': 1.0986119508743286, 'This is a nice proposal, and could lead to more efficient training of': 1.0986123085021973, 'recurrent nets.': 1.0986120700836182, 'I would really love to see a bit more experimental evidence.': 1.0986123085021973, ""I asked a few questions already but didn't get any answer so far."": 1.0986123085021973, 'Here are a few other questions/concerns I have:': 1.0986123085021973, 'Is the resulting model still a universal approximator?': 1.0986084938049316, '(providing large enough hidden dimensions and number of layers)': 1.0986123085021973, 'More generally, can one compare the expressiveness of the model with the equivalent model without the orthogonal matrices?': 1.0986123085021973, 'with the same number of parameters for instance?': 1.0986123085021973, 'The experiments are a bit disappointing as the number of distinct input/output': 1.0986123085021973, 'sequences were in fact very small and as noted by the authr, training': 1.098356008529663, 'becomes unstable (I didn\'t understand what ""success"" meant in this case).': 1.0890812873840332, 'The authors point that the experiment section need to be expanded, but as': 1.0985333919525146, ""far as I can tell they still haven't unfortunately."": 1.0986123085021973}"
477,https://openreview.net/forum?id=ry_sjFqgx,"{'This paper proposes an approach to character language modeling (CLMs) based on developing a domain specific language to represent CLMs.': 1.0599955320358276, 'The experiments show mixed performance versus neural CLM approaches to modeling linux kernel data and wikipedia text, however the proposed DSL models are slightly more compact and fast to query as compared with neural CLMs.': 1.098290205001831, 'The proposed approach is difficult to understand overall and perhaps is aimed towards the sub-community already working on this sort of approach but lacks sufficient explanation for the ICLR audience.': 1.0986123085021973, 'Critically the paper glosses over the major issues of demonstrating the proposed DSL is a valid probabilistic model and how training is performed to fit the model to data (there is clearly not a gradient-based training approach used).': 1.0986123085021973, 'FInally the experiments feel incomplete without showing samples drawn from the generative model or analyzing the learned model to determine what it has learned.': 1.0985969305038452, 'Overall I feel this paper does not describe the approach in enough depth for readers to understand or re-implement it.': 1.0985699892044067, 'Almost all of the model section is devoted to exposition of the DSL without specifying how probabilities are computed using this model and how training is performed.': 1.0985952615737915, 'How are probabilities actually encoded?': 1.0976732969284058, 'The DSL description seems to have only discrete decisions rather than probabilities.': 1.0985475778579712, 'Training is perhaps covered in previous papers but there needs to be some discussion of how it works here.': 1.094754695892334, 'Section 2.5 does not do enough to explain how training works or how any measure of optimality is achieved.': 1.0986121892929077, 'Given this model is quite a different hypothesis space from neural models or n-grams, looking and samples drawn from the model seems critical.': 1.0986111164093018, 'The current experiments show it can score utterances relatively well but it would be very interesting if the model can sample more structured samples than neural approaches (for example long-range syntax constraints like brackets)': 1.09861159324646, 'The authors propose a method for language modeling by first generating a program from a DSL, then learning the count-based parameters of that program.': 1.0680675506591797, 'Pros include: The proposed method is innovative and highly different from standard LSTM-based approaches of late.': 1.0982636213302612, 'The model should also be much quicker to apply at query time.': 1.078798532485962, 'Strong empirical results are obtained on modeling code, though there is some gap between the synthesis method and neural methods on the Hutter task.': 1.0983518362045288, 'A detailed description of the language syntax is provided.': 1.0986123085021973, 'Cons/suggestions:': 1.0986123085021973, 'The synthesis procedure using MCMC is left very vague, even though being able to make this procedure efficient is one of the key questions.': 1.0986121892929077, 'The work builds on work from the PL literature; surely the related work could also be expanded and this work better put in context.': 1.0986117124557495, 'More compact/convincing examples of human interpretability would be helpful.': 0.9440751075744629, 'Other comments': 1.0986123085021973, 'Training time evaluation in Table 1 should give basic information such as whether training was done on GPU/CPU, CPU specs, etc.': 1.0499398708343506, 'This paper introduces a novel method for language modeling which is suitable for both modeling programming language as well as natural language.': 1.0986123085021973, 'The approach uses a program synthesis algorithm to search over program space and uses count-based estimation of the weights of the program.': 1.0986123085021973, 'This is a departure from neural network-based approaches which rely on gradient descent, and thus are extremely slow to estimate.': 1.0986123085021973, 'Count-based method such as regular n-gram models suffer because of their simplicity, i.e. not being able to model large context, and scaling badly as context increases.': 1.0986123085021973, 'The proposed approach synthesizes programs using MCMC which learn context-sensitive probabilities using count-based estimation, and thus is both fast and able to model long-range context.': 1.0986123085021973, 'Experiments on a programming language datasets, the linux kernel corpus, show that this method is vastly better than both LSTM and n-gram language models.': 1.0986123085021973, 'Experiments on the Wikipedia corpus show that the method is competitive, but not better, to SOTA models.': 1.0986123085021973, 'Both estimation and query time are significantly better than LSTM LMs, and competitive to n-gram LMs.': 1.0986123085021973, ""It's debatable whether this paper is suitable for ICLR, due to ICLR's focus on neural network-based approaches."": 1.0986123085021973, 'However, in the interest of diversity and novelty, such ""outside"" papers should be accepted to ICLR.': 1.0986123085021973, 'This paper is likely to inspire more research into fusion of program synthesis and machine learning methods, which was a popular theme at NIPS 2016.': 1.0986123085021973, '*Pros*': 1.0986123085021973, '1. Novel approach.': 1.0986123085021973, '2. Good results.': 1.0986123085021973, '*Cons*': 1.0986123085021973, '1. Some significant algorithmic details are not included in the paper. They should at least be included in an appendix for comprehensiveness.': 1.0986003875732422, '*Comments*': 1.0986123085021973, '1. Please include n-gram results in the table for Wikipedia results.': 1.0986123085021973}"
478,https://openreview.net/forum?id=ryaFG5ige,"{'The paper proposes to perform active learning using pool selection of deep learning mini-batches using an approximation of the bayesian posterior.': 1.0986123085021973, 'Several terms are in turn approximated.': 1.0986123085021973, 'The Maximum Likelihood Estimation (MLE) bayesian inference approach to active learning, the various approximations, and more generally the theoretical framework is very interesting but difficult to follow.': 1.095717430114746, 'The paper is written in poor English and is sometimes a bit painful to read.': 0.839824914932251, 'Alternative Active learning strategies and techniques do not need to be described with such detail.': 1.0986121892929077, 'On the other hand, the proposed approach has a lot of complex approximations which would benefit from a more detailed/structured presentation.': 1.0986067056655884, 'Another dataset would be a big plus (both datasets concern gray digits and USPS and are arguably somewhat similar).': 1.092794418334961, 'Quality:': 1.0986123085021973, 'The paper initiates a framework to incorporate active learning into the deep learning framework, mainly addressing challenges such as scalability that accompanies the training of a deep neural network.': 1.0985363721847534, 'However, I think the paper is not well polished; there are quite a lot of grammatical and typing errors.': 1.0961265563964844, 'Clarity:': 1.0986123085021973, 'The paper needs major improvements in terms of clarity.': 0.6576570272445679, 'The motivations in the introduction, i.e., why it is difficult to do active learning in deep architectures, could be better explained, and tied to the explanation in Section 3 of the paper.': 1.0931224822998047, 'For example, the authors motivated the need of (mini)batch label queries, but never mention it again in Section 3, when they describe their main methodology.': 1.0986123085021973, 'The related work section, although appearing systematic and thorough, is a little detached from the main body of the paper (related work section should not be a survey of the literature, but help readers locate your work in the relevant literature, and highlight the pros and cons.': 1.0986123085021973, ""In this perspective, maybe the authors could shorten some explanations over the related work that are not directly related, while spending more time on discussing/comparing with works that are most related to your current work, e.g., that of Graves '11."": 1.0986120700836182, 'Originality & Significance:': 1.0977587699890137, 'The authors proposed an active learning training framework.': 1.0848181247711182, 'The idea is to treat the network parameter optimization problem as a Bayesian inference problem (which is proposed previously by Graves) and formulate the active learning problem as that of sampling the most informative data, where the informativeness is defined by the variational free energy, which depends on the Fisher information.': 1.0986061096191406, 'To reconcile the computational burden of computing the inverse of Fisher Information matrix, the authors proposed techniques to approximate it (which seems to be novel)': 1.0976616144180298, 'I think that this paper initiates an interesting direction: one that adapts deep learning to label-expensive problems, via active learning.': 1.0986114740371704, 'But the paper needs to be improved in terms of presentation.': 0.29958948493003845, 'This paper introduces a mechanism for active learning with convolutional neural networks (CNNs).': 1.0978611707687378, 'I would not go as far as the authors in calling these ""deep"", seeing that they seem to have only 2 hidden layers with only 20 filters each.': 1.0986123085021973, 'The active learning criterion is a greedy selection scheme based on variational free energy and a series of approximations.': 1.0986104011535645, 'The paper is sometimes hard to read, due to (a) many grammatical errors and (b) sloppy notation in some places (e.g., on page 5, line 1, f is used but never introduced before).': 1.0986123085021973, 'Overall, I give an accepting score, but a weak one because of the grammatical errors.': 1.098608374595642, 'If the paper is accepted, these should be fixed for the final version, optimally by a native speaker.': 1.098610520362854, ""The paper's topic is interesting, and the paper appears to succeed in its goal of showing a proof of concept for active learning in CNNs (if only on toy datasets)."": 1.0986123085021973, ""I'm surprised by the new results on uncertainty sampling and curriculum learning the authors added: why do these methods both break for USPS?"": 1.0986123085021973, ""In particular, uncertainty sampling did very well (in fact, better than the authors' new method) on MNIST, but apparently horribly on USPS; some explanation for this would be useful."": 1.0986123085021973, 'I have one more question: why is it necessary to first sample a larger subset D \\subset U, from which we select using active learning?': 1.0986123085021973, 'Is this merely done for reasons of computational efficiency, or can it actually somehow improve results?': 1.0986123085021973, '(If so, it would be instrumental to see the worse results when this is not done.)': 1.0986123085021973}"
479,https://openreview.net/forum?id=ryb-q1Olg,"{'The paper presents a repurposing of rectified factor networks proposed': 1.0986123085021973, 'earlier by the same authors to biclustering.': 0.709368109703064, 'The method seems': 1.0984623432159424, 'potentially quite interesting but the paper has serious problems in': 1.0788096189498901, 'the presentation.': 1.0986123085021973, 'Quality:': 1.0985945463180542, 'The method relies mainly on techniques presented in a NIPS 2015 paper': 1.0985461473464966, 'by (mostly) the same authors.': 1.0986123085021973, 'The experimental procedure should be': 0.89185631275177, 'clarified further.': 1.0804468393325806, 'The results (especially Table 2) seem to depend': 1.0949164628982544, 'critically upon the sparsity of the reported clusters, but the authors': 1.0376402139663696, 'do not explain in sufficient detail how the sparsity hyperparameter is': 1.0976427793502808, 'determined.': 1.0986123085021973, 'Clarity:': 1.0985876321792603, 'The style of writing is terrible and completely unacceptable as a': 0.856843888759613, 'scientific publication.': 1.0896073579788208, 'The text looks more like an industry white': 1.0981214046478271, 'paper or advertisement, not an objective scientific paper.': 0.476884663105011, 'A complete': 1.0986123085021973, 'rewrite would be needed before the paper can be considered for': 0.858654797077179, 'publication.': 1.0986123085021973, 'Specifically, all references to companies using your': 1.0986123085021973, 'methods must be deleted.': 0.03592251241207123, 'Additionally, Table 1 is essentially unreadable.': 1.0834777355194092, 'I would recommend': 1.0986050367355347, 'using a figure or cleaning up the table by removing all engineering': 0.4163203537464142, 'notation and reporting numbers per 1000 so that e.g. ""0.475 +/- 9e-4""': 0.4999748766422272, 'would become ""475 +/- 0.9"".': 1.0964670181274414, 'In general figures would be preferred as a': 1.098603367805481, 'primary means for presenting the results in text while tables can be': 0.8477254509925842, 'included as supplementary information.': 1.0986114740371704, 'Originality:': 1.0986123085021973, 'The novelty of the work appears limited: the method is mostly based on': 1.0986123085021973, 'a NIPS 2015 paper by the same authors.': 1.0986123085021973, 'The experimental evaluation': 1.0986123085021973, 'appears at least partially novel, but for example the IBD detection is': 1.0986123085021973, 'very similar to Hochreiter (2013)': 1.0986123085021973, 'but without any comparison.': 1.0986123085021973, 'Significance:': 1.0986123085021973, ""The authors' strongest claim is based on strong empirical performance"": 1.0986123085021973, 'in their own benchmark problems.': 1.0986123085021973, 'It is however unclear how useful this': 1.0986123085021973, 'would be to others as there is no code available and the details of': 1.0986123085021973, 'the implementation are less than complete.': 1.0986123085021973, 'Furthermore, the method': 1.0986123085021973, 'depends on many specific tuning parameters whose tuning method is not': 1.082635760307312, 'fully defined, leaving it unclear how to guarantee the generalisation': 1.0986121892929077, 'of the good performance.': 1.0985850095748901, 'Clarity: The novel contribution of the paper': 0.48704594373703003, 'Section 2.2': 0.5516116619110107, 'was very difficult to understand.': 1.0976592302322388, 'The notation seemed inconsistent (particularly the use of l, p, and m), and I am still not confident that I understand the model being used.': 1.0986123085021973, 'Originality: The novelty comes from applying the RFN model (including the ReLU non-linearity and dropout training) to the problem of biclustering.': 1.0935097932815552, 'It sounds like a good idea.': 1.0891822576522827, 'Significance: The proposed algorithm appears to be a useful tool for unsupervised data modelling, and the authors make a convincing argument that it is significant.': 1.0986121892929077, '(I.E. The previous state-of-the-art, FABIA, is widely used and this method both outperforms and addresses some of the practical difficulties with that method.)': 0.2883552312850952, 'Quality: The experiments are high-quality.': 0.5538896322250366, 'Comments:': 1.094652771949768, '1) The introduction claims that this method is much faster than FABIA because the use of rectified units allow it to be run on GPUs.': 0.535054087638855, 'It is not clear to me how this works.': 1.0986123085021973, 'How many biclusters can be supported with this method?': 1.0986123085021973, 'It looks like the number of biclusters used for this method in the experiments is only 3-5?': 1.0951178073883057, '2) The introduction claims that using dropout during training increases sparsity in the bicluster assignments.': 0.8575307130813599, 'This seems like a reasonable hypothesis, but this claim should be supported with a better argument or experiments.': 0.7459934949874878, '3) How is the model deep?': 1.0986123085021973, ""The model isn't deep just because it uses a relu and dropout."": 1.0560044050216675, 'This paper applies RFN for biclustering to overcome the drawbacks in FABIA.': 1.0976519584655762, 'The proposed method performs best among 14 biclustering methods, However, my first concern is that from the methodological point of view, the novelty of the proposed method seems small.': 1.0986123085021973, 'The authors replied to the same question which another reviewer gave, but the replies were not so convincing.': 1.0986121892929077, 'This paper was actually difficult for me to follow.': 1.0986123085021973, 'For instance, in Figure 1, a bicluster matrix is constructed as an outer product of  and .': 1.0986123085021973, 'is a hidden unit, but what is ?': 1.0776792764663696, 'I could not find any definition.': 1.0986123085021973, 'Furthermore, I could not know how  is estimated in this method.': 1.0986104011535645, 'Therefore, I do NOT understand how this method performs biclustering.': 1.0986108779907227, 'Totally, I am not sure that this paper is suitable for publication.': 1.0986120700836182, 'Prons:': 0.5092105865478516, 'Empirical performance is good.': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'Novelty of the proposed method': 1.0986123085021973, 'Some description in the paper is unclear.': 1.0986123085021973}"
480,https://openreview.net/forum?id=rye9LT8cee,"{'The authors use the Alternating Direction Method of Multipliers (ADMM) algorithm for the first time on CNN models, allowing them to perform model compression without any appreciable loss on the CIFAR-10, CIFAR-100, and SVHN tasks.': 1.386293888092041, 'The algorithmic details and the intuition behind their algorithm are generally well presented, (although there are occasional typos).': 1.3862943649291992, 'Pros:': 1.3862943649291992, '1) Put an old algorithm to good use in a new setting': 1.3862943649291992, '2)': 1.3862943649291992, 'The algorithm has the nice property that it is partially analytically solvable (due to the separability property the authors mention).': 1.3862943649291992, 'This contributes to the efficient trainability of the model': 1.3862943649291992, '3) Seems to dovetail nicely with other results to encourage sparsity': 1.3862943649291992, 'that is, it can be used simultaneously': 1.3862943649291992, 'and is quite generalizable.': 1.3862943649291992, 'Cons:': 1.3862943649291992, '1) It would be nice to see a more thorough analysis of the performance gains for using this method, beyond raw data about % sparsity': 1.3862943649291992, 'some sort of comparison involving training time would be great, and is currently lacking.': 1.3862943649291992, 'EDIT: Authors addressed this by addition of results in Appendix B.': 1.3862943649291992, '2) I would very much like to see some discussion about why the sparsity seems to *improve* the test performance, as mentioned in my previous comment.': 1.3862943649291992, 'Is this a general feature?': 1.3862943649291992, 'Is this a statistical fluke?': 1.3862943649291992, 'etc.': 1.3862943649291992, 'Even if the answer is ""it is not obvious, and determining why goes outside the scope of this work"", I would like to know it!': 1.2562569379806519, 'EDIT: Authors addressed this by addition of statistical significance tests in the new Appendix A.': 1.267393708229065, '3) Based on the current text, and some of the other reviewer comments, I would appreciate an expanded discussion on how this work compares with other methods in the field.': 0.8409873843193054, ""I don't think a full numerical comparison is necessary, but some additional text discussing some of the other papers mentioned in the other reviews would greatly benefit the paper."": 1.284461259841919, 'EDIT: Authors addressed this by followup to question and additional text in the paper.': 1.2829402685165405, 'Additional comments: If my Cons are addressed, I would definitely raise my score to a 6 or even a 7.': 1.3579124212265015, 'The core of this paper is quite solid, it just needs a little bit more polishing.': 1.3858847618103027, 'EDIT: Score has been updated.': 1.3862941265106201, 'Note: the authors probably meant ""In order to verify"" in the first sentence of Appendix A.': 1.3843510150909424, 'This paper presents a framework to use sparsity to reduce the parameters and computations of a pre-trained CNN.': 1.3856849670410156, 'The results are only reported for small datasets and networks, while it is now imperative to be able to report results on larger datasets and production-size networks.': 1.3862943649291992, 'The biggest problem with this paper is that it does not report numbers of inference time and gains, which is very important in production.': 1.3862941265106201, 'And similarly for disk pace.': 1.3850640058517456, 'Parameters reduction is useful only if it leads to a large decrease in space or inference time.': 1.3335760831832886, 'This paper reduces the computational complexity of CNNs by minimizing the recognition loss and a sparsity-promoting penalty term together.': 1.3862943649291992, 'The authors use ADMM, which is widely used in optimization problems but not used with CNNs before.': 1.3862943649291992, 'The paper has a good motivation and well written.': 0.9637228846549988, 'The experiments show that the proposed approach increases the sparsity in a network as well as increases the performance.': 0.9723158478736877, 'As the authors stated, ADMM approach is not guaranteed to converge in non-convex problems.': 1.1776344776153564, 'The authors used pre-trained networks to mitigate the problem of trapping into a local optimum.': 1.382875680923462, 'However, the datasets that are used are very small.': 1.3862918615341187, 'It would be good to investigate how the proposed approach works on bigger datasets such as ImageNet.': 1.3839794397354126, 'Authors should compare their results with previous studies that use pruning or sparsity regularizers (Liu et al. (2015); Han et al. (2015); Collins & Kohli (2014)).': 1.3652019500732422, 'In the discussion section, authors stated that the proposed approach is efficient in training because of the separability property.': 1.101993203163147, 'Could you elaborate on that?': 1.3862943649291992, 'Lets say this work uses two phases; phase 1 is pre-training a network, phase 2 is using sparsity and performance promoting steps.': 1.3808232545852661, 'Phase 2 also includes fine-tuning the network based on the new sparse structure.': 1.1933221817016602, 'How long does phase 2 take compare to phase 1?': 1.1104457378387451, 'How many epochs is needed to fine-tune the network?': 0.892927348613739, 'The paper presents a method for sparsifying the weights of the convolutional': 0.7938915491104126, 'filters and fully-connected layers of a CNN without loss of performance.': 1.3862937688827515, 'Sparsification is achieved by using augmenting the CNN objective function with a regularization term promoting sparisty on groups of weights.': 1.3862943649291992, 'The authors use ADMM for solving this optimization task, which allows decoupling of the two terms.': 1.3862943649291992, 'The method alternates between promoting the sparsity of the network and optimizing the recognition performance': 1.3862943649291992, 'The method is technically sound and clearly explained.': 1.3862943649291992, 'The paper is well organised and the ideas presented in a structured manner.': 1.3862943649291992, 'I believe that sometimes the wording could be improved.': 0.9226054549217224, 'The proposed method is simple and effective.': 1.3839083909988403, 'Using it in combination with other CNN compression techniques such as quantization/encoding is a promising direction for future research.': 1.3862943649291992, 'The experimental evaluation is convincing in the sense that the method seems to work well.': 1.3862762451171875, ""The authors do not use state-of-the-art CNNs architectures, but I don't see this a requirement to deliver the message."": 1.3862924575805664, 'On the other hand, the proposed method is closely related to recent works (as shown in the references posted in the public comment titled ""Comparison with structurally-sparse DNNs using group Lasso), that should be cited and compared against (at least at a conceptual level).': 1.3862943649291992, 'I will of course consider the authors rebuttal on this matter.': 1.3856403827667236, 'It would be very important for the authors to comment on the differences between these works and the proposed approach.': 1.3514190912246704, 'It seems that both of these references use sparsity regularized training for neural networks, with a very similar formulation.': 0.8403995037078857, 'The authors choose to optimize the proposed objective function using ADMM.': 1.3851664066314697, 'It is not clear to me, why this approach should be more effective than proximal gradient descent methods.': 1.3862943649291992, 'Could you please elaborate on this?': 1.3862943649291992, 'ADMM is more demanding in terms of memory (2 copies of the parameters need to be stored).': 1.383851408958435, 'The claimed contributions (Section 5) seem a bit misleading in my opinion.': 1.3862911462783813, 'Using sparsity promoting regularization in the parameters of the model has been used by many works, in particular in the dictionary learning literature but also in the neural network community (as stated above).': 1.3834370374679565, 'Claims 1,2 and 3, are well understood properties of L1-type regularizers.': 1.2773817777633667, 'As written in the current version of the manuscript, it seems that these are claimed contributions of this particular work.': 1.2907177209854126, 'A discussion on why sparsity sometimes helps improve performance could be interesting.': 1.3466142416000366, 'In the experimental section, the authors mainly concentrate on comparing accuracy vs parameter reduction.': 1.3162175416946411, 'While this is naturally very relevant property to report, it would be also interesting to show more results in terms of speed-up.': 1.369384527206421, 'which should also be improved by the proposed approach.': 1.3862943649291992, 'Other minor issues:': 1.3862943649291992, 'In (3): I think a ""^2"" is missing in the augmented term (the rightmost term).': 1.3856884241104126, 'The authors could cite the approach by Han et all for compressing DNNS:': 0.7086180448532104, 'Han, ICLR 2016 ""Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding""': 1.243711233139038}"
481,https://openreview.net/forum?id=ryelgY5eg,"{'The paper presents a novel look at binary auto-encoders, formulating the objective function as a min-max reconstruction error over a training set given the observed intermediate representations.': 1.0986123085021973, 'The author shows that this formulation leads to a bi-convex problem that can be solved by alternating minimisation methods; this part is non-trivial and is the main contribution of the paper.': 1.0986123085021973, 'Proof-of-concept experiments are performed, showing improvements for 1-hidden layer auto-encoders with respect to a vanilla approach.': 1.0986123085021973, 'The experimental section is fairly weak because the literature on auto-encoders is huge and many variants were shown to perform better than straightforward approaches without being more complicated (e.g., denoising auto-encoders).': 1.0986123085021973, 'Yet, the paper presents an analysis that leads to a new learning algorithm for an old problem, and is likely worth discussing.': 1.098610520362854, 'The author attacks the problem of shallow binary autoencoders using a minmax game approach.': 1.0986123085021973, 'The algorithm, though simple, appears to be very effective.': 1.0985796451568604, 'The paper is well written and has sound analyses.': 1.039098858833313, 'Although the work does not extend to deep networks immediately, its connections with other popular minmax approaches (eg GANs) could be fruitful in the future.': 1.0986123085021973, 'The paper propose to find an optimal decoder for binary data using a min-max decoder on the binary hypercube given a linear constraint on the correlation between the encoder and the  data.': 1.0985660552978516, 'The paper gives finally that the optimal decoder as logistic of the lagragian W multiplying the encoding e.': 1.0986108779907227, 'Given the weights of the ‘min-max’decoder W the paper finds the best encoding for the data distribution considered, by minimizing that error as a function of the encoding.': 1.0985865592956543, 'The paper then alternates that optimization between the encoding and the min-max decoding, starting from random weights W.': 1.0986123085021973, 'clarity:': 1.0986123085021973, 'The paper would be easier to follow if the real data (x in section 3 ) is differentiated from the worst case data played by the model (x in section 2).': 1.0986123085021973, 'significance': 1.0986123085021973, 'Overall I like the paper, however I have some doubts on what the alternating optimization optimum ends up being.': 1.0986123085021973, 'The paper ends up implementing a single layer network.': 1.0986123085021973, 'The correlation constraints while convenient in the derivation, is  a bit intriguing.': 1.0986123085021973, 'Since linear relation between the encoding and the data  seems to be weak modeling constraint and might be not different from what PCA would implement.': 1.0986123085021973, 'what is the performance of PCA on those tasks?': 1.0986123085021973, 'one could you use a simple sign function to decode.': 1.0986123085021973, 'This is related to one bit compressive sensing.': 1.0986123085021973, 'what happens if you initialize W in algorithm one with PCA weights?': 1.0986123085021973, 'or weighted pca weights?': 1.0986123085021973, 'Have you tried on more complex datasets such as cifar?': 1.0986123085021973}"
482,https://openreview.net/forum?id=ryh9pmcee,"{'This paper is a parallel work to Improving Generative Adversarial Networks with Denoising Feature Matching.': 1.0986123085021973, 'The main solution of both papers is introducing autoencoder into discriminator to improve the stability and quality of GAN.': 1.0986123085021973, 'Different to Denoising Feature Matching, EBGAN uses encoder-decoder instead of denoising only, and use hingle loss to replace original loss function.': 1.0986123085021973, 'The theoretical results are good, and empirical result of high resolution image is unique among all recent GAN advantages.': 1.0986123085021973, 'I suggest to introduce Improving Generative Adversarial Networks with Denoising Feature Matching as related work.': 1.0986123085021973, 'This paper introduces an energy-based Generative Adversarial Network (GAN) and provides theoretical and empirical results modeling a number of image datasets (including large-scale versions of categories of ImageNet).': 1.0986123085021973, 'As far as I know energy-based GANs (EBGAN) were introduced in Kim and Bengio (2016), but the proposed version makes a number of different design choices.': 1.0986123085021973, 'First, it does away with the entropy regularization term that Kim and Bengio (2016) introduced to ensure that the GAN discriminator converged to an energy function proportional to the log density of the data (at optimum).': 1.0986123085021973, 'This implies that the discriminator in the proposed scheme will become uniform at convergence as discussed in the theoretical section of the paper, however the introductory text seems to imply otherwise': 1.0986123085021973, 'that one could recover a meaningful score function from the trained energy-function (discriminator).': 1.0986123085021973, 'This should be clarified.': 1.0986123085021973, 'Second, this version of the EBGAN setting includes two innovations: (1) the introduction of the hinge loss in the value function, and (2) the use of an auto-encoder parametrization for the energy function.': 1.0986117124557495, 'These innovations are not empirically justified in any way - this is disappointing, as it would be really good to see empirical results supporting the arguments made in support of their introduction.': 1.0986123085021973, 'The two significant contributions of this paper are the theoretical analysis of the energy-baesd GAN formalism (showing that the optimum corresponds to a Nash equilibrium) and the impressive empirical results on large images that set a new standard in what straight GAN-style models can achieve.': 1.0985921621322632, 'The theoretical results seem solid to me and make a nice contribution.': 0.461794376373291, 'Regarding the quantitative results in Table 2, it seems not appropriate to bold the EBGAN line when it seems to be statistically indistinguishable form the Rasmus et al (2015) results.': 1.0974338054656982, 'Though it is not mentioned here, the use of bold typically indicates the state-of-the-art.': 0.9055944681167603, 'I think this paper could be be much stronger if the two novel contributions to the energy-based GAN setting were more thoroughly explored with ablation experiments.': 1.0986123085021973, 'That being said, I think this paper has already become a contribution other are building on (including at least two other ICLR submissions) and so I think it should be accepted for publication at ICLR.': 1.0985524654388428, ""This paper proposes a novel extension of generative adversarial networks that replaces the traditional binary classifier discriminator with one that assigns a scalar energy to each point in the generator's output domain."": 1.098609209060669, 'The discriminator minimizes a hinge loss while the generator attempts to generate samples with low energy under the discriminator.': 1.096289873123169, 'The authors show that a Nash equilibrium under these conditions yields a generator that matches the data distribution (assuming infinite capacity).': 1.0986123085021973, 'Experiments are conducted with the discriminator taking the form of an autoencoder, optionally including a regularizer that penalizes generated samples having a high cosine similarity to other samples in the minibatch.': 1.0986123085021973, 'Pros:': 1.0986123085021973, '*': 1.0986123085021973, 'The paper is well-written.': 1.0986123085021973, 'The topic will be of interest to many because it sets the stage for the exploration of a wider variety of discriminators than currently used for training GANs.': 1.0986123085021973, '* The theorems regarding optimality of the Nash equilibrium appear to be correct.': 1.0986123085021973, '* Thorough exploration of hyperparameters in the MNIST experiments.': 1.0986123085021973, '* Semi-supervised results show that contrastive samples from the generator improve classification performance.': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'The relationship to other works that broaden the scope of the discriminator (e.g. [1]) or use a generative network to provide contrastive samples to an energy-based model ([2]) is not made clear in the paper.': 1.0986123085021973, '* From visual inspection alone it is difficult to conclude whether EB-GANs produce better samples than DC-GANs on the LSUN and CelebA datasets.': 1.098600149154663, '* It is difficult to assess the effect of the PT regularizer beyond visual inspection as the Inception score results are computed with the vanilla EB-GAN.': 1.0984314680099487, 'Specific Comments': 1.0986123085021973, '* Sec 2.3: It is unclear to me why a reconstruction loss will necessarily produce very different gradient directions.': 1.0986123085021973, '* Sec 2.4: It is confusing that ""pulling-away"" is abbreviated as ""PT"".': 1.0986123085021973, '* Sec 4.1: It seems strange that the Inception model (trained on natural images) is being used to compute KL scores for MNIST.': 1.0986117124557495, 'Using an MNIST-trained CNN to compute Inception-style scores seems to be more appropriate here.': 1.0986052751541138, '* Figure 3: There is little variation across the histograms, so this figure is not very enlightening.': 1.0981203317642212, '* Appendix A:': 1.0986123085021973, 'In the proof of theorem 2, it is unclear to me why a Nash equilibrium of the system exists.': 1.0986121892929077, 'Typos / Minor Comments': 1.0986123085021973, 'Abstract: ""probabilistic GANs"" should probably be ""traditional"" or ""classical"" GANs.': 1.0986123085021973, '* Theorem 2: ""A Nash equilibrium ... exists""': 1.0986123085021973, '* Sec 3: Should be ""Several papers were presented""': 1.0986123085021973, 'Overall, I have some concerns with the related work and experimental evaluation sections, but I feel the model is novel enough and is well-justified by the optimality proofs and the quality of the generated samples.': 1.0751776695251465, '[1] Springenberg, Jost Tobias.': 1.0986123085021973, '""Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks.""': 1.0986123085021973, 'arXiv preprint arXiv:1511.06390 (2015).': 1.0986123085021973, '[2] Kim, Taesup, and Yoshua Bengio.': 1.0986123085021973, '""Deep Directed Generative Models with Energy-Based Probability Estimation.""': 1.0986123085021973, 'arXiv preprint arXiv:1606.03439 (2016).': 1.0986123085021973}"
483,https://openreview.net/forum?id=ryh_8f9lg,"{'The paper presents an alternative way of supervising the training of neural network without explicitly using labels when only link/not-link information is available between pairs of examples.': 1.0986123085021973, 'A pair of network is trained each of which is used to supervise the other one.': 1.0986123085021973, 'The presentation of the paper is not very clear, the writing can be improved.': 1.0952285528182983, 'Some design choice are not explained: Why is the power function used in the E-step for approximating the distribution (section 2.1)?': 1.0986120700836182, 'Why do the authors only consider a uniform distribution?': 1.0986121892929077, 'I understand that using a different prior breaks the assumption that nothing is known about the classes.': 1.0986123085021973, 'However I do not see a practical situations where the proposed setting/work would be useful.': 1.0986123085021973, 'Also, there exist a large body of work in semi-supervised learning with co-training based on a similar idea.': 1.0986123085021973, 'Overall, I think this work should be clarified and improved to be a good fit for this venue.': 1.098610758781433, 'The paper explores a new technique for classless association, a milder unsupervised learning where we do not know the class labels exactly, but we have a prior about the examples that belong to the same class.': 1.0986064672470093, 'Authors proposed a two stream architecture with two neural networks, as streams process examples from the same class simultaneously.': 1.0986120700836182, 'Both streams rely on the target (pseudo classes or cluster indices) of each other, and the outputs an intermediate representation z, which is forced to match with a statistical distribution (uniform in their case).': 1.0986123085021973, 'The model is trained with EM where the E step obtains the current statistical distribution given output vectors z, and M step updates the weights of the architecture given z and pseudo-classes.': 1.0986067056655884, 'Experimental results on re-organized MNIST exhibits better performance compared to classical clustering algorithms (in terms of association accuracy and purity).': 1.0852534770965576, 'The authors further provide comparison against a supervised method, where proposed architecture expectedly performs worse but with promising results.': 1.0986123085021973, 'The basic motivation of the architecture apparently relies on unlabeled data and agreement of the same pseudo-labels generated by two streams.': 1.0986123085021973, 'But the paper is hard to follow and the motivation for the proposed architecture itself, is hidden in details.': 1.0986119508743286, 'What is trying to be achieved by matching distributions and using the pseudo-targets of the each other?': 1.0986121892929077, 'Perhaps the statistical distribution of the classes is assumed to be uniform but how will it extend to other priors, or even the case where we do not assume that we know the prior?': 1.0985954999923706, 'The current setup needs justifications.': 0.7315106391906738, 'What would be very interesting is to see two examples having the same class but one from MNIST, the other from Rotated-MNIST or Background-MNIST.': 1.0906039476394653, 'Because it is hard to guess how different the examples in two streams.': 0.9635229110717773, 'At the end, I feel like the authors have found a very interesting approach for classless association which can be extended to lots of many-to-one problems.': 1.0982335805892944, 'This is a good catch.': 1.0986123085021973, 'I would like to see the idea in the future with some extensive experiments on large scale datasets and tasks.': 1.0986123085021973, 'But the current version lacks the theoretical motivations and convincing experiments.': 1.0986123085021973, 'I would definitely recommend this paper to be presented in ICLR workshop.': 1.0986123085021973, 'Few more points:': 1.0986123085021973, 'Typo: Figure1.': 1.0986123085021973, 'second line in the caption ""that"" -> ""than""': 1.0986123085021973, 'Necessity of Equation 2 is not clear': 1.0986123085021973, 'Batch size M is enormous compared to classical models, there is no explanation for this': 1.0986123085021973, 'Why uniform? should be clarified (of course it is the simplest prior to pick but just a few words about it would be good for completeness)': 1.0986123085021973, 'Typo: Page 6, second paragraph line 3: ""that"" -> ""than""': 1.0986123085021973, 'The paper looks correct but still i am not convinced about the experimentation performed.': 1.0986123085021973, 'Perhaps another experiment with more challenging data would be welcome.': 1.0986123085021973, ""Honestly i don't find a clear motivation for this work however it could have some potential and it would be interested to be presented in conference."": 1.0986123085021973}"
484,https://openreview.net/forum?id=ryhqQFKgl,"{'After the discussion below, I looked at previous work by the authors (MUS-ROVER) on which this paper was based.': 1.098544716835022, 'On one hand, this was very helpful for me to better understand the current paper.': 1.0986121892929077, 'On the other hand, this was very needed for me to better understand the current paper.': 1.0986123085021973, 'Overall, while I think that I like this work, and while I am familiar with the JSB chorales, with probabilistic approaches, with n-grams, etc, I did find the paper quite hard to follow at various parts.': 1.0986123085021973, 'The extensive use of notation did not help the clarity.': 1.0986078977584839, 'I think the ideas and approaches are good, and certainly worth publishing and worth pursuing.': 1.0986123085021973, ""I am not sure that, in the paper's current form, ICLR is an appropriate venue."": 1.0981814861297607, '(Incidentally, the issue is not the application as I think that music applications can be very appropriate, nor is the problem necessarily with the approach... see my next suggestion..).': 0.6880042552947998, 'I get the sense that a long-form journal publication would actually give the authors the space necessary to fully explain these ideas, provide clearer running examples where needed, provide the necessary background for the appropriate readership, provide the necessary background on the previous system, perhaps demonstrating results on a second dataset to show generality of the approach, etc.': 1.0986123085021973, 'A short conference paper just seems to me to be too dense a format for giving this project the description it merits.': 1.0986123085021973, 'If it were possible to focus on just one aspect of this system, then that might work, but I do not have good suggestions for exactly how to do that.': 1.0938937664031982, 'If the paper were revised substantially (though I cannot suggest details for how to do this within the appropriate page count), I would consider raising my score.': 1.0986123085021973, 'I do think that the effort would be better invested in turning this into a long (and clearer) journal submission.': 1.0986123085021973, '[Addendum: based on discussions here & revisions, I have revised my score]': 1.0986123085021973, 'Summary:': 1.0986123085021973, ""The paper presents an advanced self-learning model that extracts compositional rules from Bach's chorales, which extends their previous work in: 1) the rule hierarchy in both conceptual and informational dimensions; 2) adaptive 2-D memory selection which assumes the features follow Dirichlet Distribution."": 1.0986123085021973, 'Sonority (column of 4 MIDI numbers) acts as word in language model: unigram statistics have been used to learn the fundamental rules in music theory, while n-grams with higher order help characterize part writing.': 1.0986123085021973, 'Sonorities have been clustered together based on feature functions through iterations.': 1.0986123085021973, 'The partition induced by the features is recognized as a rule if it is sufficiently significant.': 1.0986123085021973, 'As a result, two sample syllabi with different difficulty strides and ""satisfactory gaps"" have been generated in terms of sets of learned rules.': 1.0986123085021973, '1. Quality:': 1.0986123085021973, 'a) Strengths: In the paper, the exploration of hierarchies in two dimensions makes the learning process more cognitive and interpretable.': 1.0986123085021973, 'The authors also demonstrate an effective memory selection to speed up the learning.': 1.0986123085021973, 'b) Flaws: The paper only discussed N<=5, which might limit the learning and interpretation capacities of the proposed model, failing to capture long-distance dependence of music.': 1.0986123085021973, ""(In the replies to questions, the authors mentioned they had experimented with max N=10, but I'm not sure why related results were not included in the paper)."": 1.096315860748291, 'Besides the elaborated interpretation of results, a survey seeking the opinions of students in a music department might make the evaluation of system performance more persuasive.': 1.0985811948776245, '2. Clarity:': 0.5239258408546448, 'a) Pros: The paper clearly delivers an improved automatic theorist system which learns and represents music concepts as well as thoroughly interprets and compares the learned rules with music theory.': 1.0986100435256958, 'Proper analogies and examples help the reader perceive the ideas more easily.': 0.7673900723457336, ""b) Cons: Although detailed definitions can be found in the authors' previous MUS-ROVER I papers, it would be great if they had described the optimization more clearly (in Figure 1. and related parts)."": 1.098273754119873, 'The ""(Conceptual-Hierarchy Filter)"" row in equations (3): the prime symbol should appear in the subscript.': 1.0986123085021973, '3. Originality:': 0.9973797798156738, 'The representation of music concepts and rules is still an open area, the paper investigate the topic in a novel way.': 0.4056324064731598, 'It illustrates an alternative besides other interpretable feature learning methods such as autoencoders, GAN, etc.': 0.7752779126167297, '4. Significance:': 0.9361836314201355, 'It is good to see some corresponding interpretations for the learned rules from music theory.': 1.0893075466156006, 'The authors mentioned students in music could and should be involved in the self-learning loop to interact, which is very interesting.': 1.0652835369110107, 'I hope their advantages can be combined in the practice of music theory teaching and learning.': 0.409670352935791, ""This paper proposes an interesting framework (as a follow-up work of the author's previous paper) to learn compositional rules used to compose better music."": 1.0986123085021973, 'The system consists of two components, a generative component (student) and a discriminative component (teacher).': 1.0983003377914429, 'The generative component is a Probabilistic Graphical Models, generating the music following learned rules.': 1.0986018180847168, 'The teacher compares the generated music with the empirical distribution of exemplar music (e.g, Bach’s chorales) and propose new rules for the student to learn so that it could improve.': 1.09861159324646, 'The framework is different from GANs that the both the generative and discriminative components are interpretable.': 1.0986121892929077, 'From the paper, it seems that the system can indeed learn sensible rules from the composed music and apply them in the next iteration, if trained in a curriculum manner.': 1.0985534191131592, 'However, there is no comparison between the proposed system and its previous version, nor comparison between the proposed system and other simple baselines, e.g., an LSTM generative model.': 1.0986123085021973, 'This might pose a concern here.': 1.0986123085021973, 'I found this paper a bit hard to read, partly due to (1) lots of music terms (e.g, Tbl. 1 does not make sense to me) that hinders understanding of how the system performs, and (2) over-complicated math symbols and concept.': 1.0986123085021973, 'For example, In Page 4, the concept of raw/high-level feature, Feature-Induced Partition and Conceptual Hierarchy, all means a non-overlapping hierarchical clustering on the 4-dimensional feature space.': 1.0986123085021973, 'Also, there seems to be no hierarchy in Informational Hierarchy, but a list of rules.': 1.0986123085021973, 'It would be much clearer if the authors write the paper in a plain way.': 1.0986123085021973, 'Overall, the paper proposes a working system that seems to be interesting.': 1.0986123085021973, 'But I am not confident enough to give strong conclusions.': 1.0986123085021973}"
485,https://openreview.net/forum?id=ryjp1c9xg,"{'Overall the paper has the feel of a status update by some of the best researchers in the field.': 1.0986123085021973, ""The paper is very clear, the observations are interesting, but the remarks are scattered and don't add up to a quantum of progress in the study of what can be done with the Neural GPU model."": 1.0986120700836182, 'Minor remark on the use of the term RNN in Table 1: I found Table 1 confusing because several of the columns are for models that are technically RNNs, and use of RNNs for e.g. translation and word2vec highlight that RNNs can be characterized in terms of the length of their input sequence, the length of their input, and the sizes (per step) of their input, output, and working memories.': 1.0986120700836182, 'Basic model question: How are inputs presented (each character 1-hot?) and outputs retrieved when there are e.g. 512 “filters” in the model ?': 1.0986123085021973, 'If inputs and outputs are 1-hot encoded, and treated with the same filters as intermediate layers, then the intermediate activation functions should be interpretable as digits, and we should be able to interpret the filters as implementing a reliable e.g. multiplication-with-carry algorithm.': 1.0986123085021973, 'Looking at the intermediate values may shed some light on why the usually-working models fail on e.g. the pathological cases identified in Table 3.': 1.0986123085021973, 'The preliminary experiment on input alignment is interesting in two ways: the seeds for effective use of an attentional mechanism are there, but also, it suggests that the model is not presently dealing with general expression evaluation the way a correct algorithm should.': 1.0986123085021973, 'The remarks in the abstract about improving the memory efficiency of Neural GPU seem overblown': 1.091722011566162, 'the paragraph at the top of page 6 describes the improvements as using tf.while_loop instead of unrolling the graph, and using swap_memory to use host memory when GPU memory runs short.': 1.0857112407684326, 'These both seem like good practice, but not a remarkable improvement to the efficiency of the model, in fact it would likely slow down training and inference when memory does in fact fit in the GPU.': 1.0986123085021973, 'The point about trying many of random seeds to get convergence makes me wonder if the Neural GPU is worth its computational cost at all, when evaluated as means of learning algorithms that are already well understood (e.g. parsing and evaluating S-exprs).': 1.0986123085021973, 'Consider spending all of the computational cycles that go into training one of these models (with the multiple seeds) on a traditional search through program space (e.g. sampling lisp programs or something).': 1.098598837852478, 'The notes on the curriculum strategies employed to get the presented results were interesting to read, as an indication of the lengths to which someone might have to go to train this sort of model, but it does leave this reviewer with the impression that despite the stated extensions of the Neural GPU model it remains unclear how useful it might be to practical problems.': 1.0973252058029175, 'The authors investigate the neural GPU model introduced by Kaiser and Sutskever.': 1.0802950859069824, 'In section 3 they claim its performance is due to the O(n^2) number of steps it can perform for each example.': 1.0986123085021973, 'In the subsequent section they highlight the importance of curriculum training and empirically show that larger models generalize better.': 1.0986123085021973, 'In section 5 they construct examples that reveal failure modes.': 1.0986123085021973, 'In the last section they compare the performance given different input formats.': 1.0986123085021973, 'The paper is well written.': 0.8510245084762573, 'It contains an exhaustive set of experiments which provide insight into the details of training the neural GPU model.': 1.0986098051071167, 'It pushes the boundary of algorithms that can be learned further.': 1.09848153591156, 'On the other hand, the paper seems to lack a coherent message.': 0.53877854347229, 'It also fails to provide any insight into the how and why of the observations made (i.e. why curriculum training is essential and why certain failure modes exist).': 1.0986120700836182, 'The introduction contains several statements which should be qualified or explained further.': 1.098249077796936, 'As far as I am aware statistical learning theory does not guarantee that empirical risk minimization is consistent when the number of parameters is larger than the number of examples; the generalization performance depends on the VC dimension of the function space instead.': 1.0986112356185913, 'Furthermore, the suggested link between adversarial examples and learning algorithms is tenuous, and references or a further explanation should be provided for the contentious statement that deep neural networks are able to match the performance of any parallel machine learning algorithm.': 1.0986112356185913, 'The authors argue that the neural GPU performs O(n^2) “steps” on each example, which allows it to learn algorithms with super-linear complexity such as multiplication.': 1.0986117124557495, 'This analysis seems to overlook the parallel nature of the neural GPU architecture: Both addition and multiplication have O(log n) time complexity when parallelism is used (cf.': 1.0986065864562988, 'a carry-lookahead adder and a Wallace tree respectively).': 1.0986123085021973, 'In section 4 the authors show that their larger models generalize better, which they argue is not self-evident.': 1.0986123085021973, 'However, since both training and test error decrease it is likely that the smaller models are underfitting, in which case it is not counter-intuitive at all that a larger model would have better generalization error.': 1.0985970497131348, 'It is interesting to see that progressively decreasing the number of terms and increasing the radix of the number system works well as a learning curriculum, although it would be nice to have a stronger intuitive or theoretical justification for the latter.': 1.085733413696289, 'The final section claims that neural GPUs are cellular automata.': 0.45323389768600464, 'Further justification for this statement would be useful, since cellular automata are discrete models, and the equivalence between both models is in no way obvious.': 0.7538512945175171, 'The relationship between global operations and changing the input format is circuitous.': 0.283685564994812, 'In conclusion, the paper provides some useful insights into the neural GPU model, but does not introduce original extensions to the model and does not explain any fundamental limitations.': 1.0947564840316772, 'Several statements require stronger substantiation.': 1.0559935569763184, 'Pro:': 1.098510503768921, '* Well written': 1.0986123085021973, '* Exhaustive set of experiments': 1.0986082553863525, '* Learning algorithms with decimal representation': 0.4059385061264038, '* Available source code': 1.0986123085021973, 'Cons:': 1.0986123085021973, '* No coherent hypothesis/premise advanced': 1.0985729694366455, '* Two or three bold statements without explanation or references': 1.0969855785369873, '*': 1.0986120700836182, 'Some unclarity in experimental details': 0.539451539516449, '* Limited novelty and originality factor': 1.0986123085021973, 'Typos: add minus in “chance of carrying k digits is 10^k” (section 5); remove “are” from “the larger models with 512 filters are achieve” (section 4); add “a” in “such model doesn’t generalize” (section 4).': 1.098330020904541, 'The paper investigates on better training strategies for the Neural GPU models as well as studies the limitations of the model.': 1.0986123085021973, 'Pros:': 1.0986123085021973, '* Well written.': 1.09822416305542, 'Many investigations.': 0.5348048806190491, '* Available source code.': 1.0984437465667725, '* Misleading title, there is no extension to the Neural GPU model, just to its training strategies.': 0.643517255783081, 'No comparisons to similar architectures (e.g. Grid LSTM, NTM, Adaptive Computation Time).': 1.0986028909683228, 'More experiments on other tasks would be nice, it is only tested on some toy tasks.': 1.0986100435256958, '* No positive results, only negative results.': 1.0986123085021973, 'To really understand the negative results, it would be good to know what is missing to make it work.': 1.0986121892929077, 'This has not been studied further.': 1.098610520362854, 'Some details remain unclear or missing, e.g. if gradient noise was used in all experiments, or the length of sequences e.g. in Figure 3.': 1.0986123085021973, '* Misleading number of NTM computation steps.': 1.0986123085021973, 'You write O(n)': 1.0986123085021973, 'but it is actually variable.': 1.0986123085021973, 'After the results from the paper, the limitations still remain unclear because it is not clear exactly why the model fails.': 1.0986123085021973, 'Despite showing some examples which make it fail, it was not studied in more detail why it failed for those examples, and how you could fix the problem.': 1.0986123085021973}"
486,https://openreview.net/forum?id=ryrGawqex,"{'Authors describe implementation of TensorFlow Fold which allows one to run various computations without modifying computation graph.': 1.0986104011535645, 'They achieve this by creating a generic scheduler as a TensorFlow computation graph, which can accept graph description as input and execute it.': 1.0986123085021973, 'They show clear benefits to this approach for tasks where computation changes for each datapoint, such as the case with TreeRNN.': 1.0986123085021973, 'In the experiments, they compare against having static batch (same graph structure repeated many times) and batch size 1.': 1.0986123085021973, 'The reason my score is 7 and not higher is because they do not provide comparison to the main alternative of their method': 1.0986123085021973, 'someone could create a new TensorFlow graph for each dynamic batch.': 1.0891212224960327, 'In other words, instead of using their graph as the scheduling algorithm, one could explicitly create each non-uniform batch as a TensorFlow graph, and run that using standard TensorFlow.': 1.09861159324646, 'The paper presents a novel strategy to deal with dynamic computation graphs.': 1.0986086130142212, 'They arise, when the computation is dynamically influenced by the input data, such as in LSTMs.': 1.0986123085021973, ""The authors propose an `unrolling' strategy over the operations done at every step, which allows a new kind of batching of inputs."": 1.0986121892929077, 'The presented idea is novel and the results clearly indicate the potential of the approach.': 1.0897555351257324, 'For the sake of clarity of the presentation I would drop parts of Section 3 (""A combinator library for neural networks"") which presents technical details that are in general interesting, but do not help the understanding of the core idea of the paper.': 1.0986123085021973, 'The presented experimental results on the ""Stanford Sentiment Treebank"" are in my opinion not supporting the claim of the paper, which is towards speed, than a little bit confusing.': 1.0986123085021973, 'It is important to point out that even though the presented ensemble ""[...] variant sets a new state-of-the-art on both subtasks""': 1.0986123085021973, '[p. 8], this is not due to the framework, not even due to the model (comp.': 0.7813310623168945, 'lines 4 and 2 of Tab. 2), but probably, and this can only be speculated about, due to the ensemble averaging.': 0.4048253297805786, 'I would appreciate a clearer argumentation in this respect.': 1.0986123085021973, 'Update on Jan. 17th:': 1.0985082387924194, 'after the authors update for their newest revision, I increase my rating to 8 due to the again improved, now very clear argumentation.': 0.7589089870452881, 'The paper describes a novel technique to improve the efficiency of computation graphs in deep learning frameworks.': 1.0986123085021973, 'An impressive speedup can be observed in their implementation within TensorFlow.': 1.0986123085021973, 'The content is presented with sufficient clarity, although some more graphical illustrations could be useful.': 1.0986123085021973, 'This work is relevant in order to achieve highest performance in neural network training.': 1.098482370376587, 'Pros:': 1.0986123085021973, 'significant speed improvements through dynamic batching': 1.0876916646957397, 'source code provided': 1.0986123085021973, 'Cons:': 1.0986123085021973, 'the effect on a large real-world (ASR, SMT) would allow the reader to put the improvements better into context': 1.029970645904541, 'presentation/vizualisation can be improved': 0.5926690697669983}"
487,https://openreview.net/forum?id=ryuxYmvel,"{'Use of ML in ITP is an interesting direction of research.': 1.0986123085021973, 'Authors consider the problem of predicting whether a given statement would be useful in a proof of a conjecture or not.': 1.0986123085021973, 'This is posed as a binary classification task and authors propose a dataset and some deep learning based baselines.': 1.0986123085021973, 'I am not an expert on ITP or theorem proving, so I will present a review from more of a ML perspective.': 1.0986123085021973, 'I feel one of the goals of the paper should be to present the problem to a ML audience in a way that is easy for them to grasp.': 1.0986123085021973, 'While most of the paper is well written, there are some sections that are not clear (especially section 2):': 1.0986123085021973, 'Terms such as LCF, OCaml-top level, deBruijn indices have been used without explaining or any references.': 1.0986123085021973, 'These terms might be trivial in ITP literature, but were hard for me to follow.': 1.0986123085021973, 'Section 2 describes how the data was splits into train and test set.': 1.0986123085021973, 'One thing which is unclear is – can the examples in the train and test set be statements about the same conjecture or are they always statements about different conjectures?': 1.0986123085021973, 'It also unclear how the deep learning models are applied.': 1.0986123085021973, 'Let’s consider the leftmost architecture in Figure 1.': 1.0986123085021973, 'Each character is embedded into 256-D vector – and processed until the global max-pooling layer.': 1.0986123085021973, 'Does this layer take a max along each feature and across all characters in the input?': 1.0986123085021973, 'My another concern is only deep learning methods are presented as baselines.': 1.0986123085021973, 'It would be great to compare with standard NLP techniques such as Bag of Words followed by SVM.': 1.0986123085021973, 'I am sure these would be outperformed by neural networks, but the numbers would give a sense of how easy/hard the current problem setup is.': 1.0986123085021973, 'Did the authors look at the success and failure cases of the algorithm?': 1.0986123085021973, 'Are there any insights that can be drawn from such analysis that can inform design of future models?': 1.0986123085021973, 'Overall I think the research direction of using ML for theorem proving is an interesting one.': 1.0986123085021973, 'However, I also feel the paper is quite opaque.': 1.0986123085021973, 'Many parts of how the data is constructed is unclear (atleast to someone with little knowledge in ITPs).': 1.0986123085021973, 'If authors can revise the text to make it clearer – it would be great.': 1.0986123085021973, 'The baseline models seem to perform quite well, however there are no insights into what kind of ability the models are lacking.': 1.0986123085021973, 'Authors mention that they are unable to perform logical reasoning – but that’s a very vague statement.': 1.0986123085021973, 'Some examples of mistakes might help make the message clearer.': 1.0986123085021973, 'Further, since I am not well versed with the ITP literature it’s not possible for me to judge how valuable is this dataset.': 1.0986123085021973, 'From the references, it seems like it’s drawn from a set of benchmark conjectures/proofs used in the ITP community – so its possibly a good dataset.': 1.0986123085021973, 'My current rating is a weak reject, but if the authors address my concerns I would change to an accept.': 1.0986123085021973, 'The authors present a dataset extraction method, dataset and first interesting results for machine-learning supported higher order logic theorem proving.': 1.0038096904754639, 'The experimental results are impressively good for a first baseline and with an accuracy higher than 0.83 in relevance classification a lot better than chance, and encourage future research in this direction.': 1.0402227640151978, 'The paper is well-written in terms of presentation and argumentation and leaves little room for criticism.': 1.0986123085021973, 'The related work seems to be well-covered, though I have to note that I am not an expert for automated theorem proving.': 1.0986120700836182, 'The authors describe a dataset of proof steps in higher order logic derived from a set of proven theorems.': 1.098591923713684, 'The success of methods like AlphaGo suggests that for hard combinatorial style problems, having a curated set of expert data (in this case the sequence of subproofs) is a good launching point for possibly super-human performance.': 1.0986123085021973, 'Super-human ATPs are clearly extremely valuable.': 1.0984833240509033, 'Although relatively smaller than the original Go datasets, this dataset seems to be a great first step.': 0.822124719619751, 'Unfortunately, the ATP and HOL aspect of this work is not my area of expertise.': 0.7198121547698975, ""I can't comment on the quality of this aspect."": 0.5822244882583618, 'It would be great to see future work scale up the baselines and integrate the networks into state of the art ATPs.': 1.0986120700836182, ""The capacity of deep learning methods to scale and take advantage of larger datasets means there's a possibility of an iterative approach to improving ATPs: as the ATPs get stronger they may generate more data in the form of new theorems."": 1.0985926389694214, 'This may be a long way off, but the possibility is exciting.': 1.0972529649734497}"
488,https://openreview.net/forum?id=rywUcQogx,"{'It is not clear to me at all what this paper is contributing.': 1.0986123085021973, 'Deep CCA (Andrew et al, 2013) already gives the gradient derivation of the correlation objective with respect to the network outputs which are then back-propagated to update the network weights.': 0.8298295736312866, 'Again, the paper gives the gradient of the correlation (i.e. the CCA objective) w.r.t.': 1.0986123085021973, 'the network outputs, so it is confusing to me when authors say that their differentiable version enables them to back-propagate directly through the computation of CCA.': 1.0986123085021973, 'After a second look of the paper, I am still confused what the authors are trying to achieve.': 1.0986123085021973, 'The CCA objective is not differentiable in the sense that the sum of singular values (trace norm) of T is not differentiable.': 1.0986123085021973, 'It appears to me (from the title, and section 3), the authors are trying to solve this problem.': 1.0986123085021973, 'However,': 1.0986123085021973, 'Did the authors simply reformulate the CCA objective or change the objective?': 1.0986123085021973, 'The authors need to be explicit here.': 1.0986123085021973, 'What is the relationship between the retrieval objective and the ""CCA layer""?': 1.0986123085021973, 'I could imagine different ways of combining them, such as combination or bi-level optimization.': 1.0986123085021973, 'And I could not find discussion about this in section 3.': 1.0986123085021973, 'For this, equations would be helpful.': 1.0986123085021973, 'Even though the CCA objective is not differentiable in the above sense, it has not caused major problem for training (e.g., in principle we need batch training, but empirically using large minibatches works fine).': 1.098260760307312, 'The authors need to justify why the original gradient computation is problematic for what the authors are trying to achieve.': 1.0986123085021973, ""From the authors' response to my question 2, it seems they still use SVD of T, so I am not sure if the proposed method has advantage in computational efficiency."": 1.0986123085021973, 'In terms of paper organization, it is better to describe the retrieval objective earlier than in the experiments.': 1.0986123085021973, 'And I still encourage the authors to conduct the comparison with contrastive loss that I mentioned in my previous comments.': 1.0985376834869385, 'The authors propose to combine a CCA objective with a downstream loss.': 1.0986123085021973, 'This is a really nice and natural idea.': 1.0744813680648804, 'However, both the execution and presentation leave a lot to be desired in the current version of the paper.': 1.0978413820266724, 'It is not clear what the overall objective is.': 0.8489057421684265, 'This was asked in a pre-review question but the answer did not fully clarify it for me.': 1.0986123085021973, 'Is it the sum of the CCA objective and the final (top-layer) objective, including the CCA constraints?': 1.0986123085021973, 'Is there some interpolation of the two objectives?': 1.0986123085021973, 'By saying that the top-layer objective is ""cosine distance"" or ""squared cosine distance"", do you really mean you are just minimizing this distance between the matched pairs in the two views?': 1.0986123085021973, 'If so, then of course that does not work out of the box without the intervening CCA layer:  You could minimize it by setting all of the projections to a single point.': 1.098595142364502, 'A better comparison would be against a contrastive loss like the Hermann & Blunsom one mentioned in the reviewer question, which aims to both minimize the distance for matched pairs and separate mismatched ones (where ""mismatched"" ones can be uniformly drawn, or picked in some cleverer way).': 1.0986123085021973, 'But other discriminative top-layer objectives that are tailored to a downstream task could make sense.': 1.0986123085021973, 'There is some loose terminology in the paper.': 1.0831122398376465, 'The authors refer to the ""correlation"" and ""cross-correlation"" between two vectors.': 1.0985535383224487, '""Correlation"" normally applies to scalars, so you need to define what you mean here.': 1.0986123085021973, '""Cross-correlation"" typically refers to time series.': 1.0986123085021973, 'In eq.': 1.0986123085021973, '(2) you are taking the max of a matrix.': 1.0986123085021973, 'Finally I am not too sure in what way this approach is ""fully differentiable"" while regular CCA is not': 1.0986123085021973, 'perhaps it is worth revisiting this term as well.': 1.0986123085021973, 'Also just a small note about the relationship between cosine distance and correlation:  they are related when we view the dimensions of each of the two vectors as samples of a single random variable.': 1.0986123085021973, 'In that case the cosine distance of the (mean-normalized) vectors is the same as the correlation between the two corresponding random variables.': 1.0986123085021973, 'In CCA we are viewing each dimension of the vectors as its own random variable.': 1.0986123085021973, 'So I fear the claim about cosine distance and correlation is a bit of a red herring here.': 1.0986123085021973, 'A couple of typos:': 1.0986123085021973, '""prosed""': 1.0986123085021973, '> ""proposed""': 1.0986123085021973, '""allong""': 1.0986123085021973, '> ""along""': 1.0986123085021973}"
489,https://openreview.net/forum?id=ryxB0Rtxx,"{'This paper provides some theoretical guarantees for the identity parameterization by showing that 1) arbitrarily deep linear residual networks have no spurious local optima; and 2) residual networks with ReLu activations have universal finite-sample expressivity.': 1.0986123085021973, 'This paper is well written and studied a fundamental problem in deep neural network.': 1.0855867862701416, 'I am very positive on this paper overall and feel that this result is quite significant by essentially showing the stability of auto-encoder, given the fact that it is hard to provide concrete theoretical guarantees for deep neural networks.': 1.0986123085021973, 'One of key questions is how to extent the result in this paper to the more general nonlinear actuation function case.': 1.0985947847366333, 'Minors: one line before Eq. (3.1), U \\in R ? \\times k': 1.098405122756958, 'Paper Summary:': 1.0986123085021973, 'Authors investigate identity re-parametrization in the linear and the non linear case.': 1.0986123085021973, 'Detailed comments:': 1.0986123085021973, '— Linear Residual Network:': 1.0986051559448242, 'The paper shows that for a linear residual network any critical point is a global optimum.': 1.098214864730835, 'This problem is non convex': 1.0986123085021973, 'it is interesting that this simple re-parametrization leads to such a result.': 0.5976919531822205, '— Non linear Residual Network:': 1.0985404253005981, 'Authors propose a construction that maps the points to their labels via a resnet , using an initial random projection, followed by a residual block that clusters the data based on their label, and a last layer that maps the clusters to the label.': 1.0986123085021973, '1- In Eq 3.4  seems the dimensions are not matching q_j in R^k and e_j in R^r.': 0.6727566123008728, 'please clarify': 1.0986123085021973, '2-': 0.651583731174469, 'The construction seems fine, but what is special about the resnet here in this construction?': 1.0986123085021973, 'One can do a similar construction if we did not have the identity?': 1.0940897464752197, 'can you discuss this point?': 1.0986123085021973, 'In the linear case it is clear from a spectral point of view how the identity is helping the optimization.': 1.0986123085021973, 'Please provide some intuition.': 0.4224196672439575, '3-   Existence of a network in the residual  class that overfits does it give us any intuition on why residual network outperform other architectures?': 1.0810086727142334, 'What does an existence result of such a network tell us about its representation power ?': 1.0986123085021973, 'A simple linear model under the assumption that points can not be too close can overfit the data, and get fast convergence rate (see for instance tsybakov noise condition).': 1.0986123085021973, '4- What does the construction tell us about the number of layers?': 0.4963088929653168, '5- clustering the activation independently from the label, is an old way to pretrain the network.': 0.7001111507415771, 'One could use those centroids as weights for the next layer (this is also related to Nystrom approximation see for instance https://www.cse.ust.hk/~twinsen/nystrom.pdf ).': 1.0986123085021973, 'Your clustering is very strongly connected to the label at each residual block.': 1.0978726148605347, ""I don't think this is appealing or useful since no feature extraction is happening."": 1.09861159324646, 'Moreover the number of layers in this construction': 1.0986120700836182, 'does not matter.': 0.9805749654769897, 'Can you weaken the clustering to be independent to the label at least in the early layers?': 1.09861159324646, 'then one could you use your construction as an initialization in the training.': 1.0986123085021973, '— Experiments :': 1.0986123085021973, 'last layer is not trained means the layer before the linear layer preceding the softmax?': 1.0986082553863525, 'Minor comments:': 0.8474355340003967, 'Abstract:  how  the identity mapping motivated batch normalization?': 0.9674443602561951, 'This paper investigates the identity parametrization also known as shortcuts where the output of each layer has the form h(x)+x instead of h(x).': 1.0939035415649414, 'This has been shown to perform well in practice (eg. ResNet).': 1.0986121892929077, 'The discussions and experiments in the paper are interesting.': 1.098611831665039, ""Here's a few comments on the paper:"": 0.568114697933197, 'Section 2: Studying the linear networks is interesting by itself.': 1.0986120700836182, 'However, it is not clear that how this could translate to any insight about non-linear networks.': 1.098604440689087, 'For example, you have proved that every critical point is global minimum.': 1.0970686674118042, 'I think it is helpful to add some discussion about the relationship between linear and non-linear networks.': 1.0984055995941162, ""Section 3: The construction is interesting but the expressive power of residual network is within a constant factor of general feedforward networks and I don't see why we need a different proof given all the results on finite sample expressivity of feedforward networks."": 1.0986123085021973, 'I appreciate if you clarify this.': 1.093271255493164, 'Section 4: I like the experiments.': 1.071651577949524, 'The choice of random projection on the top layer is brilliant.': 1.0986120700836182, 'However, since you have combined this choice with all-convolutional residual networks, it is hard for the reader to separate the affect of each of them.': 1.0986123085021973, 'Therefore, I suggest reporting the numbers for all-convolutional residual networks with learned top layer and also ResNet with random projection on the top layer.': 1.0986121892929077, ""1- I don't agree that Batch Normalization can be reduced to identity transformation and I don't know if bringing that in the abstract without proper discussion is a good idea."": 1.0986123085021973, '2- Page 5 above assumption 3.1 : x^(i)=1 ==> ||x^(i)||_2=1': 1.0986123085021973}"
